# -*- coding: utf-8 -*-
"""additional_vqvae_loss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L4PjyJ6uA7VYeWHzMo4htEFErwF8xfVt

# 0) Globals Params
"""

#!pip install python-box
#!pip install neural_structured_learning

"""
### STORED All variables inside special dict cc


"""
#!pip install python-box
import os
import glob

from box import Box
cc = Box({})


### Output naming
cc.dname       = 'fashion_kaggle_full_40k'
cc.tag         = "test02_300epoch_"
tag = cc.tag


### Epoch
cc.epoch_start = 0
cc.epoch_end   = 300
num_epochs        = 1  # 80 overfits

cc.compute_mode   = ""  # "tpu"


### Input file
image_size     = 64   ### 64
xdim           = 64
ydim           = 64
cdim           = 3


#### Data Size
nmax           = 100000

#### Model Parameters
n_filters      = 12  #  12  # Base number of convolutional filters
latent_dim     = 200  # Number of latent dimensions
num_embeddings = 64


#### Training parmeter
batch_size     = 64
learning_rate  = 1e-3
schedule_type = 'poly'

cc.patience = 300
cc.kloop    = 200
# cc.kloop = 15
cc.class_dict = {'gender': 5, 'masterCategory': 7, 'subCategory': 45, 'articleType': 141, 'baseColour': 46}


#### Paths data  #####################
cc.root = "/content/"
cc.root2 = "/content/drive/MyDrive/v3/"
cc.root3 = ""
# os.makedirs(os.path.join(cc.root2, 'fashion_data'), exist_ok=True)

cc.path_img_all   = cc.root + "fashion_data/images/"
cc.path_img_train = cc.root + "fashion_data/images/"
cc.path_img_test  = cc.root + "fashion_data/images/"
cc.img_suffix = ".jpg"


# cc.path_label_raw   = cc.root + 'raw_fashion_data/styles.csv'
cc.path_label_raw   = cc.root2  + 'fashion_data/styles.csv'
cc.path_label_train = cc.root2 + 'fashion_data/styles_train.csv'
cc.path_label_test  = cc.root2 + 'fashion_data/styles_test.csv'
cc.path_label_val   = cc.root2 + 'fashion_data/styles_test.csv'

cc.model_dir     =  cc.root2 + 'saved_models/'


##### Output
model_dir  = cc.model_dir
model_dir2 = model_dir + f"/m_{tag}-{cc.dname}/"
cc.model_dir2 = model_dir2
# os.makedirs(model_dir2, exist_ok=True)

print('params', cc)

"""# 1) Connect the Google Drive"""

from google.colab import drive
drive.mount('/content/drive/')

"""# 1) Common"""

#!nvidia-smi

"""
Actually, Google Colab automatically disconnects the notebook if we leave it idle for more than 30 minutes. ðŸ•‘
Open your Chrome DevTools by pressing F12 or ctrl+shift+i on Linux and enter the following JavaScript snippet in your console:
function KeepClicking(){
console.log("Clicking");
document.querySelector("colab-connect-button").click()
}
setInterval(KeepClicking,60000)



"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install albumentations
# !pip install keras-adabound
# !pip install keras-rectified-adam
# !pip install tf-madgrad
# !pip install tf_sprinkles
# !pip install utilmy
# !pip install python-box

# Commented out IPython magic to ensure Python compatibility.
import os
import shutil
import glob
import sys
import time
import math
import string
import time
import json
from pathlib import Path
import random
import logging
import functools

import tensorflow as tf
from tensorflow.keras import layers, regularizers

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import cv2
import scipy
import pandas as pd
import scipy.stats
import h5py
import yaml
import seaborn as sns

from tensorflow import keras
from PIL import Image
from tqdm import tqdm
from scipy.stats import norm
from sklearn import manifold
from utilmy import pd_read_file
from matplotlib.offsetbox import OffsetImage, AnnotationBbox
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas

from tf_sprinkles import Sprinkles
from madgrad import MadGrad

from sklearn.preprocessing import OneHotEncoder
from albumentations.core.transforms_interface import ImageOnlyTransform
from albumentations import (
    Compose, HorizontalFlip, CLAHE, HueSaturationValue,
    RandomBrightness, RandomContrast, RandomGamma,
    ToFloat, ShiftScaleRotate, Resize,
    GridDistortion, ElasticTransform, OpticalDistortion, Cutout
)
from scipy.spatial.distance import cdist

import numpy as np
import tensorflow as tf
import neural_structured_learning as nsl
from neural_structured_learning.keras import layers as nsl_layers
import neural_structured_learning.configs as nsl_configs


# %matplotlib inline

"""## 1) Define some helper functions"""

def print_log(*s):   #name changed
    """Log decorator"""
    print(*s, flush=True)

from utilmy.deeplearning.keras.util_graph_loss import *;
from utilmy.deeplearning.keras.util_loss import metric_accuracy,clf_loss_macro_soft_f1;


"""## 1-3) Define VQ-VAE model"""

class Quantizer(layers.Layer):
    def __init__(self, number_of_embeddings, embedding_dimensions, beta=0.25, **kwargs):
        super().__init__(**kwargs)
        self.embedding_dimensions = embedding_dimensions
        self.number_of_embeddings = number_of_embeddings
        self.beta = (
            beta  
            # This parameter are set as described int the paper
        )

        # Initializing the embeddings for quantization
        initializer  = tf.random_uniform_initializer()
        self.embeddings = tf.Variable(
            initial_value=initializer (
                shape=(self.embedding_dimensions, self.number_of_embeddings), dtype="float32"
            ),
            trainable=True,
            name="embeddings_for_vq_vae",
        )

    def call(self, x):
        # flattening the input
        input_shape = tf.shape(x)
        flattened = tf.reshape(x, [-1, self.embedding_dimensions])

        # Quantization.
        encoding_indices = self.get_code_indices(flattened)
        encodings = tf.one_hot(encoding_indices, self.number_of_embeddings)
        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)
        quantized = tf.reshape(quantized, input_shape)

        # Computing quantization loss 
        
        commitment_loss = self.beta * tf.reduce_mean(
            (tf.stop_gradient(quantized) - x) ** 2
        )
        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)
        # adding it to the layer. 

        self.add_loss(commitment_loss)
        self.add_loss(codebook_loss)

        quantized = x + tf.stop_gradient(quantized - x)
        return quantized

    def get_code_indices(self, flattened_inputs):
        similarity = tf.matmul(flattened_inputs, self.embeddings)
        distances = (
            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)
            + tf.reduce_sum(self.embeddings ** 2, axis=0)
            - 2 * similarity
        )
        encoding_indices = tf.argmin(distances, axis=1)
        return encoding_indices


def encoder_base(input_shape, latent_dim):
    """VQ-VAE encoder base model

    Parameters

    latent_dim: int
        Latent dim.
    """
    encoder_A_inputs = keras.Input(shape=input_shape)
    Encoder_A = layers.Conv2D(32, 3, activation="relu", strides=2, padding="same")(encoder_A_inputs)
    Encoder_A = layers.Conv2D(64, 3, activation="relu", strides=2, padding="same")(Encoder_A)
    encoder_A_outputs = layers.Conv2D(latent_dim, 1, padding="same")(Encoder_A)
    return keras.Model(encoder_A_inputs, encoder_A_outputs, name="encoder")


def decoder_base(latent_dim, shape):
    """VQ-VAE decoder base model.

    Parameters

    latent_dim: int
        Latent dim.
    shape: tuple
        The input shape.
    """
    latent_inputs = keras.Input(shape=shape[1:])
    Decoder_B = layers.Conv2DTranspose(64, 3, activation="relu", strides=2, padding="same")(latent_inputs)
    Decoder_B = layers.Conv2DTranspose(32, 3, activation="relu", strides=2, padding="same")(Decoder_B)
    decoder_B_outputs = layers.Conv2DTranspose(1, 3, padding="same")(Decoder_B)
    return keras.Model(latent_inputs, decoder_B_outputs, name="decoder")


def make_vqvae_encoder(input_shape, latent_dim):
    """
    """
    encoder_A_inputs = keras.Input(shape=input_shape)
    encoder = encoder_base(input_shape, latent_dim)
    encoder_A_outputs = encoder(encoder_A_inputs)

    Encoder_B = layers.Conv2D(32, 3, activation="relu", strides=1, padding="same")(encoder_A_outputs)
    Encoder_B = layers.Conv2D(64, 3, activation="relu", strides=1, padding="same")(Encoder_B)
    encoder_B_outputs = layers.Conv2D(latent_dim, 3, padding="same", name="encoder_B")(Encoder_B)
    return tf.keras.Model(encoder_A_inputs, [encoder_A_outputs, encoder_B_outputs], name='vqvae_encoder')


def make_vqvae_decoder(input_shape, latent_dim):
    """
    """
    quantized_latents_b = tf.keras.layers.Input(input_shape)
    quantized_latents_t = tf.keras.layers.Input(input_shape)

    Decoder_T = layers.Conv2DTranspose(64, 3, activation="relu", strides=2, padding="same")(quantized_latents_t)
    Decoder_T = layers.Conv2DTranspose(32, 3, activation="relu", strides=2, padding="same")(Decoder_T)
    decoder_T_outputs = layers.Conv2DTranspose(1, 3, padding="same", name="decoder_B")(Decoder_T)

    Decoder_B = layers.Conv2DTranspose(64, 3, activation="relu", strides=2, padding="same")(quantized_latents_b)
    Decoder_B = layers.Conv2DTranspose(32, 3, activation="relu", strides=2, padding="same")(Decoder_B)
    decoder_B_outputs = layers.Conv2DTranspose(1, 3, padding="same", name="decoder_A")(Decoder_B)

    reconstructions_of_t = layers.Conv2D(16, 3, activation="relu", strides=2, padding="same")(decoder_T_outputs)
    reconstructions_of_t = layers.Conv2D(16, 3, activation="relu", strides=2, padding="same")(reconstructions_of_t)

    concat = tf.keras.layers.Concatenate(axis=-1)([reconstructions_of_t,quantized_latents_b])
    Decoder_B = decoder_base(latent_dim, concat.shape)
    decoder_B_outputs = Decoder_B(concat)

    return keras.Model([quantized_latents_b, quantized_latents_t], decoder_B_outputs, name="decoder")


def make_vqvae_classifier(class_dict):
    """ Supervised multi class
            self.gender         = nn.Linear(self.inter_features, self.num_classes['gender'])
            self.masterCategory = nn.Linear(self.inter_features, self.num_classes['masterCategory'])
            self.subCategory    = nn.Linear(self.inter_features, self.num_classes['subCategory'])
            self.articleType    = nn.Linear(self.inter_features, self.num_classes['articleType'])

    """    
    Input = tf.keras.layers.InputLayer
    Dense = functools.partial(tf.keras.layers.Dense, activation='relu', 
                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                bias_regularizer=regularizers.l2(1e-4), 
                                activity_regularizer=regularizers.l2(1e-5))
    Reshape = tf.keras.layers.Reshape
    BatchNormalization = tf.keras.layers.BatchNormalization

    # if xdim == 64 :   #### 64 x 64 img          
    base_model = tf.keras.Sequential([
        Input(input_shape=(image_size // 4, image_size // 4, latent_dim,)),
        layers.Conv2D(128, 3, activation='relu'),
        layers.Flatten(),
        Dense(units=512),
        layers.Dropout(0.10),         
        Dense(units=512),
        layers.Dropout(0.10), 
        Dense(units=512),
    ])

    x = base_model.output
    ## x = layers.Flatten()(x) already flatten
    
    #### Multi-heads
    outputs = [Dense(num_classes, activation='softmax', name= f'{class_name}_out')(x) for class_name, num_classes in class_dict.items()]
    clf = tf.keras.Model(name='clf', inputs=base_model.input , outputs=outputs)  

    return clf


class VQ_VAE(tf.keras.Model):
    """Deep Feature Consistent Variational Autoencoder Class"""
    def __init__(self, latent_dim, class_dict, num_embeddings=64, image_size=64):
        super(VQ_VAE, self).__init__()
        self.latent_dim = latent_dim
        self.encoder = make_vqvae_encoder((image_size, image_size, 3), latent_dim)
        self.vq_layer = Quantizer(num_embeddings, latent_dim, name="vector_quantizer")
        self.decoder = make_vqvae_decoder((image_size//4, image_size//4, latent_dim), latent_dim)
        self.classifier = make_vqvae_classifier(class_dict)

    def encode(self, x):
        # z_mean, z_logsigma = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
        # return z_mean, z_logsigma
        return self.encoder(x)
  
    def reparameterize(self, z_mean, z_logsigma):
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return eps * tf.exp(z_logsigma * 0.5) + z_mean

    def decode(self, encoder_A_outputs, encoder_B_outputs, apply_sigmoid=False):
        quantized_latents_b = self.vq_layer(encoder_A_outputs)
        quantized_latents_t = self.vq_layer(encoder_B_outputs)

        decoder_B_outputs = self.decoder((quantized_latents_b, quantized_latents_t))
        if apply_sigmoid:
            decoder_B_outputs = tf.sigmoid(decoder_B_outputs)
        return decoder_B_outputs
        # x_recon = self.decoder(z)
        # if apply_sigmoid:
        #     new_x_recon = tf.sigmoid(x_recon)
        #     return new_x_recon
        # return x_recon

    def call(self, x,training=True, mask=None):
        # # out_classes = None
        # z_mean, z_logsigma = self.encode(x)
        # z = self.reparameterize(z_mean, z_logsigma)
        # x_recon = self.decode(z)
        encoder_A_outputs, encoder_B_outputs = self.encode(x)
        x_recon = self.decode(encoder_A_outputs, encoder_B_outputs)
        
        #### Classifier
        # z = encoder_A_outputs
        # z = encoder_B_outputs
        z = encoder_A_outputs + encoder_B_outputs
        out_classes = self.classifier(z)
        
        # return z_mean, z_logsigma, x_recon, out_classes
        return x_recon, out_classes

"""## DFC-VAE model"""

class DFC_VAE(tf.keras.Model):
    """Deep Feature Consistent Variational Autoencoder Class"""
    def __init__(self, latent_dim, class_dict):
        super(DFC_VAE, self).__init__()
        self.latent_dim = latent_dim

        ## Uncomment to switch between DFC-VAE and VQ-VAE
        # self.encoder = make_encoder()
        # self.decoder = make_decoder()
        
        self.classifier = make_classifier(class_dict)

    def encode(self, x):
        z_mean, z_logsigma = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
        return z_mean, z_logsigma
  
    def reparameterize(self, z_mean, z_logsigma):
        eps = tf.random.normal(shape=tf.shape(z_mean))
        return eps * tf.exp(z_logsigma * 0.5) + z_mean

    def decode(self, z, apply_sigmoid=False):
        x_recon = self.decoder(z)
        if apply_sigmoid:
            new_x_recon = tf.sigmoid(x_recon)
            return new_x_recon
        return x_recon

    def call(self, x,training=True, mask=None):
        # out_classes = None
        z_mean, z_logsigma = self.encode(x)
        z = self.reparameterize(z_mean, z_logsigma)
        x_recon = self.decode(z)
        
        #### Classifier
        out_classes = self.classifier(z)
        
        return z_mean, z_logsigma, x_recon, out_classes


def make_encoder(n_outputs=1):
    #Functionally define the different layer types
    Input = tf.keras.layers.InputLayer
    Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu',
                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001), 
                                activity_regularizer=regularizers.l2(1e-5))
    BatchNormalization = tf.keras.layers.BatchNormalization
    Flatten = tf.keras.layers.Flatten
    Dense = functools.partial(tf.keras.layers.Dense, activation='relu',
                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                bias_regularizer=regularizers.l2(1e-4), activity_regularizer=regularizers.l2(1e-5))

    ##### Build the encoder network using the Sequential API
    encoder = tf.keras.Sequential([
        Input(input_shape=(xdim, cc.ydim, 3)),

        Conv2D(filters=n_filters, kernel_size=5,  strides=2),
        BatchNormalization(),

        Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),
        BatchNormalization(),

        Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),
        BatchNormalization(),

        Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),
        BatchNormalization(),

        Flatten(),
        Dense(512*1, activation='relu'),
        layers.Dropout(0.2),  
        Dense(512*1, activation='relu'),
        Dense(2*latent_dim, activation=None),
    ])

    return encoder


def make_decoder():
    """
    ValueError: Dimensions must be equal, but are 3 and 4 
    for '{{node sub}} = Sub[T=DT_FLOAT](x, sequential_1/conv2d_transpose_3/Relu)' with input shapes: [8,256,256,3], [8,256,256,4].

    """    
    #Functionally define the different layer types
    Input = tf.keras.layers.InputLayer

    # bias_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001)
    Dense = functools.partial(tf.keras.layers.Dense, activation='relu', 
                                kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                bias_regularizer=regularizers.l2(1e-4), activity_regularizer=regularizers.l2(1e-5))
    Reshape = tf.keras.layers.Reshape
    Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu',
                                        kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001), 
                                        activity_regularizer=regularizers.l2(1e-5))
    BatchNormalization = tf.keras.layers.BatchNormalization
    Flatten = tf.keras.layers.Flatten

    #Build the decoder network using the Sequential API
    if xdim == 64 :   #### 64 x 64 img         
        decoder = tf.keras.Sequential([
            Input(input_shape=(latent_dim,)),

            Dense(units= 4*4*6*n_filters),
            Dense(units= 4*4*6*n_filters),
            layers.Dropout(0.2),   
            Dense(units= 4*4*6*n_filters),    
            Reshape(target_shape=(4, 4, 6*n_filters)),
            #### ValueError: total size of new array must be unchanged, input_shape = [2304], output_shape = [7, 4, 144]

            #### Conv. layer      
            Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),
            Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),
            Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),

            Conv2DTranspose(filters=3, kernel_size=5,  strides=2),      
            # Conv2DTranspose(filters=4, kernel_size=5,  strides=2),

        ])

    if ydim == 256 :  ### 256 8 256 img          
        decoder = tf.keras.Sequential([
            Input(input_shape=(latent_dim,)),

            Dense(units=16*16*6*n_filters),
            Dense(units=16*16*6*n_filters),
            layers.Dropout(0.2),   
            Dense(units=16*16*6*n_filters),    
            Reshape(target_shape=(16, 16, 6*n_filters)),

            #### Conv. layer      
            Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),
            Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),
            Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),
            Conv2DTranspose(filters=3, kernel_size=5,  strides=2),      

        ])
    return decoder

from utilmy.deeplearning.keras.util_layers import make_classifier_2 as make_classifier

"""## 1-4) Build loss function"""

percep_model = tf.keras.applications.EfficientNetB0(
    include_top=False, weights='imagenet', input_tensor=None,
    input_shape=(xdim, ydim, cdim), pooling=None, classes=1000,
    classifier_activation='softmax'
)


def clf_loss_crossentropy(y_true, y_pred):
    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)


def perceptual_loss_function(x, x_recon, z_mean, z_logsigma, kl_weight=0.00005, 
                             y_label_heads=None, y_pred_heads=None, clf_loss_fn=None):
    ### log( 'x_recon.shae',  x_recon.shape )  
    ### VAE Loss
    reconstruction_loss = tf.reduce_mean(tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3)))
    latent_loss = 0.5 * tf.reduce_sum(tf.exp(z_logsigma) + tf.square(z_mean) - 1.0 - z_logsigma, axis=1)

    ### Efficient Head Loss
    perceptual_loss = tf.reduce_sum(tf.square(tf.subtract(tf.stop_gradient(percep_model(x)),percep_model(x_recon))))
    loss_all = kl_weight*latent_loss + reconstruction_loss + 0.015*perceptual_loss  

    ### Classifier Loss
    loss_clf = [0.]
    if y_label_heads is not None:
        for i in range(len(y_pred_heads)):
            head_loss = clf_loss_fn(y_label_heads[i], y_pred_heads[i])
            loss_clf.append(head_loss)

    loss_clf = tf.reduce_mean(loss_clf)
    loss_all = loss_all + loss_clf * 0.01 

    return loss_all

"""Data Loader"""

class SprinklesTransform(ImageOnlyTransform):
    def __init__(self, num_holes=100, side_length=10, always_apply=False, p=1.0):
        super(SprinklesTransform, self).__init__(always_apply, p)
        self.sprinkles = Sprinkles(num_holes=num_holes, side_length=side_length)
    
    def apply(self, image, **params):
        if isinstance(image, Image.Image):
            image = tf.constant(np.array(image), dtype=tf.float32)
        elif isinstance(image, np.ndarray):
            image = tf.constant(image, dtype=tf.float32)

        return self.sprinkles(image).numpy()


class CustomDataGenerator0(tf.keras.utils.Sequence):
    def __init__(self, x, y, batch_size=32, augmentations=None):
        self.x = x
        self.y = y
        self.batch_size = batch_size
        self.augment = augmentations

    def __len__(self):
        return int(np.ceil(len(self.x) / float(self.batch_size)))

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = []
        for y_head in self.y:
            batch_y.append(y_head[idx * self.batch_size:(idx + 1) * self.batch_size])
        
        if self.augment is not None:
            batch_x = np.stack([self.augment(image=x)['image'] for x in batch_x], axis=0)
        return (batch_x, *batch_y)


class CustomDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, image_dir, label_path, class_dict,
                 split='train', batch_size=8, transforms=None):
        self.image_dir = image_dir
        self.labels = np.loadtxt(label_path, delimiter=' ', dtype=np.object)
        self.class_dict = class_dict
        self.num_classes = len(class_dict)
        self.batch_size = batch_size
        self.transforms = transforms
    
    def on_epoch_end(self):
        np.random.seed(12)
        np.random.shuffle(self.labels)

    def __len__(self):
        return int(np.ceil(len(self.labels) / float(self.batch_size)))

    def __getitem__(self, idx):
        batch_raw_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]
        num_samples = len(batch_raw_labels)

        # Create batch targets
        batch_x = []
        batch_y = [np.zeros((num_samples, num_labels), dtype='int32') for _, num_labels in self.class_dict.items()]
        for sample in batch_raw_labels:
            # Load image
            image_filename = str(sample[0])
            image = np.array(Image.open(os.path.join(self.image_dir, image_filename)).convert('RGB'))
            batch_x.append(image)

            # Create labels
            class_id = int(sample[1])
            label_id = list(map(int, sample[2:]))
            batch_y[class_id][label_id] = 1
        
        if self.transforms is not None:
            batch_x = np.stack([self.transforms(image=x)['image'] for x in batch_x], axis=0)
        return (batch_x, *batch_y)

"""# 3) Kaggle Fashion dataset

## Code for generating custom data from the Kaggle dataset
"""

# Download data
os.chdir(cc.dir_local)
os.system("gdown --id 1AExPSGhWOGPA79McvAOEFKXO_MCgkAVY -O raw_fashion_data.zip")
os.system("unzip -qq raw_fashion_data.zip")

"""### Old"""

import os
import glob


def apply_func(s, values):
    if s.lower() in values:
        return s.lower()
    return 'other'


df = pd.read_csv('raw_fashion_data/styles.csv', error_bad_lines=False, warn_bad_lines=False)
df = df.dropna()
df_img_ids = set(df['id'].tolist())
img_ids = set([int(os.path.splitext(filename)[0]) for filename in os.listdir(os.path.join('raw_fashion_data/images'))])
available_img_ids = df_img_ids & img_ids
print('Total valid images:', len(available_img_ids))

df = df[df['id'].isin(available_img_ids)]
print(df.shape)

# Edit these categories as you need
value_map = {
    'gender': ['men', 'women'],
    'masterCategory': ['apparel', 'accessories'],
    'subCategory': ['topwear', 'shoes'],
    'articleType': ['tshirts', 'shirts'],
    'baseColour': ['black', 'white']
}
cols = ['id'] + list(value_map.keys())
df = df[cols]
for col, values in value_map.items():
    df[col] = df[col].apply(apply_func, args=(values,))

    print(df[col].unique())
shuffled = df.sample(frac=1)

n = df.shape[0]
num_train = int(0.65 * n)
num_val = n - num_train

df_train = df.iloc[:num_train, :]
df_val = df.iloc[num_train:, :]
for col in df_train.columns:
    print(df_train[col].unique())
for col in df_val.columns:
    print(df_val[col].unique())

#!mkdir -p fashion_data
#!cp -r raw_fashion_data/images fashion_data
df_train.to_csv('fashion_data/styles_train.csv', index=False)
df_val.to_csv('fashion_data/styles_val.csv', index=False)

"""### New"""

import os
import glob


df = pd.read_csv('raw_fashion_data/styles.csv', error_bad_lines=False, warn_bad_lines=False)
df = df.dropna()
df_img_ids = set(df['id'].tolist())
img_ids = set([int(os.path.splitext(filename)[0]) for filename in os.listdir(os.path.join('raw_fashion_data/images'))])
available_img_ids = df_img_ids & img_ids
print('Total valid images:', len(available_img_ids))

df = df[df['id'].isin(available_img_ids)]
print(df.shape)

# # Edit these categories as you need
# def apply_func(s, values):
#     if s.lower() in values:
#         return s.lower()
#     return 'other'


# value_map = {
#     'gender': ['men', 'women'],
#     'masterCategory': ['apparel', 'accessories'],
#     'subCategory': ['topwear', 'shoes'],
#     'articleType': ['tshirts', 'shirts'],
#     'baseColour': ['black', 'white']
# }
# cols = ['id'] + list(value_map.keys())
# df = df[cols]
# for col, values in value_map.items():
#     df[col] = df[col].apply(apply_func, args=(values,))

#     print(df[col].unique())
shuffled = df.sample(frac=1)

n = df.shape[0]
num_train = int(0.65 * n)
num_val = n - num_train

df_train = df.iloc[:num_train, :]
df_val = df.iloc[num_train:, :]
# for col in df_train.columns:
#     print(df_train[col].unique())
# for col in df_val.columns:
#     print(df_val[col].unique())

#!mkdir -p fashion_data
#!cp -r raw_fashion_data/images fashion_data
df_train.to_csv('fashion_data/styles_train.csv', index=False)
df_val.to_csv('fashion_data/styles_val.csv', index=False)

#!zip -r fashion_data.zip fashion_data

from google.colab import drive
drive.mount('/content/drive')

#!cp fashion_data.zip /content/drive/MyDrive/v3/

"""## Download the clean Kaggle dataset"""

# Download data
# !gdown --id 16i8Y7yQXkOdv-QwMcPusrKOd9ICyT07i -O fashion_data.zip
# !unzip -qq fashion_data.zip
# os.chdir('/content/')
# os.system('gdown --id 1Jf2XOJb078Mu75oUCJjBfxM36TGZ8SFv -O fashion_data.zip')
# os.system('unzip -qq fashion_data.zip')
#!cp {cc.root2}fashion_data.zip  /content/fashion_data.zip
#!unzip -o -qq  /content/fashion_data.zip   -d /content

#### https://www.kaggle.com/paramaggarwal/fashion-product-images-dataset

"""#### Visualization"""

df_train = pd.read_csv('fashion_data/styles_train.csv', error_bad_lines=False, warn_bad_lines=False)
df_val = pd.read_csv('fashion_data/styles_val.csv', error_bad_lines=False, warn_bad_lines=False)

sns.countplot(data=df_train, x='gender')

print('Total: ', df_train.masterCategory.nunique())

plt.figure(figsize=(10, 10))
sns.countplot(data=df_train, x='masterCategory')

# print(df_train.articleType.value_counts())
print('Total: ', df_train.articleType.nunique())

plt.figure(figsize=(20, 10))
sns.countplot(data=df_train, x='articleType', order=df_train.articleType.value_counts().iloc[:20].index)

# print(df_train.baseColour.value_counts())
# print(df_train.baseColour.nunique())
print('Total: ', df_train.baseColour.nunique())

#plt.figure(figsize=(20, 10))
# sns.countplot(data=df_train, x='baseColour', order=df_train.baseColour.value_counts().iloc[:20].index)

# print(df_train.subCategory.value_counts())
# print(df_train.subCategory.nunique())
print('Total: ', df_train.subCategory.nunique())

#plt.figure(figsize=(20, 10))
# sns.countplot(data=df_train, x='subCategory', order=df_train.subCategory.value_counts().iloc[:20].index)
from utilmy.deeplearning.keras.util_dataloader import RealCustomDataGenerator;

df = pd.read_csv(cc.path_label_train )
df = pd.concat((df,  pd.read_csv(cc.path_label_test )))
df = pd.concat((df,  pd.read_csv(cc.path_label_val )))
df = df.fillna('')
print(df.dtypes)

for ci in df.columns :
    if 'id' not in ci:
       df[ci] = df[ci].astype(str).str.lower()

       print(ci, df[ci].unique(), len(df[ci].unique()))



df.to_csv( cc.path_label_raw , index=False )

print(df.head(3).T)

df = pd.read_csv( cc.path_label_raw )

df = pd.read_csv(cc.path_label_raw, error_bad_lines=False, warn_bad_lines=False)
df = df.fillna('')
df = df.dropna()
df['id'] = df['id'].astype('int')
df = df.drop_duplicates('id')
df_img_ids  = set(df['id'].tolist())

image_list    = glob.glob(cc.path_img_all + "/*" + cc.img_suffix )
image_list    = [filename.split("/")[-1] for filename in image_list]
log('N images', len(image_list))

img_ids = set([int(filename.split(".")[0]) for filename in image_list])
available_img_ids = df_img_ids & img_ids
print('Total valid images:', len(available_img_ids))

df = df[df['id'].isin(available_img_ids)]
df = df.drop_duplicates('id')
shuffled  = df.sample(frac=1)

n         = df.shape[0]
num_train = int(0.9 * n)
num_val   = n - num_train

dftmp = df.drop_duplicates(list(cc.class_dict.keys()))
df_train  = pd.concat( (df.iloc[:num_train, :], dftmp))
df_val    = pd.concat( (df.iloc[num_train:, :], dftmp))
num_train = len(df_train)
num_val   = len(df_val)      


train_transforms = Compose([
    Resize(image_size, image_size, p=1),
    HorizontalFlip(p=0.5),
    RandomContrast(limit=0.2, p=0.5),
    RandomGamma(gamma_limit=(80, 120), p=0.5),
    RandomBrightness(limit=0.2, p=0.5),
    HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,
                       val_shift_limit=10, p=.9),
    ShiftScaleRotate(
        shift_limit=0.0625, scale_limit=0.1, 
        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), 
    ToFloat(max_value=255),
    SprinklesTransform(num_holes=10, side_length=10, p=0.5),
])

test_transforms = Compose([
    Resize(image_size, image_size, p=1),
    ToFloat(max_value=255)
])

# data_dir         = Path('./fashion_data')
image_dir        = cc.path_img_all # str(data_dir / 'images')
train_label_path = cc.path_label_train # str(data_dir / 'styles_train.csv')
val_label_path   = cc.path_label_val  # str(data_dir / 'styles_val.csv')

train_data       = RealCustomDataGenerator(image_dir, train_label_path, cc.class_dict,
                                     split='train', batch_size=batch_size, transforms=train_transforms)
val_data         = RealCustomDataGenerator(image_dir, val_label_path, cc.class_dict,
                                   split='val', batch_size=batch_size, transforms=test_transforms, shuffle=False)

log("Train Model")
log('Number of training batches:', len(train_data))
log('Number of test batches:', len(val_data))

"""### Augmentation Performance Test"""

all_transforms = {
    'resize': Resize(image_size, image_size, p=1),
    'hozirontal_flip': HorizontalFlip(p=0.5),
    'random_contrast': RandomContrast(limit=0.2, p=0.5),
    'random_gamma': RandomGamma(gamma_limit=(80, 120), p=0.5),
    'random_brightness': RandomBrightness(limit=0.2, p=0.5),
    'hue_saturation': HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,
                       val_shift_limit=10, p=.9),
    'shift_scale_rotate': ShiftScaleRotate(
        shift_limit=0.0625, scale_limit=0.1, 
        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), 
    'tofloat': ToFloat(max_value=255),
    'grid_distortion': GridDistortion(num_steps=5),
    'elastic_transform': ElasticTransform(sigma=10, alpha_affine=10),
    'optical_distortion': OpticalDistortion(distort_limit=0.1, shift_limit=0.1),
    # 'sprinkles_transform': SprinklesTransform(num_holes=10, side_length=10, p=0.5),
    'cutout': Cutout(num_holes=15),
}

def plot_grid(images, title=''):
    num_samples = len(images)
    n = int(np.sqrt(num_samples))
    _, axes = plt.subplots(n, n, figsize=(10, 10))
    for i in range(n):
        for j in range(n):
            axes[i, j].imshow(images[i*n+j])
            axes[i, j].axis('off')
    plt.suptitle(title)
    plt.show()
    plt.close()


for name, transform in all_transforms.items():
    if name not in ('resize', 'tofloat'):
        test_perf_transforms = Compose([
            all_transforms['resize'],
            transform,
        ])
        test_perf_data = RealCustomDataGenerator(cc.path_img_all, val_label_path, cc.class_dict,
                                                    split='val', batch_size=batch_size, transforms=test_perf_transforms, shuffle=False)
        images, *labels = test_perf_data[0]
        plot_grid(images, title=name)

t0 = time.time()

transform_times = {}
for name, transform in all_transforms.items():
    if name not in ('resize', 'tofloat'):
        test_perf_transforms = Compose([
            all_transforms['resize'],
            transform,
        ])

        test_perf_data = RealCustomDataGenerator(cc.path_img_all, val_label_path, cc.class_dict,
                                                 split='val', batch_size=batch_size, transforms=test_perf_transforms, shuffle=False)
        # Run over 10 batches and get average running time
        times = []
        for i in range(10):
            t0 = time.time()
            test_perf_data[i]
            times.append(time.time() - t0)
        
        avg_time = np.mean(times)
        transform_times[name] = avg_time
        print('Transform type: {} - Average time: {} on batch size: {}'.format(name, avg_time, batch_size))

time_df = pd.DataFrame(data=list(transform_times.items()), columns=['Transform', 'Time (s)'])
time_df.plot(kind='barh', x='Transform', figsize=(15, 10))

"""## Train

### Learning rate scheduler
"""

# https://www.pyimagesearch.com/2019/07/22/keras-learning-rate-schedules-and-decay/

class LearningRateDecay:
	def plot(self, epochs, title="Learning Rate Schedule"):
		# compute the set of learning rates for each corresponding
		# epoch
		lrs = [self(i) for i in epochs]
		# the learning rate schedule
		plt.style.use("ggplot")
		plt.figure()
		plt.plot(epochs, lrs)
		plt.title(title)
		plt.xlabel("Epoch #")
		plt.ylabel("Learning Rate")


class StepDecay(LearningRateDecay):
	def __init__(self, init_lr=0.01, factor=0.25, drop_every=10):
		# store the base initial learning rate, drop factor, and
		# epochs to drop every
		self.init_lr = init_lr
		self.factor = factor
		self.drop_every = drop_every

	def __call__(self, epoch):
		# compute the learning rate for the current epoch
		exp = np.floor((1 + epoch) / self.drop_every)
		alpha = self.init_lr * (self.factor ** exp)
		# return the learning rate
		return float(alpha)


class PolynomialDecay(LearningRateDecay):
	def __init__(self, max_epochs=100, init_lr=0.01, power=1.0):
		# store the maximum number of epochs, base learning rate,
		# and power of the polynomial
		self.max_epochs = max_epochs
		self.init_lr = init_lr
		self.power = power

	def __call__(self, epoch):
		# compute the new learning rate based on polynomial decay
		decay = (1 - (epoch / float(self.max_epochs))) ** self.power
		alpha = self.init_lr * decay
		# return the new learning rate
		return float(alpha)

def visualize_imgs(img_list, path, tag, y_labels, n_sample=None):
    """Assess image validity"""
    os.makedirs(path, exist_ok=True)
    if n_sample is not None and isinstance(n_sample, int):
        img_list = img_list[:n_sample]
        y_labels = [y[:n_sample].tolist() for y in y_labels]

    for i in range(len(img_list)) :
        img = img_list[i]
        if not isinstance(img, np.ndarray) :
            img = img.numpy()
        
        img = img[:, :, ::-1]
        img = np.clip(img * 255, 0, 255).astype('uint8')
        label_tag = 'label_{' + '-'.join([str(y[i]) for y in y_labels]) + '}'
        save_path = f"{path}/img_{cc.tag}_nimg_{i}_{tag}_{label_tag}.png"
        cv2.imwrite(save_path, img)
        img = None


@tf.function
def train_step(x, model, train_variance, y_label_list=None):
    with tf.GradientTape() as tape:
        x_recon, out_classes = model(x, training=True)      #Forward pass through the VAE
        # loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma,
        #     y_label_heads=y_label_list, 
        #     y_pred_heads=out_classes, 
        #     clf_loss_fn=clf_loss_crossentropy
        # )

        # Calculate the losses.
        commitment_loss = model.losses[0:2]
        codebook_loss =  model.losses[2:4]
    
        reconstruction_loss = (tf.reduce_mean((x - x_recon) ** 2) / train_variance)
        loss = reconstruction_loss + sum(commitment_loss+codebook_loss)

    grads = tape.gradient(loss, model.trainable_variables)   #Calculate gradients
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss


@tf.function
def validation_step(x, model, train_variance):
    x_recon, out_classes = model(x, training=False)  #Forward pass through the VAE
    # loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma)

     # Calculate the losses.
    commitment_loss = model.losses[0:2]
    codebook_loss =  model.losses[2:4]

    reconstruction_loss = (tf.reduce_mean((x - x_recon) ** 2) / train_variance)
    loss = reconstruction_loss + sum(commitment_loss+codebook_loss)
    return loss, x_recon, out_classes


##### Output
model_dir  = os.path.join(cc.root2, 'saved_models')
model_dir2 = model_dir + f"/m_{cc.tag}-{cc.dname}/"
os.makedirs(model_dir2, exist_ok=True)
check_dir = os.path.join(model_dir2, 'check')

## Do not delete
##if os.path.isdir(check_dir):
###    shutil.rmtree(check_dir)  # Delete previous dir


#### Instantiate a new VQ_CVAE model and optimizer
log('Current directory:', os.getcwd())
log("\nBuild the VQ-VAE Model")
model = VQ_VAE(latent_dim, cc.class_dict, num_embeddings, image_size)
optimizer = tf.keras.optimizers.Adam(learning_rate)


### Training loop
kbatch = len(train_data)
train_loss_hist = []
valid_loss_hist = []
counter = 0
dostop = False
best_loss = 100000000.0

# Setup learning rate scheduler
if schedule_type == 'step':
	print("Using 'step-based' learning rate decay")
	schedule = StepDecay(init_lr=learning_rate, factor=0.25, drop_every=15)
elif schedule_type == "linear":
	print("Using 'linear' learning rate decay")
	schedule = PolynomialDecay(max_epochs=num_epochs, init_lr=learning_rate, power=1)
elif schedule_type == "poly":
	print("Using 'polynomial' learning rate decay")
	schedule = PolynomialDecay(max_epochs=num_epochs, init_lr=learning_rate, power=5)

# ==============================================================================
# ### Validation dataset
# # This might take long time
# for batch_idx, (x_val, *_) in enumerate(val_data):
#     x_val = np.clip(x_val * 255, 0, 255).astype('uint8')
#     visualize_imgs(x_val, path=model_dir2 + "/check/",
#                     batch_idx=batch_idx, n_sample=3)
# ==============================================================================


def metric_accuracy(y_val, y_pred_head, class_dict):
    # Val accuracy
    val_accuracies = {class_name: 0. for class_name in class_dict}
    for i, class_name in enumerate(class_dict):
        y_pred = np.argmax(y_pred_head[i], 1)
        y_true = np.argmax(y_val[i], 1)
        val_accuracies[class_name] = (y_pred == y_true).mean()
    print('\n %s\n' % val_accuracies)
    return val_accuracies


# Calculate the variance of the training set
print('Calculating the variance of the training set...')
sum_all = None
n = 0
for i, (x, *_) in enumerate(train_data):
    if sum_all is None:
        sum_all = np.sum(x, 0)
    else:
        sum_all += np.sum(x, 0)
    n += len(x)
    if i >= 10:
        break
mean = sum_all / n

v = None
for i, (x, *_) in enumerate(train_data):
    if v is None:
        v = np.sum((x - mean)**2, 0)
    else:
        v += np.sum((x - mean)**2, 0)
    if i >= 10:
        break
train_variance = v / n


for epoch in range(num_epochs):
    log(f"Starting epoch {epoch+1}/{num_epochs}, in {kbatch} kbatches ")
    if dostop: break   

    # Set learning rate
    lr = schedule(epoch)
    optimizer.learning_rate = lr
    print('[Epoch {:03d}] Learning rate: {}'.format(epoch, optimizer.learning_rate.numpy()))
    for batch_idx, (x, *y_label_list) in enumerate(train_data):        
        if dostop:
            break

        # log("x", x)
        # log("y_label_list", y_label_list)
        train_loss = train_step(x, model, train_variance, y_label_list=y_label_list)
        train_loss = np.mean(train_loss.numpy())
        train_loss_hist.append(train_loss)
        if (batch_idx + 1) % (kbatch // 10) == 0:
            log('[Epoch {:03d} batch {:04d}/{:04d}]'.format(epoch + 1, batch_idx+1, kbatch))

        if (batch_idx + 1) % cc['kloop'] == 0:
        # if (batch_idx + 1) % 2 == 0:
            for (x_val, *y_val) in val_data:
                valid_loss, x_recon, y_pred = validation_step(x_val, model, train_variance)
                valid_loss = np.mean(valid_loss.numpy())

                # test_accuracy = metric_accuracy(y_test, y_pred, dd)

            valid_loss_hist.append(valid_loss)
        
            log(epoch+1, batch_idx, 'train,valid', train_loss, valid_loss)        
            best_loss, counter = save_best(model, model_dir2, valid_loss, best_loss, counter)
            dostop = train_stop(counter, cc['patience'])

        if (batch_idx + 1) % 200 == 0:
        # if (batch_idx + 1) % 2 == 0:
            for (x_val, *y_val) in val_data:
                _, x_recon, y_pred_head = validation_step(x_val, model, train_variance)
                break

            y_pred     = [np.argmax(y, 1) for y in y_pred_head]
            valid_image_check(x_recon, path=model_dir2 + "/check/",
                              tag=f"e{epoch+1}_b{batch_idx+1}", y_labels=y_pred, n_sample=20, renorm=True)

            if epoch == 0 and batch_idx < 300 : 
              y_val_true = [np.argmax(y, 1) for y in y_val]
              visualize_imgs(x_val, path=model_dir2 + "/check/",
                            tag=f'e{epoch+1}_b{batch_idx+1}', y_labels=y_val_true, n_sample=20)

            # Val accuracy
            val_accuracies = metric_accuracy(y_val, y_pred_head, cc.class_dict)
            # print('\n %s\n' % val_accuracies)


log('Final valid_loss', str(valid_loss_hist)[:200])

"""### Plot learning rate"""

schedule.plot(np.arange(num_epochs))

"""## Save the model"""

log("\nSaving Model")
os.makedirs(model_dir, exist_ok=True)
tf.saved_model.save(model, model_dir2)
model.save_weights( model_dir2 + f'/model_keras_weights.h5')
dd = {"pars" : [ learning_rate, latent_dim, num_epochs ]}
json.dump(dd, open(model_dir2 +"/info.json" , mode='w'))
log(model_dir2)

"""## Reload the model"""

log('\nReload Model')
model2 = VQ_VAE(latent_dim, cc.class_dict, num_embeddings, image_size)
input_shape = (batch_size, xdim, ydim, cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
model2.build(input_shape)
print(model_dir2)
model2.load_weights( model_dir2 + f'/model_keras_weights.h5')
log('# Reloaded', model2)
# log('rerun eval', validation_step(x_val, model2))

"""## Plot"""

"""Plot Original Input Images vs. Reconstructed Images"""
logger = logging.getLogger()
old_level = logger.level
logger.setLevel(100)

# test_sample = random.sample(list(test_images), 1)
for test_images, *_ in val_data:
    break
plot_original_images([test_images[0]])
_, _, x_recon, _ = model2(tf.expand_dims(test_images[0], 0), training=False)
plot_reconstructed_images(model, x_recon)


logger.setLevel(old_level)
log('Plot Loss Over Time During the Training Process')
plt.figure()
plt.plot(train_loss_hist)
plt.plot(valid_loss_hist)
plt.title('Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()


log('Plot Tail-End of Logarithmic of Loss Over Time During the Training Process')
plt.figure()
plt.plot(np.log(train_loss_hist[5000:]))
plt.plot(np.log(valid_loss_hist[5000:]))
plt.title('Logarithmic Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()

"""## Classification Evaluation"""

val_accuracies = {class_name: 0. for class_name in class_dict}
for (x_val, *y_val) in val_data:
    _, _, _, predictions = model(x_val, training=False)

    for i, class_name in enumerate(class_dict):
        y_pred = np.argmax(predictions[i], 1)
        y_true = np.argmax(y_val[i], 1)
        val_accuracies[class_name] = (y_pred == y_true).mean()
print(val_accuracies)

"""# 4) Fashion MNIST TF"""

data_type = 'fashion_tf'
# if data_type == 'random' :
#     num_train = 100
#     num_val = 50
#     x_train = np.random.randint(0, 256, size=(num_train, image_size, image_size, 3), dtype='uint8')
#     y_train = [np.random.randint(0, 2,  size=(num_train, n_labels)) for _, n_labels in class_dict.items()]

#     x_val = np.random.randint(0, 256, size=(num_val, image_size, image_size, 3), dtype='uint8')
#     y_val = [np.random.randint(0, 2,  size=(num_val, n_labels)) for _, n_labels in class_dict.items()]

#     train_data = CustomDataGenerator0(x_train, y_train, augmentations=train_transforms)
#     val_data = CustomDataGenerator0(x_val, y_val, augmentations=test_transforms)
# elif data_type == 'fashion_tf':

# ==============================================================================
(x_train, train_labels), (x_val, val_labels) = tf.keras.datasets.fashion_mnist.load_data()

# Convert to RGB
x_train = np.stack([x_train, x_train, x_train], axis=-1)
x_val = np.stack([x_val, x_val, x_val], axis=-1)
image_size = 28
num_train = train_labels.shape[0]
num_val = val_labels.shape[0]
y_train = np.zeros((len(class_dict), num_train, 1), dtype='int32')
y_val = np.zeros((len(class_dict), num_val, 1), dtype='int32')

y_train[train_labels, :, :] = 1
y_val[val_labels, :, :] = 1
train_data = CustomDataGenerator0(x_train, y_train, augmentations=train_transforms)
val_data = CustomDataGenerator0(x_val, y_val, augmentations=test_transforms)
# ==============================================================================

# elif data_type == 'fashion':
#     data_dir = Path('./data')
#     image_dir = str(data_dir / 'images')
#     train_label_path = str(data_dir / 'labels' / 'train.txt')
#     val_label_path = str(data_dir / 'labels' / 'val.txt')
#     train_data = CustomDataGenerator(image_dir, train_label_path, class_dict, split='train', transforms=train_transforms)
#     val_data = CustomDataGenerator(image_dir, val_label_path, class_dict, split='val', transforms=test_transforms)


log("Train Model")
log('Number of training batches:', len(train_data))
log('Number of test batches:', len(val_data))

"""Sanity check"""

for images, *labels in train_data:
    break

plt.imshow(images[0])
plt.show()

@tf.function
def train_step(x, model, y_label_list=None):
    with tf.GradientTape() as tape:
        z_mean, z_logsigma, x_recon, out_classes = model(x, training=True)      #Forward pass through the VAE
        loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma,
            y_label_heads=y_label_list, 
            y_pred_heads=out_classes, 
            clf_loss_fn=clf_loss_crossentropy
        )  

    grads = tape.gradient(loss, model.trainable_variables)   #Calculate gradients
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss


@tf.function
def validation_step(x, model):
    z_mean, z_logsigma, x_recon, out_classes = model(x, training=False)  #Forward pass through the VAE
    loss = perceptual_loss_function(x, x_recon, z_mean, z_logsigma)
    return loss, x_recon, out_classes


##### Output
cc.dname = 'model_fashion_tf'
model_dir = './saved_models'
model_dir2 = model_dir + f"/m_{cc.tag}-{cc.dname}/"
os.makedirs(model_dir2, exist_ok=True)


#### Instantiate a new DFC_CVAE model and optimizer
log("\nBuild the DFC-VAE Model")
model = DFC_VAE(latent_dim, class_dict)
optimizer = tf.keras.optimizers.Adam(learning_rate)


### Training loop
kbatch = len(train_data)
train_loss_hist = []
valid_loss_hist = []
counter = 0 
dostop = False
best_loss = 100000000.0

### Validation dataset
# x_val = test_images
# valid_image_check(x_val, path= model_dir2 + "/check/", 
#                   tag= f"0ref", n_sample=10, renorm=True)


for epoch in range(num_epochs):
    log(f"Starting epoch {epoch+1}/{num_epochs}, in {kbatch} kbatches ")
    if dostop: break   

    for batch_idx, (x, *y_label_list) in enumerate(train_data):        
        if dostop:
            break

        # log("x", x)
        # log("y_label_list", y_label_list)
        log('[Epoch {:03d} batch {:04d}/{:04d}]'.format(epoch + 1, batch_idx+1, kbatch))

        train_loss = train_step(x, model, y_label_list=y_label_list)
        train_loss = np.mean(train_loss.numpy())
        train_loss_hist.append(train_loss)

        if (batch_idx + cc['kloop'] ) % cc['kloop']  == 0 :
            for (x_val, *_) in val_data:
                valid_loss, x_recon, _ = validation_step(x_val, model)    
                valid_loss = np.mean(valid_loss.numpy())
            valid_loss_hist.append(valid_loss)
        
            log(epoch+1, batch_idx, 'train,valid', train_loss, valid_loss)        
            best_loss, counter = save_best(model, model_dir2, valid_loss, best_loss, counter)
            dostop = train_stop(counter, cc['patience']) 

        if (batch_idx + 500) % 500 == 0 :
            for (x_val, *_) in val_data:
                _, x_recon, _ = validation_step(x_val, model)
                break
            valid_image_check(x_recon, path=model_dir2 + "/check/",
                              tag=f"e{epoch+1}_b{batch_idx+1}", n_sample=10, renorm=True)


log('Final valid_loss', str(valid_loss_hist)[:200])

"""Save the model"""

log("\nSaving Model")
os.makedirs(model_dir, exist_ok=True)
tf.saved_model.save(model, model_dir2)
model.save_weights(model_dir2 + f'/model_keras_weights.h5')
dd = {"pars" : [ learning_rate, latent_dim, num_epochs ]}
json.dump(dd, open(model_dir2 +"/info.json" , mode='w'))
log(model_dir2)

"""Reload the model"""

log('\nReload Model')
model2 = DFC_VAE(latent_dim, class_dict)
input_shape = (batch_size, xdim, ydim, cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
model2.build(input_shape)
model2.load_weights( model_dir2 + f'/model_keras_weights.h5')
log('# Reloaded', model2)
log('rerun eval', validation_step(x_val, model2))

"""Plot"""

"""Plot Original Input Images vs. Reconstructed Images"""
logger = logging.getLogger()
old_level = logger.level
logger.setLevel(100)

# test_sample = random.sample(list(test_images), 1)
for test_images, *_ in val_data:
    break
plot_original_images([test_images[0]])
_, _, x_recon, _ = model2(tf.expand_dims(test_images[0], 0), training=False)
plot_reconstructed_images(model, x_recon)


logger.setLevel(old_level)
log('Plot Loss Over Time During the Training Process')
plt.figure()
plt.plot(train_loss_hist)
plt.plot(valid_loss_hist)
plt.title('Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()


log('Plot Tail-End of Logarithmic of Loss Over Time During the Training Process')
plt.figure()
plt.plot(np.log(train_loss_hist[5000:]))
plt.plot(np.log(valid_loss_hist[5000:]))
plt.title('Logarithmic Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()

# def get_data_sample(batch_size, train_images, labels_val):
#     """Data sampler"""
#     i_select = np.random.choice(np.arange(len(labels_val['gender'])), size=batch_size, replace=False)
    
#     #### Images
#     x = np.array([train_images[i]  for i in i_select ])

#     #### y_onehot Labels  [y1, y2, y3, y4]
#     labels_col = ['gender', 'masterCategory', 'subCategory', 'articleType']
#     y_label_list = []
#     for ci in labels_col :
#         v =  labels_val[ci][i_select]
#         y_label_list.append(v)

#     return x, y_label_list

"""# 5) Random data"""

num_train = 100
num_val = 50
x_train = np.random.randint(0, 256, size=(num_train, image_size, image_size, 3), dtype='uint8')
y_train = [np.random.randint(0, 2,  size=(num_train, n_labels)) for _, n_labels in class_dict.items()]

x_val = np.random.randint(0, 256, size=(num_val, image_size, image_size, 3), dtype='uint8')
y_val = [np.random.randint(0, 2,  size=(num_val, n_labels)) for _, n_labels in class_dict.items()]

train_data = CustomDataGenerator0(x_train, y_train, augmentations=train_transforms)
val_data = CustomDataGenerator0(x_val, y_val, augmentations=test_transforms)

log("Train Model")
log('Number of training batches:', len(train_data))
log('Number of test batches:', len(val_data))


##### Output
cc.dname = 'model_random'
model_dir = './saved_models'
model_dir2 = model_dir + f"/m_{cc.tag}-{cc.dname}/"
os.makedirs(model_dir2, exist_ok=True)


#### Instantiate a new DFC_CVAE model and optimizer
log("\nBuild the DFC-VAE Model")
model = DFC_VAE(latent_dim, class_dict)
optimizer = tf.keras.optimizers.Adam(learning_rate)


### Training loop
kbatch = len(train_data)
train_loss_hist = []
valid_loss_hist = []
counter = 0 
dostop = False
best_loss = 100000000.0

### Validation dataset
# x_val = test_images
# valid_image_check(x_val, path= model_dir2 + "/check/", 
#                   tag= f"0ref", n_sample=10, renorm=True)


for epoch in range(num_epochs):
    log(f"Starting epoch {epoch+1}/{num_epochs}, in {kbatch} kbatches ")
    if dostop: break   

    for batch_idx, (x, *y_label_list) in enumerate(train_data):        
        if dostop:
            break

        # log("x", x)
        # log("y_label_list", y_label_list)
        log('[Epoch {:03d} batch {:04d}/{:04d}]'.format(epoch + 1, batch_idx+1, kbatch))

        train_loss = train_step(x, model, y_label_list=y_label_list)
        train_loss = np.mean(train_loss.numpy())
        train_loss_hist.append(train_loss)

        if (batch_idx + cc['kloop'] ) % cc['kloop']  == 0 :
            for (x_val, *_) in val_data:
                valid_loss, x_recon, _ = validation_step(x_val, model)    
                valid_loss = np.mean(valid_loss.numpy())
            valid_loss_hist.append(valid_loss)
        
            log(epoch+1, batch_idx, 'train,valid', train_loss, valid_loss)        
            best_loss, counter = save_best(model, model_dir2, valid_loss, best_loss, counter)
            dostop = train_stop(counter, cc['patience']) 

        if (batch_idx + 500) % 500 == 0 :
            for (x_val, *_) in val_data:
                _, x_recon, _ = validation_step(x_val, model)
                break
            valid_image_check(x_recon, path=model_dir2 + "/check/",
                              tag=f"e{epoch+1}_b{batch_idx+1}", n_sample=10, renorm=True)


log('Final valid_loss', str(valid_loss_hist)[:200])

"""Save the model"""

log("\nSaving Model")
os.makedirs(model_dir, exist_ok=True)
tf.saved_model.save(model, model_dir2)
model.save_weights( model_dir2 + f'/model_keras_weights.h5')
dd = {"pars" : [ learning_rate, latent_dim, num_epochs ]}
json.dump(dd, open(model_dir2 +"/info.json" , mode='w'))
log(model_dir2)

"""Reload the model"""

log('\nReload Model')
model2 = DFC_VAE(latent_dim, class_dict)
input_shape = (batch_size, xdim, ydim, cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
model2.build(input_shape)
model2.load_weights( model_dir2 + f'/model_keras_weights.h5')
log('# Reloaded', model2)
log('rerun eval', validation_step(x_val, model2))

"""Plot"""

"""Plot Original Input Images vs. Reconstructed Images"""
logger = logging.getLogger()
old_level = logger.level
logger.setLevel(100)

# test_sample = random.sample(list(test_images), 1)
for test_images, *_ in val_data:
    break
plot_original_images([test_images[0]])
_, _, x_recon, _ = model2(tf.expand_dims(test_images[0], 0), training=False)
plot_reconstructed_images(model, x_recon)


logger.setLevel(old_level)
log('Plot Loss Over Time During the Training Process')
plt.figure()
plt.plot(train_loss_hist)
plt.plot(valid_loss_hist)
plt.title('Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()


log('Plot Tail-End of Logarithmic of Loss Over Time During the Training Process')
plt.figure()
plt.plot(np.log(train_loss_hist[5000:]))
plt.plot(np.log(valid_loss_hist[5000:]))
plt.title('Logarithmic Loss Over Time During Training Process')
plt.xlabel('Time')
plt.ylabel('Loss')
plt.show()











# -*- coding: utf-8 -*-
"""


from utilmy import pd_read_file
import tensorflow as tf, pandas as pd 
from sklearn.metrics import accuracy_score
print(tf, tf.keras)

from importall import *
from utils import *
from importall import xdim, ydim, cdim
from utils import log, valid_image_check
from util_train import *
from util_train import save_model_state, save_best, metric_accuracy

###########################################################################################
from box import Box
cc = Box({})

### Data Size
cc.nmax        = 100000
cc.max_batch   = 300
cc.data_type   = 'npz'



### model 
cc.n_filters  = 12    #  12  # Base number of convolutional filters
cc.latent_dim = 256   # Number of latent dimensions


### Train
cc.opt_name       = ''   # 'madgrad'  ### Optimizer 
cc.data_gen_name  = 'album1'  ###  'album1'  ### Data Generator Name 

cc.batch_size     = 32
cc.learning_rate  = 1e-3
cc.num_epochs     = 100  # 80 overfits
cc.patience       = 200
cc.kloop          = 200


### Output file
cc.tag        = "test17"



############################################################################
log("\n\n##### Load Data   ###############################################")
cc.dname = "train_test-alllabel_nobg-256_256-100000"




if cc.data_type == 'npz' :
    log("\n\n######## Image dataset   ####################################################")
    data    = np.load( data_train + f"/{cc.dname}.npz")

        
    log("##### Xtrain ")    
    x_train = data['train']
    # x_test  = data['test']
    i_test = np.random.randint(0, len(x_train), 1000)  
    
    x_test = x_train[i_test]
    
    log( f"{cc.dname}")
    log( 'dataset shape', x_train.shape, )

    log( 'img shape', x_test[0].shape )
    (xdim, ydim, cdim) = x_test[0].shape
 

    log("#### Labels into OneHot #########################################################")    
    dfref      = pd_read_file( data_train + "/preprocessed_df.csv" )
    labels_col = [  'gender', 'masterCategory', 'subCategory', 'articleType' ]
    

    df       = pd.DataFrame(data['train_label'], columns=['uri'])    
    # log5(df)
    df['id'] = df['uri'].apply(lambda x : int(x.split("/")[-1].split(".")[0])    ) 
    labels_val, cc.labels_cnt  = data_to_y_onehot_list(df, dfref, labels_col)     
    y_train  = [ labels_val[ci][:, :]  for ci in labels_col  ]  ### list of oneHot (kbatch, nlabels)
    log5(y_train)
    
    
    # df       = pd.DataFrame(data['test_label'], columns=['uri'])
    df = pd.DataFrame(data['train_label'], columns=['uri'])
    df = df.iloc[i_test, :]
    
    df['id']       = df['uri'].apply(lambda x : int(x.split("/")[-1].split(".")[0])    )    
    labels_val, _  = data_to_y_onehot_list(df, dfref, labels_col)         
    y_test         = [ labels_val[ci][:, :]  for ci in labels_col  ]  #### list of oneHot (kbatch, nlabels)
    log5(y_test)    
    
    del data; gc.collect()
    

elif cc.data_type == 'parquet' :
    log("\n\n######## Image dataset   ####################################################")
    data    = np.load( data_train + f"/{cc.dname}.npz")

    
    i_test = np.random.randint(0, len(x_train), 1000)
    
    
    log("#### Labels into OneHot #########################################################")    
    dfref      = pd_read_file( data_train + "/preprocessed_df.csv" )
    # dfref      = data['df_master']
    labels_col = [  'gender', 'masterCategory', 'subCategory', 'articleType' ]
    
    
    id_filter = dfref[dfref['masterCategory'] == 0]['id'].values
    #id_filter = dfref[dfref['masterCategory'] > -1 ]['id'].values
    

    df       = pd.DataFrame(data['train_label'], columns=['uri'])    
    # log5(df)
    df['id'] = df['uri'].apply(lambda x : int(x.split("/")[-1].split(".")[0])    ) 
    df       = df[df['id'].isin(id_filter) ]
    
    # i_train =
    
    labels_val, cc.labels_cnt  = data_to_y_onehot_list(df, dfref, labels_col)     
    y_train  = [ labels_val[ci][:, :]  for ci in labels_col  ]  #### list of oneHot (kbatch, nlabels)
    log5(y_train)
    
    
    # df       = pd.DataFrame(data['test_label'], columns=['uri'])
    df = pd.DataFrame(data['train_label'], columns=['uri'])
    df = df.iloc[i_test, :]
    
    df['id']       = df['uri'].apply(lambda x : int(x.split("/")[-1].split(".")[0])    )    
    labels_val, _  = data_to_y_onehot_list(df, dfref, labels_col)         
    y_test         = [ labels_val[ci][:, :]  for ci in labels_col  ]  #### list of oneHot (kbatch, nlabels)
    log5(y_test)    
    
    
    
    log("### Xtrain ")    
    x_train = data['train']
    # x_test  = data['test']
    
    x_test = x_train[i_test]
    
    log( f"{cc.dname}")
    log( 'dataset shape', x_train.shape, )

    log( 'img shape', x_test[0].shape )
    (xdim, ydim, cdim) = x_test[0].shape
 
    # time.sleep(3)
    #np.savez_compressed( data_train + f"/{cc.dname}_check2.npz" ,
    #                     train=x_train[:100], test=x_test[:100])


    
elif cc.data_type == 'direct' :
     from util_train import CustomDataGenerator_img, train_transforms, test_transforms
     dfref      = pd_read_file( data_train + "/preprocessed_df.csv" )
     labels_col = [  'gender', 'masterCategory', 'subCategory', 'articleType' ]
     cc.labels_cnt = {}
     for ci in labels_col :
        cc.labels_cnt[ci] = dfref[ci].nunique()

        
else :    ###### Fake
    log("####### Fake Data  ")    
    image_size   = 64; num_train= 100 ; num_val = 10
    num_classes  = [ 7, 8, 9, 10]

    x_train =  np.random.randint(0, 256, size=(num_train, image_size, image_size, 3), dtype='uint8')
    y_train = [np.random.randint(0, 2,   size=(num_train, n_classes)) for n_classes in num_classes]

    x_test  =  np.random.randint(0, 256, size=(num_val, image_size, image_size, 3), dtype='uint8')
    y_test  = [np.random.randint(0, 2,   size=(num_val, n_classes)) for n_classes in num_classes]


    (xdim, ydim, cdim) = x_test[0].shape
    #### Mapping classname : nb of labels
    labels_col = ['gender', 'masterCategory', 'subCategory', 'articleType']
    cc.labels_cnt = { labels_col[i]: num_classes[i] for i in range(len(labels_col))  }


    
log('##### Check data ########################################################')    
log('xtrain, xtest', len(x_train) , len(x_test), x_train[0].shape)    
log('ytrain', len(y_train) , len(y_train[0]) )
log('ytest',  len(y_test)  , len(y_test[0]) )

log('cc.labels_cnt'   , cc.labels_cnt)
train_size = min(cc.nmax, x_train.shape[0])
test_size  = min(cc.nmax, x_test.shape[0])
log('Actual train, test: ', train_size, test_size)
log('x_train:',      str(x_train[0])[:10])

log( 'xdim, ydim', xdim, ydim )
cc.xdim, cc.ydim, cc.cdim = xdim, ydim, cdim
time.sleep(4)

    


################################################################################
log("\n\n##### Build Model   #################################################")
from tensorflow.keras import layers,regularizers

#### Deep Feature Consistent Variational Autoencoder Class
class DFC_VAE(tf.keras.Model):
  def __init__(self, latent_dim, labels_cnt):
    super(DFC_VAE, self).__init__()
    self.latent_dim = cc.latent_dim
    self.encoder    = make_encoder()
    self.decoder    = make_decoder()
    
    self.classifier = make_classifier(labels_cnt)

  def encode(self, x):
    z_mean, z_logsigma = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
    return z_mean, z_logsigma
  
  def reparameterize(self, z_mean, z_logsigma):
    eps = tf.random.normal(shape=z_mean.shape)
    return eps * tf.exp(z_logsigma * 0.5) + z_mean

  def decode(self, z, apply_sigmoid=False):
    x_recon = self.decoder(z)
    if apply_sigmoid:
      new_x_recon = tf.sigmoid(x_recon)
      return new_x_recon
    return x_recon

  ### bug when saving https://github.com/tensorflow/tensorflow/issues/37439
  def call(self, x,training=True, mask=None): 
    # out_classes = None
    z_mean, z_logsigma = self.encode(x)
    z                  = self.reparameterize(z_mean, z_logsigma)
    x_recon            = self.decode(z)
    
    #### Classifier
    out_classes = self.classifier(z)
    return z_mean, z_logsigma, x_recon, out_classes


def make_encoder(n_outputs = 1):
  #Functionally define the different layer types
  Input              = tf.keras.layers.InputLayer
  Conv2D             = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu',
                                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001), 
                                         activity_regularizer=regularizers.l2(1e-5)
                                        )
  BatchNormalization = tf.keras.layers.BatchNormalization
  Flatten            = tf.keras.layers.Flatten
  Dense              = functools.partial(tf.keras.layers.Dense, activation='relu',
                                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                         bias_regularizer=regularizers.l2(1e-4), activity_regularizer=regularizers.l2(1e-5)
                                        )

  ##### Build the encoder network using the Sequential API
  encoder = tf.keras.Sequential([
    Input(input_shape=(xdim, ydim, 3)),

    Conv2D(filters=1*cc.n_filters, kernel_size=5,  strides=2),
    BatchNormalization(),
    
    Conv2D(filters=2*cc.n_filters, kernel_size=5,  strides=2),
    BatchNormalization(),

    Conv2D(filters=4*cc.n_filters, kernel_size=3,  strides=2),
    BatchNormalization(),

    Conv2D(filters=6*cc.n_filters, kernel_size=3,  strides=2),
    BatchNormalization(),

    Flatten(),
    Dense(512*1, activation='relu'),
    layers.Dropout(0.2),  
    Dense(512*1, activation='relu'),
    Dense(2*cc.latent_dim, activation=None),
  ])

  return encoder


def make_decoder():
    
  #Functionally define the different layer types
  Input              = tf.keras.layers.InputLayer

  # bias_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001)
  Dense              = functools.partial(tf.keras.layers.Dense, activation='relu', 
                                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                         bias_regularizer=regularizers.l2(1e-4), activity_regularizer=regularizers.l2(1e-5)
                                        )
  Reshape            = tf.keras.layers.Reshape
  Conv2DTranspose    = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu',
                                         kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001), 
                                         activity_regularizer=regularizers.l2(1e-5)
                                        )
  BatchNormalization = tf.keras.layers.BatchNormalization
  Flatten            = tf.keras.layers.Flatten

  #Build the decoder network using the Sequential API
  if xdim == 64 :   #### 64 x 64 img         
      decoder = tf.keras.Sequential([
        Input(input_shape=(cc.latent_dim,)),

        Dense(units= 4*4  *6*cc.n_filters),
        Dense(units= 4*4  *6*cc.n_filters),
        layers.Dropout(0.2),   
        Dense(units= 4*4  *6*cc.n_filters),    
        Reshape(target_shape=(4, 4, 6*cc.n_filters)),
        #### ValueError: total size of new array must be unchanged, input_shape = [2304], output_shape = [7, 4, 144]

        #### Conv. layer      
        Conv2DTranspose(filters=4*cc.n_filters, kernel_size=3,  strides=2),
        Conv2DTranspose(filters=2*cc.n_filters, kernel_size=3,  strides=2),
        Conv2DTranspose(filters=1*cc.n_filters, kernel_size=5,  strides=2),

        Conv2DTranspose(filters=3, kernel_size=5,  strides=2),      
        # Conv2DTranspose(filters=4, kernel_size=5,  strides=2),

      ])


  if ydim == 256 :  ### 256 8 256 img          
      decoder = tf.keras.Sequential([
        Input(input_shape=(cc.latent_dim,)),

        Dense(units=16*16  *6*cc.n_filters),
        Dense(units=16*16  *6*cc.n_filters),
        layers.Dropout(0.2),   
        Dense(units=16*16  *6*cc.n_filters),    
        Reshape(target_shape=(16, 16, 6*cc.n_filters)),

        #### Conv. layer      
        Conv2DTranspose(filters=4*cc.n_filters, kernel_size=3,  strides=2),
        Conv2DTranspose(filters=2*cc.n_filters, kernel_size=3,  strides=2),
        Conv2DTranspose(filters=1*cc.n_filters, kernel_size=5,  strides=2),
        Conv2DTranspose(filters=3, kernel_size=5,  strides=2),      

      ])
  return decoder



#### Classifier
def clf_loss_crossentropy(y_true, y_pred):
    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)


def make_classifier(nclass_dict):
  
  Input              = tf.keras.layers.InputLayer
  Dense              = functools.partial(tf.keras.layers.Dense, activation='relu', 
                                         kernel_regularizer   = tf.keras.regularizers.L1L2(l1=0.01, l2=0.001),
                                         bias_regularizer     = regularizers.l2(1e-4), 
                                         activity_regularizer = regularizers.l2(1e-5)
                                        )
  Reshape            = tf.keras.layers.Reshape
  BatchNormalization = tf.keras.layers.BatchNormalization


  label_list  = [  'gender', 'masterCategory', 'subCategory', 'articleType' ]
  if xdim == 64 or xdim == 256 :   #### 64 x 64 img                 
        base_model = tf.keras.Sequential([
            Input(input_shape=(cc.latent_dim,)),
            Dense(units= 256 ),
            layers.Dropout(0.15),         
            Dense(units= 128),
        ])

        x = base_model.output
        ## x = layers.Flatten()(x) already flatten
        
        #### Multi-heads
        outputs = [ Dense( nclass_dict[ci], activation='softmax', name= f'out_{ci}')(x)  for ci in label_list  ] 
        clf     = tf.keras.Model(name='clf', inputs= base_model.input , outputs= outputs)  
        
  return clf



#### Loss Function  ########################################################################
#### Pretrained EfficientNetB0 Network 
percep_model = tf.keras.applications.EfficientNetB0(
    include_top=False, weights='imagenet', input_tensor=None,
    input_shape=(xdim, ydim, cdim), pooling=None, classes=1000,
    classifier_activation='softmax'
)

from util_train import clf_loss_macro_soft_f1

def loss_total_function(x, x_recon, z_mean, z_logsigma, kl_weight=0.00005, 
                        y_label_heads=None, y_pred_heads=None, clf_loss_fn=None):
      ### log( 'x_recon.shae',  x_recon.shape )  
      ### VAE Loss
      reconstruction_loss = tf.reduce_mean(tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3)))
      latent_loss         = 0.5 * tf.reduce_sum(tf.exp(z_logsigma) + tf.square(z_mean) - 1.0 - z_logsigma, axis=1)

      ### Efficient Head Loss
      a = 0.015  # 0.015
      perceptual_loss  = tf.reduce_sum(tf.square(tf.subtract(tf.stop_gradient(percep_model(x)),percep_model(x_recon))))
      loss_all         = kl_weight*latent_loss + reconstruction_loss + a*perceptual_loss  

      ### Classifier Loss 
      if y_label_heads is not None :
          loss_clf = []
          for i in range(len(y_pred_heads) ):
                # loss_clf.append( 0.0 )                        
                head_loss = clf_loss_fn(y_label_heads[i], y_pred_heads[i])
                # head_loss = clf_loss_macro_soft_f1(y_label_heads[i], y_pred_heads[i])
                loss_clf.append(head_loss)

          loss_clf = tf.reduce_mean(loss_clf) * 0.01 # + 10 # 000.0
          loss_all = loss_all * 2.0 + loss_clf    

      return loss_all



###########################################################################################
### Training setup
@tf.function
def train_step(x, model, optimizer, y_label_list=None):
  with tf.GradientTape() as tape:
    z_mean, z_logsigma, x_recon, y_pred = model.call(x)      #Forward pass through the VAE
    loss = loss_total_function(x, x_recon, z_mean, z_logsigma,
              y_label_heads = y_label_list, 
              y_pred_heads  = y_pred, 
              clf_loss_fn   = clf_loss_crossentropy
    )  

  grads = tape.gradient(loss, model.trainable_variables)   #Calculate gradients
  optimizer.apply_gradients(zip(grads, model.trainable_variables))
  return loss


@tf.function
def validation_step(x, model):
  z_mean, z_logsigma, x_recon, y_pred = model.call(x)  #Forward pass through the VAE
  loss                        = loss_total_function(x, x_recon, z_mean, z_logsigma)
  return loss, x_recon, y_pred



def model_reload(cc,):    
    model2      = DFC_VAE(cc.latent_dim, cc.labels_cnt)
    input_shape = (cc.batch_size, cc.xdim, cc.ydim, cc.cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
    model2.build(input_shape)
    model2.load_weights( cc.model_dir2 + f'/best/model_keras_weights.h5')
    return model2




    
###########################################################################################
####### Dataset ###########################################################################  
if cc.data_gen_name == 'album1':
   from util_train import CustomDataGenerator, train_augments, test_augments
   train_data = CustomDataGenerator(x_train, y_train, augmentations=train_augments)
   test_data  = CustomDataGenerator(x_test, y_test, augmentations=test_augments)


if cc.data_gen_name == 'direct':
    from util_train import CustomDataGenerator_img, train_transforms, test_transforms
    mg_dir  = data_dir + '/train/*'
    test_img_dir   = data_dir + '/test/*'    
    label_file = data_label  +"/preprocessed_df.csv"
    labels_col = [ 'gender', 'masterCategory', 'subCategory', 'articleType' ]
    
    train_data = CustomDataGenerator_img(train_img_dir, label_file, labels_col, split='train', transforms= train_transforms)
    test_data  = CustomDataGenerator_img(test_img_dir,  label_file, labels_col, split='val',   transforms= test_transforms)

    


    
###########################################################################################
###### Optimizer ##########################################################################
if   cc.opt_name == 'ada-bound':  
  from keras_adabound import AdaBound
  optimizer = AdaBound(lr=1e-3, final_lr=0.1)

elif cc.opt_name == 'radam':      
  from keras_radam import RAdam
  optimizer = RAdam(learning_rate=0.001)

elif cc.opt_name == 'madgrad':    
  from madgrad import MadGrad
  optimizer = MadGrad(lr=0.01)
else :    optimizer = tf.keras.optimizers.Adam(learning_rate=cc.learning_rate)




log("\n\n### Train Model ###############################################################")
log('x_train',      str(x_train)[:10])
log('shape train, test', train_size, test_size)


##### Hyperparameters  ###################################################################
cc.model_dir2 = model_dir + f"/m_{cc.tag}-{cc.dname}/"
os.makedirs(cc.model_dir2, exist_ok=True)


#### Model Instance
try :
   model  = model_reload(cc)
   log('\n\n model RELOADED', model)
except :    
   model          = DFC_VAE(cc.latent_dim, cc.labels_cnt)
   log('\n\n model NEW', model)

    
#### Training loop
from box import Box
dd = Box({})
dd.kbatch          = train_size // cc.batch_size
dd.train_loss_hist = []
dd.valid_loss_hist = []
dd.counter         = 0 
dd.dostop          = False
dd.best_loss       = 100000000.0
dd.test_accuracy   = {}
dd.labels_col      = labels_col
### Validation dataset
valid_image_check(x_test, path= cc.model_dir2 + "/check/", 
                  tag= f"0ref", n_sample=10, renorm=True)
### Dump Data
json.dump(cc, open(cc.model_dir2 +"/info.json" , mode='w'))


for i in range(cc.num_epochs):
    log(f"Starting epoch {i+1}/{cc.num_epochs}, in {dd.kbatch} kbatches ")
    if dd.dostop: break   

    for j, (x, *y_label_list) in enumerate(train_data):        
        if j > cc.max_batch : dostop= True
        if dd.dostop:  break 
        #(x, *y_label_list) = get_data_sample(cc.batch_size, x_train, labels_val)                
        # log5(x) ; log5(y_label_list )

        train_loss = train_step(x, model, optimizer, y_label_list = y_label_list)
        dd.train_loss_hist.append( np.mean( train_loss.numpy() ) )        


        if (j + cc['kloop'] ) % cc['kloop']  == 0 :
           valid_loss, x_recon, y_pred  = validation_step(x_test, model)    
           dd.valid_loss_hist.append( np.mean( valid_loss.numpy()  ) )

           log(i, j, 'train,valid', dd.train_loss_hist[-1], dd.valid_loss_hist[-1] )                               
           dd.test_accuracy = metric_accuracy(y_test, y_pred, dd)

            
           dd.best_loss, dd.counter = save_best(model, cc.model_dir2, dd.valid_loss_hist[-1], dd.best_loss, dd.counter)
           dd.dostop                = train_stop(dd.counter, cc['patience']) 
           json.dump({'res' : str(dd)}, open(cc.model_dir2 +"/results.json" , mode='w'))           
                    

        if (j + 400 ) % 400 == 0 :
            valid_image_check(x_recon, path= cc.model_dir2 + "/check/", 
                              tag= f"e{i}_b{j}", n_sample=10, renorm=True)

        
log('counter', counter) 
log(dd)
log('Final valid_loss', str(valid_loss_hist)[:200] )





    
log("\n\n#### Saving Model ######################################################")    
os.makedirs(cc.model_dir2, exist_ok=True)
#model.save( model_dir + "/model.h5")
# input_shape = (cc.batch_size, xdim, ydim, cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
# model.build(input_shape)
# model.save( model_dir + "/model_tf", save_format='tf') 
tf.saved_model.save(model,   cc.model_dir2 )
model.save_weights( cc.model_dir2 + f'/model_keras_weights.h5')



log(cc.model_dir2)



log('\n\n##### Reload Model ####################################################',)
##### model2    = tf.saved_model.load(cc.model_dir2)
model2      = DFC_VAE(cc.latent_dim, cc.labels_cnt)
input_shape = (cc.batch_size, xdim, ydim, cdim)   ### x_train = x_train.reshape(-1, 28, 28, 1)
model2.build(input_shape)
model2.load_weights( cc.model_dir2 + f'/model_keras_weights.h5')
# model2  = tf.keras.models.load_model(cc.model_dir2) 
log('##### Reloaded', model2)
log('rerun eval', validation_step(x_test, model2))


"""

















"""
# OLD"""

# Dummy data
num_train = 1000
num_val = 500
num_classes = [7, 7, 7, 7, 7]  # Number of classes each head
image_size = 224

# The image values should be range in [0, 1]
# We're using sparse form for targets
x_train = np.random.randint(0, 256, size=(num_train, image_size, image_size, 3), dtype='uint8')
y_train = [np.random.randint(0, 2, size=(num_train, n_classes)) for n_classes in num_classes]

x_val = np.random.randint(0, 256, size=(num_val, image_size, image_size, 3), dtype='uint8')
y_val = [np.random.randint(0, 2, size=(num_val, n_classes)) for n_classes in num_classes]

print(f'x_train: {x_train.shape}')
print(f'x_val: {x_val.shape}')
print(y_train[0].shape)

from albumentations.core.transforms_interface import ImageOnlyTransform

class SprinklesTransform(ImageOnlyTransform):
    def __init__(self, num_holes=100, side_length=10, always_apply=False, p=1.0):
        super(SprinklesTransform, self).__init__(always_apply, p)
        self.sprinkles = Sprinkles(num_holes=num_holes, side_length=side_length)
    
    def apply(self, image, **params):
        if isinstance(image, Image.Image):
            image = tf.constant(np.array(image), dtype=tf.float32)
        elif isinstance(image, np.ndarray):
            image = tf.constant(image, dtype=tf.float32)

        return self.sprinkles(image).numpy()


class CustomDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, x, y, batch_size=32, augmentations=None):
        self.x = x
        self.y = y
        self.batch_size = batch_size
        self.augment = augmentations

    def __len__(self):
        return int(np.ceil(len(self.x) / float(self.batch_size)))

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = []
        for y_head in self.y:
            batch_y.append(y_head[idx * self.batch_size:(idx + 1) * self.batch_size])
        
        if self.augment is not None:
            batch_x = np.stack([self.augment(image=x)['image'] for x in batch_x], axis=0)
        return (batch_x, *batch_y)


# # Data Augmentation with built-in Keras functions
# train_gen = keras.preprocessing.image.ImageDataGenerator(
#     rotation_range=20,
#     width_shift_range=20,
#     height_shift_range=20,
#     brightness_range=[0.2, 1.0],
#     shear_range=20,
#     horizontal_flip=True,
#     rescale=1./255
# )

# test_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

# train_data = train_gen.flow(x_train, y_train)
# val_data = test_gen.flow(x_val, y_val)


from albumentations import (
    Compose, HorizontalFlip, CLAHE, HueSaturationValue,
    RandomBrightness, RandomContrast, RandomGamma,
    ToFloat, ShiftScaleRotate, 
)

train_augments = Compose([
    HorizontalFlip(p=0.5),
    RandomContrast(limit=0.2, p=0.5),
    RandomGamma(gamma_limit=(80, 120), p=0.5),
    RandomBrightness(limit=0.2, p=0.5),
    HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,
                       val_shift_limit=10, p=.9),
    ShiftScaleRotate(
        shift_limit=0.0625, scale_limit=0.1, 
        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), 
    ToFloat(max_value=255),
    SprinklesTransform(p=0.5),
])

test_augments = Compose([
    ToFloat(max_value=255)
])

train_data = CustomDataGenerator(x_train, y_train, augmentations=train_augments)
val_data   = CustomDataGenerator(x_train, y_train, augmentations=test_augments)

def custom_loss(y_true, y_pred):
    return keras.losses.BinaryCrossentropy()(y_true, y_pred)


def build_model(input_shape, num_classes):
    """EfficientNet"""
    base_model = EfficientNetB0(include_top=False,
                                weights='imagenet',
                                input_shape=input_shape)

    # Freeze all layers
    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = layers.Flatten()(x)
    
    outputs = [layers.Dense(n_classes, activation='sigmoid')(x) for n_classes in num_classes]

    model = keras.Model(name='EfficientNet',
                        inputs=base_model.input,
                        outputs=outputs)
    return model

def custom_loss(y_true, y_pred):
    return keras.losses.BinaryCrossentropy()(y_true, y_pred)


def build_model_2(input_shape, num_classes):
    """Vanilla CNN"""

    base_model = tf.keras.Sequential([
        layers.InputLayer(input_shape),
        layers.ZeroPadding2D((3, 3)),
        layers.Conv2D(64, 7, 2, padding='same', activation='relu'),
        layers.MaxPooling2D(pool_size=3, strides=(2, 2)),
        layers.Conv2D(128, 3, padding='same', activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(256, 3, padding='same', activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(256, 3, padding='same', activation='relu'),
        layers.MaxPooling2D((2, 2)),
    ])
    x = base_model.output
    x = layers.Flatten()(x)
    
    outputs = [layers.Dense(n_classes, activation='sigmoid')(x) for n_classes in num_classes]

    model = tf.keras.Model(name='EfficientNet',
                        inputs=base_model.input,
                        outputs=outputs)
    return model

"""Train the model (Data augmentation)"""
from utilmy.deeplearning.keras.train_graph_loss import train_step, test_step;


input_shape = (image_size, image_size, 3)
num_classes = [7, 7, 7, 7, 7]
batch_size = 32
epochs = 2
learning_rate = 0.001

model = build_model(input_shape=input_shape,
                    num_classes=num_classes)
model.summary()

# Optimizers
opt = 'madgrad'
if opt == 'adam':
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
elif opt == 'adamw':
    optimizer = AdamW(lr=1e-4, model=model, lr_multipliers={'lstm_1': 0.5},
                    use_cosine_annealing=True, total_iterations=24)
elif opt == 'ada-bound':
    optimizer = AdaBound(lr=1e-3, final_lr=0.1)
elif opt == 'radam':
    optimizer = RAdam(learning_rate=0.001)
elif opt == 'madgrad':
    optimizer = MadGrad(lr=0.01)

accuracies = [keras.metrics.CategoricalAccuracy(name='accuracy_head_%i') for i in range(len(num_classes))]
loss = keras.metrics.Mean(name='loss')
head_losses = [keras.metrics.Mean(name='loss_head_%i') for i in range(len(num_classes))]

val_accuracies = [keras.metrics.CategoricalAccuracy(name='val_accuracy_head_%i') for i in range(len(num_classes))]
val_loss = keras.metrics.Mean(name='loss')
val_head_losses = [keras.metrics.Mean(name='val_loss_head_%i') for i in range(len(num_classes))]


print('Start training...')
for epoch in range(epochs):
    # Training
    for batch_idx, (images, *labels) in enumerate(train_data):
        batch_loss, loss_list, batch_outputs = train_step(images, labels, model, custom_loss, optimizer)

        for i in range(len(batch_outputs)):
            accuracies[i].update_state(labels[i], batch_outputs[i])
            head_losses[i].update_state(loss_list[i])
        loss.update_state(batch_loss)

        if (batch_idx + 1) % 10 == 0:
            print('[Epoch {:03d} iter {:04d}] - Loss: {:.3f} - {} - {}'.format(
                epoch + 1, batch_idx + 1, loss.result(),
                ' - '.join(['Head_{}_Loss: {:.3f}'.format(i, l.result()) for i, l in enumerate(head_losses)]),
                ' - '.join(['Head_{}_Accuracy: {:.3f}'.format(i, accuracy.result()) for i, accuracy in enumerate(accuracies)])
            ))
    
    # Validation
    print('\nEvaluating...')
    for i, (images, *labels) in enumerate(val_data):
        batch_loss, loss_list, batch_outputs = test_step(images, labels, model, custom_loss)

        for i in range(len(batch_outputs)):
            val_accuracies[i].update_state(labels[i], batch_outputs[i])
            val_head_losses[i].update_state(loss_list[i])
        loss.update_state(batch_loss)
    
    print('[Epoch {:03d}] - Loss: {:.3f} - {} - {}\n'.format(
        epoch + 1, val_loss.result(),
        ' - '.join(['Head_{}_Loss: {:.3f}'.format(i, l.result()) for i, l in enumerate(val_head_losses)]),
        ' - '.join(['Head_{}_Accuracy: {:.3f}'.format(i, accuracy.result()) for i, accuracy in enumerate(val_accuracies)])
    ))
    
    for acc in accuracies:
        acc.reset_state()

    for val_acc in val_accuracies:
        val_acc.reset_state()

"""Train the model [Original]"""

# @tf.function
# def train_step(x, y, model, loss_fn, optimizer):
#     """
#     """
#     with tf.GradientTape() as tape:
#         outputs = model(x, training=True)

#         all_losses = []
#         for y_true_head, y_pred_head in zip(y, outputs):
#             head_loss = loss_fn(y_true_head, y_pred_head)
#             all_losses.append(head_loss)
        
#         loss = tf.reduce_mean(all_losses)
    
#     grad = tape.gradient(loss, model.trainable_variables)
#     optimizer.apply_gradients(zip(grad, model.trainable_variables))
#     return loss, all_losses, outputs


# @tf.function
# def test_step(x, y, model, loss_fn):
#     """
#     """
#     with tf.GradientTape() as tape:
#         outputs = model(x, training=False)
        
#         all_losses = []
#         for y_true_head, y_pred_head in zip(y, outputs):
#             head_loss = loss_fn(y_true_head, y_pred_head)
#             all_losses.append(head_loss)
        
#         loss = tf.reduce_mean(all_losses)
#     return loss, all_losses, outputs


# input_shape = (image_size, image_size, 3)
# num_classes = [7, 7, 7, 7, 7]
# batch_size = 32
# epochs = 2
# learning_rate = 0.001

# model = build_model(input_shape=input_shape,
#                     num_classes=num_classes)
# model.summary()
# optimizer = keras.optimizers.Adam(learning_rate=learning_rate)

# accuracies = [keras.metrics.CategoricalAccuracy(name='accuracy_head_%i') for i in range(len(num_classes))]
# loss = keras.metrics.Mean(name='loss')
# head_losses = [keras.metrics.Mean(name='loss_head_%i') for i in range(len(num_classes))]

# val_accuracies = [keras.metrics.CategoricalAccuracy(name='val_accuracy_head_%i') for i in range(len(num_classes))]
# val_loss = keras.metrics.Mean(name='loss')
# val_head_losses = [keras.metrics.Mean(name='val_loss_head_%i') for i in range(len(num_classes))]

# train_data = tf.data.TFRecordDataset.from_tensor_slices((x_train, *y_train))\
#     .shuffle(buffer_size=num_train)\
#     .batch(batch_size, num_parallel_calls=2)\
#     .prefetch(1)

# val_data = tf.data.TFRecordDataset.from_tensor_slices((x_val, *y_val)) \
#     .batch(batch_size, num_parallel_calls=2) \
#     .prefetch(1)

# print('Start training...')
# for epoch in range(epochs):
#     # Training
#     for batch_idx, (images, *labels) in enumerate(train_data):
#         batch_loss, loss_list, batch_outputs = train_step(images, labels, model, custom_loss, optimizer)

#         for i in range(len(batch_outputs)):
#             accuracies[i].update_state(labels[i], batch_outputs[i])
#             head_losses[i].update_state(loss_list[i])
#         loss.update_state(batch_loss)

#         if (batch_idx + 1) % 10 == 0:
#             print('[Epoch {:03d} iter {:04d}] - Loss: {:.3f} - {} - {}'.format(
#                 epoch + 1, batch_idx + 1, loss.result(),
#                 ' - '.join(['Head_{}_Loss: {:.3f}'.format(i, l.result()) for i, l in enumerate(head_losses)]),
#                 ' - '.join(['Head_{}_Accuracy: {:.3f}'.format(i, accuracy.result()) for i, accuracy in enumerate(accuracies)])
#             ))
    
#     # Validation
#     print('\nEvaluating...')
#     for i, (images, *labels) in enumerate(val_data):
#         batch_loss, loss_list, batch_outputs = test_step(images, labels, model, custom_loss)

#         for i in range(len(batch_outputs)):
#             val_accuracies[i].update_state(labels[i], batch_outputs[i])
#             val_head_losses[i].update_state(loss_list[i])
#         loss.update_state(batch_loss)
    
#     print('[Epoch {:03d}] - Loss: {:.3f} - {} - {}\n'.format(
#         epoch + 1, val_loss.result(),
#         ' - '.join(['Head_{}_Loss: {:.3f}'.format(i, l.result()) for i, l in enumerate(val_head_losses)]),
#         ' - '.join(['Head_{}_Accuracy: {:.3f}'.format(i, accuracy.result()) for i, accuracy in enumerate(val_accuracies)])
#     ))
    
#     for acc in accuracies:
#         acc.reset_state()
    
#     # for head_loss in head_losses:
#     #     head_loss.reset_state()
#     # loss.reset_state()

#     for val_acc in val_accuracies:
#         val_acc.reset_state()
#     # for val_head_loss in val_head_losses:
#     #     val_head_loss.reset_state()
#     # val_loss.reset_state()
