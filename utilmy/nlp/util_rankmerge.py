# -*- coding: utf-8 -*-
HELP = """merge ranking

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wMTlaj4PPBlpkh4VE0A3X7D0ZULWnok8
!pip install ranky

!pip install git+https://github.com/djcunningham0/rankaggregation.git
!pip install mc4


"""
import random, pandas as pd, numpy as np,  scipy, math
try :
   import ranky as rk
   from mc4.algorithm import mc4_aggregator
   import rankaggregation as ra
except :
   pass 


##################################################################################################
def log(*s): 
  print(*s, flush=True)




def test1():
    """"
        # evaluate each rank aggregation algorithm by using spearman's rho's and kendall-tau's metrics
        # for varying levels of ncorrect in the generated rankings    
    
    """"

    def rank_merge_v4(ll1, ll2):
        """ Re-rank elements of list1 using ranking of list2
        """        
        n1, n2 = len(ll1), len(ll2)
        adjust, mrank = (1.0 * n1) / n2, n2
        rank3 = np.zeros(n1, dtype='float32')
        kk = 2

        for rank1, sid in enumerate(ll1):
            rank2 = np_find(ll2, sid)
            rank2 = mrank if rank2 == -1 else rank2
            rank3[rank1] = -rank_score(rank1, rank2, adjust, kk=kk)

        # Id of ll1 sorted list
        return [ll1[i] for i in np.argsort(rank3)]

    pd.set_option('display.max_columns', 7)
    for ncorrect in [0, 20, 50, 80, 100]:
      print('ncorrect = {}'.format(ncorrect))

      #### Fake with ncorrect 
      df, rank1, rank2, dict_full = rank_generatefake(ncorrect, 100)
      rank_true = list(dict_full)
        
        
      def rank_score(rank1, rank2, adjust=1.0, kk=1.0):
            return 1.0 / (kk + rank1) + 1.0 / (kk + rank2 * adjust)
        
        
      def loss(rank_score_formulae):        
         rankmerge = rank_merge_v3(rank1, rank2, 100)  
         metric    = kendall_tau(rank_true, rankmerge)  
        
         retrun metric*metric

     #### Find rank_score formulae, such as metric is minimize
    
    
def test_use_operon():    
    # SPDX-License-Identifier: MIT
    # SPDX-FileCopyrightText: Copyright 2019-2021 Heal Research

    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import r2_score, make_scorer
    from scipy.stats import pearsonr

    from operon import RSquared
    from operon.sklearn import SymbolicRegressor

    from pmlb import fetch_data, dataset_names, classification_dataset_names, regression_dataset_names
    #print(regression_dataset_names)

    X, y = fetch_data('1027_ESL', return_X_y=True)

    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25, shuffle=True, random_state=1234)

    reg = SymbolicRegressor(
            allowed_symbols='add,sub,mul,div,constant,variable',
            offspring_generator='basic',
            local_iterations=10,
            n_threads=4,
            objectives = ['r2', 'shape'],
            random_state=1234
            )

    reg.fit(X_train, y_train)
    print(reg.get_model_string(2))
    print(reg._stats)

    y_pred_train = reg.predict(X_train)
    print('r2 train (sklearn.r2_score): ', r2_score(y_train, y_pred_train))
    # for comparison we calculate the r2 from _operon and scipy.pearsonr
    print('r2 train (operon.rsquared): ', RSquared(y_train, y_pred_train))
    r = pearsonr(y_train, y_pred_train)[0]
    print('r2 train (scipy.pearsonr): ', r * r)

    # crossvalidation
    sc = make_scorer(RSquared, greater_is_better=True)
    scores = cross_val_score(reg, X, y, cv=5, scoring=sc)
    print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

    

    
    
###################################################################################################
###################################################################################################    
def test():
    # evaluate each rank aggregation algorithm by using spearman's rho's and kendall-tau's metrics
    # for varying levels of ncorrect in the generated rankings
    pd.set_option('display.max_columns', 7)
    for ncorrect in [0, 20, 50, 80, 100]:
      print('ncorrect = {}'.format(ncorrect))

      #### Fake with ncorrect 
      df, rank1, rank2, dict_full = rank_generatefake(ncorrect, 100)
      rank_true = list(dict_full)

      #### Algo merged
      borda     = rank_merge(df, method='borda')
      dowdall   = rank_merge(df, method='dowdall')
      mc4_rank  = rank_merge(df, method='mc4')
      custom    = rank_merge_v2(rank1, rank2, 100)
      avg       = rank_merge(df, method='average')
      df2 = pd.DataFrame([borda, dowdall, avg, mc4_rank, custom, rank1, rank2],
                         index = ['borda', 'dowdall', 'average', 'MC4', 'algo v2', 'rank1', 'rank2'])
      df2 = df2.transpose()

      #### Fill NA with Max ranking as default 
      for idx, col in enumerate(df2.columns):
        max_value = df2[col].max()
        df2[col]  = df2[col].replace(np.nan, max_value)


      dfres = rank_eval(rank_true, df2, 100)
      log(dfres)


        
        

############################################################################################
############################################################################################
def rank_generatefake(ncorrect=30, nsize=100):
    """
      Generate a fake rank list of size nrank that contains ncorrect elements that
      are correctly ranked. Returns a dataframe where the two fake ranks are merged

      # each column of the dataframe should contain the rank each ranker assigns to each candidate
      # because of the fake generate algorithm, some items might not have ranks assigned to them by the ranker

      Keyword arguments:
      ncorrect: The number of correctly ranked elements in list
      nrank: Total number of elements to be ranked
    """
    ### Reference 1000 elements
    dict_full = {}
    for i in range(1000):
      dict_full[i] = i + 1

    list_overlap = [1, 15, 20, 30, 80]
    list1 = rank_generate_fake(dict_full, list_overlap, nsize=nsize,  ncorrect=ncorrect) 
    list2 = rank_generate_fake(dict_full, list_overlap, nsize=nsize,  ncorrect=ncorrect)
    rank1, rank2 = [None] * 1000, [None] * 1000
    for i in range(1000):
      if i in list1:
        rank1[i - 1] = list1.index(i)

      if i in list2:
        rank2[i - 1] = list2.index(i)

    df = pd.DataFrame([rank1, rank2], index=['rank1', 'rank2'])
    df = df.fillna(np.nan)
    df = df.transpose()
    # normalize by replacing missing values
    df = rank_fillna(df)
    return df, rank1, rank2, dict_full


def rank_generate_fake(dict_full, list_overlap, nsize=100, ncorrect=20):
    """  Returns a list of random rankings of size nsize where ncorrect
         elements have correct ranks

        Keyword arguments:
        dict_full    : a dictionary of 1000 objects and their ranks
        list_overlap : list items common to all lists
        nsize        : the total number of elements to be ranked
        ncorrect     : the number of correctly ranked objects
    """
    # first randomly sample nsize - len(list_overlap) elements from dict_full
    # of those, ncorrect of them must be correctly ranked
    random_vals = []
    while len(random_vals) <= nsize - len(list_overlap):
      rand = random.sample(list(dict_full), 1)
      if (rand not in random_vals and rand not in list_overlap):
        random_vals.append(rand[0])

    # next create list as aggregate of random_vals and list_overlap
    list2 = random_vals + list_overlap
    
    # shuffle nsize - ncorrect elements from list2 
    copy = list2[0:nsize - ncorrect]
    random.shuffle(copy)
    list2[0:nsize - ncorrect] = copy

    # ensure there are ncorrect elements in correct places
    if ncorrect == 0: 
      return list2
    rands = random.sample(list(dict_full)[0:nsize + 1], ncorrect + 1)
    for r in rands:
      list2[r] = list(dict_full)[r]
    return list2



def rank_fillna(df):
    """Replace NaN value with maximum value of column
        Keyword arguments:
        df: a dataframe where each row represents an item and each column represents it's
            assigned rank by a ranker
    """
    cols = df.columns
    for col in cols:
      max_val = df[col].max()
      df[col] = df[col].replace(np.nan, max_val)
    return df





############################################################################################
def rank_eval(rank_true, dfmerged, nrank=100):
  """
    Returns a dataframe where the columns are the rank aggregation algorithms
    and the rows are the metrics used to evaluate the algorithms

    Keyword arguments:
    rank_true: a list containing the accurate rank for each item
    dfmerged: a dataframe where each row represents an item and each column represents
              the rank aggregation algorithms
    nrank: the total number of elements to be ranked
  """
  columns = dfmerged.columns
  df = pd.DataFrame([], index=['spearman_rho', 'kendall-tau'])
  for col in columns:
    rank_list    = dfmerged[col].tolist()
    spearman_rho = scipy.stats.spearmanr(rank_true[0:nrank],  rank_list[0:nrank])
    kendall_tau  = scipy.stats.kendalltau(rank_true[0:nrank], rank_list[0:nrank])
    df[col]      = [spearman_rho[0], kendall_tau[0]]
  return df





#####################################################################################
#####################################################################################
def rank_score(rank1, rank2, adjust=1.0, kk=1.0):
    return 1.0 / (kk + rank1) + 1.0 / (kk + rank2 * adjust)



def rank_merge_v4(ll1, ll2):
    """ Re-rank elements of list1 using ranking of list2
    """        
    n1, n2 = len(ll1), len(ll2)
    adjust, mrank = (1.0 * n1) / n2, n2
    rank3 = np.zeros(n1, dtype='float32')
    kk = 2

    for rank1, sid in enumerate(ll1):
        rank2 = np_find(ll2, sid)
        rank2 = mrank if rank2 == -1 else rank2
        rank3[rank1] = -rank_score(rank1, rank2, adjust, kk=kk)

    # Id of ll1 sorted list
    return [ll1[i] for i in np.argsort(rank3)]



def rank_merge_v3(list1, list2, maxrank=100):
    """ Inver MRR cores
      A custom rank aggregation algorithm that assigns a score to each item 
      based on its rankings. The algorithm sorts the items by their score inescending order and returns them
      Keyword arguments:
      list1: a list containing the items
      list2: a list containing the items
      nrank: total number of elements to be ranked
    """
    keys, scores = [], []    
    adjust = len(list1) / len(list2) * 1.0 
    kk     = 1

    for i, x in enumerate(list1):
        rank1 = i + 1
        rank2 = np_find(x, list2)
        rank2 = maxrank if rank2 < 0 else rank2+1
        score  = rank_score(rank1, rank2, adjust, kk= kk)
        keys.append(x)
        scores.append(score)

    for i, x in enumerate(list2):
        if x in keys:
            continue
        rank_2 = i + 1
        rank_1 = maxrank
        score  = rank_score(rank1, rank2, adjust, kk= kk)
        keys.append(x)
        scores.append(score)

    # sort list by rank in descending order
    return [keys[i] for i in np.argsort(np.array(scores))[::-1]]

    
    
    
############################################################################################
############ Algo 2#########################################################################
def rank_merge(df, method='borda'):
    """
      Returns a list of merged ranks for each item in the dataframe

      Keyword arguments:
      df: a dataframe in which each row represents an item and each column
          represents it's rank assigned by a ranker
      method: A rank aggregation algorithm
    """
    columns = df.columns
    # rank aggregation algorithm for dowdall and average doesn't accept dataframes
    # so convert each column to list
    rank_list = [df[column].tolist() for column in columns]
    if method == 'borda':
      rank = rk.rank(rk.borda(df)).tolist()

    elif method == 'majority':
      rank = rk.majority(df).tolist()

    elif method == 'center':
      rank = rk.center(df).tolist()

    elif method == 'instant_runoff':
      agg = ra.RankAggregator()   
      rank = agg.instant_runoff(rank_list)

    elif method == 'dowdall':
      agg = ra.RankAggregator()
      rank = agg.dowdall(rank_list)
      res = []
      for r in rank:
        res.append(r[0])
      return res

    elif method == 'average':
      agg = ra.RankAggregator()
      rank = agg.average_rank(rank_list)
      res = [None] * 1000
      for r in rank:
        res[int(r[0]) - 1] = r[1]
      return res

    elif method == 'mc4':
      aggregated_ranks = mc4_aggregator(df, header_row = 0, index_col = 0) 
      return list(aggregated_ranks.values())
    return rank




def rank_merge_v2(list1, list2, nrank):
  """ Inver MRR cores
    A custom rank aggregation algorithm that assigns a score to each item 
    based on its rankings. The algorithm sorts the items by their score in
    descending order and returns them

    Keyword arguments:
    list1: a list containing the ranks of items
    list2: a list containing the ranks of items
    nrank: total number of elements to be ranked
  """
  scores = []
  for i in range(1, nrank + 1):
    rank_1, rank_2 = nrank, nrank
    if i in list1:
      rank_1 = list1.index(i) + 1

    if i in list2:
      rank_2 = list2.index(i) + 1

    score = ((1 / rank_1) + (1 / rank_2)) / 2.0
    tup = (i, score)
    scores.append(tup)
  
  # sort list by rank in descending order
  scores.sort(key=lambda tup: tup[1], reverse=True)
  rank = []
  for (item, score) in scores:
    rank.append(item)
  return rank



##########################################################################################
if __name__ == '__main__':
    import fire
    fire.Fire()


