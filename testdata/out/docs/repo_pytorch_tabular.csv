uri,name,type,n_variable,n_words,n_words_unique,n_characters,avg_char_per_word,n_loop,n_ifthen,arg_name,arg_type,arg_value,line,docs,list_functions,n_functions
setup.py:read_requirements,read_requirements,function,8,15,13,186,12.4,0,1,"['thelibFolder', 'filename']","[None, None]","[None, None]",8,[],"['open', 'f.read']",2
examples/adhoc_scaffold.py:make_mixed_classification,make_mixed_classification,function,26,58,44,645,11.12,3,2,"['n_samples', 'n_features', 'n_categories']","[None, None, None]","[None, None, None]",11,[],"['make_classification', 'random.choices', 'range', 'pd.qcut', 'col_names.append', 'cat_col_names.append', 'num_col_names.append', 'pd.DataFrame', 'pd.Series', 'X.join']",10
examples/adhoc_scaffold.py:print_metrics,print_metrics,function,14,35,26,364,10.4,0,4,"['y_true', 'y_pred', 'tag']","[None, None, None]","[None, None, None]",32,[],"['isinstance', 'accuracy_score', 'f1_score', 'print']",4
examples/to_test_node.py:regression_data,regression_data,function,14,23,22,369,16.04,0,0,[],[],[],14,[],"['fetch_california_housing', 'pd.qcut', 'df.sample', 'len']",4
examples/to_test_node.py:classification_data,classification_data,function,19,36,34,526,14.61,1,0,[],[],[],25,[],"['fetch_covtype', 'np.hstack', 'range', 'pd.DataFrame', 'pd.qcut', 'data.sample', 'len']",7
examples/to_test_node.py:test_regression,test_regression,function,20,63,54,1001,15.89,0,2,"['regression_data', 'multi_target', 'embed_categorical', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', '']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",39,[],"['len', 'DataConfig', 'dict', 'NodeConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",10
examples/to_test_node.py:test_classification,test_classification,function,19,51,48,921,18.06,0,1,"['classification_data', 'continuous_cols', 'categorical_cols', 'embed_categorical', 'continuous_feature_transform', 'normalize_continuous_features', '']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",84,[],"['len', 'DataConfig', 'dict', 'NodeConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",10
examples/to_test_regression.py:fake_metric,fake_metric,function,1,3,3,22,7.33,0,0,"['y_hat', 'y']","[None, None]","[None, None]",96,[],[],0
examples/to_test_regression_custom_models.py:MultiStageModelConfig,MultiStageModelConfig,class,88,475,238,2968,6.25,0,1,[],[],[],51,[],[],0
pytorch_tabular/categorical_encoders.py:BaseEncoder,BaseEncoder,class,42,170,108,1426,8.39,2,7,[],[],[],22,[],[],0
pytorch_tabular/categorical_encoders.py:OrdinalEncoder,OrdinalEncoder,class,11,37,35,440,11.89,1,0,[],[],[],100,[],[],0
pytorch_tabular/categorical_encoders.py:CategoricalEmbeddingTransformer,CategoricalEmbeddingTransformer,class,49,182,127,1944,10.68,5,4,[],[],[],133,[],[],0
categorical_encoders.py:BaseEncoder:__init__,BaseEncoder:__init__,method,9,11,11,122,11.09,0,0,"['self', 'cols', 'handle_unseen', 'min_samples', 'imputed']","[None, None, None, None, None]","[None, None, None, None, None]",23,[],['max'],1
categorical_encoders.py:BaseEncoder:transform,BaseEncoder:transform,method,14,54,44,523,9.69,1,3,"['self', 'X']","[None, None]","[None, None]",32,"['        """"""Transform categorical data based on mapping learnt at fitting time.\n', '        :param pandas.DataFrame X: DataFrame of features, shape (n_samples, n_features). Must contain columns to encode.\n', '        :return: encoded DataFrame of shape (n_samples, n_features), initial categorical columns are dropped, and\n', '            replaced with encoded columns. DataFrame passed in argument is unchanged.\n', '        :rtype: pandas.DataFrame\n', '        """"""\n']","['ValueError', 'all', 'X.copy', 'np.unique']",4
categorical_encoders.py:BaseEncoder:fit_transform,BaseEncoder:fit_transform,method,3,4,4,37,9.25,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",59,"['        """"""Encode given columns of X according to y, and transform X based on the learnt mapping.\n', '        :param pandas.DataFrame X: DataFrame of features, shape (n_samples, n_features). Must contain columns to encode.\n', '        :param pandas.Series y: pandas Series of target values, shape (n_samples,).\n', '            Required only for encoders that need it: TargetEncoder, WeightOfEvidenceEncoder\n', '        :return: encoded DataFrame of shape (n_samples, n_features), initial categorical columns are dropped, and\n', '            replaced with encoded columns. DataFrame passed in argument is unchanged.\n', '        :rtype: pandas.DataFrame\n', '        """"""\n']","['self.fit', 'self.transform']",2
categorical_encoders.py:BaseEncoder:_input_check,BaseEncoder:_input_check,method,3,17,16,95,5.59,0,1,"['self', 'name', 'value', 'options']","[None, None, None, None]","[None, None, None, None]",71,[],['ValueError'],1
categorical_encoders.py:BaseEncoder:_before_fit_check,BaseEncoder:_before_fit_check,method,7,31,21,186,6.0,0,2,"['self', 'X', 'y']","[None, None, None]","[None, None, None]",77,[],['all'],1
categorical_encoders.py:BaseEncoder:save_as_object_file,BaseEncoder:save_as_object_file,method,3,16,16,135,8.44,0,1,"['self', 'path']","[None, None]","[None, None]",88,[],"['ValueError', 'pickle.dump', 'open']",3
categorical_encoders.py:BaseEncoder:load_from_object_file,BaseEncoder:load_from_object_file,method,4,9,8,63,7.0,1,0,"['self', 'path']","[None, None]","[None, None]",95,[],"['pickle.load', 'setattr']",2
categorical_encoders.py:OrdinalEncoder:__init__,OrdinalEncoder:__init__,method,2,10,9,147,14.7,0,0,"['self', 'cols', 'handle_unseen']","[None, None, None]","[None, 'None', '""impute""']",105,"['        """"""Instantiation\n', '        :param [str] cols: list of columns to encode, or None (then all dataset columns will be encoded at fitting time)\n', '        :param str handle_unseen:\n', ""            'error'  - raise an error if a category unseen at fitting time is found\n"", ""            'ignore' - skip unseen categories\n"", ""            'impute' - impute new categories to a predefined value, which is same as NAN_CATEGORY\n"", '        :return: None\n', '        """"""\n']","['self._input_check', 'super']",2
categorical_encoders.py:OrdinalEncoder:fit,OrdinalEncoder:fit,method,7,19,19,217,11.42,1,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",117,"['        """"""Label Encode given columns of X.\n', '        :param pandas.DataFrame X: DataFrame of features, shape (n_samples, n_features). Must contain columns to encode.\n', '        :return: None\n', '        """"""\n']","['self._before_fit_check', 'pd.Series', 'map.set_index']",3
categorical_encoders.py:CategoricalEmbeddingTransformer:__init__,CategoricalEmbeddingTransformer:__init__,method,6,7,7,187,26.71,0,0,"['self', 'tabular_model']","[None, None]","[None, None]",137,"['        """"""Initializes the Transformer and extracts the neural embeddings\n', '\n', '        Args:\n', '            tabular_model (TabularModel): The trained TabularModel object\n', '        """"""\n']",['self._extract_embedding'],1
categorical_encoders.py:CategoricalEmbeddingTransformer:_extract_embedding,CategoricalEmbeddingTransformer:_extract_embedding,method,14,47,39,584,12.43,2,2,"['self', 'model']","[None, None]","[None, None]",150,[],"['hasattr', 'model.extract_embedding', 'enumerate']",3
categorical_encoders.py:CategoricalEmbeddingTransformer:fit,CategoricalEmbeddingTransformer:fit,method,1,2,2,10,5.0,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",117,"['        """"""Label Encode given columns of X.\n', '        :param pandas.DataFrame X: DataFrame of features, shape (n_samples, n_features). Must contain columns to encode.\n', '        :return: None\n', '        """"""\n']",[],0
categorical_encoders.py:CategoricalEmbeddingTransformer:transform,CategoricalEmbeddingTransformer:transform,method,15,69,54,656,9.51,2,1,"['self', 'X', 'y']","[None, ' pd.DataFrame', None]","[None, None, 'None']",179,"['        """"""Transforms the categorical columns specified to the trained neural embedding from the model\n', '\n', '        Args:\n', '            X (pd.DataFrame): DataFrame of features, shape (n_samples, n_features). Must contain columns to encode.\n', '            y ([type], optional): Only for compatibility. Not used. Defaults to None.\n', '\n', '        Raises:\n', '            ValueError: [description]\n', '\n', '        Returns:\n', '            pd.DataFrame: The encoded dataframe\n', '        """"""\n']","['ValueError', 'all', 'X.copy', 'tqdm', 'total=len', 'range', 'mapping.items', 'X_encoded.drop']",8
categorical_encoders.py:CategoricalEmbeddingTransformer:fit_transform,CategoricalEmbeddingTransformer:fit_transform,method,3,4,4,37,9.25,0,0,"['self', 'X', 'y']","[None, ' pd.DataFrame', None]","[None, None, 'None']",217,"['        """"""Encode given columns of X based on the learned embedding.\n', '\n', '        Args:\n', '            X (pd.DataFrame): DataFrame of features, shape (n_samples, n_features). Must contain columns to encode.\n', '            y ([type], optional): Only for compatibility. Not used. Defaults to None.\n', '\n', '        Returns:\n', '            pd.DataFrame: The encoded dataframe\n', '        """"""\n']","['self.fit', 'self.transform']",2
categorical_encoders.py:CategoricalEmbeddingTransformer:save_as_object_file,CategoricalEmbeddingTransformer:save_as_object_file,method,3,16,16,135,8.44,0,1,"['self', 'path']","[None, None]","[None, None]",88,[],"['ValueError', 'pickle.dump', 'open']",3
categorical_encoders.py:CategoricalEmbeddingTransformer:load_from_object_file,CategoricalEmbeddingTransformer:load_from_object_file,method,4,9,8,63,7.0,1,0,"['self', 'path']","[None, None]","[None, None]",95,[],"['pickle.load', 'setattr']",2
pytorch_tabular/feature_extractor.py:DeepFeatureExtractor,DeepFeatureExtractor,class,53,169,122,1714,10.14,6,6,[],[],[],22,[],[],0
feature_extractor.py:DeepFeatureExtractor:__init__,DeepFeatureExtractor:__init__,method,8,30,27,317,10.57,0,0,"['self', 'tabular_model', 'extract_keys', 'drop_original']","[None, None, None, None]","[None, None, '[""backbone_features""]', 'True']",23,"['        """"""Initializes the Transformer and extracts the neural features\n', '\n', '        Args:\n', '            tabular_model (TabularModel): The trained TabularModel object\n', '        """"""\n']",['isinstance'],1
feature_extractor.py:DeepFeatureExtractor:fit,DeepFeatureExtractor:fit,method,1,2,2,10,5.0,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",40,"['        """"""Just for compatibility. Does not do anything""""""\n']",[],0
feature_extractor.py:DeepFeatureExtractor:transform,DeepFeatureExtractor:transform,method,35,81,58,875,10.8,5,5,"['self', 'X', 'y']","[None, ' pd.DataFrame', None]","[None, None, 'None']",44,"['        """"""Transforms the categorical columns specified to the trained neural features from the model\n', '\n', '        Args:\n', '            X (pd.DataFrame): DataFrame of features, shape (n_samples, n_features). Must contain columns to encode.\n', '            y ([type], optional): Only for compatibility. Not used. Defaults to None.\n', '\n', '        Raises:\n', '            ValueError: [description]\n', '\n', '        Returns:\n', '            pd.DataFrame: The encoded dataframe\n', '        """"""\n']","['X.copy', 'defaultdict', 'tqdm', 'batch.items', 'isinstance', 'v.to', 'ret_value.keys', 'logits_predictions.items', 'torch.cat', 'v.reshape', 'range', 'X_encoded.drop']",12
feature_extractor.py:DeepFeatureExtractor:fit_transform,DeepFeatureExtractor:fit_transform,method,3,4,4,37,9.25,0,0,"['self', 'X', 'y']","[None, ' pd.DataFrame', None]","[None, None, 'None']",92,"['        """"""Encode given columns of X based on the learned features.\n', '\n', '        Args:\n', '            X (pd.DataFrame): DataFrame of features, shape (n_samples, n_features). Must contain columns to encode.\n', '            y ([type], optional): Only for compatibility. Not used. Defaults to None.\n', '\n', '        Returns:\n', '            pd.DataFrame: The encoded dataframe\n', '        """"""\n']","['self.fit', 'self.transform']",2
feature_extractor.py:DeepFeatureExtractor:save_as_object_file,DeepFeatureExtractor:save_as_object_file,method,3,16,16,135,8.44,0,1,"['self', 'path']","[None, None]","[None, None]",105,[],"['ValueError', 'pickle.dump', 'open']",3
feature_extractor.py:DeepFeatureExtractor:load_from_object_file,DeepFeatureExtractor:load_from_object_file,method,4,9,8,63,7.0,1,0,"['self', 'path']","[None, None]","[None, None]",112,[],"['pickle.load', 'setattr']",2
pytorch_tabular/tabular_datamodule.py:TabularDatamodule,TabularDatamodule,class,153,741,378,9000,12.15,4,30,[],[],[],32,[],[],0
pytorch_tabular/tabular_datamodule.py:TabularDataset,TabularDataset,class,25,98,65,1100,11.22,0,8,[],[],[],539,[],[],0
tabular_datamodule.py:TabularDatamodule:__init__,TabularDatamodule:__init__,method,4,10,10,168,16.8,0,1,"['self', 'train', 'config', 'validation', 'test', 'target_transform', 'Tuple]] ', 'train_sampler', ').__init__())self.validation = validationif target_transform is not None']","[None, ' pd.DataFrame', ' DictConfig', ' pd.DataFrame ', ' pd.DataFrame ', ' Optional[Union[TransformerMixin', None, ' Optional[torch.utils.data.Sampler] ', '']","[None, None, None, ' None', ' None', None, ' None', ' None', ' validationif target_transform is not None:']",53,"['        """"""The Pytorch Lightning Datamodule for Tabular Data\n', '\n', '        Args:\n', '            train (pd.DataFrame): The Training Dataframe\n', '            config (DictConfig): Merged configuration object from ModelConfig, DataConfig,\n', '            TrainerConfig, OptimizerConfig & ExperimentConfig\n', '            validation (pd.DataFrame, optional): Validation Dataframe.\n', '            If left empty, we use the validation split from DataConfig to split a random sample as validation.\n', '            Defaults to None.\n', '            test (pd.DataFrame, optional): Holdout DataFrame to check final performance on.\n', '            Defaults to None.\n', '            target_transform (Optional[Union[TransformerMixin, Tuple(Callable)]], optional): If provided, applies the transform to the target before modelling\n', '            and inverse the transform during prediction. The parameter can either be a sklearn Transformer which has an inverse_transform method, or\n', '            a tuple of callables (transform_func, inverse_transform_func)\n', '        """"""\n']","['isinstance', 'FunctionTransformer']",2
tabular_datamodule.py:TabularDatamodule:update_config,TabularDatamodule:update_config,method,8,42,34,522,12.43,1,3,['self'],[None],[None],96,"['        """"""Calculates and updates a few key information to the config object\n', '\n', '        Raises:\n', '            NotImplementedError: [description]\n', '        """"""\n']","['len', 'self.do_leave_one_out_encoder', 'int', 'hasattr', 'min']",5
tabular_datamodule.py:TabularDatamodule:do_leave_one_out_encoder,TabularDatamodule:do_leave_one_out_encoder,method,2,8,8,83,10.38,0,0,['self'],[None],[None],117,"['        """"""Checks the special condition for NODE where we use a LeaveOneOutEncoder to encode categorical columns\n', '\n', '        Returns:\n', '            bool\n', '        """"""\n']",[],0
tabular_datamodule.py:TabularDatamodule:preprocess_data,TabularDatamodule:preprocess_data,method,51,224,129,2853,12.74,2,16,"['self', 'data', 'stage']","[None, ' pd.DataFrame', ' str ']","[None, None, ' ""inference""']",127,"['        """"""The preprocessing, like Categorical Encoding, Normalization, etc. which any dataframe should undergo before feeding into the dataloder\n', '\n', '        Args:\n', '            data (pd.DataFrame): A dataframe with the features and target\n', '            stage (str, optional): Internal parameter. Used to distinguisj between fit and inference. Defaults to ""inference"".\n', '\n', '        Returns:\n', '            tuple[pd.DataFrame, list]: Returns the processed dataframe and the added features(list) as a tuple\n', '        """"""\n']","['logger.info', 'self.make_date', 'self.add_datepart', 'logger.debug', 'len', 'self.do_leave_one_out_encoder', 'ce.LeaveOneOutEncoder', 'logger.warning', 'target', 'OrdinalEncoder', 'StandardScaler', 'LabelEncoder', 'all', 'copy.deepcopy', '_target_transform.fit_transform', 'target_transforms.append']",16
tabular_datamodule.py:TabularDatamodule:setup,TabularDatamodule:setup,method,17,59,45,678,11.49,0,3,"['self', 'stage']","[None, ' Optional[str] ']","[None, ' None']",245,"['        """"""Data Operations you want to perform on all GPUs, like train-test split, transformations, etc.\n', '        This is called before accessing the dataloaders\n', '\n', '        Args:\n', '            stage (Optional[str], optional): Internal parameter to distinguish between fit and inference. Defaults to None.\n', '        """"""\n']","['logger.debug', 'int', 'len', 'self.preprocess_data', 'self.update_config']",5
tabular_datamodule.py:TabularDatamodule:time_features_from_frequency_str,TabularDatamodule:time_features_from_frequency_str,method,45,138,71,1434,10.39,1,1,"['cls', 'freq_str']","[None, ' str']","[None, None]",277,"['        """"""\n', '        Returns a list of time features that will be appropriate for the given frequency string.\n', '\n', '        Parameters\n', '        ----------\n', '\n', '        freq_str\n', '            Frequency string of the form [multiple][granularity] such as ""12H"", ""5min"", ""1D"" etc.\n', '\n', '        """"""\n']","['to_offset', 'features_by_offsets.items', 'isinstance']",3
tabular_datamodule.py:TabularDatamodule:val_dataloader,TabularDatamodule:val_dataloader,method,5,17,17,332,19.53,0,0,['self'],[None],[None],470,"['        """""" Function that loads the validation set. """"""\n']","['TabularDataset', 'self.do_leave_one_out_encoder', 'DataLoader']",3
tabular_datamodule.py:TabularDatamodule:test_dataloader,TabularDatamodule:test_dataloader,method,6,22,22,352,16.0,0,1,['self'],[None],[None],484,"['        """""" Function that loads the validation set. """"""\n']","['TabularDataset', 'self.do_leave_one_out_encoder', 'DataLoader']",3
tabular_datamodule.py:TabularDatamodule:prepare_inference_dataloader,TabularDatamodule:prepare_inference_dataloader,method,17,52,46,658,12.65,0,3,"['self', 'df']","[None, ' pd.DataFrame']","[None, None]",502,"['        """"""Function that prepares and loads the new data.\n', '\n', '        Args:\n', '            df (pd.DataFrame): Dataframe with the features and target\n', '\n', '        Returns:\n', '            DataLoader: The dataloader for the passed in dataframe\n', '        """"""\n']","['df.copy', 'len', 'set', 'np.array', 'np.zeros', 'self.preprocess_data', 'TabularDataset', 'self.do_leave_one_out_encoder', 'all', 'DataLoader']",10
tabular_datamodule.py:TabularDataset:__init__,TabularDataset:__init__,method,21,54,35,710,13.15,0,8,"['self', 'data', 'task', 'continuous_cols', 'categorical_cols', 'embed_categorical', 'target', '']","[None, ' pd.DataFrame', ' str', ' List[str] ', ' List[str] ', ' bool ', ' List[str] ', None]","[None, None, None, ' None', ' None', ' True', ' None', None]",53,"['        """"""The Pytorch Lightning Datamodule for Tabular Data\n', '\n', '        Args:\n', '            train (pd.DataFrame): The Training Dataframe\n', '            config (DictConfig): Merged configuration object from ModelConfig, DataConfig,\n', '            TrainerConfig, OptimizerConfig & ExperimentConfig\n', '            validation (pd.DataFrame, optional): Validation Dataframe.\n', '            If left empty, we use the validation split from DataConfig to split a random sample as validation.\n', '            Defaults to None.\n', '            test (pd.DataFrame, optional): Holdout DataFrame to check final performance on.\n', '            Defaults to None.\n', '            target_transform (Optional[Union[TransformerMixin, Tuple(Callable)]], optional): If provided, applies the transform to the target before modelling\n', '            and inverse the transform during prediction. The parameter can either be a sklearn Transformer which has an inverse_transform method, or\n', '            a tuple of callables (transform_func, inverse_transform_func)\n', '        """"""\n']","['isinstance', 'np.zeros']",2
tabular_datamodule.py:TabularDataset:__len__,TabularDataset:__len__,method,2,2,2,12,6.0,0,0,['self'],[None],[None],587,"['        """"""\n', '        Denotes the total number of samples.\n', '        """"""\n']",[],0
tabular_datamodule.py:TabularDataset:__getitem__,TabularDataset:__getitem__,method,1,17,14,164,9.65,0,0,"['self', 'idx']","[None, None]","[None, None]",593,"['        """"""\n', '        Generates one sample of data.\n', '        """"""\n']",[],0
pytorch_tabular/tabular_model.py:TabularModel,TabularModel,class,245,1417,751,16933,11.95,13,62,[],[],[],39,[],[],0
tabular_model.py:TabularModel:__init__,TabularModel:__init__,method,27,139,81,1781,12.81,0,5,"['self', 'config', 'data_config', 'str]] ', 'model_config', 'str]] ', 'optimizer_config', 'str]] ', 'trainer_config', 'str]] ', 'experiment_config', 'str]] ', 'model_callable', '']","[None, ' Optional[DictConfig] ', ' Optional[Union[DataConfig', None, ' Optional[Union[ModelConfig', None, ' Optional[Union[OptimizerConfig', None, ' Optional[Union[TrainerConfig', None, ' Optional[Union[ExperimentConfig', None, ' Optional[Callable] ', None]","[None, ' None', None, ' None', None, ' None', None, ' None', None, ' None', None, ' None', ' None', None]",40,"['        """"""The core model which orchestrates everything from initializing the datamodule, the model, trainer, etc.\n', '\n', '        Args:\n', '            config (Optional[Union[DictConfig, str]], optional): Single OmegaConf DictConfig object or\n', '                the path to the yaml file holding all the config parameters. Defaults to None.\n', '\n', '            data_config (Optional[Union[DataConfig, str]], optional): DataConfig object or path to the yaml file. Defaults to None.\n', '\n', '            model_config (Optional[Union[ModelConfig, str]], optional): A subclass of ModelConfig or path to the yaml file.\n', '                Determines which model to run from the type of config. Defaults to None.\n', '\n', '            optimizer_config (Optional[Union[OptimizerConfig, str]], optional): OptimizerConfig object or path to the yaml file.\n', '                Defaults to None.\n', '\n', '            trainer_config (Optional[Union[TrainerConfig, str]], optional): TrainerConfig object or path to the yaml file.\n', '                Defaults to None.\n', '\n', '            experiment_config (Optional[Union[ExperimentConfig, str]], optional): ExperimentConfig object or path to the yaml file.\n', '                If Provided configures the experiment tracking. Defaults to None.\n', '\n', '            model_callable (Optional[Callable], optional): If provided, will override the model callable that will be loaded from the config.\n', '                Typically used when providing Custom Models\n', '        """"""\n']","['super', 'ExperimentRunManager', 'self._read_parse_config', 'logger.info', 'OmegaConf.merge', 'OmegaConf.to_container', 'hasattr', 'self._get_run_name_uid', 'self._setup_experiment_tracking', 'getattr', 'self._run_validation']",11
tabular_model.py:TabularModel:_run_validation,TabularModel:_run_validation,method,6,73,51,539,7.38,0,5,['self'],[None],[None],135,"['        """"""Validates the Config params and throws errors if something is wrong\n', '\n', '        Raises:\n', '            NotImplementedError: If you provide a multi-target config to a classification task\n', '            ValueError: If there is a problem with Target Range\n', '        """"""\n']","['len', 'NotImplementedError', 'any', 'ValueError', 'two']",5
tabular_model.py:TabularModel:_read_parse_config,TabularModel:_read_parse_config,method,12,46,38,398,8.65,0,4,"['self', 'config', 'cls']","[None, None, None]","[None, None, None]",160,[],"['isinstance', 'OmegaConf.load', 'getattr', 'cls', '_config.items', 'ValueError', 'OmegaConf.structured']",7
tabular_model.py:TabularModel:_get_run_name_uid,TabularModel:_get_run_name_uid,method,9,28,20,297,10.61,0,1,['self'],[None],[None],181,"['        """"""Gets the name of the experiment and increments version by 1\n', '\n', '        Returns:\n', '            tuple[str, int]: Returns the name and version number\n', '        """"""\n']",['hasattr'],1
tabular_model.py:TabularModel:_setup_experiment_tracking,TabularModel:_setup_experiment_tracking,method,5,31,28,412,13.29,0,1,['self'],[None],[None],196,"['        """"""Sets up the Experiment Tracking Framework according to the choices made in the Experimentconfig\n', '\n', '        Raises:\n', '            NotImplementedError: Raises an Error for invalid choices of log_target\n', '        """"""\n']",['NotImplementedError'],1
tabular_model.py:TabularModel:_prepare_callbacks,TabularModel:_prepare_callbacks,method,13,44,39,861,19.57,0,2,['self'],[None],[None],217,"['        """"""Prepares the necesary callbacks to the Trainer based on the configuration\n', '\n', '        Returns:\n', '            List: A list of callbacks\n', '        """"""\n']","['callbacks.append', 'ckpt_name.replace', 'logger.debug']",3
tabular_model.py:TabularModel:_prepare_dataloader,TabularModel:_prepare_dataloader,method,14,44,40,548,12.45,0,1,"['self', 'train', 'validation', 'test', 'target_transform', 'train_sampler']","[None, None, None, None, None, None]","[None, None, None, None, 'None', 'None']",250,[],"['logger.info', 'hasattr', 'logger.debug', 'TabularDatamodule']",4
tabular_model.py:TabularModel:_prepare_model,TabularModel:_prepare_model,method,8,38,33,480,12.63,0,1,"['self', 'loss', 'metrics', 'optimizer', 'optimizer_params', 'reset']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",275,[],"['logger.info', 'hasattr', 'logger.debug', 'self.model_callable']",4
tabular_model.py:TabularModel:_prepare_trainer,TabularModel:_prepare_trainer,method,18,49,39,508,10.37,2,2,"['self', 'max_epochs', 'min_epochs']","[None, None, None]","[None, 'None', 'None']",294,[],"['logger.info', 'inspect.signature', 'pl.Trainer']",3
tabular_model.py:TabularModel:load_best_model,TabularModel:load_best_model,method,8,38,37,376,9.89,0,1,['self'],[None],[None],314,"['        """"""Loads the best model after training is done""""""\n']","['logger.info', 'logger.debug', 'pl_load']",3
tabular_model.py:TabularModel:_pre_fit,TabularModel:_pre_fit,method,17,102,79,963,9.44,0,4,"['self', 'train', 'validation', 'test', 'loss', 'metrics', 'optimizer', 'optimizer_params', 'train_sampler', 'target_transform', 'Tuple]]', 'max_epochs', 'min_epochs', 'reset']","[None, ' pd.DataFrame', ' Optional[pd.DataFrame]', ' Optional[pd.DataFrame]', ' Optional[torch.nn.Module]', ' Optional[List[Callable]]', ' Optional[torch.optim.Optimizer]', ' Dict', ' Optional[torch.utils.data.Sampler]', ' Optional[Union[TransformerMixin', None, ' int', ' int', ' bool']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None]",327,"['        """"""Prepares the dataloaders, trainer, and model for the fit process""""""\n']","['isinstance', 'len', 'ValueError', 'logger.warning', 'self._prepare_dataloader', 'self._prepare_model', 'self._prepare_callbacks', 'self._prepare_trainer']",8
tabular_model.py:TabularModel:fit,TabularModel:fit,method,13,45,39,581,12.91,0,2,"['self', 'train', 'validation', 'test', 'loss', 'metrics', 'optimizer', 'optimizer_params', 'train_sampler', 'target_transform', 'Tuple]] ', 'max_epochs', 'min_epochs', 'reset', 'seed', '']","[None, ' pd.DataFrame', ' Optional[pd.DataFrame] ', ' Optional[pd.DataFrame] ', ' Optional[torch.nn.Module] ', ' Optional[List[Callable]] ', ' Optional[torch.optim.Optimizer] ', ' Dict ', ' Optional[torch.utils.data.Sampler] ', ' Optional[Union[TransformerMixin', None, ' Optional[int] ', ' Optional[int] ', ' bool ', ' Optional[int] ', None]","[None, None, ' None', ' None', ' None', ' None', ' None', ' {}', ' None', None, ' None', ' None', ' None', ' False', ' None', None]",372,"['        """"""The fit method which takes in the data and triggers the training\n', '\n', '        Args:\n', '            train (pd.DataFrame): Training Dataframe\n', '\n', '            valid (Optional[pd.DataFrame], optional): If provided, will use this dataframe as the validation while training.\n', '                Used in Early Stopping and Logging. If left empty, will use 20% of Train data as validation. Defaults to None.\n', '\n', '            test (Optional[pd.DataFrame], optional): If provided, will use as the hold-out data,\n', ""                which you'll be able to check performance after the model is trained. Defaults to None.\n"", '\n', '            loss (Optional[torch.nn.Module], optional): Custom Loss functions which are not in standard pytorch library\n', '\n', '            metrics (Optional[List[Callable]], optional): Custom metric functions(Callable) which has the\n', '                signature metric_fn(y_hat, y) and works on torch tensor inputs\n', '\n', '            optimizer (Optional[torch.optim.Optimizer], optional): Custom optimizers which are a drop in replacements for standard PyToch optimizers.\n', '                This should be the Class and not the initialized object\n', '\n', '            optimizer_params (Optional[Dict], optional): The parmeters to initialize the custom optimizer.\n', '\n', '            train_sampler (Optional[torch.utils.data.Sampler], optional): Custom PyTorch batch samplers which will be passed to the DataLoaders. Useful for dealing with imbalanced data and other custom batching strategies\n', '\n', '            target_transform (Optional[Union[TransformerMixin, Tuple(Callable)]], optional): If provided, applies the transform to the target before modelling\n', '                and inverse the transform during prediction. The parameter can either be a sklearn Transformer which has an inverse_transform method, or\n', '                a tuple of callables (transform_func, inverse_transform_func)\n', '\n', '            max_epochs (Optional[int]): Overwrite maximum number of epochs to be run\n', '\n', '            min_epochs (Optional[int]): Overwrite minimum number of epochs to be run\n', '\n', '            reset: (bool): Flag to reset the model and train again from scratch\n', '\n', '            seed: (int): If you have to override the default seed set as part of of ModelConfig\n', '        """"""\n']","['seed_everything', 'self._pre_fit', 'logger.info', 'self.load_best_model']",4
tabular_model.py:TabularModel:find_learning_rate,TabularModel:find_learning_rate,method,17,42,39,496,11.81,0,1,"['self', 'train', 'validation', 'test', 'loss', 'metrics', 'optimizer', 'optimizer_params', 'min_lr', 'max_lr', 'num_training', 'mode', 'early_stop_threshold', 'plot', '']","[None, ' pd.DataFrame', ' Optional[pd.DataFrame] ', ' Optional[pd.DataFrame] ', ' Optional[torch.nn.Module] ', ' Optional[List[Callable]] ', ' Optional[torch.optim.Optimizer] ', ' Dict ', ' float ', ' float ', ' int ', ' str ', ' float ', None, None]","[None, None, ' None', ' None', ' None', ' None', ' None', ' {}', ' 1e-8', ' 1', ' 100', ' ""exponential""', ' 4.0', 'True', None]",449,"['        """"""Enables the user to do a range test of good initial learning rates, to reduce the amount of guesswork in picking a good starting learning rate.\n', '\n', '        Args:\n', '            train (pd.DataFrame): Training Dataframe\n', '\n', '            valid (Optional[pd.DataFrame], optional): If provided, will use this dataframe as the validation while training.\n', '                Used in Early Stopping and Logging. If left empty, will use 20% of Train data as validation. Defaults to None.\n', '\n', '            test (Optional[pd.DataFrame], optional): If provided, will use as the hold-out data,\n', ""                which you'll be able to check performance after the model is trained. Defaults to None.\n"", '\n', '            loss (Optional[torch.nn.Module], optional): Custom Loss functions which are not in standard pytorch library\n', '\n', '            metrics (Optional[List[Callable]], optional): Custom metric functions(Callable) which has the signature metric_fn(y_hat, y)\n', '\n', '            optimizer (Optional[torch.optim.Optimizer], optional): Custom optimizers which are a drop in replacements for standard PyToch optimizers.\n', '                This should be the Class and not the initialized object\n', '\n', '            optimizer_params (Optional[Dict], optional): The parmeters to initialize the custom optimizer.\n', '\n', '            min_lr (Optional[float], optional): minimum learning rate to investigate\n', '\n', '            max_lr (Optional[float], optional): maximum learning rate to investigate\n', '\n', '            num_training (Optional[int], optional): number of learning rates to test\n', '\n', ""            mode (Optional[str], optional): search strategy, either 'linear' or 'exponential'. If set to\n"", ""                'linear' the learning rate will be searched by linearly increasing\n"", ""                after each batch. If set to 'exponential', will increase learning\n"", '                rate exponentially.\n', '\n', '            early_stop_threshold(Optional[float], optional): threshold for stopping the search. If the\n', '                loss at any point is larger than early_stop_threshold*best_loss\n', '                then the search is stopped. To disable, set to None.\n', '\n', '            plot(bool, optional): If true, will plot using matplotlib\n', '        """"""\n']","['self._pre_fit', 'lr_finder.plot', 'fig.show', 'lr_finder.suggestion', 'pd.DataFrame']",5
tabular_model.py:TabularModel:evaluate,TabularModel:evaluate,method,8,28,21,286,10.21,0,1,"['self', 'test']","[None, ' Optional[pd.DataFrame]']","[None, None]",536,"['        """"""Evaluates the dataframe using the loss and metrics already set in config\n', '\n', '        Args:\n', '            test (Optional[pd.DataFrame]): The dataframe to be evaluated. If not provided, will try to use the\n', '                test provided during fit. If that was also not provided will return an empty dictionary\n', '\n', '        Returns:\n', '            Union[dict, list]: The final test result dictionary.\n', '        """"""\n']",[],0
tabular_model.py:TabularModel:predict,TabularModel:predict,method,66,240,134,2901,12.09,10,16,"['self', 'test', 'quantiles', '0.5', '0.75]', 'n_samples', 'ret_logits', '']","[None, ' pd.DataFrame', ' Optional[List] ', None, None, ' Optional[int] ', None, None]","[None, None, ' [0.25', None, None, ' 100', 'False', None]",558,"['        """"""Uses the trained model to predict on new data and return as a dataframe\n', '\n', '        Args:\n', '            test (pd.DataFrame): The new dataframe with the features defined during training\n', '            quantiles (Optional[List]): For probabilistic models like Mixture Density Networks, this specifies\n', '                the different quantiles to be extracted apart from the `central_tendency` and added to the dataframe.\n', '                For other models it is ignored. Defaults to [0.25, 0.5, 0.75]\n', '            n_samples (Optional[int]): Number of samples to draw from the posterior to estimate the quantiles.\n', '                Ignored for non-probabilistic models. Defaults to 100\n', '            ret_logits (bool): Flag to return raw model outputs/logits except the backbone features along\n', '                with the dataframe. Defaults to False\n', '\n', '        Returns:\n', '            pd.DataFrame: Returns a dataframe with predictions and features.\n', '                If classification, it returns probabilities and final prediction\n', '        """"""\n']","['all', 'defaultdict', 'hasattr', 'tqdm', 'batch.items', 'isinstance', 'v.to', 'torch.mean', 'quantile_preds.append', 'torch.quantile', 'ret_value.items', 'point_predictions.append', 'quantile_predictions.append', 'torch.cat', 'point_predictions.unsqueeze', 'quantile_predictions.unsqueeze', 'test.copy', 'point_predictions.numpy', 'quantile_predictions.numpy', 'enumerate', 'nn.Softmax', 'np.argmax', 'logits_predictions.items', 'v.reshape', 'range']",25
tabular_model.py:TabularModel:save_model,TabularModel:save_model,method,16,70,60,1062,15.17,1,4,"['self', 'dir']","[None, ' str']","[None, None]",681,"['        """"""Saves the model and checkpoints in the specified directory\n', '\n', '        Args:\n', '            dir (str): The path to the directory to save the model\n', '        """"""\n']","['logger.warning', 'os.listdir', 'os.remove', 'os.makedirs', 'open', 'OmegaConf.save', 'joblib.dump', 'hasattr']",8
tabular_model.py:TabularModel:load_from_checkpoint,TabularModel:load_from_checkpoint,method,38,140,85,2040,14.57,0,11,"['cls', 'dir']","[None, ' str']","[None, None]",712,"['        """"""Loads a saved model from the directory\n', '\n', '        Args:\n', '            dir (str): The directory where the model wa saved, along with the checkpoints\n', '\n', '        Returns:\n', '            TabularModel: The saved TabularModel\n', '        """"""\n']","['OmegaConf.load', 'joblib.load', 'hasattr', 'getattr', 'custom_params.get', 'model_callable.load_from_checkpoint', 'model._setup_loss', 'model._setup_metrics', 'cls', 'tabular_model._prepare_trainer']",10
pytorch_tabular/utils.py:_make_smooth_weights_for_balanced_classes,_make_smooth_weights_for_balanced_classes,function,15,34,28,275,8.09,2,0,"['y_train', 'mu']","[None, None]","[None, '0.15']",14,[],"['zip', 'np.bincount', 'np.sum', 'sorted', 'np.log', 'float', 'weight.append']",7
pytorch_tabular/utils.py:get_class_weighted_cross_entropy,get_class_weighted_cross_entropy,function,8,19,18,271,14.26,0,0,"['y_train', 'mu']","[None, None]","[None, '0.15']",27,[],"['LabelEncoder', '_make_smooth_weights_for_balanced_classes']",2
pytorch_tabular/utils.py:get_balanced_sampler,get_balanced_sampler,function,13,30,28,395,13.17,1,0,['y_train'],[None],[None],35,[],"['LabelEncoder', 'np.bincount', 'torch.Tensor', 'len']",4
pytorch_tabular/utils.py:_initialize_layers,_initialize_layers,function,7,53,35,625,11.79,0,4,"['hparams', 'layer']","[None, None]","[None, None]",50,[],['logger.warning'],1
pytorch_tabular/utils.py:_linear_dropout_bn,_linear_dropout_bn,function,9,18,16,259,14.39,0,2,"['hparams', 'in_units', 'out_units', 'activation', 'dropout']","[None, None, None, None, None]","[None, None, None, None, None]",77,[],"['layers.append', 'nn.Linear', '_initialize_layers', 'layers.extend', 'activation']",5
pytorch_tabular/utils.py:get_gaussian_centers,get_gaussian_centers,function,9,19,16,207,10.89,0,2,"['y', 'n_components']","[None, None]","[None, None]",89,[],"['isinstance', 'y.reshape', 'KMeans']",3
tests/conftest.py:load_regression_data,load_regression_data,function,14,23,22,369,16.04,0,0,[],[],[],11,[],"['fetch_california_housing', 'pd.qcut', 'df.sample', 'len']",4
tests/conftest.py:load_classification_data,load_classification_data,function,19,36,34,526,14.61,1,0,[],[],[],22,[],"['fetch_covtype', 'np.hstack', 'range', 'pd.DataFrame', 'pd.qcut', 'data.sample', 'len']",7
tests/conftest.py:load_timeseries_data,load_timeseries_data,function,10,21,19,375,17.86,0,0,[],[],[],36,[],"['urlopen', 'ZipFile', 'pd.read_csv']",3
tests/conftest.py:regression_data,regression_data,function,2,2,2,28,14.0,0,0,[],[],[],47,[],['load_regression_data'],1
tests/conftest.py:classification_data,classification_data,function,2,2,2,32,16.0,0,0,[],[],[],52,[],['load_classification_data'],1
tests/conftest.py:timeseries_data,timeseries_data,function,2,2,2,28,14.0,0,0,[],[],[],57,[],['load_timeseries_data'],1
tests/test_autoint.py:test_regression,test_regression,function,26,80,71,1336,16.7,1,2,"['regression_data', 'multi_target', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', 'target_range', 'deep_layers', 'batch_norm_continuous_input', 'attention_pooling']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None]",33,[],"['len', 'DataConfig', 'dict', '_target_range.append', 'AutoIntConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",11
tests/test_autoint.py:test_classification,test_classification,function,20,57,53,1042,18.28,0,1,"['classification_data', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', 'deep_layers', 'batch_norm_continuous_input']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",102,[],"['len', 'DataConfig', 'dict', 'AutoIntConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",10
tests/test_categorical_embedding.py:fake_metric,fake_metric,function,1,3,3,22,7.33,0,0,"['y_hat', 'y']","[None, None]","[None, None]",16,[],[],0
tests/test_categorical_embedding.py:test_regression,test_regression,function,27,96,78,1401,14.59,1,3,"['regression_data', 'multi_target', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', 'target_range', 'target_transform', 'custom_metrics', 'custom_loss', 'custom_optimizer', '']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None]",46,[],"['len', 'DataConfig', 'dict', '_target_range.append', 'CategoryEmbeddingModelConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",11
tests/test_categorical_embedding.py:test_classification,test_classification,function,19,53,49,931,17.57,0,1,"['classification_data', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', '']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",123,[],"['len', 'DataConfig', 'dict', 'CategoryEmbeddingModelConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",10
tests/test_categorical_embedding.py:test_embedding_transformer,test_embedding_transformer,function,21,73,63,1032,14.14,1,0,['regression_data'],[None],[None],163,[],"['DataConfig', 'dict', 'CategoryEmbeddingModelConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'CategoricalEmbeddingTransformer', 'transformer.fit_transform', 'len', 'all']",11
tests/test_common.py:fake_metric,fake_metric,function,1,3,3,22,7.33,0,0,"['y_hat', 'y']","[None, None]","[None, None]",30,[],[],0
tests/test_common.py:test_save_load,test_save_load,function,23,61,58,1087,17.82,0,0,"['regression_data', 'model_config_class', 'continuous_cols', 'categorical_cols', 'custom_metrics', 'custom_loss', 'custom_optimizer', 'tmpdir', '']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None]",55,[],"['DataConfig', 'model_config_class', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tmpdir.mkdir', 'tabular_model.save_model', 'TabularModel.load_from_checkpoint', 'new_mdl.evaluate']",11
tests/test_common.py:test_feature_extractor,test_feature_extractor,function,17,49,45,706,14.41,0,0,"['regression_data', 'model_config_class', 'continuous_cols', 'categorical_cols', '']","[None, None, None, None, None]","[None, None, None, None, None]",125,[],"['DataConfig', 'dict', 'model_config_class', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'DeepFeatureExtractor', 'dt.fit_transform', 'any']",10
tests/test_datamodule.py:test_dataloader,test_dataloader,function,36,105,84,1703,16.22,0,4,"['regression_data', 'validation_split', 'multi_target', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', 'target_transform', 'embedding_dims', '']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None]",43,[],"['train_test_split', 'len', 'DataConfig', 'dict', 'CategoryEmbeddingModelConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'TabularDatamodule', 'datamodule.prepare_data', 'datamodule.setup', 'round', 'datamodule.val_dataloader', 'datamodule.prepare_inference_dataloader', 'next', 'np.not_equal']",16
tests/test_datamodule.py:test_date_encoding,test_date_encoding,function,24,87,67,1127,12.95,0,2,"['timeseries_data', 'freq']","[None, None]","[None, None]",113,[],"['train_test_split', 'DataConfig', 'dict', 'CategoryEmbeddingModelConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'TabularDatamodule', 'datamodule.prepare_data', 'datamodule.setup']",10
tests/test_mdn.py:test_regression,test_regression,function,22,62,57,1055,17.02,0,1,"['regression_data', 'multi_target', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', 'variant', 'num_gaussian']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",31,[],"['len', 'DataConfig', 'dict', 'MixtureDensityHeadConfig', 'variant', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",11
tests/test_mdn.py:test_classification,test_classification,function,17,48,46,922,19.21,0,1,"['classification_data', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', 'num_gaussian']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",87,[],"['len', 'DataConfig', 'dict', 'MixtureDensityHeadConfig', 'CategoryEmbeddingMDNConfig', 'TrainerConfig', 'OptimizerConfig', 'pytest.raises', 'TabularModel', 'tabular_model.fit']",10
tests/test_node.py:test_regression,test_regression,function,26,79,69,1211,15.33,1,2,"['regression_data', 'multi_target', 'embed_categorical', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', 'target_range', '']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None]",30,[],"['len', 'DataConfig', 'dict', '_target_range.append', 'NodeConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",11
tests/test_node.py:test_classification,test_classification,function,19,58,53,976,16.83,0,1,"['classification_data', 'continuous_cols', 'categorical_cols', 'embed_categorical', 'continuous_feature_transform', 'normalize_continuous_features', '']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",97,[],"['len', 'DataConfig', 'dict', 'NodeConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",10
tests/test_node.py:test_embedding_transformer,test_embedding_transformer,function,21,78,67,1060,13.59,1,0,['regression_data'],[None],[None],142,[],"['DataConfig', 'dict', 'NodeConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'CategoricalEmbeddingTransformer', 'transformer.fit_transform', 'len', 'all']",11
tests/test_tabnet.py:test_regression,test_regression,function,26,72,64,1153,16.01,1,2,"['regression_data', 'multi_target', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', 'target_range']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",29,[],"['len', 'DataConfig', 'dict', '_target_range.append', 'TabNetModelConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",11
tests/test_tabnet.py:test_classification,test_classification,function,19,51,48,918,18.0,0,1,"['classification_data', 'continuous_cols', 'categorical_cols', 'continuous_feature_transform', 'normalize_continuous_features', '']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",87,[],"['len', 'DataConfig', 'dict', 'TabNetModelConfig', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'tabular_model.predict']",10
pytorch_tabular/config/config.py:_read_yaml,_read_yaml,function,27,307,194,2280,7.43,0,0,['filename'],[None],[None],13,[],"['loader.add_implicit_resolver', 're.compile', 'field', 'column', 'features', '__post_init__', 'len', '_validate_choices']",8
pytorch_tabular/config/config.py:TrainerConfig,TrainerConfig,class,32,586,300,3779,6.45,0,0,[],[],[],162,[],[],0
pytorch_tabular/config/config.py:ExperimentConfig,ExperimentConfig,class,16,178,116,1164,6.54,0,1,[],[],[],353,[],[],0
pytorch_tabular/config/config.py:OptimizerConfig,OptimizerConfig,class,15,122,78,1154,9.46,0,1,[],[],[],425,[],[],0
pytorch_tabular/config/config.py:ExperimentRunManager,ExperimentRunManager,class,18,52,37,704,13.54,0,2,[],[],[],479,[],[],0
pytorch_tabular/config/config.py:ModelConfig,ModelConfig,class,21,312,174,2181,6.99,0,1,[],[],[],513,[],[],0
config/config.py:TrainerConfig:__post_init__,TrainerConfig:__post_init__,method,1,1,1,23,23.0,0,0,['self'],[None],[None],145,[],['_validate_choices'],1
config/config.py:ExperimentConfig:__post_init__,ExperimentConfig:__post_init__,method,5,28,25,192,6.86,0,1,['self'],[None],[None],145,[],"['_validate_choices', 'ImportError']",2
config/config.py:OptimizerConfig:read_from_yaml,OptimizerConfig:read_from_yaml,method,5,10,9,131,13.1,0,1,['filename'],[' str '],"[' ""config/optimizer_config.yml""']",472,[],"['_read_yaml', 'OptimizerConfig']",2
config/config.py:ExperimentRunManager:__init__,ExperimentRunManager:__init__,method,11,19,18,352,18.53,0,1,"['self', 'exp_version_manager', '']","[None, ' str ', None]","[None, ' "".tmp/exp_version_manager.yml""', None]",480,"['        """"""The manages the versions of the experiments based on the name. It is a simple dictionary(yaml) based lookup.\n', '        Primary purpose is to avoid overwriting of saved models while runing the training without changing the experiment name.\n', '\n', '        Args:\n', '            exp_version_manager (str, optional): The path of the yml file which acts as version control.\n', '            Defaults to "".tmp/exp_version_manager.yml"".\n', '        """"""\n']","['super', 'OmegaConf.load', 'OmegaConf.create', 'os.makedirs', 'open', 'OmegaConf.save']",6
config/config.py:ExperimentRunManager:update_versions,ExperimentRunManager:update_versions,method,8,21,17,236,11.24,0,1,"['self', 'name']","[None, None]","[None, None]",501,[],"['open', 'OmegaConf.save']",2
config/config.py:ModelConfig:__post_init__,ModelConfig:__post_init__,method,6,84,52,685,8.15,0,1,['self'],[None],[None],145,[],"['NotImplementedError', 'len', '_validate_choices']",3
pytorch_tabular/models/base_model.py:BaseModel,BaseModel,class,104,554,290,5885,10.62,7,14,[],[],[],27,[],[],0
models/base_model.py:BaseModel:__init__,BaseModel:__init__,method,19,51,31,663,13.0,0,4,"['self', 'config', 'custom_loss', 'custom_metrics', 'custom_optimizer', 'custom_optimizer_params', '**kwargs']","[None, ' DictConfig', ' Optional[torch.nn.Module] ', ' Optional[List[Callable]] ', ' Optional[torch.optim.Optimizer] ', ' Dict ', None]","[None, None, ' None', ' None', ' None', ' {}', None]",28,[],"['super', 'str', 'self.save_hyperparameters', 'self._build_network', 'self._setup_loss', 'self._setup_metrics']",6
models/base_model.py:BaseModel:_build_network,BaseModel:_build_network,method,0,1,1,4,4.0,0,0,['self'],[None],[None],59,[],[],0
models/base_model.py:BaseModel:_setup_loss,BaseModel:_setup_loss,method,5,30,27,217,7.23,0,1,['self'],[None],[None],62,[],"['getattr', 'logger.error']",2
models/base_model.py:BaseModel:_setup_metrics,BaseModel:_setup_metrics,method,11,44,38,393,8.93,2,1,['self'],[None],[None],74,[],['logger.error'],1
models/base_model.py:BaseModel:calculate_loss,BaseModel:calculate_loss,method,13,46,41,511,11.11,1,1,"['self', 'y', 'y_hat', 'tag']","[None, None, None, None]","[None, None, None, None]",90,[],"['range', 'self.loss', 'losses.append', 'self.log', 'torch.stack', 'y.squeeze']",6
models/base_model.py:BaseModel:calculate_metrics,BaseModel:calculate_metrics,method,19,77,57,887,11.52,2,2,"['self', 'y', 'y_hat', 'tag']","[None, None, None, None]","[None, None, None, None]",119,[],"['zip', 'range', 'metric', 'torch.clamp', 'self.log', '_metrics.append', 'torch.stack', 'nn.Softmax', 'y.squeeze', 'metrics.append']",10
models/base_model.py:BaseModel:data_aware_initialization,BaseModel:data_aware_initialization,method,0,1,1,4,4.0,0,0,"['self', 'datamodule']","[None, None]","[None, None]",163,[],[],0
models/base_model.py:BaseModel:forward,BaseModel:forward,method,0,1,1,4,4.0,0,0,"['self', 'x']","[None, ' Dict']","[None, None]",167,[],[],0
models/base_model.py:BaseModel:predict,BaseModel:predict,method,5,10,9,121,12.1,0,1,"['self', 'x', 'ret_model_output']","[None, ' Dict', ' bool ']","[None, None, ' False']",170,[],"['self.forward', 'ret_value.get']",2
models/base_model.py:BaseModel:training_step,BaseModel:training_step,method,8,14,11,148,10.57,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",177,[],"['self', 'self.calculate_loss', 'self.calculate_metrics']",3
models/base_model.py:BaseModel:validation_step,BaseModel:validation_step,method,8,15,10,148,9.87,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",184,[],"['self', 'self.calculate_loss', 'self.calculate_metrics']",3
models/base_model.py:BaseModel:test_step,BaseModel:test_step,method,8,15,10,146,9.73,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",191,[],"['self', 'self.calculate_loss', 'self.calculate_metrics']",3
models/base_model.py:BaseModel:configure_optimizers,BaseModel:configure_optimizers,method,11,103,58,1105,10.73,0,3,['self'],[None],[None],198,[],"['getattr', 'self._optimizer', 'self.parameters', 'logger.error', 'isinstance', 'self._lr_scheduler']",6
models/base_model.py:BaseModel:create_plotly_histogram,BaseModel:create_plotly_histogram,method,8,37,32,360,9.73,1,0,"['self', 'arr', 'name', 'bin_dict']","[None, None, None, None]","[None, None, None, 'None']",250,[],"['go.Figure', 'range', 'fig.add_trace', 'go.Histogram', 'dict', 'fig.update_layout', 'legend=dict', 'fig.update_traces']",8
models/base_model.py:BaseModel:validation_epoch_end,BaseModel:validation_epoch_end,method,12,34,31,375,11.03,1,1,"['self', 'outputs']","[None, None]","[None, None]",272,[],"['hasattr', 'torch.cat', 'self.create_plotly_histogram', 'wandb.log']",4
pytorch_tabular/models/autoint/autoint.py:AutoIntBackbone,AutoIntBackbone,class,67,215,133,3076,14.31,5,16,[],[],[],21,[],[],0
pytorch_tabular/models/autoint/autoint.py:AutoIntModel,AutoIntModel,class,24,65,62,713,10.97,1,1,[],[],[],141,[],[],0
models/autoint/autoint.py:AutoIntBackbone:__init__,AutoIntBackbone:__init__,method,3,3,3,74,24.67,0,0,"['self', 'config']","[None, ' DictConfig']","[None, None]",22,[],"['super', 'self.save_hyperparameters', 'self._build_network']",3
models/autoint/autoint.py:AutoIntBackbone:_build_network,AutoIntBackbone:_build_network,method,38,97,71,1599,16.48,3,6,['self'],[None],[None],29,[],"['len', 'nn.ModuleList', 'nn.Embedding', 'nn.BatchNorm1d', 'nn.Dropout', 'getattr', 'layers.extend', '_linear_dropout_bn', 'int', 'nn.Sequential', 'nn.Linear', '_initialize_layers', 'nn.MultiheadAttention', 'range']",14
models/autoint/autoint.py:AutoIntBackbone:forward,AutoIntBackbone:forward,method,37,105,70,1314,12.51,2,10,"['self', 'x']","[None, ' Dict']","[None, None]",91,[],"['len', 'embedding_layer', 'enumerate', 'torch.cat', 'torch.arange', 'self.normalizing_batch_norm', 'torch.mul', 'continuous_data.unsqueeze', 'self.cont_embedding_layer', 'self.embed_dropout', 'self.linear_layers', 'self.attn_proj', 'self_attn', 'attention_ops.append', 'cross_term.transpose', 'self.V_res_embedding', 'nn.ReLU']",17
models/autoint/autoint.py:AutoIntModel:__init__,AutoIntModel:__init__,method,1,2,2,33,16.5,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",142,[],['super'],1
models/autoint/autoint.py:AutoIntModel:_build_network,AutoIntModel:_build_network,method,7,17,17,261,15.35,0,0,['self'],[None],[None],29,[],"['AutoIntBackbone', 'nn.Dropout', 'nn.Linear', '_initialize_layers']",4
models/autoint/autoint.py:AutoIntModel:forward,AutoIntModel:forward,method,16,35,34,321,9.17,1,1,"['self', 'x']","[None, ' Dict']","[None, None]",91,[],"['self.backbone', 'self.dropout', 'self.output_layer', 'range', 'nn.Sigmoid']",5
pytorch_tabular/models/autoint/config.py:AutoIntConfig,AutoIntConfig,class,21,340,139,2344,6.89,0,0,[],[],[],12,[],[],0
pytorch_tabular/models/category_embedding/category_embedding_model.py:FeedForwardBackbone,FeedForwardBackbone,class,26,58,49,748,12.9,1,1,[],[],[],19,[],[],0
pytorch_tabular/models/category_embedding/category_embedding_model.py:CategoryEmbeddingModel,CategoryEmbeddingModel,class,40,129,98,1429,11.08,2,6,[],[],[],52,[],[],0
models/category_embedding/category_embedding_model.py:FeedForwardBackbone:__init__,FeedForwardBackbone:__init__,method,5,10,10,135,13.5,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",20,[],"['sum', 'super', 'self.save_hyperparameters', 'self._build_network']",4
models/category_embedding/category_embedding_model.py:FeedForwardBackbone:_build_network,FeedForwardBackbone:_build_network,method,17,34,30,489,14.38,1,1,['self'],[None],[None],26,[],"['getattr', 'layers.append', 'layers.extend', '_linear_dropout_bn', 'int', 'nn.Sequential']",6
models/category_embedding/category_embedding_model.py:FeedForwardBackbone:forward,FeedForwardBackbone:forward,method,3,4,3,31,7.75,0,0,"['self', 'x']","[None, None]","[None, None]",47,[],['self.linear_layers'],1
models/category_embedding/category_embedding_model.py:CategoryEmbeddingModel:__init__,CategoryEmbeddingModel:__init__,method,3,9,9,94,10.44,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",20,[],"['sum', 'super']",2
models/category_embedding/category_embedding_model.py:CategoryEmbeddingModel:_build_network,CategoryEmbeddingModel:_build_network,method,10,29,29,429,14.79,0,1,['self'],[None],[None],26,[],"['nn.ModuleList', 'nn.BatchNorm1d', 'FeedForwardBackbone', 'nn.Linear', '_initialize_layers']",5
models/category_embedding/category_embedding_model.py:CategoryEmbeddingModel:unpack_input,CategoryEmbeddingModel:unpack_input,method,14,41,29,451,11.0,1,4,"['self', 'x']","[None, ' Dict']","[None, None]",74,[],"['embedding_layer', 'enumerate', 'torch.cat', 'self.normalizing_batch_norm']",4
models/category_embedding/category_embedding_model.py:CategoryEmbeddingModel:forward,CategoryEmbeddingModel:forward,method,16,35,34,326,9.31,1,1,"['self', 'x']","[None, ' Dict']","[None, None]",96,[],"['self.unpack_input', 'self.backbone', 'self.output_layer', 'range', 'nn.Sigmoid']",5
pytorch_tabular/models/category_embedding/config.py:CategoryEmbeddingModelConfig,CategoryEmbeddingModelConfig,class,13,142,89,1202,8.46,0,0,[],[],[],12,[],[],0
pytorch_tabular/models/mixture_density/config.py:MixtureDensityHeadConfig,MixtureDensityHeadConfig,class,21,337,163,2195,6.51,0,0,[],[],[],14,[],[],0
pytorch_tabular/models/mixture_density/config.py:CategoryEmbeddingMDNConfig,CategoryEmbeddingMDNConfig,class,9,27,25,330,12.22,0,0,[],[],[],121,[],[],0
pytorch_tabular/models/mixture_density/config.py:NODEMDNConfig,NODEMDNConfig,class,9,27,25,304,11.26,0,0,[],[],[],165,[],[],0
pytorch_tabular/models/mixture_density/config.py:AutoIntMDNConfig,AutoIntMDNConfig,class,9,27,25,310,11.48,0,0,[],[],[],238,[],[],0
pytorch_tabular/models/mixture_density/mdn.py:MixtureDensityHead,MixtureDensityHead,class,61,206,137,2137,10.37,2,4,[],[],[],35,[],[],0
pytorch_tabular/models/mixture_density/mdn.py:BaseMDN,BaseMDN,class,78,436,207,4671,10.71,4,9,[],[],[],135,[],[],0
pytorch_tabular/models/mixture_density/mdn.py:CategoryEmbeddingMDN,CategoryEmbeddingMDN,class,28,81,58,1017,12.56,1,5,[],[],[],337,[],[],0
pytorch_tabular/models/mixture_density/mdn.py:NODEMDN,NODEMDN,class,21,53,45,612,11.55,0,0,[],[],[],379,[],[],0
pytorch_tabular/models/mixture_density/mdn.py:AutoIntMDN,AutoIntMDN,class,12,21,19,298,14.19,0,0,[],[],[],409,[],[],0
models/mixture_density/mdn.py:MixtureDensityHead:__init__,MixtureDensityHead:__init__,method,4,4,4,60,15.0,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",36,[],"['super', 'self._build_network']",2
models/mixture_density/mdn.py:MixtureDensityHead:_build_network,MixtureDensityHead:_build_network,method,10,26,24,436,16.77,1,1,['self'],[None],[None],41,[],"['nn.Linear', 'enumerate']",2
models/mixture_density/mdn.py:MixtureDensityHead:forward,MixtureDensityHead:forward,method,10,14,12,95,6.79,0,0,"['self', 'x']","[None, None]","[None, None]",55,[],"['self.pi', 'self.sigma', 'nn.ELU', 'self.mu']",4
models/mixture_density/mdn.py:MixtureDensityHead:gaussian_probability,MixtureDensityHead:gaussian_probability,method,8,32,26,213,6.66,0,1,"['self', 'sigma', 'mu', 'target', 'log']","[None, None, None, None, None]","[None, None, None, None, 'False']",63,"['        """"""Returns the probability of `target` given MoG parameters `sigma` and `mu`.\n', '\n', '        Arguments:\n', '            sigma (BxGxO): The standard deviation of the Gaussians. B is the batch\n', '                size, G is the number of Gaussians, and O is the number of\n', '                dimensions per Gaussian.\n', '            mu (BxGxO): The means of the Gaussians. B is the batch size, G is the\n', '                number of Gaussians, and O is the number of dimensions per Gaussian.\n', '            target (BxI): A batch of target. B is the batch size and I is the number of\n', '                input dimensions.\n', '        Returns:\n', '            probabilities (BxG): The probability of each point in the probability\n', '                of the distribution in the corresponding sigma/mu index.\n', '        """"""\n']","['target.expand_as', 'torch.pow', 'torch.exp', 'torch.prod']",4
models/mixture_density/mdn.py:MixtureDensityHead:log_prob,MixtureDensityHead:log_prob,method,6,18,17,239,13.28,0,0,"['self', 'pi', 'sigma', 'mu', 'y']","[None, None, None, None, None]","[None, None, None, None, None]",91,[],"['self.gaussian_probability', 'torch.log', 'torch.logsumexp']",3
models/mixture_density/mdn.py:MixtureDensityHead:sample,MixtureDensityHead:sample,method,8,15,12,188,12.53,0,0,"['self', 'pi', 'sigma', 'mu']","[None, None, None, None]","[None, None, None, None]",101,"['        """"""Draw samples from a MoG.""""""\n']","['Categorical', 'categorical.sample', 'Variable', 'sigma.gather', 'mu.gather']",5
models/mixture_density/mdn.py:MixtureDensityHead:generate_samples,MixtureDensityHead:generate_samples,method,10,39,35,346,8.87,1,1,"['self', 'pi', 'sigma', 'mu', 'n_samples']","[None, None, None, None, None]","[None, None, None, None, 'None']",110,[],"['range', 'samples.append', 'torch.cat']",3
models/mixture_density/mdn.py:MixtureDensityHead:generate_point_predictions,MixtureDensityHead:generate_point_predictions,method,7,19,16,225,11.84,0,1,"['self', 'pi', 'sigma', 'mu', 'n_samples']","[None, None, None, None, None]","[None, None, None, None, 'None']",125,[],"['self.generate_samples', 'torch.mean', 'torch.median']",3
models/mixture_density/mdn.py:BaseMDN:__init__,BaseMDN:__init__,method,5,33,25,253,7.67,0,1,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",36,"['        """"""Returns the probability of `target` given MoG parameters `sigma` and `mu`.\n', '\n', '        Arguments:\n', '            sigma (BxGxO): The standard deviation of the Gaussians. B is the batch\n', '                size, G is the number of Gaussians, and O is the number of\n', '                dimensions per Gaussian.\n', '            mu (BxGxO): The means of the Gaussians. B is the batch size, G is the\n', '                number of Gaussians, and O is the number of dimensions per Gaussian.\n', '            target (BxI): A batch of target. B is the batch size and I is the number of\n', '                input dimensions.\n', '        Returns:\n', '            probabilities (BxG): The probability of each point in the probability\n', '                of the distribution in the corresponding sigma/mu index.\n', '        """"""\n']","['logger.warning', 'super']",2
models/mixture_density/mdn.py:BaseMDN:unpack_input,BaseMDN:unpack_input,method,0,1,1,4,4.0,0,0,"['self', 'x']","[None, ' Dict']","[None, None]",144,[],[],0
models/mixture_density/mdn.py:BaseMDN:forward,BaseMDN:forward,method,8,17,14,125,7.35,0,0,"['self', 'x']","[None, ' Dict']","[None, None]",147,[],"['self.unpack_input', 'self.backbone', 'self.mdn']",3
models/mixture_density/mdn.py:BaseMDN:predict,BaseMDN:predict,method,4,8,8,121,15.12,0,0,"['self', 'x']","[None, ' Dict']","[None, None]",153,[],['self.forward'],1
models/mixture_density/mdn.py:BaseMDN:sample,BaseMDN:sample,method,7,17,15,187,11.0,0,1,"['self', 'x', 'n_samples', 'ret_model_output']","[None, ' Dict', ' Optional[int] ', None]","[None, None, ' None', 'False']",159,[],['self.forward'],1
models/mixture_density/mdn.py:BaseMDN:calculate_loss,BaseMDN:calculate_loss,method,21,85,50,1003,11.8,0,4,"['self', 'y', 'pi', 'sigma', 'mu', 'tag']","[None, None, None, None, None, None]","[None, None, None, None, None, '""train""']",169,[],"['torch.mean', 'torch.cat', 'torch.norm', 'self.log']",4
models/mixture_density/mdn.py:BaseMDN:training_step,BaseMDN:training_step,method,11,28,24,340,12.14,0,1,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",208,[],"['self', 'self.calculate_loss', 'self.calculate_metrics']",3
models/mixture_density/mdn.py:BaseMDN:validation_step,BaseMDN:validation_step,method,11,26,19,295,11.35,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",223,[],"['self', 'self.calculate_loss', 'self.calculate_metrics']",3
models/mixture_density/mdn.py:BaseMDN:test_step,BaseMDN:test_step,method,10,25,19,283,11.32,0,0,"['self', 'batch', 'batch_idx']","[None, None, None]","[None, None, None]",235,[],"['self', 'self.calculate_loss', 'self.calculate_metrics']",3
models/mixture_density/mdn.py:BaseMDN:validation_epoch_end,BaseMDN:validation_epoch_end,method,23,146,74,1619,11.09,4,2,"['self', 'outputs']","[None, None]","[None, None]",247,[],"['hasattr', 'torch.cat', 'range', 'self.log', 'self.create_plotly_histogram', 'wandb.log', 'bin_dict=dict']",7
models/mixture_density/mdn.py:CategoryEmbeddingMDN:__init__,CategoryEmbeddingMDN:__init__,method,3,9,9,94,10.44,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",36,"['        """"""Returns the probability of `target` given MoG parameters `sigma` and `mu`.\n', '\n', '        Arguments:\n', '            sigma (BxGxO): The standard deviation of the Gaussians. B is the batch\n', '                size, G is the number of Gaussians, and O is the number of\n', '                dimensions per Gaussian.\n', '            mu (BxGxO): The means of the Gaussians. B is the batch size, G is the\n', '                number of Gaussians, and O is the number of dimensions per Gaussian.\n', '            target (BxI): A batch of target. B is the batch size and I is the number of\n', '                input dimensions.\n', '        Returns:\n', '            probabilities (BxG): The probability of each point in the probability\n', '                of the distribution in the corresponding sigma/mu index.\n', '        """"""\n']","['sum', 'super']",2
models/mixture_density/mdn.py:CategoryEmbeddingMDN:_build_network,CategoryEmbeddingMDN:_build_network,method,11,20,20,369,18.45,0,1,['self'],[None],[None],41,[],"['nn.ModuleList', 'nn.BatchNorm1d', 'FeedForwardBackbone', 'MixtureDensityHead']",4
models/mixture_density/mdn.py:CategoryEmbeddingMDN:unpack_input,CategoryEmbeddingMDN:unpack_input,method,14,41,29,451,11.0,1,4,"['self', 'x']","[None, ' Dict']","[None, None]",144,[],"['embedding_layer', 'enumerate', 'torch.cat', 'self.normalizing_batch_norm']",4
models/mixture_density/mdn.py:NODEMDN:__init__,NODEMDN:__init__,method,1,2,2,33,16.5,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",36,"['        """"""Returns the probability of `target` given MoG parameters `sigma` and `mu`.\n', '\n', '        Arguments:\n', '            sigma (BxGxO): The standard deviation of the Gaussians. B is the batch\n', '                size, G is the number of Gaussians, and O is the number of\n', '                dimensions per Gaussian.\n', '            mu (BxGxO): The means of the Gaussians. B is the batch size, G is the\n', '                number of Gaussians, and O is the number of dimensions per Gaussian.\n', '            target (BxI): A batch of target. B is the batch size and I is the number of\n', '                input dimensions.\n', '        Returns:\n', '            probabilities (BxG): The probability of each point in the probability\n', '                of the distribution in the corresponding sigma/mu index.\n', '        """"""\n']",['super'],1
models/mixture_density/mdn.py:NODEMDN:subset,NODEMDN:subset,method,2,3,3,27,9.0,0,0,"['self', 'x']","[None, None]","[None, None]",383,[],[],0
models/mixture_density/mdn.py:NODEMDN:_build_network,NODEMDN:_build_network,method,11,16,16,327,20.44,0,0,['self'],[None],[None],41,[],"['NODEBackbone', 'utils.Lambda', 'nn.Sequential', 'MixtureDensityHead']",4
models/mixture_density/mdn.py:NODEMDN:unpack_input,NODEMDN:unpack_input,method,4,18,14,102,5.67,0,0,"['self', 'x']","[None, ' Dict']","[None, None]",144,[],"['len', 'torch.cat']",2
models/mixture_density/mdn.py:AutoIntMDN:__init__,AutoIntMDN:__init__,method,1,2,2,33,16.5,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",36,"['        """"""Returns the probability of `target` given MoG parameters `sigma` and `mu`.\n', '\n', '        Arguments:\n', '            sigma (BxGxO): The standard deviation of the Gaussians. B is the batch\n', '                size, G is the number of Gaussians, and O is the number of\n', '                dimensions per Gaussian.\n', '            mu (BxGxO): The means of the Gaussians. B is the batch size, G is the\n', '                number of Gaussians, and O is the number of dimensions per Gaussian.\n', '            target (BxI): A batch of target. B is the batch size and I is the number of\n', '                input dimensions.\n', '        Returns:\n', '            probabilities (BxG): The probability of each point in the probability\n', '                of the distribution in the corresponding sigma/mu index.\n', '        """"""\n']",['super'],1
models/mixture_density/mdn.py:AutoIntMDN:_build_network,AutoIntMDN:_build_network,method,6,6,6,155,25.83,0,0,['self'],[None],[None],41,"['        """"""Returns the probability of `target` given MoG parameters `sigma` and `mu`.\n', '\n', '        Arguments:\n', '            sigma (BxGxO): The standard deviation of the Gaussians. B is the batch\n', '                size, G is the number of Gaussians, and O is the number of\n', '                dimensions per Gaussian.\n', '            mu (BxGxO): The means of the Gaussians. B is the batch size, G is the\n', '                number of Gaussians, and O is the number of dimensions per Gaussian.\n', '            target (BxI): A batch of target. B is the batch size and I is the number of\n', '                input dimensions.\n', '        Returns:\n', '            probabilities (BxG): The probability of each point in the probability\n', '                of the distribution in the corresponding sigma/mu index.\n', '        """"""\n']","['AutoIntBackbone', 'MixtureDensityHead']",2
models/mixture_density/mdn.py:AutoIntMDN:unpack_input,AutoIntMDN:unpack_input,method,2,2,2,7,3.5,0,0,"['self', 'x']","[None, ' Dict']","[None, None]",144,[],[],0
pytorch_tabular/models/node/architecture_blocks.py:DenseODSTBlock,DenseODSTBlock,class,37,115,89,1207,10.5,2,4,[],[],[],13,[],[],0
models/node/architecture_blocks.py:DenseODSTBlock:__init__,DenseODSTBlock:__init__,method,18,39,35,448,11.49,1,0,"['self', 'input_dim', 'num_trees', 'num_layers', 'tree_output_dim', 'max_features', 'input_dropout', 'flatten_output', 'Module', '**kwargs']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, '1', 'None', '0.0', 'False', 'ODST', None]",14,[],"['range', 'Module', 'min', 'float', 'layers.append', 'super']",6
models/node/architecture_blocks.py:DenseODSTBlock:forward,DenseODSTBlock:forward,method,19,60,48,581,9.68,1,4,"['self', 'x']","[None, None]","[None, None]",49,[],"['min', 'torch.cat', 'F.dropout', 'layer', 'outputs.view']",5
pytorch_tabular/models/node/config.py:NodeConfig,NodeConfig,class,87,475,238,2968,6.25,0,1,[],[],[],8,[],[],0
pytorch_tabular/models/node/node_model.py:NODEBackbone,NODEBackbone,class,16,49,42,1013,20.67,0,0,[],[],[],22,[],[],0
pytorch_tabular/models/node/node_model.py:NODEModel,NODEModel,class,61,194,127,2080,10.72,3,11,[],[],[],58,[],[],0
models/node/node_model.py:NODEBackbone:__init__,NODEBackbone:__init__,method,3,3,3,74,24.67,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",23,[],"['super', 'self.save_hyperparameters', 'self._build_network']",3
models/node/node_model.py:NODEBackbone:_build_network,NODEBackbone:_build_network,method,8,32,29,817,25.53,0,0,['self'],[None],[None],28,[],"['DenseODSTBlock', 'choice_function=getattr', 'bin_function=getattr', 'initialize_response_=getattr', 'initialize_selection_logits_=getattr']",5
models/node/node_model.py:NODEBackbone:forward,NODEBackbone:forward,method,3,4,3,29,7.25,0,0,"['self', 'x']","[None, None]","[None, None]",53,[],['self.dense_block'],1
models/node/node_model.py:NODEModel:__init__,NODEModel:__init__,method,4,11,11,122,11.09,0,1,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",23,[],"['sum', 'super']",2
models/node/node_model.py:NODEModel:subset,NODEModel:subset,method,3,4,4,50,12.5,0,0,"['self', 'x']","[None, None]","[None, None]",64,[],[],0
models/node/node_model.py:NODEModel:data_aware_initialization,NODEModel:data_aware_initialization,method,14,24,24,255,10.62,1,1,"['self', 'datamodule']","[None, None]","[None, None]",67,"['        """"""Performs data-aware initialization for NODE""""""\n']","['logger.info', 'datamodule.train_dataloader', 'next', 'batch.items', 'isinstance', 'v.to', 'torch.no_grad', 'self']",8
models/node/node_model.py:NODEModel:_build_network,NODEModel:_build_network,method,13,35,29,524,14.97,0,2,['self'],[None],[None],28,"['        """"""Performs data-aware initialization for NODE""""""\n']","['nn.ModuleList', 'nn.Dropout', 'NODEBackbone', 'utils.Lambda']",4
models/node/node_model.py:NODEModel:unpack_input,NODEModel:unpack_input,method,13,54,35,476,8.81,1,4,"['self', 'x']","[None, ' Dict']","[None, None]",104,[],"['embedding_layer', 'enumerate', 'torch.cat', 'len']",4
models/node/node_model.py:NODEModel:forward,NODEModel:forward,method,21,45,40,456,10.13,1,3,"['self', 'x']","[None, ' Dict']","[None, None]",131,[],"['self.unpack_input', 'self.embedding_dropout', 'self.backbone', 'self.output_response', 'range', 'nn.Sigmoid']",6
pytorch_tabular/models/node/odst.py:check_numpy,check_numpy,function,5,12,9,111,9.25,0,1,['x'],[None],[None],15,"['    """""" Makes sure x is a numpy array """"""\n']","['isinstance', 'x.detach', 'np.asarray']",3
pytorch_tabular/models/node/odst.py:ODST,ODST,class,75,287,225,3640,12.68,0,2,[],[],[],24,[],[],0
models/node/odst.py:ODST:__init__,ODST:__init__,method,37,94,75,1201,12.78,0,0,"['self', 'in_features', 'num_trees', 'depth', 'tree_output_dim', 'flatten_output', 'choice_function', 'bin_function', 'initialize_response_', 'initialize_selection_logits_', 'threshold_init_beta', 'threshold_init_cutoff', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, '6', '1', 'True', 'sparsemax', 'sparsemoid', 'nn.init.normal_', 'nn.init.uniform_', '1.0', '1.0', None]",25,"['        """"""\n', '        Oblivious Differentiable Sparsemax Trees. http://tinyurl.com/odst-readmore\n', '        One can drop (sic!) this module anywhere instead of nn.Linear\n', '        :param in_features: number of features in the input tensor\n', '        :param num_trees: number of trees in this layer\n', '        :param tree_dim: number of response channels in the response of individual tree\n', '        :param depth: number of splits in every tree\n', '        :param flatten_output: if False, returns [..., num_trees, tree_dim],\n', '            by default returns [..., num_trees * tree_dim]\n', '        :param choice_function: f(tensor, dim) -> R_simplex computes feature weights s.t. f(tensor, dim).sum(dim) == 1\n', '        :param bin_function: f(tensor) -> R[0, 1], computes tree leaf weights\n', '\n', '        :param initialize_response_: in-place initializer for tree output tensor\n', '        :param initialize_selection_logits_: in-place initializer for logits that select features for the tree\n', '        both thresholds and scales are initialized with data-aware init (or .load_state_dict)\n', '        :param threshold_init_beta: initializes threshold to a q-th quantile of data points\n', '            where q ~ Beta(:threshold_init_beta:, :threshold_init_beta:)\n', '            If this param is set to 1, initial thresholds will have the same distribution as data points\n', '            If greater than 1 (e.g. 10), thresholds will be closer to median data value\n', '            If less than 1 (e.g. 0.1), thresholds will approach min/max data values.\n', '\n', '        :param threshold_init_cutoff: threshold log-temperatures initializer, in (0, inf)\n', '            By default(1.0), log-remperatures are initialized in such a way that all bin selectors\n', '            end up in the linear region of sparse-sigmoid. The temperatures are then scaled by this parameter.\n', '            Setting this value > 1.0 will result in some margin between data points and sparse-sigmoid cutoff value\n', '            Setting this value < 1.0 will cause (1 - value) part of data points to end up in flat sparse-sigmoid region\n', '            For instance, threshold_init_cutoff = 0.9 will set 10% points equal to 0.0 or 1.0\n', '            Setting this value > 1.0 will result in a margin between data points and sparse-sigmoid cutoff value\n', '            All points will be between (0.5 - 0.5 / threshold_init_cutoff) and (0.5 + 0.5 / threshold_init_cutoff)\n', '        """"""\n']","['super', 'nn.Parameter', 'torch.zeros', 'initialize_response_', 'initialize_selection_logits_', 'torch.full', 'float', 'torch.no_grad', 'torch.arange', 'offsets.view', 'torch.stack']",11
models/node/odst.py:ODST:forward,ODST:forward,method,17,52,46,755,14.52,0,1,"['self', 'input']","[None, None]","[None, None]",112,[],"['len', 'self.forward', 'self.choice_function', 'torch.einsum', 'torch.exp', 'torch.stack', 'self.bin_function', 'torch.prod', 'response.flatten']",9
models/node/odst.py:ODST:initialize,ODST:initialize,method,23,104,92,1101,10.59,0,1,"['self', 'input', 'eps']","[None, None, None]","[None, None, '1e-6']",148,[],"['len', 'warn', 'torch.no_grad', 'self.choice_function', 'torch.einsum', 'torch.as_tensor', 'list', 'map', 'check_numpy', 'percentiles_q.flatten', 'np.percentile', 'min', 'max', 'torch.log']",14
models/node/odst.py:ODST:__repr__,ODST:__repr__,method,7,13,13,217,16.69,0,0,['self'],[None],[None],197,[],[],0
pytorch_tabular/models/node/utils.py:to_one_hot,to_one_hot,function,72,162,134,1350,8.33,2,0,"['y', 'depth']","[None, None]","[None, 'None']",13,[],"['input.max', 'SparsemaxFunction._threshold_and_support', 'torch.clamp', 'ctx.save_for_backward', 'backward', 'grad_output.clone', 'grad_input.sum', 'supp_size.to', 'v_hat.unsqueeze', 'torch.where', '_threshold_and_support', 'torch.sort', 'input_srt.cumsum', '_make_ix_like', 'support.sum', 'input_cumsum.gather', 'support_size.to']",17
pytorch_tabular/models/node/utils.py:sparsemax,sparsemax,function,2,3,3,40,13.33,0,0,"['input', 'dim']","[None, None]","[None, '-1']",100,[],['SparsemaxFunction.apply'],1
pytorch_tabular/models/node/utils.py:sparsemoid,sparsemoid,function,1,5,5,33,6.6,0,0,['input'],[None],[None],104,[],[],0
pytorch_tabular/models/node/utils.py:entmax15,entmax15,function,2,3,3,39,13.0,0,0,"['input', 'dim']","[None, None]","[None, '-1']",193,[],['Entmax15Function.apply'],1
pytorch_tabular/models/node/utils.py:Entmax15Function,Entmax15Function,class,50,119,88,956,8.03,1,0,[],[],[],110,[],[],0
pytorch_tabular/models/node/utils.py:Entmoid15,Entmoid15,class,24,70,49,647,9.24,0,0,[],[],[],161,[],[],0
pytorch_tabular/models/node/utils.py:Lambda,Lambda,class,6,13,12,122,9.38,0,0,[],[],[],199,[],[],0
pytorch_tabular/models/node/utils.py:ModuleWithInit,ModuleWithInit,class,12,41,33,556,13.56,0,2,[],[],[],208,[],[],0
models/node/utils.py:Entmax15Function:forward,Entmax15Function:forward,method,24,41,33,305,7.44,1,0,"['ctx', 'input', 'dim']","[None, None, None]","[None, None, '-1']",47,"['        """"""sparsemax: normalizing sparse transform (a la softmax)\n', '\n', '        Parameters:\n', '            input (Tensor): any shape\n', '            dim: dimension along which to apply sparsemax\n', '\n', '        Returns:\n', '            output (Tensor): same shape as input\n', '        """"""\n']","['input.max', 'Entmax15Function._threshold_and_support', 'torch.clamp', 'ctx.save_for_backward']",4
models/node/utils.py:Entmax15Function:backward,Entmax15Function:backward,method,9,23,19,150,6.52,0,0,"['ctx', 'grad_output']","[None, None]","[None, None]",66,"['        """"""Sparsemax building block: compute the threshold\n', '\n', '        Args:\n', '            input: any dimension\n', '            dim: dimension along which to apply the sparsemax\n', '\n', '        Returns:\n', '            the threshold value\n', '        """"""\n']","['Y.sqrt', 'dX.sum', 'gppr.sum', 'q.unsqueeze']",4
models/node/utils.py:Entmax15Function:_threshold_and_support,Entmax15Function:_threshold_and_support,method,19,42,34,356,8.48,0,0,"['input', 'dim']","[None, None]","[None, '-1']",78,"['        """"""Sparsemax building block: compute the threshold\n', '\n', '        Args:\n', '            input: any dimension\n', '            dim: dimension along which to apply the sparsemax\n', '\n', '        Returns:\n', '            the threshold value\n', '        """"""\n']","['torch.sort', '_make_ix_like', 'Xsrt.cumsum', 'torch.clamp', 'torch.sqrt', 'tau.gather']",6
models/node/utils.py:Entmoid15:forward,Entmoid15:forward,method,4,5,4,75,15.0,0,0,"['ctx', 'input']","[None, None]","[None, None]",165,[],"['Entmoid15._forward', 'ctx.save_for_backward']",2
models/node/utils.py:Entmoid15:_forward,Entmoid15:_forward,method,9,27,22,195,7.22,0,0,['input'],[None],[None],172,[],"['abs', 'torch.sqrt', 'tau.masked_fill_', 'F.relu', 'torch.where']",5
models/node/utils.py:Entmoid15:backward,Entmoid15:backward,method,2,3,3,59,19.67,0,0,"['ctx', 'grad_output']","[None, None]","[None, None]",66,"['        """"""Sparsemax building block: compute the threshold\n', '\n', '        Args:\n', '            input: any dimension\n', '            dim: dimension along which to apply the sparsemax\n', '\n', '        Returns:\n', '            the threshold value\n', '        """"""\n']",['Entmoid15._backward'],1
models/node/utils.py:Entmoid15:_backward,Entmoid15:_backward,method,8,18,13,136,7.56,0,0,"['output', 'grad_output']","[None, None]","[None, None]",185,[],['output.sqrt'],1
models/node/utils.py:Lambda:__init__,Lambda:__init__,method,3,3,3,33,11.0,0,0,"['self', 'func']","[None, None]","[None, None]",200,[],['super'],1
models/node/utils.py:Lambda:forward,Lambda:forward,method,2,3,3,31,10.33,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",204,[],['self.func'],1
models/node/utils.py:ModuleWithInit:__init__,ModuleWithInit:__init__,method,4,9,9,147,16.33,0,0,['self'],[None],[None],211,[],"['super', 'nn.Parameter', 'torch.tensor']",3
models/node/utils.py:ModuleWithInit:initialize,ModuleWithInit:initialize,method,1,4,4,43,10.75,0,0,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",222,"['        """""" initialize module tensors using first batch of data """"""\n']",['NotImplementedError'],1
models/node/utils.py:ModuleWithInit:__call__,ModuleWithInit:__call__,method,6,18,15,275,15.28,0,2,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",226,[],"['bool', 'self.initialize', 'super']",3
pytorch_tabular/models/tabnet/config.py:TabNetModelConfig,TabNetModelConfig,class,13,159,82,1062,6.68,0,0,[],[],[],12,[],[],0
pytorch_tabular/models/tabnet/tabnet_model.py:TabNetBackbone,TabNetBackbone,class,24,52,44,805,15.48,2,0,[],[],[],19,[],[],0
pytorch_tabular/models/tabnet/tabnet_model.py:TabNetModel,TabNetModel,class,22,79,64,611,7.73,1,1,[],[],[],50,[],[],0
models/tabnet/tabnet_model.py:TabNetBackbone:__init__,TabNetBackbone:__init__,method,3,3,3,74,24.67,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",20,[],"['super', 'self.save_hyperparameters', 'self._build_network']",3
models/tabnet/tabnet_model.py:TabNetBackbone:_build_network,TabNetBackbone:_build_network,method,15,33,28,607,18.39,2,0,['self'],[None],[None],25,[],"['TabNet', 'range']",2
models/tabnet/tabnet_model.py:TabNetBackbone:forward,TabNetBackbone:forward,method,5,5,5,26,5.2,0,0,"['self', 'x']","[None, ' Dict']","[None, None]",44,[],['self.tabnet'],1
models/tabnet/tabnet_model.py:TabNetModel:__init__,TabNetModel:__init__,method,1,2,2,33,16.5,0,0,"['self', 'config', '**kwargs']","[None, ' DictConfig', None]","[None, None, None]",20,[],['super'],1
models/tabnet/tabnet_model.py:TabNetModel:_build_network,TabNetModel:_build_network,method,2,2,2,42,21.0,0,0,['self'],[None],[None],25,[],['TabNetBackbone'],1
models/tabnet/tabnet_model.py:TabNetModel:unpack_input,TabNetModel:unpack_input,method,4,18,14,102,5.67,0,0,"['self', 'x']","[None, ' Dict']","[None, None]",57,[],"['len', 'torch.cat']",2
models/tabnet/tabnet_model.py:TabNetModel:forward,TabNetModel:forward,method,14,42,40,305,7.26,1,1,"['self', 'x']","[None, ' Dict']","[None, None]",44,[],"['self.unpack_input', 'self.backbone', 'range', 'nn.Sigmoid']",4
