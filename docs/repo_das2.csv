uri,name,type,n_variable,n_words,n_words_unique,n_characters,avg_char_per_word,n_loop,n_ifthen,arg_name,arg_type,arg_value,line,docs,list_functions,n_functions
core_deploy.py:load_arguments,load_arguments,function,6,18,14,243,13.5,0,0,[],[],[],16,"['    """"""\n', '    Get arguments from command line.\n', '    """"""\n']","['argparse.ArgumentParser', 'p.add_argument', 'p.parse_args']",3
core_run.py:get_global_pars,get_global_pars,function,9,26,23,303,11.65,0,0,['config_uri'],[None],"['""""']",38,[],"['log', 'print', 'load_function_uri', 'model_dict_fun']",4
core_run.py:get_config_path,get_config_path,function,11,45,30,412,9.16,0,1,['config'],[None],"[""''""]",53,[],"['print', 'get_global_pars', 'len', 'config.split']",4
core_run.py:data_profile2,data_profile2,function,12,22,22,264,12.0,0,0,['config'],[None],"[""''""]",79,"['    """"""\n', '    :param config:\n', '    :return:\n', '    """"""\n']","['get_config_path', 'get_global_pars', 'log', 'run_profile']",4
core_run.py:data_profile,data_profile,function,13,29,29,337,11.62,0,0,['config'],[None],"[""''""]",96,[],"['get_config_path', 'get_global_pars', 'log', 'run_profile', 'data_profile']",5
core_run.py:preprocess,preprocess,function,13,28,27,306,10.93,0,0,"['config', 'nsample']","[None, None]","[""''"", 'None']",105,"['    """"""\n', '\n', '    :param config:\n', '    :param nsample:\n', '    :return:\n', '    """"""\n']","['get_config_path', 'get_global_pars', 'log', 'run_preprocess.run_preprocess']",4
core_run.py:train,train,function,13,27,26,270,10.0,0,0,"['config', 'nsample']","[None, None]","[""''"", 'None']",128,"['    """"""  train a model with  confi_name  and nsample\n', '    :param config:\n', '    :param nsample:\n', '    :return:\n', '    """"""\n']","['get_config_path', 'get_global_pars', 'log', 'run_train.run_train']",4
core_run.py:check,check,function,5,6,6,68,11.33,0,0,"[""config='outlier_predict.py""]",[''],"[""'outlier_predict.py::titanic_lightgbm'""]",150,[],"['get_global_pars', 'log']",2
core_run.py:predict,predict,function,13,33,32,362,10.97,0,0,"['config', 'nsample']","[None, None]","[""''"", 'None']",161,"['    """"""\n', '    :param config:\n', '    :param nsample:\n', '    :return:\n', '    """"""\n']","['get_config_path', 'get_global_pars', 'log', 'run_inference.run_predict']",4
core_run.py:train_sampler,train_sampler,function,13,27,26,274,10.15,0,0,"['config', 'nsample']","[None, None]","[""''"", 'None']",186,"['    """"""  train a model with  confi_name  and nsample\n', '    :param config:\n', '    :param nsample:\n', '    :return:\n', '    """"""\n']","['get_config_path', 'get_global_pars', 'log', 'run_sampler.run_train']",4
core_run.py:transform,transform,function,13,33,32,360,10.91,0,0,"['config', 'nsample']","[None, None]","[""''"", 'None']",208,"['    """"""\n', '    :param config:\n', '    :param nsample:\n', '    :return:\n', '    """"""\n']","['get_config_path', 'get_global_pars', 'log', 'run_sampler.run_transform']",4
core_run.py:hyperparam_wrapper,hyperparam_wrapper,function,28,74,62,973,13.15,0,2,"['config_full', 'ntrials', 'n_sample', 'debug', 'path_output         ', 'path_optuna_storage ', 'metric_name', 'mdict_range']","[None, None, None, None, None, None, None, None]","['""""', '2', '5000', '1', ' ""data/output/titanic1/""', "" 'data/output/optuna_hyper/optunadb.db'"", ""'accuracy_score'"", 'None']",234,[],"['config_full.split', 'load_function_uri', 'mdict', 'objective_fun', 'log', 'run_train', 'print', 'run_hyper_optuna', 'os.makedirs', 'json.dump', 'open']",11
core_run.py:deploy,deploy,function,3,6,6,88,14.67,0,0,[],[],[],285,"['    """"""\n', '    Simple deploy using uvicorn on runtime. U can use gunicorn instead.\n', '\n', '    #### Important info: First request will always take time\n', '\n', '    return: Service\n', '    """"""\n']",['uvicorn.run'],1
core_test.py:os_bash,os_bash,function,5,16,14,126,7.88,0,0,['cmd'],[None],[None],26,[],['subprocess.run'],1
core_test.py:log_separator,log_separator,function,1,6,6,34,5.67,0,0,['space'],[None],['140'],37,[],['print'],1
core_test.py:log_info_repo,log_info_repo,function,21,128,70,1272,9.94,0,0,['arg'],[None],['None'],41,"['   """"""\n', '      Grab Github Variables\n', '      https://help.github.com/en/actions/configuring-and-managing-workflows/using-environment-variables\n', '      log_info_repo(arg=None)\n', '   """"""\n']","['os_bash', 'repo.replace', 'workflow.replace', 'sha.replace', 'branch.replace', 'locals', 'dlocal.get', 'log_separator', 'print']",9
core_test.py:to_logfile,to_logfile,function,6,21,14,113,5.38,0,1,"['prefix', ""dateformat='+%Y-%m-%d_%H""]","[None, '']","['""""', ""'+%Y-%m-%d_%H:%M:%S,%3N'""]",91,[],[],0
core_test.py:os_file_current_path,os_file_current_path,function,7,9,7,117,13.0,0,0,[],[],[],99,[],['str'],1
core_test.py:os_system,os_system,function,4,8,7,61,7.62,0,1,"['cmd', 'dolog', 'prefix', ""dateformat='+%Y-%m-%d_%H""]","[None, None, None, '']","[None, '1', '""""', ""'+%Y-%m-%d_%H:%M:%S,%3N'""]",109,[],"['to_logfile', 'os.system']",2
core_test.py:json_load,json_load,function,2,10,8,58,5.8,0,0,['path'],[None],[None],116,[],['json.load'],1
core_test.py:log_remote_start,log_remote_start,function,4,18,16,132,7.33,0,0,['arg'],[None],['None'],124,[],"['log', 'os.system']",2
core_test.py:test_functions,test_functions,function,14,86,48,444,5.16,1,1,['arg'],[None],['None'],156,[],"['path_norm', 'json.load', 'log', 'load_function_uri', 'p.get', 'len', 'myfun']",7
core_test.py:test_model_structure,test_model_structure,function,5,11,11,163,14.82,0,0,[],[],[],185,[],"['log', 'os.getcwd', 'os.system']",3
core_test.py:test_import,test_import,function,13,34,33,338,9.94,1,0,['arg'],[None],['None'],196,[],"['log_info_repo', 'log_separator', 'log', 'os_get_file', 'print', 'f.replace', 'import_module']",7
core_test.py:test_jupyter,test_jupyter,function,21,96,72,922,9.6,2,2,"['arg', 'config_mode']","[None, None]","['None', '""test_all""']",227,"['    """"""\n', '      Tests files in dsa2/example/\n', '    """"""\n']","['log_info_repo', 'os_package_root_path', 'root.replace', 'str', 'print', 'get_recursive_files2', 'log_separator', 'file.replace', 'git.get', 'os.system', 'os_file_replace']",11
core_test.py:os_file_replace,os_file_replace,function,7,23,17,170,7.39,0,0,"['filename', 's1', 's2']","[None, None, None]","[None, '""""', '""""']",276,[],"['open', 'file.read', 'filedata.replace', 'file.write', 'print']",5
core_test.py:test_benchmark,test_benchmark,function,10,50,29,447,8.94,1,0,['arg'],[None],['None'],295,[],"['log_info_repo', 'log', 'path.replace', 'log_separator', 'print', 'os.system']",6
core_test.py:test_cli,test_cli,function,26,73,62,571,7.82,1,3,['arg'],[None],['None'],318,[],"['log', 'log_info_repo', 'path_norm', 'print', 'is_valid_cmd', 'cmd.strip', 'len', 'cmd.startswith', 'open', 'f.readlines', 'ss.strip', 'to_logfile', 'log_separator', 'os.system']",14
core_test.py:test_pullrequest,test_pullrequest,function,26,85,67,711,8.36,3,2,['arg'],[None],['None'],356,"['    """"""\n', '      Scan files in /pullrequest/ and run test on it.\n', '    """"""\n']","['log_info_repo', 'str', 'log', 'get_recursive_files', 'test_import', 'sleep', 'os.system', 'to_logfile', 'log_separator', 'open', 'f.readlines', 'Exception']",12
core_test.py:test_dataloader,test_dataloader,function,11,34,28,332,9.76,1,0,['arg'],[None],['None'],406,[],"['log_info_repo', 'json_load', 'log', 'path.replace', 'log_separator', 'os.system']",6
core_test.py:test_json_all,test_json_all,function,20,62,55,620,10.0,2,0,['arg'],[None],[None],427,[],"['log_info_repo', 'os_package_root_path', 'root.replace', 'log', 'str', 'get_recursive_files2', 'print', 'json.load', 'cfg.items', 'log_separator', 'os.system']",11
core_test.py:test_all,test_all,function,23,56,46,550,9.82,2,1,['arg'],[None],['None'],454,[],"['log_info_repo', 'log', 'model_get_list', 'json.load', 'path_norm', 'path.replace', 't.replace', 'log_separator', 'os.system', 'log_remote_push', 'sleep']",11
core_test.py:test_json,test_json,function,20,62,55,620,10.0,2,0,['arg'],[None],[None],482,[],"['log_info_repo', 'os_package_root_path', 'root.replace', 'log', 'str', 'get_recursive_files2', 'print', 'json.load', 'cfg.items', 'log_separator', 'os.system']",11
core_test.py:test_list,test_list,function,7,17,15,131,7.71,1,0,['mlist'],[None],[None],499,[],"['log_separator', 'log', 'os.system']",3
core_test.py:test_custom,test_custom,function,2,7,7,130,18.57,0,0,[],[],[],512,[],['test_list'],1
core_test.py:test_fast_linux,test_fast_linux,function,2,5,5,59,11.8,0,0,[],[],[],524,[],['test_list'],1
core_test.py:log,log,function,22,135,75,1335,9.89,0,0,['space'],[None],['140'],534,"['    """"""\n', '     https://help.github.com/en/actions/configuring-and-managing-workflows/using-environment-variables\n', '    """"""\n']","['print', 'log_info_repo', 'os_bash', 'repo.replace', 'workflow.replace', 'sha.replace', 'branch.replace', 'locals', 'dlocal.get', 'log_separator']",10
core_test.py:test_all_files,test_all_files,function,15,52,37,321,6.17,2,1,[],[],[],540,[],"['log', 'os.getcwd', 'glob.glob', 'path.replace', 'log_separator', 'os.system']",6
core_test.py:test_all_example,test_all_example,function,21,96,71,604,6.29,2,2,[],[],[],564,[],"['log', 'os.getcwd', 'glob.glob', 'path.replace', 'log_separator', 'os.system', 'open', 'fp.readlines', 'Exception']",9
core_test.py:cli_load_arguments,cli_load_arguments,function,14,66,51,591,8.95,0,0,['config_file'],[None],['None'],617,[],"['path_norm', 'argparse.ArgumentParser', 'add', 'p.add_argument', 'p.parse_args']",5
core_test.py:main,main,function,7,18,18,161,8.94,0,1,[],[],[],669,[],"['cli_load_arguments', 'log', 'test_list', 'globals']",4
titanic_classifier.py:global_pars_update,global_pars_update,function,22,62,47,1124,18.13,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",26,[],['print'],1
titanic_classifier.py:config1,config1,function,8,216,120,1966,9.1,0,0,[],[],[],82,"['    """"""\n', '       ONE SINGLE DICT Contains all needed informations for\n', '       used for titanic classification task\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
titanic_classifier.py:pd_col_myfun,pd_col_myfun,function,22,96,66,813,8.47,0,3,"['df', 'col', 'pars']","[None, None, None]","['None', 'None', '{}']",181,"['    """"""\n', '         Example of custom Processor\n', '    """"""\n']","['load', 'list', 'save', 'pars.get']",4
titanic_classifier.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],277,[],[],0
titanic_gefs.py:global_pars_update,global_pars_update,function,22,62,47,1124,18.13,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",26,[],['print'],1
titanic_gefs.py:config1,config1,function,8,184,104,1608,8.74,0,0,[],[],[],82,"['    """"""\n', '       ONE SINGLE DICT Contains all needed informations for\n', '       used for titanic classification task\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
tsampler.py:global_pars_update,global_pars_update,function,23,64,49,1156,18.06,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",22,[],['print'],1
tsampler.py:config_sampler,config_sampler,function,8,182,97,1568,8.62,0,0,[],[],[],79,"['    """"""\n', '       ONE SINGLE DICT Contains all needed informations for\n', '       used for titanic classification task\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
tsampler.py:test_batch,test_batch,function,19,55,47,478,8.69,1,0,[],[],[],172,[],"['config_sampler', 'get_config_path', 'get_global_pars', 'print', 'run_sampler.run_train']",5
tseries.py:global_pars_update,global_pars_update,function,21,56,41,1028,18.36,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",20,[],['print'],1
tseries.py:config1,config1,function,9,158,108,1328,8.41,0,0,[],[],[],79,"['    """"""\n', '       ONE SINGLE DICT Contains all needed informations for\n', '       used for tseries_demand classification task\n', '    """"""\n']","['post_process_fun', 'float', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
tseries.py:pd_dsa2_custom,pd_dsa2_custom,function,37,104,85,810,7.79,1,1,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",162,"['    """"""\n', '    Example of custom Processor Combining\n', '    Usage :\n', '    ,{""uri"":  THIS_FILEPATH + ""::pd_dsa2_custom"",   ""pars"": {\'coldate\': \'date\'}, ""cols_family"": ""coldate"",   ""cols_out"": ""coldate_features1"",  ""type"": """" },\n', '\n', '    """"""\n']","['prepro_load', 'pd_ts_date', 'pd_ts_rolling', 'pd.concat', 'list', 'prepro_save']",6
ztemplate.py:fit,fit,function,3,6,6,34,5.67,0,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",41,[],[],0
ztemplate.py:predict,predict,function,5,7,7,47,6.71,0,0,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",47,[],[],0
ztemplate.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],54,[],[],0
ztemplate.py:save,save,function,6,9,9,47,5.22,0,0,"['path', 'info']","[None, None]","['None', 'None']",59,[],[],0
ztemplate.py:load_model,load_model,function,7,10,10,54,5.4,0,0,['path'],[None],"['""""']",65,[],[],0
ztemplate.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",266,[],['Model'],1
ztemplate.py:fit,fit,function,3,6,6,34,5.67,0,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",41,[],[],0
ztemplate.py:predict,predict,function,5,7,7,47,6.71,0,0,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",47,[],[],0
ztemplate.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],54,[],[],0
ztemplate.py:save,save,function,6,9,9,47,5.22,0,0,"['path', 'info']","[None, None]","['None', 'None']",59,[],[],0
ztemplate.py:load_model,load_model,function,7,10,10,54,5.4,0,0,['path'],[None],"['""""']",65,[],[],0
ztemplate.py:get_dataset_tuple,get_dataset_tuple,function,9,34,29,281,8.26,1,1,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",360,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
ztemplate.py:get_dataset,get_dataset,function,33,132,93,1285,9.73,2,5,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",380,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",7
ztemplate.py:test,test,function,41,195,150,1973,10.12,2,0,['config'],[None],"[""''""]",429,"['    """"""\n', '        Group of columns for the input model\n', '           cols_input_group = [ ]\n', '          for cols in cols_input_group,\n', '\n', '    :param config:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 'range', 'train_test_split', 'log', 'colg_input.items', 'test_helper', 'Model', 'fit', 'predict', 'save', 'load_model']",11
ztemplate.py:test_helper,test_helper,function,12,49,39,644,13.14,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",502,[],"['Model', 'log', 'fit', 'predict', 'save', 'load_model']",6
ztemplate.py:Model,Model,class,3,7,7,120,17.14,0,0,[],[],[],36,[],[],0
ztemplate.py:MY_MODEL_CLASS,MY_MODEL_CLASS,class,3,4,4,42,10.5,0,0,[],[],[],73,[],[],0
ztemplate.py:Model,Model,class,3,7,7,120,17.14,0,0,[],[],[],36,[],[],0
ztemplate.py:Model:__init__,Model:__init__,method,2,2,2,52,26.0,0,0,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",37,[],['MY_MODEL_CLASS'],1
ztemplate.py:MY_MODEL_CLASS:__init__,MY_MODEL_CLASS:__init__,method,2,2,2,22,11.0,0,0,['cpars'],[None],[None],74,"['        """"""\n', '        YOUR   custom MODEL definition\n', '\n', '        """"""\n']",[],0
ztemplate.py:Model:__init__,Model:__init__,method,2,2,2,52,26.0,0,0,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",37,[],['MY_MODEL_CLASS'],1
example/classifier_mlflow.py:global_pars_update,global_pars_update,function,22,59,44,1125,19.07,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",24,[],['print'],1
example/classifier_mlflow.py:titanic_lightgbm,titanic_lightgbm,function,12,200,126,1972,9.86,0,0,[],[],[],81,"['    """"""\n', '       ONE SINGLE DICT Contains all needed informations for\n', '       used for titanic classification task\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/classifier_mlflow.py:pd_col_myfun,pd_col_myfun,function,22,83,64,813,9.8,0,3,"['df', 'col', 'pars']","[None, None, None]","['None', 'None', '{}']",176,"['    """"""\n', '         Example of custom Processor\n', '    """"""\n']","['load', 'list', 'save', 'pars.get']",4
example/classifier_mlflow.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],265,[],[],0
example/test.py:os_get_function_name,os_get_function_name,function,4,4,4,47,11.75,0,0,[],[],[],23,[],['sys._getframe'],1
example/test.py:global_pars_update,global_pars_update,function,20,48,33,960,20.0,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",28,[],[],0
example/test.py:titanic1,titanic1,function,9,99,64,908,9.17,0,0,['path_model_out'],[None],"['""""']",89,"['    """"""\n', '       Contains all needed informations for Light GBM Classifier model,\n', '       used for titanic classification task\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
example/test.py:data_profile,data_profile,function,4,12,12,143,11.92,0,0,"['path_data_train', 'path_model', 'n_sample']","[None, None, None]","['""""', '""""', ' 5000']",188,[],['run_profile'],1
example/test.py:preprocess,preprocess,function,13,33,26,312,9.45,0,1,"['config', 'nsample']","[None, None]","['None', 'None']",198,[],"['globals', 'print', 'run_preprocess.run_preprocess']",3
example/test.py:train,train,function,13,32,25,276,8.62,0,1,"['config', 'nsample']","[None, None]","['None', 'None']",215,[],"['globals', 'print', 'run_train.run_train']",3
example/test.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],231,[],[],0
example/test.py:predict,predict,function,12,37,30,355,9.59,0,1,"['config', 'nsample']","[None, None]","['None', 'None']",239,[],"['globals', 'run_inference.run_predict']",2
example/test.py:run_all,run_all,function,5,5,5,53,10.6,0,0,[],[],[],257,[],"['data_profile', 'preprocess', 'train', 'check', 'predict']",5
example/test_automl.py:global_pars_update,global_pars_update,function,22,62,47,1124,18.13,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",49,[],['print'],1
example/test_automl.py:config1,config1,function,10,186,111,1613,8.67,0,0,[],[],[],106,"['    """"""      ONE SINGLE DICT Contains all needed informations for\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/test_features.py:global_pars_update,global_pars_update,function,21,55,40,1028,18.69,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",22,[],['print'],1
example/test_features.py:config1,config1,function,8,247,151,2499,10.12,0,0,['path_model_out'],[None],"['""""']",138,"['    """"""\n', '       Contains all needed informations\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
example/test_features.py:config2,config2,function,8,104,78,1072,10.31,0,0,['path_model_out'],[None],"['""""']",255,"['    """"""\n', '       Contains all needed informations\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
example/test_features.py:config4,config4,function,8,80,64,726,9.07,0,0,['path_model_out'],[None],"['""""']",322,"['    """"""\n', '\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
example/test_features.py:config9,config9,function,8,102,82,998,9.78,0,0,['path_model_out'],[None],"['""""']",403,"['    """"""\n', '       python  example/test_features.py  train       --nsample 500 --config config1\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
example/test_features.py:pd_col_amyfun,pd_col_amyfun,function,29,79,64,556,7.04,1,1,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",472,"['    """"""\n', '    Example of custom Processor\n', '    Used at prediction time\n', '        ""path_pipeline""  :\n', '\n', '    Training time :\n', '        ""path_features_store"" :  to store intermediate dataframe\n', '        ""path_pipeline_export"":  to store pipeline  for later usage\n', '\n', '    """"""\n']","['prepro_load', 'prepro', 'list', 'prepro_save']",4
example/test_features.py:config3,config3,function,8,80,64,726,9.07,0,0,['path_model_out'],[None],"['""""']",559,"['    """"""\n', '       Contains all needed informations\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
example/test_hyperopt.py:global_pars_update,global_pars_update,function,21,56,41,1028,18.36,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",22,[],['print'],1
example/test_hyperopt.py:post_process_fun,post_process_fun,function,6,77,55,687,8.92,0,0,['y)'],[' return  int(y)y):  return  int(y)path_model_out'],"['"""") :']",79,"['    """""" One big dict\n', '    """"""\n']",['os_get_function_name'],1
example/test_hyperopt.py:pre_process_fun,pre_process_fun,function,6,77,55,687,8.92,0,0,['y)'],['  return  int(y)path_model_out'],"['"""") :']",80,"['    """""" One big dict\n', '    """"""\n']",['os_get_function_name'],1
example/test_hyperopt.py:titanic1,titanic1,function,6,77,55,687,8.92,0,0,['path_model_out'],[None],"['""""']",82,"['    """""" One big dict\n', '    """"""\n']",['os_get_function_name'],1
example/test_hyperopt.py:hyperparam,hyperparam,function,29,145,121,1603,11.06,0,1,"['config_full', 'ntrials', 'n_sample', 'debug', 'path_output         ', 'path_optuna_storage ']","[None, None, None, None, None, None]","['""""', '2', '5000', '1', ' ""data/output/titanic1/""', "" 'data/output/optuna_hyper/optunadb.db'""]",142,"['    """"""\n', '        python test_hyperopt.py  hyperparam  --ntrials 2\n', '    """"""\n']","['hyperparam_wrapper', 'config_full.split', 'load_function_uri', 'mdict', 'objective_fun', 'log', 'run_train', 'run_hyper_optuna', 'os.makedirs', 'json.dump', 'open']",11
example/test_hyperopt.py:hyperparam_wrapper,hyperparam_wrapper,function,24,72,63,892,12.39,0,1,"['config_full', 'ntrials', 'n_sample', 'debug', 'path_output         ', 'path_optuna_storage ', 'metric_name', 'mdict_range']","[None, None, None, None, None, None, None, None]","['""""', '2', '5000', '1', ' ""data/output/titanic1/""', "" 'data/output/optuna_hyper/optunadb.db'"", ""'accuracy_score'"", 'None']",188,[],"['config_full.split', 'load_function_uri', 'mdict', 'objective_fun', 'log', 'run_train', 'run_hyper_optuna', 'os.makedirs', 'json.dump', 'open']",10
example/test_mkeras.py:global_pars_update,global_pars_update,function,22,62,47,1124,18.13,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",20,[],['print'],1
example/test_mkeras.py:config1,config1,function,8,177,99,1636,9.24,0,0,[],[],[],77,"['    """"""\n', '       ONE SINGLE DICT Contains all needed informations for  used for titanic classification task\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/zfraud.py:global_pars_update,global_pars_update,function,19,53,38,1016,19.17,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",37,[],['print'],1
source/prepro.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],15,[],"['print', 'log2', 'log3']",3
source/prepro.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],18,[],['print'],1
source/prepro.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],21,[],['print'],1
source/prepro.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],24,[],['os.makedirs'],1
source/prepro.py:log4,log4,function,3,18,14,142,7.89,0,2,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '1']",37,[],"['print', 'log4_pd', 'df.head', 'df.reset_index']",4
source/prepro.py:log4_pd,log4_pd,function,2,9,9,76,8.44,0,1,"['name', 'df', '*s']","[None, None, None]","[None, None, None]",41,[],"['print', 'df.head', 'df.reset_index']",3
source/prepro.py:_pd_colnum,_pd_colnum,function,10,19,17,185,9.74,2,0,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",45,[],['_pd_colnum_fill_na_median'],1
source/prepro.py:_pd_colnum_fill_na_median,_pd_colnum_fill_na_median,function,3,6,6,78,13.0,1,0,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",51,[],[],0
source/prepro.py:prepro_load,prepro_load,function,7,27,17,283,10.48,0,1,"['prefix', 'pars']","[None, None]","[None, None]",60,"['    """"""  Load previously savec preprocessors\n', '    :param prefix:\n', '    :param pars:\n', '    :return:\n', '    """"""\n']",['load'],1
source/prepro.py:prepro_save,prepro_save,function,17,58,44,519,8.95,1,2,"['prefix', 'pars', 'df_new', 'cols_new', 'prepro']","[None, None, None, None, None]","[None, None, None, None, None]",77,"['    """"""  Save preprocessors and export\n', '    :param prefix:\n', '    :param pars:\n', '    :param df_new:\n', '    :param cols_new:\n', '    :param prepro:\n', '    :param pars_prepro:\n', '    :return:\n', '    """"""\n']","['pars.items', 'isinstance', 'save', 'pars.get']",4
source/prepro.py:pd_col_atemplate,pd_col_atemplate,function,23,61,51,443,7.26,0,1,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",107,"['    """"""\n', '    Example of custom Processor\n', '    Used at prediction time\n', '        ""path_pipeline""  :\n', '\n', '    Training time :\n', '        ""path_features_store"" :  to store intermediate dataframe\n', '        ""path_pipeline_export"":  to store pipeline  for later usage\n', '\n', '    """"""\n']","['prepro_load', 'prepro', 'list', 'prepro_save']",4
source/prepro.py:pd_coly_clean,pd_coly_clean,function,12,25,23,282,11.28,0,1,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",148,[],"['log', 'pars.get', 'y_norm_fun', 'save_features']",4
source/prepro.py:pd_coly,pd_coly,function,28,105,67,1010,9.62,0,3,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",163,[],"['log', 'pars.get', 'y_norm_fun', 'save_features', 'pd_coly', 'isfloat', 'float', 'log2']",8
source/prepro.py:pd_colnum_normalize,pd_colnum_normalize,function,24,109,85,1079,9.9,0,3,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",200,"['    """""" Float num INTO [0,1]\n', ""      'quantile_cutoff', 'quantile_cutoff_2', 'minmax'      \n"", ""      'name': 'fillna', 'na_val' : 0.0 \n"", '\n', '    """"""\n']","['log2', 'load', 'pd_normalize_fun', 'log3', 'dfnum_norm.head', 'dfnum_norm.isna', 'save_features', 'save', 'pars.get']",9
source/prepro.py:pd_colnum_quantile_norm,pd_colnum_quantile_norm,function,47,235,139,2343,9.97,1,7,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",236,"['  """"""\n', '     colnum normalization by quantile\n', '  """"""\n']","['load', 'pars2.get', 'df.quantile', 'pd.DataFrame', 'len', 'np.where', 'list', 'save_features', 'save', 'pars.get']",10
source/prepro.py:pd_colnum_bin,pd_colnum_bin,function,29,152,106,1830,12.04,1,2,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",319,"['    """"""  float column into  binned columns\n', '    :param df:\n', '    :param col:\n', '    :param pars:\n', '    :return:\n', '    """"""\n']","['pars.get', 'load', 'log2', 'pd_colnum_tocat', 'log3', 'list', 'save_features', 'save', 'pd_colnum_binto_onehot', 'isinstance', 'log', 'pd_col_to_onehot']",12
source/prepro.py:pd_colnum_binto_onehot,pd_colnum_binto_onehot,function,21,64,58,800,12.5,0,1,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",361,[],"['isinstance', 'pars.get', 'load', 'log', 'pd_col_to_onehot', 'log2', 'save_features', 'save']",8
source/prepro.py:pd_colcat_to_onehot,pd_colcat_to_onehot,function,18,86,58,906,10.53,0,4,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",391,"['    """"""\n', '\n', '    """"""\n']","['log', 'isinstance', 'len', 'load', 'util_feature.pd_col_to_onehot', 'log3', 'save_features', 'save']",8
source/prepro.py:pd_colcat_bin,pd_colcat_bin,function,18,73,61,837,11.47,0,2,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",434,[],"['pars.get', 'load', 'isinstance', 'log', 'util_feature.pd_colcat_toint', 'list', 'util_feature.pd_colcat_mapping', 'log2', 'save_features', 'save']",10
source/prepro.py:pd_colcross,pd_colcross,function,37,169,131,2111,12.49,2,3,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",467,"['    """"""\n', '     cross_feature_new =  feat1 X feat2  (pair feature)\n', '\n', '    """"""\n']","['log', 'dfnum_hot.drop_duplicates', 'dfcat_hot.reset_index', 'set', 'log4', 'log4_pd', 'copy.deepcopy', 'load', 'len', 'list', 'colcross_single_onehot_select.append', 'sorted', 'pd_feature_generate_cross', 'save_features', 'save']",15
source/prepro.py:pd_coldate,pd_coldate,function,17,56,49,497,8.88,1,1,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",545,[],"['log', 'util_date.pd_datestring_split', 'pd.concat', 'save_features', 'list']",5
source/prepro.py:pd_colcat_encoder_generic,pd_colcat_encoder_generic,function,28,113,82,1222,10.81,0,2,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",568,"['    """"""\n', '        Create a Class or decorator\n', '        https://pypi.org/project/category-encoders/\n', '        encoder = ce.BackwardDifferenceEncoder(cols=[...])\n', '        encoder = ce.BaseNEncoder(cols=[...])\n', '        encoder = ce.BinaryEncoder(cols=[...])\n', '        encoder = ce.CatBoostEncoder(cols=[...])\n', '        encoder = ce.CountEncoder(cols=[...])\n', '        encoder = ce.GLMMEncoder(cols=[...])\n', '        encoder = ce.HashingEncoder(cols=[...])\n', '        encoder = ce.HelmertEncoder(cols=[...])\n', '        encoder = ce.JamesSteinEncoder(cols=[...])\n', '        encoder = ce.LeaveOneOutEncoder(cols=[...])\n', '        encoder = ce.MEstimateEncoder(cols=[...])\n', '        encoder = ce.OneHotEncoder(cols=[...])\n', '        encoder = ce.OrdinalEncoder(cols=[...])\n', '        encoder = ce.SumEncoder(cols=[...])\n', '        encoder = ce.PolynomialEncoder(cols=[...])\n', '        encoder = ce.TargetEncoder(cols=[...])\n', '        encoder = ce.WOEEncoder(cols=[...])\n', '    """"""\n']","['load', 'pars.get', 'model_class', 'modelx.fit_transform', 'list', 'save_features', 'save']",7
source/prepro.py:pd_colcat_minhash,pd_colcat_minhash,function,20,84,65,976,11.62,0,2,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",630,"['    """"""\n', '       MinHash Algo for category\n', '       https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087\n', '\n', '    """"""\n']","['load', 'log', 'util_text.pd_coltext_minhash', 'list', 'log2', 'save_features', 'save']",7
source/prepro.py:os_convert_topython_code,os_convert_topython_code,function,0,1,1,4,4.0,0,0,['txt'],[None],[None],668,[],[],0
source/prepro.py:save_json,save_json,function,5,10,10,58,5.8,0,0,"['js', 'pfile', 'mode']","[None, None, None]","[None, None, ""'a'""]",683,[],"['open', 'json.dump']",2
source/prepro.py:pd_col_genetic_transform,pd_col_genetic_transform,function,63,244,182,2405,9.86,3,3,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",689,"['    """"""\n', '        Find Symbolic formulae for faeture engineering\n', '\n', '    """"""\n']","['squaree', 'make_function', 'pars.get', 'load', 'SymbolicTransformer', 'pars_genetic.get', 'gp.fit', 'gp.transform', 'random.randint', 'range', 'pd.DataFrame', 'str', 'formula.split', 'enumerate', 'log', 'save_features', 'save', 'save_json']",18
source/prepro.py:test,test,function,14,43,38,296,6.88,1,0,[],[],[],777,"['    """"""\n', '      python example/prepro.py test\n', '    :return:\n', '    """"""\n']","['test_get_classification_data', 'list', 'globals', 'myfun', 'log']",5
source/prepro_rec.py:_preprocess_criteo,_preprocess_criteo,function,33,112,62,1047,9.35,2,1,"['df', '**kw']","[None, None]","[None, None]",30,[],"['kw.get', 'str', 'range', 'MinMaxScaler', 'mms.fit_transform', 'LabelEncoder', 'lbe.fit_transform', 'enumerate', 'train_test_split']",9
source/prepro_rec.py:_preprocess_movielens,_preprocess_movielens,function,53,244,135,2546,10.43,5,4,"['df', '**kw']","[None, None]","[None, None]",66,[],"['kw.get', 'LabelEncoder', 'lbe.fit_transform', 'train_test_split', 'split', 'x.split', 'len', 'list', 'np.array', 'max', 'pad_sequences', 'vocabulary_size=len', 'SparseFeat', 'VarLenSparseFeat', 'get_feature_names']",15
source/prepro_text.py:log,log,function,6,26,20,183,7.04,0,1,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '1']",27,[],"['print', 'logs', 'log_pd', 'df.head']",4
source/prepro_text.py:logs,logs,function,2,4,4,30,7.5,0,1,['*s'],[None],[None],33,[],['print'],1
source/prepro_text.py:log_pd,log_pd,function,2,6,6,47,7.83,0,0,"['df', '*s', 'n', 'm']","[None, None, None, None]","[None, None, '0', '1']",38,[],"['print', 'df.head']",2
source/prepro_text.py:pd_coltext_clean,pd_coltext_clean,function,21,73,51,748,10.25,1,1,"['df', 'col', 'stopwords', 'pars']","[None, None, None, None]","[None, None, ' None', 'None']",48,[],"['pars.get', 'df.fillna', 'log', 'coltext_remove_stopwords', 'text.split', 't.strip', 'str', 'x.translate', 're.sub']",9
source/prepro_text.py:pd_coltext_wordfreq,pd_coltext_wordfreq,function,13,33,26,379,11.48,1,1,"['df', 'col', 'stopwords', 'ntoken']","[None, None, None, None]","[None, None, None, '100']",79,"['    """"""\n', '    :param df:\n', '    :param coltext:  text where word frequency should be extracted\n', '    :param nb_to_show:\n', '    :return:\n', '    """"""\n']","['logs', 'pd.value_counts', 'coltext_freq.sort_values', 'log']",4
source/prepro_text.py:nlp_get_stopwords,nlp_get_stopwords,function,10,42,30,301,7.17,1,0,[],[],[],99,[],"['json.load', 'stopwords.sort', 'print', 'set']",4
source/prepro_text.py:pd_coltext,pd_coltext,function,32,110,73,1181,10.74,2,2,"['df', 'col', 'stopwords', 'pars']","[None, None, None, None]","[None, None, ' None', 'None']",112,"['    """"""\n', '    df : Datframe\n', '    col : list of columns\n', '    pars : dict of pars\n', '\n', '    """"""\n']","['pars.get', 'df.fillna', 'log', 'coltext_remove_stopwords', 'text.split', 't.strip', 'str', 'x.translate', 're.sub', 'pd_coltext_wordfreq', 'logs', 'pd.value_counts', 'coltext_freq.sort_values']",13
source/prepro_text.py:pd_coltext_universal_google,pd_coltext_universal_google,function,38,136,98,1163,8.55,2,3,"['df', 'col', 'pars']","[None, None, None]","[None, None, '{}']",182,"['    """"""\n', '     # Universal sentence encoding from Tensorflow\n', '       Text ---> Vectors\n', '    from source.preprocessors import  pd_coltext_universal_google\n', '    https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n', '\n', '    #latest Tensorflow that supports sentencepiece is 1.13.1\n', '    !pip uninstall --quiet --yes tensorflow\n', '    !pip install --quiet tensorflow-gpu==1.13.1\n', '    !pip install --quiet tensorflow-hub\n', '    pip install --quiet tf-sentencepiece, simpleneighbors\n', '    !pip install --quiet simpleneighbors\n', '\n', '    # df : dataframe\n', '    # col : list of text colnum names\n', '    pars\n', '    """"""\n']","['load', 'pars.get', 'hub.load', 'pd.isnull', 'model', 'tf.reshape', 'X.append', 'pd.DataFrame', 'str', 'range', 'len', 'pd.concat', 'list', 'save_features', 'save']",15
source/prepro_tseries.py:log,log,function,5,22,15,123,5.59,0,1,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '0']",39,[],"['print', 'logd']",2
source/prepro_tseries.py:logd,logd,function,4,11,11,55,5.0,0,1,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '0']",45,[],['print'],1
source/prepro_tseries.py:pd_prepro_custom,pd_prepro_custom,function,56,202,132,1850,9.16,2,1,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",66,"['    """"""\n', '    Example of custom Processor Combining\n', '    Usage :\n', '    ,{""uri"":  THIS_FILEPATH + ""::pd_dsa2_custom"",   ""pars"": {\'coldate\': \'date\'}, ""cols_family"": ""coldate"",   ""cols_out"": ""coldate_features1"",  ""type"": """" },\n', '\n', '    """"""\n']","['prepro_load', 'df.set_index', 'pd_ts_date', 'pd_ts_rolling', 'pd.concat', 'list', 'prepro_save', 'pd_prepro_custom2', 'log', 'df1.join', 'pd_ts_groupby', 'df1.set_index', 'pars.get', 'pd_ts_deltapy_generic']",14
source/prepro_tseries.py:pd_prepro_custom2,pd_prepro_custom2,function,37,97,74,960,9.9,1,0,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",111,"['    """"""   Generic template for feature generation\n', ""       'colnum' : ['sales1' 'units' ]\n"", ""      'pars_function_list' :  [\n"", '       { \'name\': \'deltapy.transform::robust_scaler\',                 \'pars\': {\'drop\':[""Close_1""]} },\n', '       { \'name\': \'deltapy.transform::standard_scaler\',               \'pars\': {\'drop\':[""Close_1""]} },\n', '       ]e\n', '\n', '    :param df:\n', '    :param col:\n', '    :param pars:\n', '    :return:\n', '    """"""\n']","['prepro_load', 'log', 'pd_ts_date', 'list', 'df1.join', 'pd_ts_groupby', 'df1.set_index', 'pars.get', 'pd_ts_deltapy_generic', 'prepro_save']",10
source/prepro_tseries.py:pd_ts_date,pd_ts_date,function,21,116,71,852,7.34,1,9,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",173,[],"['isinstance', 'pars.get', 'pd.DataFrame', 'pd.to_datetime', 'pd.concat', 'list']",6
source/prepro_tseries.py:pd_ts_groupby,pd_ts_groupby,function,14,63,43,507,8.05,3,0,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",203,"['    """"""\n', '        Generates features by groupBy over category\n', '        groupby(shop_id) ---> per each date,  mean, max, min ...\n', '        groupby(zone_id) ---> per each date,  mean, max, min ...\n', '\n', '        groupby(key_lis).agg( col_stat )\n', '\n', '    """"""\n']","['pars.get', 'list', 'df.groupby', 'set', 'dfall.join']",5
source/prepro_tseries.py:pd_ts_onehot,pd_ts_onehot,function,3,4,3,50,12.5,0,0,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",227,"['    """"""\n', '       category to oneHot (ie week, weekday, shop, ..)\n', '    :param df:\n', '    :param col:\n', '    :param pars:\n', '    :return:\n', '    """"""\n']",['pd.get_dummies'],1
source/prepro_tseries.py:pd_ts_autoregressive,pd_ts_autoregressive,function,0,1,1,4,4.0,0,0,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",241,"['    """"""\n', '        Using past data for same column ; Rolling\n', '        item_id sales -->  per each date, Moving Average, Min, Max over 1month, ...\n', '        shop_id sales -->  per each date, Moving Average, Min, Max over 1month, ...\n', '    """"""\n']",[],0
source/prepro_tseries.py:pd_ts_rolling,pd_ts_rolling,function,26,81,57,992,12.25,4,0,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",251,"['    """"""\n', '      Rolling statistics\n', '\n', '    """"""\n']","['pars.get', 'print', 'str', 'df.groupby', 'x.shift', 'col_new.append']",6
source/prepro_tseries.py:pd_ts_lag,pd_ts_lag,function,17,38,31,362,9.53,3,0,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",295,[],"['pars.get', 'range', 'col_new.append', 'str', 'df.groupby', 'x.shift']",6
source/prepro_tseries.py:pd_ts_difference,pd_ts_difference,function,9,14,13,86,6.14,1,0,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",314,[],['pars.get'],1
source/prepro_tseries.py:pd_ts_tsfresh_features,pd_ts_tsfresh_features,function,19,31,29,396,12.77,0,1,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",364,[],"['range', 'len', 'pars.get', 'pars.keys', 'extract_features']",5
source/prepro_tseries.py:pd_ts_deltapy_generic,pd_ts_deltapy_generic,function,17,48,39,447,9.31,0,2,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",377,"['    """"""\n', '       { \'name\': \'deltapy.transform::robust_scaler\',                 \'pars\': {\'drop\':[""Close_1""]} },\n', '\n', '    """"""\n']","['pars.get', 'dfin.fillna', 'dfin.min', 'load_function_uri', 'model', 'model_name.replace']",6
source/prepro_tseries.py:test_get_sampledata,test_get_sampledata,function,11,15,12,191,12.73,0,0,"['url=""https']",[''],"['""https://github.com/firmai/random-assets-two/raw/master/numpy/tsla.csv""']",416,[],"['pd.read_csv', 'pd.option_context', 'df.dropna', 'pd.to_datetime', 'df.set_index']",5
source/prepro_tseries.py:test_deltapy_all,test_deltapy_all,function,80,194,137,3583,18.47,0,0,[],[],[],426,[],"['test_get_sampledata', 'df.head', 'transform.robust_scaler', 'transform.standard_scaler', 'transform.fast_fracdiff', 'transform.operations', 'transform.triple_exponential_smoothing', 'transform.naive_dec', 'transform.bkb', 'transform.butter_lowpass_filter', 'transform.instantaneous_phases', 'transform.kalman_feat', 'transform.perd_feat', 'transform.fft_feat', 'transform.harmonicradar_cw', 'transform.saw', 'transform.modify', 'transform.multiple_rolling', 'transform.multiple_lags', 'transform.prophet_feat', 'interact.lowess', 'interact.autoregression', 'interact.muldiv', 'interact.decision_tree_disc', 'interact.quantile_normalize', 'interact.tech', 'interact.genetic_feat', 'mapper.pca_feature', 'mapper.cross_lag', 'mapper.a_chi', 'df.min', 'mapper.encoder_dataset', 'mapper.lle_feat', 'mapper.feature_agg', 'mapper.neigh_feat', 'extract.abs_energy', 'extract.cid_ce', 'extract.mean_abs_change', 'extract.mean_second_derivative_central', 'extract.variance_larger_than_standard_deviation', 'extract.symmetry_looking', 'extract.has_duplicate_max', 'extract.partial_autocorrelation', 'extract.augmented_dickey_fuller', 'extract.gskew', 'extract.stetson_mean', 'extract.length', 'extract.count_above_mean', 'extract.longest_strike_below_mean', 'extract.wozniak', 'extract.last_location_of_maximum', 'extract.fft_coefficient', 'extract.ar_coefficient', 'extract.index_mass_quantile', 'extract.number_cwt_peaks', 'extract.spkt_welch_density', 'extract.linear_trend_timewise', 'extract.c3', 'extract.binned_entropy', 'extract.svd_entropy', 'extract.hjorth_complexity', 'extract.max_langevin_fixed_point', 'extract.percent_amplitude', 'extract.cad_prob', 'extract.zero_crossing_derivative', 'extract.detrended_fluctuation_analysis', 'extract.fisher_information', 'extract.higuchi_fractal_dimension', 'extract.petrosian_fractal_dimension', 'extract.hurst_exponent', 'extract.largest_lyauponov_exponent', 'extract.whelch_method', 'extract.find_freq', 'extract.flux_perc', 'extract.range_cum_s', 'extract.structure_func', 'extract.kurtosis', 'extract.stetson_k']",78
source/prepro_tseries.py:test_prepro_v1,test_prepro_v1,function,8,15,14,145,9.67,0,0,[],[],[],529,[],"['test_get_sampledata', 'pd_ts_date', 'pd_ts_onehot', 'pd_ts_difference']",4
source/prepro_tseries.py:test_deltapy_get_method,test_deltapy_get_method,function,2,370,135,4836,13.07,0,0,['df'],[None],[None],560,[],[],0
source/prepro_tseries.py:test_deltapy_all2,test_deltapy_all2,function,12,35,30,374,10.69,1,1,[],[],[],647,[],"['test_get_sampledata', 'test_deltapy_get_method', 'copy.deepcopy', 'df_input.min', 'pd_ts_deltapy_generic']",5
source/prepro_tseries.py:m5_dataset,m5_dataset,function,122,563,373,7204,12.8,6,1,[],[],[],669,"['    """"""\n', '     https://www.kaggle.com/ratan123/m5-forecasting-lightgbm-with-timeseries-splits\n', '\n', '\n', '    """"""\n']","['read_df', 'print', 'pd.read_csv', 'reduce_mem_usage', 'melt_and_merge', 'pd.melt', 'test1.merge', 'test2.merge', 'pd.concat', 'calendar.drop', 'pd.merge', 'df.drop', 'df.merge', 'gc.collect', 'transform', 'preprocessing.LabelEncoder', 'encoder.fit_transform', 'simple_fe', 'df.groupby', 'x.shift', 'x.rolling', 'pd.to_datetime', 'x.sort_values', 'test.sort_values', 'TimeSeriesSplit', 'folds.split', 'np.zeros', 'pd.dfFrame', 'enumerate', 'lgb.dfset', 'lgb.train', 'clf.feature_importance', 'clf.predict', 'np.sqrt', 'mean_score.append']",35
source/run_bentoml.py:log,log,function,1,1,1,8,8.0,0,0,['*s'],[None],[None],69,[],['print'],1
source/run_bentoml.py:load_model_dsa,load_model_dsa,function,21,40,38,604,15.1,0,0,"['dir_model', 'model_uri=""source/models/model_sklearn.py']","[None, '']","[""''"", '""source/models/model_sklearn.py::LightGBM""']",73,"['    """"""\n', '       Return the model loaded from disk\n', '    :param dir_model:\n', '    :param model_uri:\n', '    :return:\n', '    """"""\n']","['model_dict_load', 'model_uri.split', 'log', 'map_model', 'modelx.reset', 'modelx.init', 'modelx.load_model']",7
source/run_bentoml.py:bento_save,bento_save,function,31,53,48,579,10.92,0,1,[],[],[],320,[],"['saveToBento', 'utils.load_model', 'models.resnet18', 'nn.Linear', 'AntOrBeeClassifier', 'bento_svc.pack', 'bento_svc.save', 'print', 'argparse.ArgumentParser', 'parser.add_argument', 'parser.parse_args']",11
source/run_bentoml.py:dsaModelArtifact,dsaModelArtifact,class,20,54,43,682,12.63,0,0,[],[],[],120,[],[],0
source/run_bentoml.py:dsaModelArtifact:__init__,dsaModelArtifact:__init__,method,4,6,6,104,17.33,0,0,"['self', 'name', 'pickle_extension']","[None, None, None]","[None, None, '"".pkl""']",157,[],['super'],1
source/run_bentoml.py:dsaModelArtifact:_model_file_path,dsaModelArtifact:_model_file_path,method,2,4,4,62,15.5,0,0,"['self', 'base_path']","[None, None]","[None, None]",163,[],[],0
source/run_bentoml.py:dsaModelArtifact:pack,dsaModelArtifact:pack,method,6,8,8,130,16.25,0,0,"['self', 'sklearn_model', 'metadata=None)', 'path)']","[None, None, '', '']","[None, None, 'None):  # pylint:disable=arguments-differself._model = sklearn_modelreturn selfself', None]",166,"['        """"""\n', '         \n', '        """"""\n']","['self._model_file_path', 'load_model_dsa', 'self.pack']",3
source/run_bentoml.py:dsaModelArtifact:load,dsaModelArtifact:load,method,6,8,8,130,16.25,0,0,"['self', 'path']","[None, None]","[None, None]",170,"['        """"""\n', '         \n', '        """"""\n']","['self._model_file_path', 'load_model_dsa', 'self.pack']",3
source/run_bentoml.py:dsaModelArtifact:get,dsaModelArtifact:get,method,2,2,2,17,8.5,0,0,['self'],[None],[None],178,[],[],0
source/run_bentoml.py:dsaModelArtifact:save,dsaModelArtifact:save,method,1,2,2,34,17.0,0,0,"['self', 'dst', 'stats']","[None, None, None]","[None, None, 'None']",181,[],[],0
source/run_bentoml.py:dsaModelArtifact:set_dependencies,dsaModelArtifact:set_dependencies,method,1,1,1,38,38.0,0,0,"['self', 'env']","[None, None]","[None, None]",185,[],['env.add_pip_packages'],1
source/run_feature_profile.py:log,log,function,3,11,10,65,5.91,0,0,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '0']",23,[],['print'],1
source/run_feature_profile.py:run_profile,run_profile,function,41,179,133,1645,9.19,0,1,"['path_data', 'path_output', 'n_sample']","[None, None, None]","['None', '""data/out/ztmp/""', '5000']",32,"['    """"""\n', '      Use folder , filename are fixed.\n', '    """"""\n']","['os.makedirs', 'log', 'json.load', 'cols_group.get', 'pd_read_file', 'list', 'open', 'json.dump', 'fp.write', 'pd.read_csv', 'pd.merge', 'df.profile_report', 'profile.to_file']",13
source/run_hyperopt.py:log,log,function,1,1,1,9,9.0,0,0,['*s'],[None],[None],22,[],['print'],1
source/run_hyperopt.py:run_hyper_optuna,run_hyper_optuna,function,52,167,124,1588,9.51,2,6,"['obj_fun', 'pars_dict_init', 'pars_dict_range', 'engine_pars', 'ntrials']","[None, None, None, None, None]","[None, None, None, None, '3']",27,"['    """"""\n', ""      pars_dict_init =  {  'boosting_type':'gbdt',\n"", ""\t\t\t\t\t\t'importance_type':'split', 'learning_rate':0.001, 'max_depth':10,\n"", ""\t\t\t\t\t\t'n_estimators': 50, 'n_jobs':-1, 'num_leaves':31 }\n"", ""\t  pars_dict_range =   {  'boosting_type':  ( 'categorical',  ['gbdt', 'gbdt']      ) ,\n"", ""\t\t\t\t\t\t 'importance_type':'split',\n"", ""\t\t\t\t\t\t 'learning_rate':  ('log_uniform' , 0.001, 0.1,  ),\n"", ""\t\t\t\t\t\t 'max_depth':      ('int',  1, 10, 'uniform')\n"", ""\t\t\t\t\t\t 'n_estimators':   ('int', 0, 10,  'uniform' )\n"", ""\t\t\t\t\t\t 'n_jobs':-1,\n"", ""\t\t\t\t\t\t 'num_leaves':31 }\n"", '      obj_fun(pars_dict) :  Objective function\n', '      engine_pars :    {   }  optuna parameters\n', '      \n', '      \n', '      API interface integration :\n', '           https://optuna.readthedocs.io/en/stable/reference/generated/optuna.storages.RDBStorage.html\n', '      \n', '    """"""\n']","['print', 'parse_dict', 'copy.deepcopy', 'mdict.items', 'isinstance', 'trial.suggest_loguniform', 'trial.suggest_int', 'trial.suggest_uniform', 'trial.suggest_categorical', 'trial.suggest_discrete_uniform', 'log', 'merge_dict', 'src.items', 'dst.setdefault', 'obj_custom', 'obj_fun', 'engine_pars.get', 'optuna.load_study', 'optuna.create_study', 'study.optimize']",20
source/run_hyperopt.py:test_hyper,test_hyper,function,51,162,108,2016,12.44,0,0,[],[],[],126,[],"['objective1', 'run_train.run_train', 'run_hyper_optuna', 'log', 'test_hyper3', 'test_hyper2', 'objective', 'model=RandomForestClassifier', 'model.fit', 'result_p=run_hyper_optuna']",10
source/run_hyperopt.py:test_hyper3,test_hyper3,function,17,42,40,337,8.02,0,0,[],[],[],155,[],"['objective1', 'run_hyper_optuna', 'log']",3
source/run_hyperopt.py:test_hyper2,test_hyper2,function,35,88,63,1376,15.64,0,0,[],[],[],179,[],"['objective', 'model=RandomForestClassifier', 'model.fit', 'objective1', 'result_p=run_hyper_optuna']",5
source/run_hyperopt.py:eval_dict,eval_dict,function,21,57,43,463,8.12,1,3,"['src', 'dst']","[None, None]","[None, '{}']",235,[],"['src.items', 'isinstance', 'dst.setdefault', 'eval_dict', 'key.split', 'pd.read_csv', 'pd.read_parquet', 'log', 'optuna.create_study']",9
source/run_inference.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],17,[],"['print', 'log2', 'log3']",3
source/run_inference.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],20,[],['print'],1
source/run_inference.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],23,[],['print'],1
source/run_inference.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],26,[],['os.makedirs'],1
source/run_inference.py:model_dict_load,model_dict_load,function,6,27,23,255,9.44,0,2,"['model_dict', 'config_path', 'config_name', 'verbose']","[None, None, None, None]","[None, None, None, 'True']",39,"['    """"""model_dict_load\n', '    Args:\n', '        model_dict ([type]): [description]\n', '        config_path ([type]): [description]\n', '        config_name ([type]): [description]\n', '        verbose (bool, optional): [description]. Defaults to True.\n', '\n', '    Returns:\n', '        [type]: [description]\n', '    """"""\n']","['log', 'load_function_uri', 'model_dict_fun']",3
source/run_inference.py:map_model,map_model,function,10,38,24,432,11.37,0,2,['model_name'],[None],[None],58,"['    """""" Get the Class of the object stored in source/models/\n', '    :param model_name:   model_sklearn\n', '    :return: model module\n', '    """"""\n']","['importlib.import_module', 'model_name.split']",2
source/run_inference.py:predict,predict,function,16,88,63,848,9.64,0,0,"['model_name', 'path_model', 'dfX', 'cols_family', 'model_dict']","[None, None, None, None, None]","[None, None, None, None, None]",91,"['    """"""\n', '    Arguments:\n', '        model_name {[str]} -- [description]\n', '        path_model {[str]} -- [description]\n', '        dfX {[DataFrame]} -- [description]\n', '        cols_family {[dict]} -- [description]\n', '\n', '    Returns: ypred\n', '        [numpy.array] -- [vector of prediction]\n', '    """"""\n']","['log', 'map_model', 'modelx.reset', 'log2', 'modelx.load_model', 'load', 'dfX.reindex', 'modelx.predict', 'str']",9
source/run_inference.py:run_predict,run_predict,function,41,162,122,2202,13.59,1,1,"['config_name', 'config_path', 'n_sample', 'path_data', 'path_output', 'pars', 'model_dict']","[None, None, None, None, None, None, None]","[None, None, '-1', 'None', 'None', '{}', 'None']",130,[],"['log', 'model_dict_load', 'load', 'load_dataset', 'preprocess', 'list', 'predict', 'post_process_fun', 'os.makedirs', 'df.to_csv']",10
source/run_inference.py:run_data_check,run_data_check,function,33,59,53,806,13.66,0,0,"['path_data', 'path_data_ref', 'path_model', 'path_output', 'sample_ratio']","[None, None, None, None, None]","[None, None, None, None, '0.5']",194,"['    """"""\n', '     Calcualata Dataset Shift before prediction.\n', '    """"""\n']","['os.makedirs', 'load', 'load_dataset', 'preprocess', 'int', 'len', 'pd_stat_dataset_shift', 'metrics_psi.to_csv', 'log']",9
source/run_inpection.py:log,log,function,3,11,10,65,5.91,0,0,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '0']",20,[],['print'],1
source/run_inpection.py:save_features,save_features,function,3,11,11,109,9.91,0,1,"['df', 'name', 'path']","[None, None, None]","[None, None, None]",27,[],"['os.makedirs', 'df.to_parquet']",2
source/run_inpection.py:model_dict_load,model_dict_load,function,6,27,23,255,9.44,0,2,"['model_dict', 'config_path', 'config_name', 'verbose']","[None, None, None, None]","[None, None, None, 'True']",33,"['    """"""\n', '       load the model dict from the python config file.\n', '    :param model_dict:\n', '    :param config_path:\n', '    :param config_name:\n', '    :param verbose:\n', '    :return:\n', '    """"""\n']","['log', 'load_function_uri', 'model_dict_fun']",3
source/run_mlflow.py:register,register,function,14,27,27,488,18.07,0,0,"['run_name', 'params', 'metrics', 'signature', 'model_class', 'tracking_uri= ""sqlite']","[None, None, None, None, None, '']","[None, None, None, None, None, ' ""sqlite:///local.db""']",8,"['    """"""\n', '    :run_name: Name of model\n', '    :log_params: dict with model params\n', '    :metrics: dict with model evaluation metrics\n', '    :signature: Its a signature that describes model input and output Schema\n', '    :model_class: Type of class model\n', '    :return:\n', '    """"""\n']","['mlflow.set_tracking_uri', 'mlflow.start_run', 'load', 'mlflow.log_params', 'metrics.apply', 'mlflow.log_metric', 'log', 'mlflow.end_run']",8
source/run_preprocess.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],18,[],"['print', 'log2', 'log3']",3
source/run_preprocess.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],21,[],['print'],1
source/run_preprocess.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],24,[],['print'],1
source/run_preprocess.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],27,[],['os.makedirs'],1
source/run_preprocess.py:log_pd,log_pd,function,2,6,6,47,7.83,0,0,"['df', '*s', 'n', 'm']","[None, None, None, None]","[None, None, '0', '1']",44,[],"['print', 'df.head']",2
source/run_preprocess.py:save_features,save_features,function,7,30,26,275,9.17,0,2,"['df', 'name', 'path']","[None, None, None]","[None, None, 'None']",53,"['    """""" Save dataframe on disk\n', '    :param df:\n', '    :param name:\n', '    :param path:\n', '    :return:\n', '    """"""\n']","['os.makedirs', 'isinstance', 'log', 'list', 'df0.to_parquet']",5
source/run_preprocess.py:load_features,load_features,function,3,10,9,110,11.0,0,0,"['name', 'path']","[None, None]","[None, None]",73,[],"['pd.read_parquet', 'log']",2
source/run_preprocess.py:model_dict_load,model_dict_load,function,6,27,23,255,9.44,0,2,"['model_dict', 'config_path', 'config_name', 'verbose']","[None, None, None, None]","[None, None, None, 'True']",81,"['    """"""\n', '    :param model_dict:\n', '    :param config_path:\n', '    :param config_name:\n', '    :param verbose:\n', '    :return:\n', '    """"""\n']","['log', 'load_function_uri', 'model_dict_fun']",3
source/run_preprocess.py:preprocess,preprocess,function,90,740,319,7520,10.16,11,20,"['path_train_X', 'path_train_y', 'path_pipeline_export', 'cols_group', 'n_sample', 'preprocess_pars', 'path_features_store']","[None, None, None, None, None, None, None]","['""""', '""""', '""""', 'None', '5000', '{}', 'None']",99,"['    """"""\n', '      Used for trainiing only\n', '      Save params on disk\n', '\n', '    :param path_train_X:\n', '    :param path_train_y:\n', '    :param path_pipeline_export:\n', '    :param cols_group:\n', '    :param n_sample:\n', '    :param preprocess_pars:\n', '    :param path_features_store:\n', '    :return:\n', '    """"""\n']","['log', 'os.makedirs', 'save', 'preprocess_pars.get', 'task.get', 'load_dataset', 'len', 'load_function_uri', 'pipe_fun', 'list', 'pipe_i.get', 'log3', 'save_features', 'cols_group.get', 'cols_family_all.get', 'pd.concat', 'dfi_all.keys', 'colX.remove', 'preprocess_inference', 'cols_family_full.get', 'preprocess_load', 'pd.read_parquet', 'dfXy.join', 'load']",24
source/run_preprocess.py:preprocess_inference,preprocess_inference,function,68,298,196,2819,9.46,5,9,"['df', 'path_pipeline', 'preprocess_pars', 'cols_group']","[None, None, None, None]","[None, '""data/pipeline/pipe_01/""', '{}', 'None']",242,"['    """"""\n', '       At Inference time, load model, params and preprocess data.\n', '       Not saving the data, only output final dataframe\n', '    :param df: input dataframe\n', '    :param path_pipeline:  path where processors are stored\n', '    :param preprocess_pars: dict of params specific to preprocessing\n', '    :param cols_group:  dict of column family\n', '    :return: dfXy  Final dataframe,\n', '             cols_family_full : dict of column family\n', '    """"""\n']","['preprocess_pars.get', 'task.get', 'log', 'len', 'load_function_uri', 'pipe_fun', 'list', 'pipe_i.get', 'log3', 'cols_group.get', 'cols_family_full.get', 'pd.concat', 'dfi_all.keys']",13
source/run_preprocess.py:preprocess_load,preprocess_load,function,12,30,25,378,12.6,0,0,"['path_train_X', 'path_train_y', 'path_pipeline_export', 'cols_group', 'n_sample', 'preprocess_pars', 'path_features_store']","[None, None, None, None, None, None, None]","['""""', '""""', '""""', 'None', '5000', '{}', 'None']",339,"['    """"""\n', '        Load pre-computed dataframe\n', '    :param path_train_X:\n', '    :param path_train_y:\n', '    :param path_pipeline_export:\n', '    :param cols_group:\n', '    :param n_sample:\n', '    :param preprocess_pars:\n', '    :param path_features_store:\n', '    :return:\n', '    """"""\n']","['pd.read_parquet', 'dfXy.join', 'log', 'load']",4
source/run_preprocess.py:run_preprocess,run_preprocess,function,2,7,7,74,10.57,0,0,"['config_name', 'config_path', 'n_sample', 'mode', 'model_dict', 'config_path', 'config_name', 'verbose', 'path_data + ""/features.zip"") # ### Can be a list of zip or parquet files\'path_data_prepro_y\'', 'path_data + ""/target.zip"")   # ### Can be a list of zip or parquet filespath_output         ', 'path_output + ""/pipeline/"" )\'path_features_store\'', ""path_output + '/features_store/' )  #path_data_train replaced with path_output"", ""because preprocessed files are stored there'path_check_out'"", 'path_output + ""/check/"" )path_output)""#### load input column family  ###################################################"")try ']","[None, None, None, None, None, None, None, None, None, None, None, None, None, '']","[None, None, '5000', ""'run_preprocess'"", 'Nonemodel_dict', None, None, ""True)m = model_dict['global_pars']path_data         = m['path_data_preprocess']'path_data_prepro_X'"", None, "" m['path_train_output']'path_pipeline'"", None, None, None, None]",371,"['    """"""\n', '    :param config_name:   titanic_lightgbm\n', '    :param config_path:   titanic_classifier.py\n', '    :param n_sample:     nb of rows used\n', ""    :param mode:     'run_preprocess'  / 'load_prerocess'\n"", '    :param model_dict:  Optional provide the dict model\n', '    :return: None,  only show and save dataframe\n', '    """"""\n']",[],0
source/run_sampler.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],18,[],"['print', 'log2', 'log3']",3
source/run_sampler.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],21,[],['print'],1
source/run_sampler.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],24,[],['print'],1
source/run_sampler.py:save_features,save_features,function,3,11,11,109,9.91,0,1,"['df', 'name', 'path']","[None, None, None]","[None, None, None]",44,[],"['os.makedirs', 'df.to_parquet']",2
source/run_sampler.py:model_dict_load,model_dict_load,function,6,24,21,246,10.25,0,1,"['model_dict', 'config_path', 'config_name', 'verbose']","[None, None, None, None]","[None, None, None, 'True']",50,"['    """""" load the model dict from the python config file.\n', '    :return:\n', '    """"""\n']","['log', 'load_function_uri', 'model_dict_fun', 'log3']",4
source/run_sampler.py:map_model,map_model,function,10,37,23,396,10.7,0,2,['model_name'],[None],[None],64,"['    """"""\n', '      Get the Class of the object stored in source/models/\n', '    :param model_name:   model_sklearn\n', '    :return: model module\n', '\n', '    """"""\n']","['importlib.import_module', 'model_name.split']",2
source/run_sampler.py:train,train,function,50,197,157,2577,13.08,1,2,"['model_dict', 'dfX', 'cols_family', 'post_process_fun']","[None, None, None, None]","[None, None, None, None]",96,"['    """"""  Train the model using model_dict, save model, save prediction\n', '    :param model_dict:  dict containing params\n', '    :param dfX:  pd.DataFrame\n', '    :param cols_family: dict of list containing column names\n', '    :param post_process_fun:\n', '    :return: dfXtrain , dfXval  DataFrame containing prediction.\n', '    """"""\n']","['compute_pars.get', 'model_pars.get', 'log2', 'log', 'dfX.sample', 'int', 'len', 'log3', 'map_model', 'modelx.reset', 'modelx.init', 'modelx.fit', 'modelx.transform', 'pd.DataFrame', 'post_process_fun', 'modelx.eval', 'os.makedirs', 'save', 'modelx.save', 'load']",20
source/run_sampler.py:run_train,run_train,function,39,220,165,2750,12.5,1,3,"['config_name', 'config_path', 'n_sample', 'mode', 'model_dict', 'return_mode', '**kw']","[None, None, None, None, None, None, None]","[None, '""source/config_model.py""', '5000', '""run_preprocess""', 'None', ""'file'"", None]",194,"['    """"""\n', '      Configuration of the model is in config_model.py file\n', '    :param config_name:\n', '    :param config_path:\n', '    :param n_sample:\n', '    :return:\n', '    """"""\n']","['model_dict_load', 'm.get', 'log2', 'log', 'preprocess', 'preprocess_load', 'sum', 'train', 'model_dict.get', 'mlflow_register', 'os.makedirs', 'dfXy.to_parquet', 'dfXytest.to_parquet']",13
source/run_sampler.py:transform,transform,function,16,87,61,898,10.32,0,1,"['model_name', 'path_model', 'dfX', 'model_dict', 'task_type']","[None, None, None, None, None]","[None, None, None, None, ""'transform'""]",280,"['    """"""\n', '    Arguments:\n', '        model_name {[str]} -- [description]\n', '        path_model {[str]} -- [description]\n', '        dfX {[DataFrame]} -- [description]\n', '        cols_family {[dict]} -- [description]\n', '\n', '    Returns: ypred\n', '        [numpy.array] -- [vector of prediction]\n', '    """"""\n']","['map_model', 'modelx.reset', 'log', 'log2', 'modelx.load', 'load', 'modelx.encode', 'modelx.transform']",8
source/run_sampler.py:run_transform,run_transform,function,48,208,151,2568,12.35,1,3,"['config_name', 'config_path', 'n_sample', 'path_data', 'path_output', 'pars', 'model_dict', 'return_mode']","[None, None, None, None, None, None, None, None]","[None, None, '1', 'None', 'None', '{}', 'None', '""""']",328,[],"['log', 'model_dict_load', 'm.get', 'load', 'load_dataset', 'preprocess', 'list', 'transform', 'os.makedirs', 'dfX.to_parquet']",10
source/run_sampler.py:mlflow_register,mlflow_register,function,11,35,32,595,17.0,0,0,"['dfXy', 'model_dict', 'stats', 'mlflow_pars']","[None, ' dict', ' dict', 'dict']","[None, None, None, None]",404,[],"['log', 'infer_signature', 'register', 'mlflow_pars.get']",4
source/run_train.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],24,[],"['print', 'log2', 'log3']",3
source/run_train.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],27,[],['print'],1
source/run_train.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],30,[],['print'],1
source/run_train.py:save_features,save_features,function,3,11,11,109,9.91,0,1,"['df', 'name', 'path']","[None, None, None]","[None, None, None]",38,[],"['os.makedirs', 'df.to_parquet']",2
source/run_train.py:model_dict_load,model_dict_load,function,10,46,32,598,13.0,0,1,"['model_dict', 'config_path', 'config_name', 'verbose']","[None, None, None, None]","[None, None, None, 'True']",44,"['    """""" Load the model dict from the python config file.\n', '       ### Issue wiht passing function durin pickle on disk\n', '    :return:\n', '    """"""\n']","['log', 'load_function_uri', 'model_dict_fun']",3
source/run_train.py:map_model,map_model,function,10,41,26,468,11.41,0,2,['model_name'],[None],[None],70,"['    """"""\n', '      Get the Class of the object stored in source/models/\n', '    :param model_name:   model_sklearn\n', '    :return: model module\n', '\n', '    """"""\n']","['model_name.split', 'importlib.import_module']",2
source/run_train.py:train,train,function,51,243,181,2942,12.11,0,1,"['model_dict', 'dfX', 'cols_family', 'post_process_fun']","[None, None, None, None]","[None, None, None, None]",104,"['    """"""  Train the model using model_dict, save model, save prediction\n', '    :param model_dict:  dict containing params\n', '    :param dfX:  pd.DataFrame\n', '    :param cols_family: dict of list containing column names\n', '    :param post_process_fun:\n', '    :return: dfXtrain , dfXval  DataFrame containing prediction.\n', '    """"""\n']","['log2', 'log', 'dfX.sample', 'int', 'len', 'copy.deepcopy', 'map_model', 'modelx.reset', 'modelx.init', 'modelx.fit', 'modelx.predict', 'y_norm', 'post_process_fun', 'np_conv_to_one_col', 'metrics_eval', 'os.makedirs', 'save', 'modelx.save', 'modelx.load_model']",19
source/run_train.py:cols_validate,cols_validate,function,14,105,40,897,8.54,4,3,['model_dict'],[None],[None],212,"['    """"""  Validate dictionnary\n', '    :param model_dict:\n', '    :return:\n', '    """"""\n']","['sum', 'Exception', 'log']",3
source/run_train.py:run_train,run_train,function,46,244,183,3004,12.31,1,4,"['config_name', 'config_path', 'n_sample', 'mode', 'model_dict', 'return_mode', '**kw']","[None, None, None, None, None, None, None]","[None, '""source/config_model.py""', '5000', '""run_preprocess""', 'None', ""'file'"", None]",236,"['    """"""\n', '      Configuration of the model is in config_model.py file\n', '    :param config_name:\n', '    :param config_path:\n', '    :param n_sample:\n', '    :return:\n', '    """"""\n']","['model_dict_load', 'm.get', 'log', 'cols_validate', 'log2', 'preprocess', 'preprocess_load', 'list', 'log3', 'train', 'model_dict.get', 'mlflow_register', 'os.makedirs', 'colexport.append', 'dfXy.to_parquet', 'dfXytest.to_parquet']",16
source/run_train.py:run_model_check,run_model_check,function,32,85,59,809,9.52,0,0,"['path_output', 'scoring']","[None, None]","[None, None]",328,"['    """"""\n', '    :param path_output:\n', '    :param scoring:\n', '    :return:\n', '    """"""\n']","['load', 'print', 'log', 'pd.read_csv', 'feature_importance_perm']",5
source/run_train.py:mlflow_register,mlflow_register,function,11,35,32,595,17.0,0,0,"['dfXy', 'model_dict', 'stats', 'mlflow_pars']","[None, ' dict', ' dict', 'dict']","[None, None, None, None]",381,[],"['log', 'infer_signature', 'register', 'mlflow_pars.get']",4
source/util.py:log,log,function,3,12,11,70,5.83,0,0,"['*s', 'n', 'm', '**kw']","[None, None, None, None]","[None, '0', '1', None]",21,[],['print'],1
source/util.py:create_appid,create_appid,function,4,6,5,47,7.83,0,0,['filename'],[None],[None],156,[],['str'],1
source/util.py:create_logfilename,create_logfilename,function,2,3,3,50,16.67,0,0,['filename'],[None],[None],162,[],['filename.split'],1
source/util.py:create_uniqueid,create_uniqueid,function,2,6,6,87,14.5,0,0,[],[],[],166,[],['str'],1
source/util.py:logger_setup,logger_setup,function,13,55,47,571,10.38,0,3,"['logger_name', 'log_file', 'formatter', 'isrotate', 'isconsole_output', 'logging_level', '']","[None, None, None, None, None, None, None]","['None', 'None', ""'FORMATTER_0'"", 'False', 'True', ""'info'"", None]",218,"['    """"""\n', '    my_logger = util_log.logger_setup(""my module name"", log_file="""")\n', '    APP_ID    = util_log.create_appid(__file__ )\n', '    def log(*argv):\n', '      my_logger.info("","".join([str(x) for x in argv]))\n', '  \n', '   """"""\n']","['logging.getLogger', 'logger.setLevel', 'logger.addHandler', 'logger_handler_file']",4
source/util.py:logger_handler_console,logger_handler_console,function,6,13,11,163,12.54,0,1,['formatter'],[None],['None'],250,[],"['logging.StreamHandler', 'console_handler.setFormatter']",2
source/util.py:logger_handler_file,logger_handler_file,function,11,33,22,331,10.03,0,3,"['isrotate', 'rotate_time', 'formatter', 'log_file_used']","[None, None, None, None]","['False', '""midnight""', 'None', 'None']",257,[],"['print', 'TimedRotatingFileHandler', 'fh.setFormatter', 'logging.FileHandler']",4
source/util.py:logger_setup2,logger_setup2,function,13,14,13,215,15.36,0,0,"['name', 'level']","[None, None]","['__name__', 'None']",271,[],"['logging.getLogger', 'logger.setLevel', 'logging.StreamHandler', 'ch.setLevel', 'logging.Formatter', 'ch.setFormatter', 'logger.addHandler']",7
source/util.py:test_log,test_log,function,5,26,19,184,7.08,0,0,[],[],[],288,[],"['log', 'logger.log', 'log2', 'log3']",4
source/util.py:download_googledrive,download_googledrive,function,16,39,32,402,10.31,1,0,"['file_list=[ {  ""fileid""', '""path_target""', '**kw']","['', '  ""data/input/download/test.json""}]', None]","['[ {  ""fileid"": ""1-K72L8aQPsl2qt_uBF-kzbai3TYG6Qg4""', None, None]",321,"['    """"""\n', '      Use in dataloader with\n', '         ""uri"": mlmodels.data:donwload_googledrive\n', '         file_list = [ {  ""fileid"": ""1-K72L8aQPsl2qt_uBF-kzbai3TYG6Qg4"",  ""path_target"":  ""ztest/covid19/test.json""},\n', '                        {  ""fileid"" :  ""GOOGLE URL ID""   , ""path_target"":  ""dataset/test.json""},\n', '                 ]\n', '    """"""\n']","['os.system', 'd.get', 'str', 'os.makedirs', 'gdown.download', 'target_list.append']",6
source/util.py:download_dtopbox,download_dtopbox,function,18,56,46,1066,19.04,0,3,['data_pars'],[None],[None],350,"['  """"""\n', '  download_data({""from_path"" :  ""tabular"",\n', '                        ""out_path"" :  path_norm(""ztest/dataset/text/"") } )\n', '  Open URL\n', '     https://www.dropbox.com/sh/d2n3hgsq2ycpmjf/AAAoFh0aO9RqwwROksGgasIha?dl=0\n', '\n', '\n', '  """"""\n']","['data_pars.get', 'folder.split', 'os.makedirs', 'Downloader', 'downloader.download', 'zipfile.ZipFile', 'zip_ref.extractall']",7
source/util.py:load_dataset_generator,load_dataset_generator,function,23,55,41,577,10.49,3,0,['data_pars'],[None],[None],496,[],"['sent_generator', 'pd.read_csv', 'texts.extend', 'pad_sequences']",4
source/util.py:tf_dataset,tf_dataset,function,26,109,76,934,8.57,2,2,['dataset_pars'],[None],[None],523,"['    """"""\n', '        dataset_pars ={ ""dataset_id"" : ""mnist"", ""batch_size"" : 5000, ""n_train"": 500, ""n_test"": 500,\n', '                            ""out_path"" : ""dataset/vision/mnist2/"" }\n', '        tf_dataset(dataset_pars)\n', '\n', '\n', '        https://www.tensorflow.org/datasets/api_docs/python/tfds\n', '        import tensorflow_datasets as tfds\n', '        import tensorflow as tf\n', '\n', '        # Here we assume Eager mode is enabled (TF2), but tfds also works in Graph mode.\n', '        print(tfds.list_builders())\n', '\n', '        # Construct a tf.data.Dataset\n', '        ds_train = tfds.load(name=""mnist"", split=""train"", shuffle_files=True)\n', '\n', '        # Build your input pipeline\n', '        ds_train = ds_train.shuffle(1000).batch(128).prefetch(10)\n', '        for features in ds_train.take(1):\n', '          image, label = features[""image""], features[""label""]\n', '\n', '\n', '        NumPy Usage with tfds.as_numpy\n', '        train_ds = tfds.load(""mnist"", split=""train"")\n', '        train_ds = train_ds.shuffle(1024).batch(128).repeat(5).prefetch(10)\n', '\n', '        for example in tfds.as_numpy(train_ds):\n', '          numpy_images, numpy_labels = example[""image""], example[""label""]\n', '        You can also use tfds.as_numpy in conjunction with batch_size=-1 to get the full dataset in NumPy arrays from the returned tf.Tensor object:\n', '\n', '        train_ds = tfds.load(""mnist"", split=tfds.Split.TRAIN, batch_size=-1)\n', '        numpy_ds = tfds.as_numpy(train_ds)\n', '        numpy_images, numpy_labels = numpy_ds[""image""], numpy_ds[""label""]\n', '\n', '\n', '        FeaturesDict({\n', ""    'identity_attack': tf.float32,\n"", ""    'insult': tf.float32,\n"", ""    'obscene': tf.float32,\n"", ""    'severe_toxicity': tf.float32,\n"", ""    'sexual_explicit': tf.float32,\n"", ""    'text': Text(shape=(), dtype=tf.string),\n"", ""    'threat': tf.float32,\n"", ""    'toxicity': tf.float32,\n"", '})\n', '\n', '    """"""\n']","['os.system', 'd.get', 'dataset_id.replace', 'os.makedirs', 'tfds.as_numpy', 'tfds.load', 'print', 'get_keys', 'x.keys', 'np.savez_compressed', 'x.get', 'os.listdir']",12
source/util.py:dict2,dict2,class,3,5,5,36,7.2,0,0,[],[],[],28,[],[],0
source/util.py:dictLazy,dictLazy,class,12,47,29,382,8.13,0,2,[],[],[],35,[],[],0
source/util.py:logger_class,logger_class,class,22,72,51,611,8.49,0,4,[],[],[],173,[],[],0
source/util.py:Downloader,Downloader,class,58,145,113,1894,13.06,0,6,[],[],[],398,[],[],0
source/util.py:dict2:__init__,dict2:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",29,[],[],0
source/util.py:dictLazy:__init__,dictLazy:__init__,method,1,3,3,31,10.33,0,0,"['self', '*args', '**kw']","[None, None, None]","[None, None, None]",87,[],['dict'],1
source/util.py:dictLazy:__getitem__,dictLazy:__getitem__,method,6,29,17,204,7.03,0,2,"['self', 'key']","[None, None]","[None, None]",90,[],"['key.startswith', 'load_hdfs']",2
source/util.py:dictLazy:__iter__,dictLazy:__iter__,method,2,2,2,26,13.0,0,0,['self'],[None],[None],106,[],['iter'],1
source/util.py:dictLazy:__len__,dictLazy:__len__,method,1,2,2,25,12.5,0,0,['self'],[None],[None],109,[],['len'],1
source/util.py:logger_class:__init__,logger_class:__init__,method,10,14,14,177,12.64,0,1,"['self', 'config_file', 'verbose']","[None, None, None]","[None, 'None', 'True']",185,[],"['self.load_config', 'print', 'logger_setup']",3
source/util.py:logger_class:load_config,logger_class:load_config,method,5,29,24,195,6.72,0,1,"['self', 'config_file_path']","[None, None]","[None, 'None']",193,[],"['open', 'yaml.load']",2
source/util.py:logger_class:log,logger_class:log,method,3,6,6,45,7.5,0,1,"['self', '*s', 'level']","[None, None, None]","[None, None, '1']",205,[],[],0
source/util.py:logger_class:debug,logger_class:debug,method,3,6,6,46,7.67,0,1,"['self', '*s', 'level']","[None, None, None]","[None, None, '1']",210,[],[],0
source/util.py:Downloader:__init__,Downloader:__init__,method,8,16,16,174,10.88,0,1,"['self', 'url']","[None, None]","[None, None]",410,"['        """"""Make path adjustments and parse url""""""\n']","['self.clean_netloc', 'ValueError', 'self.adjust_url']",3
source/util.py:Downloader:clean_netloc,Downloader:clean_netloc,method,4,6,6,106,17.67,0,0,['self'],[None],[None],422,[],['re.sub'],1
source/util.py:Downloader:adjust_url,Downloader:adjust_url,method,7,12,9,218,18.17,0,1,['self'],[None],[None],426,[],"['self._transform_github_url', 'self._transform_gdrive_url', 'self._transform_dropbox_url']",3
source/util.py:Downloader:_transform_github_url,Downloader:_transform_github_url,method,1,8,8,96,12.0,0,0,['self'],[None],[None],434,"['        """"""Github specific changes to get link to raw file""""""\n']",[],0
source/util.py:Downloader:_transform_gdrive_url,Downloader:_transform_gdrive_url,method,4,5,5,117,23.4,0,0,['self'],[None],[None],442,"['        """"""GDrive specific changes to get link to raw file""""""\n']",[],0
source/util.py:Downloader:_transform_dropbox_url,Downloader:_transform_dropbox_url,method,2,3,3,71,23.67,0,0,['self'],[None],[None],447,"['        """"""DropBox specific changes to get link to raw file""""""\n']",[],0
source/util.py:Downloader:get_filename,Downloader:get_filename,method,11,29,21,269,9.28,0,2,"['self', 'headers']","[None, None]","[None, None]",452,"['        """"""Attempt to get filename from content-dispositions header.\n', '\n', '        If not found: get filename from parsed path\n', '        If both fail: use DEFAULT_FILENAME to save file\n', '        """"""\n']","['headers.get', 'cgi.parse_header', 'params.get']",3
source/util.py:Downloader:download,Downloader:download,method,13,27,25,323,11.96,0,2,"['self', 'filepath']","[None, None]","[None, ""''""]",471,"[""        '''Downloading and saving file'''\n""]","['os.mkdir', 'requests.get', 'self.get_filename', 'open', 'f.write', 'print']",6
source/util_feature.py:log,log,function,6,30,20,188,6.27,0,2,"['*s', 'n', 'm', '**kw']","[None, None, None, None]","[None, '0', '1', None]",12,[],"['print', 'log2', 'log3']",3
source/util_feature.py:log2,log2,function,2,7,7,40,5.71,0,1,"['*s', '**kw']","[None, None]","[None, None]",18,[],['print'],1
source/util_feature.py:log3,log3,function,2,7,7,40,5.71,0,1,"['*s', '**kw']","[None, None]","[None, None]",21,[],['print'],1
source/util_feature.py:os_get_function_name,os_get_function_name,function,4,4,4,47,11.75,0,0,[],[],[],32,[],['sys._getframe'],1
source/util_feature.py:os_getcwd,os_getcwd,function,3,6,5,66,11.0,0,0,[],[],[],37,[],[],0
source/util_feature.py:pa_read_file,pa_read_file,function,37,134,75,700,5.22,2,9,"['path', 'cols', 'n_rows', 'file_start', 'file_end', 'verbose', '']","[None, None, None, None, None, None, None]","[""  'folder_parquet/'"", 'None', '1000', '0', '100000', '1', None]",43,"['    """"""Requied HDFS connection\n', '       http://arrow.apache.org/docs/python/parquet.html\n', '\n', '       conda install libhdfs3 pyarrow\n', '       in your script.py:\n', '        import os\n', ""        os.environ['ARROW_LIBHDFS_DIR'] = '/opt/cloudera/parcels/CDH/lib64/'\n"", '\n', '       https://stackoverflow.com/questions/18123144/missing-server-jvm-java-jre7-bin-server-jvm-dll\n', '       \n', '    """"""\n']","['hdfs.ls', 'glob.glob', 'fi.split', 'print', 'pq.read_table', 'arr_table.to_pandas', 'gc.collect', 'pd.concat', 'len', 'dfall.head']",10
source/util_feature.py:pa_write_file,pa_write_file,function,23,63,46,599,9.51,0,4,"['df', 'path', 'cols', 'n_rows', 'partition_cols', 'overwrite', 'verbose', 'filesystem ']","[None, None, None, None, None, None, None, None]","[None, ""  'folder_parquet/'"", 'None', '1000', 'None', 'True', '1', "" 'hdfs'""]",97,"['    """""" Pandas to HDFS\n', ""      pyarrow.parquet.write_table(table, where, row_group_size=None, version='1.0',\n"", ""      use_dictionary=True, compression='snappy', write_statistics=True, use_deprecated_int96_timestamps=None,\n"", '      coerce_timestamps=None, allow_truncated_timestamps=False, data_page_size=None,\n', ""      flavor=None, filesystem=None, compression_level=None, use_byte_stream_split=False, data_page_version='1.0', **kwargs)\n"", '      \n', '      https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html#pyarrow.parquet.write_to_dataset\n', '       \n', '    """"""\n']","['hdfs.rm', 'hdfs.mkdir', 'pq.write_to_dataset', 'hdfs.ls', 'print', 'os.removedirs', 'os.makedirs', 'os.listdir']",8
source/util_feature.py:test_get_classification_data,test_get_classification_data,function,20,44,40,470,10.68,1,0,['name'],[None],['None'],143,[],"['make_classification', 'range', 'pd.DataFrame', 'np.arange', 'len', 'dfX.set_index', 'dfy.set_index']",7
source/util_feature.py:params_check,params_check,function,7,56,26,259,4.62,1,5,"['pars', 'check_list', 'name']","[None, None, None]","[None, None, '""""']",160,"['    """"""\n', '      Validate a dict parans\n', '    :param pars:\n', '    :param check_list:\n', '    :param name:\n', '    :return:\n', '    """"""\n']","['isinstance', 'Exception']",2
source/util_feature.py:save_features,save_features,function,7,29,26,269,9.28,0,2,"['df', 'name', 'path']","[None, None, None]","[None, None, 'None']",186,"['    """""" Save dataframe on disk\n', '    :param df:\n', '    :param name:\n', '    :param path:\n', '    :return:\n', '    """"""\n']","['os.makedirs', 'isinstance', 'log', 'list', 'df0.to_parquet']",5
source/util_feature.py:load_features,load_features,function,3,10,9,110,11.0,0,0,"['name', 'path']","[None, None]","[None, None]",206,[],"['pd.read_parquet', 'log']",2
source/util_feature.py:save_list,save_list,function,8,21,20,166,7.9,1,0,"['path', 'name_list', 'glob']","[None, None, None]","[None, None, None]",214,[],"['os.makedirs', 'log', 'pickle.dump', 'open']",4
source/util_feature.py:save,save,function,7,29,26,269,9.28,0,2,"['df', 'name', 'path']","[None, None, None]","[None, None, 'None']",221,"['  """"""\n', '      Read file in parallel from disk : very Fast\n', '  :param path_glob:\n', '  :param ignore_index:\n', '  :param cols:\n', '  :param verbose:\n', '  :param nrows:\n', '  :param concat_sort:\n', '  :param n_pool:\n', '  :param drop_duplicates:\n', '  :param shop_id:\n', '  :param kw:\n', '  :return:\n', '  """"""\n']","['os.makedirs', 'isinstance', 'log', 'list', 'df0.to_parquet']",5
source/util_feature.py:load,load,function,3,10,9,110,11.0,0,0,"['name', 'path']","[None, None]","[None, None]",228,"['  """"""\n', '      Read file in parallel from disk : very Fast\n', '  :param path_glob:\n', '  :param ignore_index:\n', '  :param cols:\n', '  :param verbose:\n', '  :param nrows:\n', '  :param concat_sort:\n', '  :param n_pool:\n', '  :param drop_duplicates:\n', '  :param shop_id:\n', '  :param kw:\n', '  :return:\n', '  """"""\n']","['pd.read_parquet', 'log']",2
source/util_feature.py:pd_read_file,pd_read_file,function,52,161,100,1083,6.73,3,10,"['path_glob', 'ignore_index', 'cols', 'verbose', 'nrows', 'concat_sort', 'n_pool', 'drop_duplicates', 'col_filter', 'col_filter_val', '**kw']","[None, None, None, None, None, None, None, None, None, None, None]","['""*.pkl""', 'True', 'None', 'False', '-1', 'True', '1', 'None', 'None', 'None', None]",233,"['  """"""\n', '      Read file in parallel from disk : very Fast\n', '  :param path_glob:\n', '  :param ignore_index:\n', '  :param cols:\n', '  :param verbose:\n', '  :param nrows:\n', '  :param concat_sort:\n', '  :param n_pool:\n', '  :param drop_duplicates:\n', '  :param shop_id:\n', '  :param kw:\n', '  :return:\n', '  """"""\n']","['ThreadPool', 'glob.glob', 'pd.DataFrame', 'len', 'log', 'range', 'job_list.append', 'pool.apply_async', 'dfi.drop_duplicates', 'gc.collect', 'pd.concat']",11
source/util_feature.py:load_dataset,load_dataset,function,34,202,125,1704,8.44,3,11,"['path_data_x', 'path_data_y', 'colid', 'n_sample']","[None, None, None, None]","[None, ""''"", '""jobId""', '-1']",300,"['    """"""\n', '      return a datraframe\n', '      https://raw.github.com/someguy/brilliant/master/somefile.txt\n', '\n', '    :param path_data_x:\n', '    :param path_data_y:\n', '    :param colid:\n', '    :param n_sample:\n', '    :return:\n', '    """"""\n']","['log', 'fetch_spark_koalas', 'fetch_dataset', 'glob.glob', 'ntpath.dirname', 'ntpath.basename', 'len', 'print', 'pd.read_csv', 'fi.endswith', 'pd.read_parquet', 'pd.read_pickle', 'pd.concat', 'df.head', 'list', 'np.arange', 'df.set_index', 'pd_read_file', 'dfy.head', 'df.join']",20
source/util_feature.py:fetch_spark_koalas,fetch_spark_koalas,function,9,11,11,109,9.91,0,0,"['path_data_x', 'path_data_y', 'colid', 'n_sample']","[None, None, None, None]","[None, ""''"", '""jobId""', '-1']",378,[],"['path_data_x.replace', 'ks.read_parquet']",2
source/util_feature.py:fetch_dataset,fetch_dataset,function,52,175,124,1966,11.23,1,6,"['url_dataset', 'path_target', 'file_target']","[None, None, None]","[None, 'None', 'None']",386,"['    """"""Fetch dataset from a given URL and save it.\n', '\n', '    Currently `github`, `gdrive` and `dropbox` are the only supported sources of\n', '    data. Also only zip files are supported.\n', '\n', '    :param url_dataset:   URL to send\n', '    :param path_target:   Path to save dataset\n', '    :param file_target:   File to save dataset\n', '\n', '    """"""\n']","['log', 'mkdtemp', 'pathlib.Path', 'mktemp', 'url_dataset.replace', 'urlx.replace', 'urlpath.split', 'os.makedirs', 'requests.Session', 's.get', 'print', 'open', 'f.write', 'res.raise_for_status', 'urlparse', 'parse_qs', 'download_googledrive', 'download_dtopbox', 'os.listdir', 'os.unlink', 'os.link']",21
source/util_feature.py:load_function_uri,load_function_uri,function,25,65,59,686,10.55,0,0,"['uri_name=""myfolder/myfile.py']",[''],"['""myfolder/myfile.py::myFunction""']",491,"['    """"""\n', '    #load dynamically function from URI pattern\n', '    #""dataset""        : ""mlmodels.preprocess.generic:pandasDataset""\n', '    ###### External File processor :\n', '    #""dataset""        : ""MyFolder/preprocess/myfile.py:pandasDataset""\n', '    """"""\n']","['uri_name.split', 'len', 'package_path.replace', 'getattr', 'str', 'log', 'Path', 'NameError']",8
source/util_feature.py:metrics_eval,metrics_eval,function,23,70,51,896,12.8,2,3,"['metric_list', 'ytrue', 'ypred', 'ypred_proba', 'return_dict']","[None, None, None, None, None]","['[""mean_squared_error""]', 'None', 'None', 'None', 'False']",531,"['    """"""\n', '      Generic metrics calculation, using sklearn naming pattern\n', '    """"""\n']","['len', 'isinstance', 'getattr', 'range', 'mval_.append', 'np.mean', 'np.sqrt', 'metric_scorer', 'pd.DataFrame']",9
source/util_feature.py:pd_stat_dataset_shift,pd_stat_dataset_shift,function,14,24,23,328,13.67,1,0,"['dftrain', 'dftest', 'colused', 'nsample', 'buckets', 'axis']","[None, None, None, None, None, None]","[None, None, None, '10000', '5', '0']",572,[],"['print', 'pd_stat_datashift_psi', 'pd.DataFrame']",3
source/util_feature.py:pd_stat_datashift_psi,pd_stat_datashift_psi,function,25,121,82,1182,9.77,1,5,"['expected', 'actual', 'buckettype', 'buckets', 'axis']","[None, None, None, None, None]","[None, None, ""'bins'"", '10', '0']",588,"[""    '''Calculate the PSI (population stability index) across all variables\n"", '    Args:\n', '       expected: numpy matrix of original values\n', '       actual: numpy matrix of new values, same size as expected\n', '       buckettype: type of strategy for creating buckets, bins splits into even splits, quantiles splits into quantile buckets\n', '       buckets: number of quantiles to use in bucketing variables\n', '       axis: axis by which variables are defined, 0 for vertical, 1 for horizontal\n', '    Returns:\n', '       psi_values: ndarray of psi values for each variable\n', ""    '''\n""]","['psi', 'scale_range', 'np.max', 'np.arange', 'np.min', 'np.stack', 'np.histogram', 'len', 'sub_psi', 'np.log', 'np.sum', 'range', 'np.empty']",13
source/util_feature.py:estimator_std_normal,estimator_std_normal,function,15,41,31,240,5.85,0,0,"['err', 'alpha', '']","[None, None, None]","[None, '0.05', None]",660,[],"['len', 'np.var', 'np.sqrt']",3
source/util_feature.py:estimator_boostrap_bayes,estimator_boostrap_bayes,function,8,13,10,89,6.85,0,0,"['err', 'alpha', '']","[None, None, None]","[None, '0.05', None]",672,[],['bayes_mvs'],1
source/util_feature.py:estimator_bootstrap,estimator_bootstrap,function,6,11,10,120,10.91,0,0,"['err', 'custom_stat', 'alpha', 'n_iter']","[None, None, None, None]","[None, 'None', '0.05', '10000']",678,"['    """"""\n', '      def custom_stat(values, axis=1):\n', '      # stat_val = np.mean(np.asmatrix(values),axis=axis)\n', '      # stat_val = np.std(np.asmatrix(values),axis=axis)p.mean\n', '      stat_val = np.sqrt(np.mean(np.asmatrix(values*values),axis=axis))\n', '      return stat_val\n', '    """"""\n']",['bs.bootstrap'],1
source/util_feature.py:test_heteroscedacity,test_heteroscedacity,function,139,342,200,2095,6.13,1,1,"['y', 'y_pred', 'pred_value_only']","[None, None, None]","[None, None, '1']",692,[],['Linear'],1
source/util_feature.py:pd_stat_distribution_colnum,pd_stat_distribution_colnum,function,18,72,56,565,7.85,1,2,['df'],[None],[None],1333,"['    """""" Describe the tables\n', '   """"""\n']","['getstat', 'list', 'len', 'pd.Series', 'pd.DataFrame', 'str', 'pd.concat']",7
source/util_feature.py:pd_stat_histogram,pd_stat_histogram,function,10,22,21,229,10.41,0,0,"['df', 'bins', 'coltarget']","[None, None, None]","[None, '50', '""diff""']",1370,"['    """"""\n', '    :param df:\n', '    :param bins:\n', '    :param coltarget:\n', '    :return:\n', '    """"""\n']","['np.histogram', 'pd.DataFrame']",2
source/util_feature.py:col_extractname,col_extractname,function,7,38,24,216,5.68,1,5,['col_onehot'],[None],[None],1385,"['    """"""\n', '    Column extraction from onehot name\n', '    :param col_onehot\n', '    :return:\n', '    """"""\n']","['len', 'colnew.append']",2
source/util_feature.py:col_remove,col_remove,function,12,42,26,214,5.1,3,4,"['cols', 'colsremove', 'mode']","[None, None, None]","[None, None, '""exact""']",1408,"['    """"""\n', '    """"""\n']","['cols.remove', 'cols3.append']",2
source/util_feature.py:pd_colnum_tocat_stat,pd_colnum_tocat_stat,function,53,152,107,1986,13.07,1,5,"['df', 'feature', 'target_col', 'bins', 'cuts']","[None, None, None, None, None]","[None, None, None, None, '0']",1433,"['    """"""\n', '    Bins continuous features into equal sample size buckets and returns the target mean in each bucket. Separates out\n', '    nulls into another bucket.\n', '    :param df: dataframe containg features and target column\n', '    :param feature: feature column name\n', '    :param target_col: target column\n', '    :param bins: Number bins required\n', '    :param cuts: if buckets of certain specific cuts are required. Used on test data to use cuts from train.\n', '    :return: If cuts are passed only df_grouped data is returned, else cuts and df_grouped data is returned\n', '    """"""\n']","['pd.isnull', 'df.reset_index', 'min', 'range', 'np.percentile', 'cuts.append', 'pd.cut', 'df.groupby', 'df_grouped.reset_index', 'list', 'df_grouped.rename', 'str', 'len', 'pd.concat']",14
source/util_feature.py:pd_stat_shift_trend_changes,pd_stat_shift_trend_changes,function,20,41,35,660,16.1,0,0,"['df', 'feature', 'target_col', 'threshold']","[None, None, None, None]","[None, None, None, '0.03']",1502,"['    """"""\n', '    Calculates number of times the trend of feature wrt target changed direction.\n', '    :param df: df_grouped dataset\n', '    :param feature: feature column name\n', '    :param target_col: target column\n', '    :param threshold: minimum % difference required to count as trend change\n', '    :return: number of trend chagnes for the feature\n', '    """"""\n']","['target_diffs.fillna', 'target_diffs.divide', 'target_diffs_norm.diff', 'target_diffs_lvl2.fillna', 'int']",5
source/util_feature.py:pd_stat_shift_trend_correlation,pd_stat_shift_trend_correlation,function,19,63,55,788,12.51,0,2,"['df', 'df_test', 'colname', 'target_col']","[None, None, None, None]","[None, None, None, None]",1526,"['    """"""\n', '    Calculates correlation between train and test trend of colname wrt target.\n', '    :param df: train df data\n', '    :param df_test: test df data\n', '    :param colname: colname column name\n', '    :param target_col: target column name\n', '    :return: trend correlation between train and test\n', '    """"""\n']","['df.merge', 'pd.isnull', 'len', 'np.corrcoef', 'print']",5
source/util_feature.py:pd_stat_shift_changes,pd_stat_shift_changes,function,33,99,81,1236,12.48,1,4,"['df', 'target_col', 'features_list', 'bins', 'df_test']","[None, None, None, None, None]","[None, None, '0', '10', '0']",1557,"['    """"""\n', '    Calculates trend changes and correlation between train/test for list of features\n', '    :param df: dfframe containing features and target columns\n', '    :param target_col: target column name\n', '    :param features_list: by default creates plots for all features. If list passed, creates plots of only those features.\n', '    :param bins: number of bins to be created from continuous colname\n', '    :param df_test: test df which has to be compared with input df for correlation\n', '    :return: dfframe with trend changes and trend correlation (if test df passed)\n', '    """"""\n']","['type', 'list', 'features_list.remove', 'ignored.append', 'pd_colnum_tocat_stat', 'pd_stat_shift_trend_correlation', 'pd_stat_shift_changes', 'stats_all.append', 'pd.DataFrame', 'len', 'print', 'str']",12
source/util_feature.py:np_conv_to_one_col,np_conv_to_one_col,function,5,11,10,137,12.45,0,0,"['np_array', 'sep_char']","[None, None]","[None, '""_""']",1602,"['    """"""\n', '    converts string/numeric columns to one string column\n', '    :param np_array: the numpy array with more than one column\n', '    :param sep_char: the separator character\n', '    """"""\n']","['row2string', 'sep_char.join']",2
source/util_feature.py:dict2,dict2,class,3,5,5,36,7.2,0,0,[],[],[],28,[],[],0
source/util_feature.py:dict2:__init__,dict2:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",29,[],[],0
ztest/core_test_encoder.py:global_pars_update,global_pars_update,function,21,55,40,1028,18.69,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",25,[],['print'],1
ztest/core_test_encoder.py:config1,config1,function,8,247,151,2499,10.12,0,0,['path_model_out'],[None],"['""""']",145,"['    """"""\n', '       Contains all needed informations\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
ztest/core_test_encoder.py:config2,config2,function,8,104,78,1072,10.31,0,0,['path_model_out'],[None],"['""""']",266,"['    """"""\n', '       Contains all needed informations\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
ztest/core_test_encoder.py:config4,config4,function,8,80,64,726,9.07,0,0,['path_model_out'],[None],"['""""']",333,"['    """"""\n', '\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
ztest/core_test_encoder.py:config9,config9,function,8,102,82,998,9.78,0,0,['path_model_out'],[None],"['""""']",411,"['    """"""\n', '       python  example/test_features.py  train       --nsample 500 --config config1\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
ztest/core_test_encoder.py:pd_col_amyfun,pd_col_amyfun,function,29,79,64,556,7.04,1,1,"['df', 'col', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",480,"['    """"""\n', '    Example of custom Processor\n', '    Used at prediction time\n', '        ""path_pipeline""  :\n', '\n', '    Training time :\n', '        ""path_features_store"" :  to store intermediate dataframe\n', '        ""path_pipeline_export"":  to store pipeline  for later usage\n', '\n', '    """"""\n']","['prepro_load', 'prepro', 'list', 'prepro_save']",4
ztest/core_test_encoder.py:get_test_data,get_test_data,function,16,29,26,263,9.07,0,1,['name'],[None],"[""'boston'""]",519,[],"['load_boston', 'pd.DataFrame', 'pd.concat']",3
ztest/core_test_encoder.py:check1,check1,function,13,37,35,302,8.16,1,0,[],[],[],531,[],"['get_test_data', 'print', 'pd_prepro']",3
data/docs/all_nlp2.py:multiclass_logloss,multiclass_logloss,function,15,30,28,250,8.33,1,1,"['actual', 'predicted', 'eps']","[None, None, None]","[None, None, '1e-15']",91,"['    """"""Multi class version of Logarithmic Loss metric.\n', '    :param actual: Array containing the actual target classes\n', '    :param predicted: Matrix with class predictions, one probability per class\n', '    """"""\n']","['len', 'np.zeros', 'enumerate', 'np.clip', 'np.sum', 'np.log']",6
data/docs/all_nlp2.py:sent2vec,sent2vec,function,16,47,28,309,6.57,3,3,['s'],[None],[None],434,[],"['str', 'word_tokenize', 'w.isalpha', 'M.append', 'np.array', 'M.sum', 'type', 'np.zeros', 'np.sqrt']",9
data/docs/all_nlp2.py:Ensembler,Ensembler,class,86,365,182,4764,13.05,9,11,[],[],[],735,[],[],0
data/docs/all_nlp2.py:Ensembler:__init__,Ensembler:__init__,method,21,30,24,369,12.3,0,0,"['self', 'model_dict', 'num_folds', 'task_type', 'optimize', 'lower_is_better', 'save_path']","[None, None, None, None, None, None, None]","[None, None, '3', ""'classification'"", 'roc_auc_score', 'False', 'None']",736,"['        """"""\n', '        Ensembler init function\n', '        :param model_dict: model dictionary, see README for its format\n', '        :param num_folds: the number of folds for ensembling\n', '        :param task_type: classification or regression\n', '        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n', '        :param lower_is_better: is lower value of optimization function better or higher\n', '        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n', '        """"""\n']",['len'],1
data/docs/all_nlp2.py:Ensembler:fit,Ensembler:fit,method,61,204,124,2624,12.86,6,5,"['self', 'training_data', 'y', 'lentrain']","[None, None, None, None]","[None, None, None, None]",765,"['        """"""\n', '        :param training_data: training data in tabular format\n', '        :param y: binary, multi-class or regression\n', '        :return: chain of models to be used in prediction\n', '        """"""\n']","['len', 'logger.info', 'LabelEncoder', 'StratifiedKFold', 'KFold', 'range', 'np.zeros', 'enumerate', 'kf.split', 'model.fit', 'type', 'model.predict_proba', 'model.predict', 'self.optimize', 'validation_scores.append', 'np.mean', 'np.std', 'pd.DataFrame', 'train_predictions_df.to_csv', 'str']",20
data/docs/all_nlp2.py:Ensembler:predict,Ensembler:predict,method,31,114,67,1564,13.72,3,6,"['self', 'test_data', 'lentest']","[None, None, None]","[None, None, None]",848,[],"['range', 'np.zeros', 'len', 'enumerate', 'logger.info', 'model.fit', 'model.predict_proba', 'model.predict', 'pd.DataFrame', 'test_predictions_df.to_csv', 'str']",11
data/docs/column_encoder.py:OneHotEncoderRemoveOne,OneHotEncoderRemoveOne,class,18,32,31,397,12.41,0,0,[],[],[],20,[],[],0
data/docs/column_encoder.py:MinHashEncoder,MinHashEncoder,class,34,145,92,1256,8.66,6,3,[],[],[],43,[],[],0
data/docs/column_encoder.py:PasstroughEncoder,PasstroughEncoder,class,10,20,17,225,11.25,0,0,[],[],[],104,[],[],0
data/docs/column_encoder.py:OneHotEncoderRemoveOne:__init__,OneHotEncoderRemoveOne:__init__,method,13,13,13,186,14.31,0,0,"['self', 'n_values', 'categorical_features', 'categories', 'sparse', 'dtype', 'handle_unknown', '']","[None, None, None, None, None, None, None, None]","[None, 'None', 'None', '""auto""', 'True', 'np.float64', '""error""', None]",21,[],['super'],1
data/docs/column_encoder.py:OneHotEncoderRemoveOne:transform,OneHotEncoderRemoveOne:transform,method,4,5,5,43,8.6,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",38,[],['super'],1
data/docs/column_encoder.py:MinHashEncoder:__init__,MinHashEncoder:__init__,method,4,4,4,59,14.75,0,0,"['self', 'n_components', 'ngram_range', '4']","[None, None, None, None]","[None, None, '(2', None]",50,[],[],0
data/docs/column_encoder.py:MinHashEncoder:get_unique_ngrams,MinHashEncoder:get_unique_ngrams,method,8,34,26,231,6.79,2,0,"['self', 'string', 'ngram_range']","[None, None, None]","[None, None, None]",54,"['        """"""\n', '        Return a list of different n-grams in a string\n', '        """"""\n']","['range', 'list']",2
data/docs/column_encoder.py:MinHashEncoder:minhash,MinHashEncoder:minhash,method,9,37,31,353,9.54,1,1,"['self', 'string', 'n_components', 'ngram_range']","[None, None, None, None]","[None, None, None, None]",66,[],"['np.ones', 'self.get_unique_ngrams', 'len', 'np.array', 'range', 'np.minimum']",6
data/docs/column_encoder.py:MinHashEncoder:fit,MinHashEncoder:fit,method,7,20,18,170,8.5,1,1,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",78,[],"['enumerate', 'self.minhash']",2
data/docs/column_encoder.py:MinHashEncoder:transform,MinHashEncoder:transform,method,10,29,21,247,8.52,2,1,"['self', 'X']","[None, None]","[None, None]",88,[],"['np.zeros', 'enumerate', 'self.minhash']",3
data/docs/column_encoder.py:PasstroughEncoder:__init__,PasstroughEncoder:__init__,method,2,2,2,28,14.0,0,0,"['self', 'passthrough']","[None, None]","[None, 'True']",105,[],[],0
data/docs/column_encoder.py:PasstroughEncoder:fit,PasstroughEncoder:fit,method,4,6,6,83,13.83,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",78,[],['FunctionTransformer'],1
data/docs/column_encoder.py:PasstroughEncoder:transform,PasstroughEncoder:transform,method,2,2,2,31,15.5,0,0,"['self', 'X']","[None, None]","[None, None]",88,[],[],0
data/docs/config_model.py:y_norm,y_norm,function,15,70,41,312,4.46,1,4,"['y', 'inverse', 'mode']","[None, None, None]","[None, 'True', ""'boxcox'""]",11,[],[],0
data/docs/config_model.py:salary_elasticnetcv,salary_elasticnetcv,function,5,77,60,710,9.22,0,0,['path_model_out'],[None],[None],54,[],"['post_process_fun', 'y_norm', 'pre_process_fun']",3
data/docs/config_model.py:salary_lightgbm,salary_lightgbm,function,7,155,62,1465,9.45,0,0,['path_model_out'],[None],[None],80,[],"['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy', 'salary_lightgbm']",5
data/docs/config_model.py:salary_lightgbm,salary_lightgbm,function,7,155,62,1465,9.45,0,0,['path_model_out'],[None],[None],80,[],"['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy', 'salary_lightgbm']",5
data/docs/config_model.py:salary_bayesian_pyro,salary_bayesian_pyro,function,6,89,70,820,9.21,0,0,['path_model_out'],[None],[None],146,[],"['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy']",4
data/docs/config_model.py:salary_glm,salary_glm,function,5,76,59,687,9.04,0,0,['path_model_out'],[None],[None],177,[],"['post_process_fun', 'y_norm', 'pre_process_fun']",3
data/docs/config_model.py:titanic_lightgbm,titanic_lightgbm,function,5,108,77,915,8.47,0,0,['path_model_out'],[None],[None],207,"['    """"""\n', '       titanic\n', '    """"""\n']","['post_process_fun', 'y.astype', 'pre_process_fun']",3
data/docs/skpipeline.py:processing,processing,function,9,63,32,507,8.05,0,0,['df'],[None],[None],37,[],"['re.sub', 'x.lower', 'len', 'x.split', 'np.mean', 'x.count', 'return']",7
data/docs/skpipeline.py:TextSelector,TextSelector,class,7,16,13,109,6.81,0,0,[],[],[],87,[],[],0
data/docs/skpipeline.py:NumberSelector,NumberSelector,class,7,16,13,111,6.94,0,0,[],[],[],101,[],[],0
data/docs/skpipeline.py:TextSelector:__init__,TextSelector:__init__,method,2,2,2,12,6.0,0,0,"['self', 'key']","[None, None]","[None, None]",92,[],[],0
data/docs/skpipeline.py:TextSelector:fit,TextSelector:fit,method,1,2,2,10,5.0,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",95,[],[],0
data/docs/skpipeline.py:TextSelector:transform,TextSelector:transform,method,2,2,2,17,8.5,0,0,"['self', 'X']","[None, None]","[None, None]",98,[],[],0
data/docs/skpipeline.py:NumberSelector:__init__,NumberSelector:__init__,method,2,2,2,12,6.0,0,0,"['self', 'key']","[None, None]","[None, None]",92,[],[],0
data/docs/skpipeline.py:NumberSelector:fit,NumberSelector:fit,method,1,2,2,10,5.0,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",95,[],[],0
data/docs/skpipeline.py:NumberSelector:transform,NumberSelector:transform,method,2,2,2,19,9.5,0,0,"['self', 'X']","[None, None]","[None, None]",98,"['    """"""\n', '    Transformer to select a single column from the data frame to perform additional transformations on\n', '    Use on numeric columns in the data\n', '    """"""\n']",[],0
data/docs/text_features2.py:pd_str_clean,pd_str_clean,function,3,10,10,54,5.4,0,0,"['df', 'coly']","[None, None]","[None, None]",265,[],"['float', 'x.replace']",2
data/docs/__preprocessor_tseries_doc.py:robust_scaler,robust_scaler,function,14,27,21,224,8.3,0,2,"['df', 'drop', 'quantile_range', '75']","[None, None, None, None]","[None, 'None', '(25', None]",3,[],"['df.drop', 'np.median', 'np.percentile', 'pd.concat']",4
data/docs/__preprocessor_tseries_doc.py:standard_scaler,standard_scaler,function,11,24,17,160,6.67,0,2,"['df', 'drop']","[None, None]","[None, None]",24,[],"['df.drop', 'np.mean', 'np.std', 'pd.concat']",4
data/docs/__preprocessor_tseries_doc.py:fast_fracdiff,fast_fracdiff,function,17,38,34,231,6.08,1,0,"['x', 'cols', 'd']","[None, None, None]","[None, None, None]",47,[],"['len', 'int', 'np.ceil', 'np.arange', 'tuple', 'pl.ifft', 'pl.fft', 'np.real']",8
data/docs/__preprocessor_tseries_doc.py:outlier_detect,outlier_detect,function,21,93,51,1214,13.05,0,4,"['data', 'col', 'threshold', 'method']","[None, None, None, None]","[None, None, '1', '""IQR""']",71,[],"['np.median', 'pd.Series', 'np.abs', 'print', 'pd.concat', 'tmp.any']",6
data/docs/__preprocessor_tseries_doc.py:windsorization,windsorization,function,8,23,15,311,13.52,0,1,"['data', 'col', 'para', 'strategy']","[None, None, None, None]","[None, None, None, ""'both'""]",100,"['    """"""\n', '    top-coding & bottom coding (capping the maximum of a distribution at an arbitrarily set value,vice versa)\n', '    """"""\n']",['data.copy'],1
data/docs/__preprocessor_tseries_doc.py:operations,operations,function,21,53,36,691,13.04,0,0,"['df', 'features']","[None, None]","[None, None]",125,[],"['df_new.min', 'pd.DataFrame', 'pd.concat']",3
data/docs/__preprocessor_tseries_doc.py:initial_trend,initial_trend,function,5,14,12,83,5.93,1,0,"['series', 'slen']","[None, None]","[None, None]",156,[],"['range', 'float']",2
data/docs/__preprocessor_tseries_doc.py:initial_seasonal_components,initial_seasonal_components,function,12,28,20,341,12.18,3,0,"['series', 'slen']","[None, None]","[None, None]",162,[],"['int', 'range', 'season_averages.append']",3
data/docs/__preprocessor_tseries_doc.py:triple_exponential_smoothing,triple_exponential_smoothing,function,27,60,50,615,10.25,2,2,"['df', 'cols', 'slen', 'alpha', 'beta', 'gamma', 'n_preds']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",177,[],"['initial_seasonal_components', 'range', 'initial_trend', 'result.append', 'len']",5
data/docs/__preprocessor_tseries_doc.py:naive_dec,naive_dec,function,7,19,18,228,12.0,1,0,"['df', 'columns', 'freq']","[None, None, None]","[None, None, '2']",211,[],[],0
data/docs/__preprocessor_tseries_doc.py:bkb,bkb,function,6,11,11,94,8.55,1,0,"['df', 'cols']","[None, None]","[None, None]",234,[],['len'],1
data/docs/__preprocessor_tseries_doc.py:butter_lowpass,butter_lowpass,function,15,34,30,276,8.12,1,0,"['cutoff', 'fs', 'order']","[None, None, None]","[None, '20', '5']",247,[],"['signal.butter', 'butter_lowpass_filter', 'butter_lowpass', 'signal.lfilter']",4
data/docs/__preprocessor_tseries_doc.py:butter_lowpass_filter,butter_lowpass_filter,function,9,15,15,110,7.33,1,0,"['df', 'cols', 'cutoff', 'fs', 'order']","[None, None, None, None, None]","[None, None, None, '20', '5']",253,[],"['butter_lowpass', 'signal.lfilter']",2
data/docs/__preprocessor_tseries_doc.py:instantaneous_phases,instantaneous_phases,function,6,10,10,98,9.8,1,0,"['df', 'cols']","[None, None]","[None, None]",269,[],['np.unwrap'],1
data/docs/__preprocessor_tseries_doc.py:kalman_feat,kalman_feat,function,10,28,26,356,12.71,1,0,"['df', 'cols']","[None, None]","[None, None]",285,[],"['UnscentedKalmanFilter', 'np.sin', 'ukf.filter', 'ukf.smooth', 'smoothed_state_means.flatten', 'filtered_state_means.flatten']",6
data/docs/__preprocessor_tseries_doc.py:perd_feat,perd_feat,function,7,13,13,128,9.85,1,0,"['df', 'cols']","[None, None]","[None, None]",306,[],['signal.periodogram'],1
data/docs/__preprocessor_tseries_doc.py:fft_feat,fft_feat,function,10,18,15,238,13.22,1,0,"['df', 'cols']","[None, None]","[None, None]",320,[],"['pd.DataFrame', 'np.abs', 'np.angle']",3
data/docs/__preprocessor_tseries_doc.py:harmonicradar_cw,harmonicradar_cw,function,15,25,24,178,7.12,1,0,"['df', 'cols', 'fs', 'fc']","[None, None, None, None]","[None, None, None, None]",338,[],"['np.sin', 'signal.welch']",2
data/docs/__preprocessor_tseries_doc.py:saw,saw,function,7,9,9,61,6.78,1,0,"['df', 'cols']","[None, None]","[None, None]",360,[],['signal.sawtooth'],1
data/docs/__preprocessor_tseries_doc.py:modify,modify,function,34,64,37,767,11.98,1,0,"['df', 'cols']","[None, None]","[None, None]",374,[],"['magnify', 'affine', 'crop', 'cross_sum', 'resample', 'trend', 'random_time_warp', 'random_crop', 'random_cross_sum', 'random_sidetrack', 'random_magnify', 'random_jitter', 'random_trend']",13
data/docs/__preprocessor_tseries_doc.py:multiple_rolling,multiple_rolling,function,24,61,46,419,6.87,4,1,"['df', 'windows ', '2]', 'functions', '""std""]', 'columns']","[None, None, None, None, None, None]","[None, ' [1', None, '[""mean""', None, 'None']",403,[],[],0
data/docs/__preprocessor_tseries_doc.py:multiple_lags,multiple_lags,function,9,30,25,192,6.4,2,1,"['df', 'start', 'end', 'columns']","[None, None, None, None]","[None, '1', '3', 'None']",428,[],"['range', 'df.assign']",2
data/docs/__preprocessor_tseries_doc.py:prophet_feat,prophet_feat,function,21,42,37,555,13.21,1,0,"['df', 'cols', 'date', 'freq', 'train_size']","[None, None, None, None, None]","[None, None, None, None, '150']",455,[],"['prophet_dataframe', 'original_dataframe', 'pd.DataFrame', 'prophet_pred.set_index', 'Prophet', 'model.fit', 'len', 'model.make_future_dataframe', 'model.predict', 'list']",10
data/docs/__preprocessor_tseries_doc.py:lowess,lowess,function,33,91,70,648,7.12,3,0,"['df', 'cols', 'y', 'f', 'iter']","[None, None, None, None, None]","[None, None, None, '2. / 3.', '3']",497,[],"['len', 'int', 'range', 'np.clip', 'np.zeros', 'np.ones', 'np.array', 'np.sum', 'linalg.solve', 'np.median']",10
data/docs/__preprocessor_tseries_doc.py:autoregression,autoregression,function,21,43,34,459,10.67,2,2,"['df', 'drop', 'settings={""autoreg_lag""']","[None, None, '']","[None, 'None', '{""autoreg_lag"":4}']",532,[],"['df.drop', 'timer', 'np.zeros', 'range', 'AR', 'len', 'np.real', 'pd.concat']",8
data/docs/__preprocessor_tseries_doc.py:muldiv,muldiv,function,11,21,18,222,10.57,2,1,"['df', 'feature_list']","[None, None]","[None, None]",567,[],[],0
data/docs/__preprocessor_tseries_doc.py:decision_tree_disc,decision_tree_disc,function,12,19,18,245,12.89,1,0,"['df', 'cols', 'depth']","[None, None, None]","[None, None, '4']",591,[],"['DecisionTreeRegressor', 'tree_model.fit', 'tree_model.predict']",3
data/docs/__preprocessor_tseries_doc.py:quantile_normalize,quantile_normalize,function,19,39,29,292,7.49,3,2,"['df', 'drop']","[None, None]","[None, None]",614,[],"['df.drop', 'dic.update', 'sorted', 'pd.DataFrame', 'sorted_df.mean', 'np.searchsorted', 'pd.concat']",7
data/docs/__preprocessor_tseries_doc.py:haversine_distance,haversine_distance,function,14,30,26,230,7.67,0,0,"['row', 'lon', 'lat']","[None, None, None]","[None, '""Open""', '""Close""']",647,[],"['radians', 'sin', 'cos', 'atan2', 'sqrt']",5
data/docs/__preprocessor_tseries_doc.py:tech,tech,function,2,7,7,96,13.71,0,0,['df'],[None],[None],671,[],['ta.add_all_ta_features'],1
data/docs/__preprocessor_tseries_doc.py:genetic_feat,genetic_feat,function,10,37,36,524,14.16,0,0,"['df', 'num_gen', 'num_comp']","[None, None, None]","[None, '20', '10']",689,[],"['SymbolicTransformer', 'gp.fit_transform', 'pd.DataFrame', 'range', 'pd.concat']",5
data/docs/__preprocessor_tseries_doc.py:pca_feature,pca_feature,function,21,69,50,803,11.64,0,5,"['df', 'memory_issues', 'mem_iss_component', 'variance_or_components', 'n_components', 'drop_cols', 'non_linear']","[None, None, None, None, None, None, None]","[None, 'False', 'False', '0.80', '5', 'None', 'True']",722,[],"['KernelPCA', 'ValueError', 'IncrementalPCA', 'PCA', 'pca.fit_transform', 'pd.concat', 'range', 'pd.DataFrame']",8
data/docs/__preprocessor_tseries_doc.py:cross_lag,cross_lag,function,18,34,23,367,10.79,0,2,"['df', 'drop', 'lags', 'components']","[None, None, None, None]","[None, 'None', '1', '4']",760,[],"['df.drop', 'df.shift', 'df_2.dropna', 'CCA', 'cca.fit', 'cca.transform', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",9
data/docs/__preprocessor_tseries_doc.py:a_chi,a_chi,function,16,31,21,394,12.71,0,2,"['df', 'drop', 'lags', 'sample_steps']","[None, None, None, None]","[None, 'None', '1', '2']",796,[],"['df.drop', 'df.shift', 'df_2.dropna', 'AdditiveChi2Sampler', 'chi2sampler.fit_transform', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",8
data/docs/__preprocessor_tseries_doc.py:encoder_dataset,encoder_dataset,function,25,66,50,989,14.98,0,2,"['df', 'drop', 'dimesions']","[None, None, None]","[None, 'None', '20']",834,[],"['minmax_scale', 'pd.DataFrame', 'encoded_train.add_prefix', 'pd.concat']",4
data/docs/__preprocessor_tseries_doc.py:lle_feat,lle_feat,function,13,21,15,242,11.52,0,2,"['df', 'drop', 'components']","[None, None, None]","[None, 'None', '4']",881,[],"['df.drop', 'LocallyLinearEmbedding', 'embedding.fit_transform', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",6
data/docs/__preprocessor_tseries_doc.py:feature_agg,feature_agg,function,14,23,17,294,12.78,0,2,"['df', 'drop', 'components']","[None, None, None]","[None, 'None', '4']",909,[],"['df.drop', 'min', 'cluster.FeatureAgglomeration', 'agglo.fit', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",7
data/docs/__preprocessor_tseries_doc.py:neigh_feat,neigh_feat,function,15,28,20,304,10.86,0,2,"['df', 'drop', 'neighbors']","[None, None, None]","[None, None, '6']",940,[],"['df.drop', 'min', 'NearestNeighbors', 'neigh.fit', 'neigh.kneighbors', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",8
data/docs/__preprocessor_tseries_doc.py:set_property,set_property,function,7,25,22,177,7.08,0,1,"['key', 'value']","[None, None]","[None, None]",977,"['    """"""\n', '    This method returns a decorator that sets the property key of the function to value\n', '    """"""\n']","['decorate_func', 'setattr']",2
data/docs/__preprocessor_tseries_doc.py:abs_energy,abs_energy,function,5,10,10,76,7.6,0,1,['x'],[None],[None],998,[],"['isinstance', 'np.asarray', 'np.dot']",3
data/docs/__preprocessor_tseries_doc.py:cid_ce,cid_ce,function,10,24,19,166,6.92,0,3,"['x', 'normalize']","[None, None]","[None, None]",1016,[],"['isinstance', 'np.asarray', 'np.std', 'np.mean', 'np.diff', 'np.sqrt']",6
data/docs/__preprocessor_tseries_doc.py:mean_abs_change,mean_abs_change,function,2,2,2,33,16.5,0,0,['x'],[None],[None],1042,[],['np.mean'],1
data/docs/__preprocessor_tseries_doc.py:_roll,_roll,function,7,13,13,105,8.08,0,1,"['a', 'shift']","[None, None]","[None, None]",1057,[],"['isinstance', 'np.asarray', 'len', 'np.concatenate']",4
data/docs/__preprocessor_tseries_doc.py:mean_second_derivative_central,mean_second_derivative_central,function,3,10,10,73,7.3,0,0,['x'],[None],[None],1063,[],"['np.array', '_roll', 'np.mean']",3
data/docs/__preprocessor_tseries_doc.py:variance_larger_than_standard_deviation,variance_larger_than_standard_deviation,function,4,6,5,30,5.0,0,0,['x'],[None],[None],1078,[],"['np.var', 'np.sqrt']",2
data/docs/__preprocessor_tseries_doc.py:var_index,var_index,function,19,52,43,377,7.25,1,0,"['time', 'param']","[None, None]","[None, 'var_index_param']",1094,[],"['param.items', 'np.power', 'np.mean', 'len', 'np.var', 'sum', 'final.append', 'keys.append', 'zip']",9
data/docs/__preprocessor_tseries_doc.py:symmetry_looking,symmetry_looking,function,10,23,22,247,10.74,1,1,"['x', 'param=[{""r""']","[None, '']","[None, '[{""r"": 0.2}]']",1125,[],"['isinstance', 'np.asarray', 'np.abs', 'np.median', 'np.max', 'np.min']",6
data/docs/__preprocessor_tseries_doc.py:has_duplicate_max,has_duplicate_max,function,5,11,11,88,8.0,0,1,['x'],[None],[None],1144,"['    """"""\n', '    Checks if the maximum value of x is observed more than once\n', '\n', '    :param x: the time series to calculate the feature of\n', '    :type x: numpy.ndarray\n', '    :return: the value of this feature\n', '    :return type: bool\n', '    """"""\n']","['isinstance', 'np.asarray', 'np.sum', 'np.max']",4
data/docs/__preprocessor_tseries_doc.py:partial_autocorrelation,partial_autocorrelation,function,6,42,29,374,8.9,0,2,"['x', 'param=[{""lag""']","[None, '']","[None, '[{""lag"": 1}]']",1171,[],"['max', 'len', 'list']",3
data/docs/__preprocessor_tseries_doc.py:augmented_dickey_fuller,augmented_dickey_fuller,function,26,64,40,410,6.41,2,4,"['x', 'param=[{""attr""']","[None, '']","[None, '[{""attr"": ""teststat""}]']",1202,[],['adfuller'],1
data/docs/__preprocessor_tseries_doc.py:gskew,gskew,function,8,21,18,250,11.9,0,0,['x'],[None],[None],1229,[],"['np.median', 'np.percentile']",2
data/docs/__preprocessor_tseries_doc.py:stetson_mean,stetson_mean,function,21,48,43,399,8.31,1,1,"['x', 'param']","[None, None]","[None, 'stestson_param']",1251,[],"['np.median', 'range', 'np.abs', 'np.sqrt', 'weight1.mean', 'np.mean']",6
data/docs/__preprocessor_tseries_doc.py:length,length,function,1,2,2,12,6.0,0,0,['x'],[None],[None],1281,[],['len'],1
data/docs/__preprocessor_tseries_doc.py:count_above_mean,count_above_mean,function,4,6,6,40,6.67,0,0,['x'],[None],[None],1294,[],"['np.mean', 'np.where']",2
data/docs/__preprocessor_tseries_doc.py:get_length_sequences_where,get_length_sequences_where,function,2,24,20,126,5.25,0,2,['x'],[None],[None],1309,[],"['len', 'itertools.groupby']",2
data/docs/__preprocessor_tseries_doc.py:longest_strike_below_mean,longest_strike_below_mean,function,5,16,14,129,8.06,0,1,['x'],[None],[None],1317,[],"['isinstance', 'np.asarray', 'np.max', 'np.mean']",4
data/docs/__preprocessor_tseries_doc.py:wozniak,wozniak,function,15,74,47,497,6.72,3,3,"['magnitude', 'param']","[None, None]","[None, 'woz_param']",1334,[],"['len', 'np.std', 'np.mean', 'range', 'if', 'iters.append', 'enumerate']",7
data/docs/__preprocessor_tseries_doc.py:last_location_of_maximum,last_location_of_maximum,function,4,12,11,71,5.92,0,0,['x'],[None],[None],1371,[],"['np.asarray', 'np.argmax', 'len']",3
data/docs/__preprocessor_tseries_doc.py:fft_coefficient,fft_coefficient,function,17,80,53,602,7.53,1,1,"['x', 'param = [{""coeff""', '""attr""']","[None, '', ' ""real""}]']","[None, ' [{""coeff"": 10', None]",1388,[],"['min', 'set', 'complex_agg', 'np.abs', 'np.angle', 'len']",6
data/docs/__preprocessor_tseries_doc.py:ar_coefficient,ar_coefficient,function,20,56,44,563,10.05,1,2,"['x', 'param=[{""coeff""', '""k""']","[None, '', ' 5}]']","[None, '[{""coeff"": 5', None]",1421,[],"['list', 'AR', 'calculated_AR.fit', 'res.items']",4
data/docs/__preprocessor_tseries_doc.py:index_mass_quantile,index_mass_quantile,function,9,28,22,259,9.25,0,1,"['x', 'param=[{""q""']","[None, '']","[None, '[{""q"": 0.3}]']",1467,[],"['np.asarray', 'np.abs', 'sum', 'np.cumsum']",4
data/docs/__preprocessor_tseries_doc.py:number_cwt_peaks,number_cwt_peaks,function,1,11,10,124,11.27,0,0,"['x', 'param']","[None, None]","[None, 'cwt_param']",1496,[],['len'],1
data/docs/__preprocessor_tseries_doc.py:spkt_welch_density,spkt_welch_density,function,11,62,46,500,8.06,2,2,"['x', 'param=[{""coeff""']","[None, '']","[None, '[{""coeff"": 5}]']",1512,[],"['welch', 'nperseg=min', 'len', 'np.max', 'zip', 'list']",6
data/docs/__preprocessor_tseries_doc.py:linear_trend_timewise,linear_trend_timewise,function,10,19,19,236,12.42,1,0,"['x', 'param= [{""attr""']","[None, '']","[None, ' [{""attr"": ""pvalue""}]']",1543,[],"['np.asarray', 'float', 'linregress', 'getattr']",4
data/docs/__preprocessor_tseries_doc.py:c3,c3,function,8,25,20,155,6.2,0,2,"['x', 'lag']","[None, None]","[None, '3']",1565,[],"['isinstance', 'np.asarray', 'np.mean', '_roll']",4
data/docs/__preprocessor_tseries_doc.py:binned_entropy,binned_entropy,function,11,24,21,170,7.08,0,1,"['x', 'max_bins']","[None, None]","[None, '10']",1586,[],"['isinstance', 'np.asarray', 'np.histogram', 'np.sum']",4
data/docs/__preprocessor_tseries_doc.py:_embed_seq,_embed_seq,function,16,53,41,218,4.11,2,2,"['X', 'Tau', 'D']","[None, None, None]","[None, None, None]",1604,[],"['print', 'exit', 'np.zeros', 'range']",4
data/docs/__preprocessor_tseries_doc.py:svd_entropy,svd_entropy,function,16,47,40,383,8.15,1,0,"['epochs', 'param']","[None, None]","[None, 'svd_param']",1621,[],"['svd_entropy_1d', '_embed_seq', 'sum', 'np.sum', 'np.log', 'final.append', 'enumerate']",7
data/docs/__preprocessor_tseries_doc.py:_hjorth_mobility,_hjorth_mobility,function,7,12,10,114,9.5,0,0,['epochs'],[None],[None],1648,[],"['np.diff', 'np.std', 'np.divide']",3
data/docs/__preprocessor_tseries_doc.py:hjorth_complexity,hjorth_complexity,function,8,16,13,179,11.19,0,0,['epochs'],[None],[None],1656,[],"['np.diff', 'np.std', 'np.divide', '_hjorth_mobility']",4
data/docs/__preprocessor_tseries_doc.py:_estimate_friedrich_coefficients,_estimate_friedrich_coefficients,function,20,49,43,475,9.69,0,0,"['x', 'm', 'r']","[None, None, None]","[None, None, None]",1675,[],"['pd.DataFrame', 'np.diff', 'pd.qcut', 'df.groupby', 'result.dropna', 'np.polyfit']",6
data/docs/__preprocessor_tseries_doc.py:max_langevin_fixed_point,max_langevin_fixed_point,function,6,14,12,176,12.57,0,0,"['x', 'r', 'm']","[None, None, None]","[None, '3', '30']",1694,[],"['_estimate_friedrich_coefficients', 'np.max']",2
data/docs/__preprocessor_tseries_doc.py:willison_amplitude,willison_amplitude,function,1,7,7,72,10.29,0,0,"['X', 'param']","[None, None]","[None, 'will_param']",1719,[],[],0
data/docs/__preprocessor_tseries_doc.py:percent_amplitude,percent_amplitude,function,13,30,28,334,11.13,1,0,"['x', 'param ']","[None, None]","[None, 'perc_param']",1734,[],"['np.max', 'np.min', 'np.median', 'final.append', 'abs', 'zip']",6
data/docs/__preprocessor_tseries_doc.py:cad_prob,cad_prob,function,1,11,11,106,9.64,0,0,"['cads', 'param']","[None, None]","[None, 'cad_param']",1761,[],"['stats.percentileofscore', 'float']",2
data/docs/__preprocessor_tseries_doc.py:zero_crossing_derivative,zero_crossing_derivative,function,5,20,20,171,8.55,0,0,"['epochs', 'param']","[None, None]","[None, 'zero_param']",1779,[],"['np.diff', 'np.apply_along_axis', 'np.sum']",3
data/docs/__preprocessor_tseries_doc.py:detrended_fluctuation_analysis,detrended_fluctuation_analysis,function,32,123,92,751,6.11,2,4,['epochs'],[None],[None],1801,[],"['dfa_1d', 'np.array', 'np.mean', 'np.cumsum', 'np.floor', 'int', 'np.zeros', 'F', 'range', 'len', 'print', 'exit', 'list', 'np.vstack', 'np.ones', 'np.sqrt', 'np.log', 'np.apply_along_axis']",18
data/docs/__preprocessor_tseries_doc.py:_embed_seq,_embed_seq,function,16,53,41,218,4.11,2,2,"['X', 'Tau', 'D']","[None, None, None]","[None, None, None]",1855,"['""""""#### **(28) Fractals**\n', '\n', 'In mathematics, more specifically in fractal geometry, a fractal dimension is a ratio providing a statistical index of complexity comparing how detail in a pattern (strictly speaking, a fractal pattern) changes with the scale at which it is measured.\n', '\n', '(i) Highuchi Fractal\n', '\n', 'Compute a Higuchi Fractal Dimension of a time series\n', '""""""\n']","['print', 'exit', 'np.zeros', 'range']",4
data/docs/__preprocessor_tseries_doc.py:fisher_information,fisher_information,function,9,37,34,319,8.62,0,0,"['epochs', 'param']","[None, None]","[None, 'fisher_param']",1865,[],"['fisher_info_1d', '_embed_seq', 'sum', 'np.sum']",4
data/docs/__preprocessor_tseries_doc.py:higuchi_fractal_dimension,higuchi_fractal_dimension,function,16,72,54,453,6.29,3,0,"['epochs', 'param']","[None, None]","[None, 'hig_para']",1892,[],"['hfd_1d', 'len', 'range', 'int', 'abs', 'np.floor', 'float', 'Lk.append', 'L.append', 'x.append', 'np.apply_along_axis']",11
data/docs/__preprocessor_tseries_doc.py:petrosian_fractal_dimension,petrosian_fractal_dimension,function,20,50,42,282,5.64,1,2,['epochs'],[None],[None],1923,[],"['pfd_1d', 'np.diff', 'D.tolist', 'range', 'len', 'np.log10', 'np.apply_along_axis']",7
data/docs/__preprocessor_tseries_doc.py:hurst_exponent,hurst_exponent,function,39,85,66,542,6.38,3,2,['epochs'],[None],[None],1958,[],"['hurst_1d', 'np.array', 'np.arange', 'np.cumsum', 'np.zeros', 'range', 'np.std', 'np.ptp', 'len', 'np.diff', 'max', 'np.log', 'np.column_stack', 'np.ones', 'np.apply_along_axis']",15
data/docs/__preprocessor_tseries_doc.py:_embed_seq,_embed_seq,function,16,53,41,218,4.11,2,2,"['X', 'Tau', 'D']","[None, None, None]","[None, None, None]",1855,"['""""""#### **(28) Fractals**\n', '\n', 'In mathematics, more specifically in fractal geometry, a fractal dimension is a ratio providing a statistical index of complexity comparing how detail in a pattern (strictly speaking, a fractal pattern) changes with the scale at which it is measured.\n', '\n', '(i) Highuchi Fractal\n', '\n', 'Compute a Higuchi Fractal Dimension of a time series\n', '""""""\n']","['print', 'exit', 'np.zeros', 'range']",4
data/docs/__preprocessor_tseries_doc.py:largest_lyauponov_exponent,largest_lyauponov_exponent,function,43,142,119,1210,8.52,0,0,"['epochs', 'param']","[None, None]","[None, 'lyaup_param']",2009,[],"['LLE_1d', '_embed_seq', 'len', 'np.tile', 'np.transpose', 'np.sqrt', 'np.tri', 'np.logical_and', 'np.sum', 'np.arange', 'np.vstack', 'np.ones', 'np.apply_along_axis']",13
data/docs/__preprocessor_tseries_doc.py:whelch_method,whelch_method,function,15,31,29,272,8.77,1,0,"['data', 'param']","[None, None]","[None, 'whelch_param']",2072,[],"['signal.welch', 'pd.DataFrame', 'df.sort_values', 'final.append', 'zip']",5
data/docs/__preprocessor_tseries_doc.py:find_freq,find_freq,function,18,39,35,441,11.31,1,0,"['serie', 'param']","[None, None]","[None, 'freq_param']",2092,[],"['np.array', 'range', 'len', 'pd.DataFrame', 'df.sort_values', 'final.append', 'zip']",7
data/docs/__preprocessor_tseries_doc.py:flux_perc,flux_perc,function,15,28,24,400,14.29,0,0,['magnitude'],[None],[None],2119,[],"['np.sort', 'len', 'int']",3
data/docs/__preprocessor_tseries_doc.py:range_cum_s,range_cum_s,function,10,18,18,138,7.67,0,0,['magnitude'],[None],[None],2143,[],"['np.std', 'len', 'np.mean', 'np.cumsum', 'np.max', 'np.min']",6
data/docs/__preprocessor_tseries_doc.py:structure_func,structure_func,function,39,116,84,1092,9.41,2,3,"['time', 'param']","[None, None]","[None, 'struct_param']",2168,[],"['param.items', 'np.zeros', 'interp1d', 'np.linspace', 'np.max', 'f', 'np.arange', 'np.mean', 'np.power', 'np.abs', 'np.log10', 'len', 'np.polyfit', 'dict_final.items', 'zip']",15
data/docs/__preprocessor_tseries_doc.py:kurtosis,kurtosis,function,5,8,8,72,9.0,0,1,['x'],[None],[None],2226,[],"['isinstance', 'pd.Series']",2
data/docs/__preprocessor_tseries_doc.py:stetson_k,stetson_k,function,8,19,18,147,7.74,0,0,['x'],[None],[None],2238,"['    """"""A robust kurtosis statistic.""""""\n']","['len', 'stetson_mean', 'np.sqrt', 'np.mean']",4
data/docs/__preprocessor_tseries_doc.py:model,model,function,12,21,13,437,20.81,0,0,['df_final'],[None],[None],2256,[],"['LGBMRegressor', 'df_final.head', 'model.fit', 'model.predict', 'mean_squared_error']",5
data/docs/__preprocessor_tseries_doc.py:model,model,function,12,21,13,437,20.81,0,0,['df_final'],[None],[None],2256,[],"['LGBMRegressor', 'df_final.head', 'model.fit', 'model.predict', 'mean_squared_error']",5
data/docs/__preprocessor_tseries_doc.py:feature_agg,feature_agg,function,14,23,17,294,12.78,0,2,"['df', 'drop', 'components']","[None, None, None]","[None, 'None', '4']",2473,[],"['df.drop', 'min', 'cluster.FeatureAgglomeration', 'agglo.fit', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",7
example/classifier/classifier_adfraud.py:global_pars_update,global_pars_update,function,22,61,48,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",31,[],['print'],1
example/classifier/classifier_adfraud.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],241,[],[],0
example/classifier/classifier_airline.py:global_pars_update,global_pars_update,function,22,61,48,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",20,[],['print'],1
example/classifier/classifier_airline.py:airline_lightgbm,airline_lightgbm,function,8,199,137,2067,10.39,0,0,['path_model_out'],[None],"['""""']",71,[],"['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/classifier/classifier_airline.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],177,[],[],0
example/classifier/classifier_bankloan.py:global_pars_update,global_pars_update,function,22,61,48,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",22,[],['print'],1
example/classifier/classifier_bankloan.py:bank_lightgbm,bank_lightgbm,function,8,153,102,1658,10.84,0,0,[],[],[],84,[],"['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/classifier/classifier_bankloan.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],192,[],[],0
example/classifier/classifier_cardiff.py:global_pars_update,global_pars_update,function,22,61,48,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",22,[],['print'],1
example/classifier/classifier_cardiff.py:cardif_lightgbm,cardif_lightgbm,function,8,145,88,1386,9.56,0,0,['path_model_out'],[None],"['""""']",90,"['    """"""\n', '       cardiff\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/classifier/classifier_cardiff.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],198,[],[],0
example/classifier/classifier_income.py:global_pars_update,global_pars_update,function,22,61,48,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",24,[],['print'],1
example/classifier/classifier_income.py:income_status_lightgbm,income_status_lightgbm,function,7,47,38,409,8.7,0,0,['path_model_out'],[None],"['""""']",83,"['    """"""\n', '\n', '\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun']",3
example/classifier/classifier_income.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],202,[],[],0
example/classifier/classifier_multi.py:global_pars_update,global_pars_update,function,22,61,48,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",26,[],['print'],1
example/classifier/classifier_multi.py:multi_lightgbm,multi_lightgbm,function,8,172,101,1662,9.66,0,0,[],[],[],87,"['    """"""\n', '       multiclass\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun_multi', 'global_pars_update', 'config_name=os_get_function_name']",5
example/classifier/classifier_multi.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],186,[],[],0
example/classifier/classifier_optuna.py:global_pars_update,global_pars_update,function,22,61,48,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",23,[],['print'],1
example/classifier/classifier_optuna.py:titanic_lightoptuna,titanic_lightoptuna,function,10,135,101,1365,10.11,0,0,[],[],[],89,"['    """"""\n', '       Contains all needed informations for Light GBM Classifier model,\n', '       used for titanic classification task\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun', 'global_pars_update']",5
example/classifier/classifier_optuna.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],205,[],[],0
example/classifier/classifier_sentiment.py:global_pars_update,global_pars_update,function,14,25,25,460,18.4,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",36,[],[],0
example/classifier/classifier_sentiment.py:os_get_function_name,os_get_function_name,function,4,4,4,47,11.75,0,0,[],[],[],53,[],['sys._getframe'],1
example/classifier/classifier_sentiment.py:sentiment_lightgbm,sentiment_lightgbm,function,11,93,79,901,9.69,0,0,['path_model_out'],[None],"['""""']",84,"['    """"""\n', '\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'copy.deepcopy', 'global_pars_update', 'config_name=os_get_function_name']",6
example/classifier/classifier_sentiment.py:data_profile,data_profile,function,4,12,12,143,11.92,0,0,"['path_data_train', 'path_model', 'n_sample']","[None, None, None]","['""""', '""""', ' 5000']",157,[],['run_profile'],1
example/classifier/classifier_sentiment.py:preprocess,preprocess,function,14,38,31,398,10.47,0,1,"['config', 'nsample']","[None, None]","['None', 'None']",167,[],"['globals', 'print', 'run_preprocess.run_preprocess']",3
example/classifier/classifier_sentiment.py:train,train,function,13,36,29,342,9.5,0,1,"['config', 'nsample']","[None, None]","['None', 'None']",184,[],"['globals', 'print', 'run_train.run_train']",3
example/classifier/classifier_sentiment.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],202,[],[],0
example/classifier/classifier_sentiment.py:predict,predict,function,13,38,31,442,11.63,0,1,"['config', 'nsample']","[None, None]","['None', 'None']",210,[],"['globals', 'print', 'run_inference.run_predict']",3
example/classifier/classifier_sentiment.py:run_all,run_all,function,5,5,5,53,10.6,0,0,[],[],[],227,[],"['data_profile', 'preprocess', 'train', 'check', 'predict']",5
example/classifier/classifier_sentiment.py:sentiment_elasticnetcv,sentiment_elasticnetcv,function,6,95,74,903,9.51,0,0,['path_model_out'],[None],"['""""']",262,[],"['post_process_fun', 'y_norm', 'pre_process_fun']",3
example/classifier/classifier_sentiment.py:sentiment_bayesian_pyro,sentiment_bayesian_pyro,function,7,74,61,771,10.42,0,0,['path_model_out'],[None],"['""""']",302,[],"['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy']",4
example/click/online_shopping.py:global_pars_update,global_pars_update,function,21,53,38,1029,19.42,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",23,[],['print'],1
example/click/online_shopping.py:online_lightgbm,online_lightgbm,function,8,163,104,1715,10.52,0,0,[],[],[],84,"['    """"""\n', '       Contains all needed informations for Light GBM Classifier model,\n', '       used for titanic classification task\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/click/online_shopping.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],224,[],[],0
example/click/outlier_predict.py:os_get_function_name,os_get_function_name,function,4,4,4,47,11.75,0,0,[],[],[],26,[],['sys._getframe'],1
example/click/outlier_predict.py:global_pars_update,global_pars_update,function,20,48,33,960,20.0,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",31,[],[],0
example/click/outlier_predict.py:titanic_pyod,titanic_pyod,function,13,171,107,1354,7.92,0,1,['path_model_out'],[None],"['""""']",81,"['    """""" All Models  :    https://pyod.readthedocs.io/en/latest/pyod.html\n', '        pyod.models.abod \n', '        pyod.models.auto_encoder \n', '        pyod.models.cblof \n', '        pyod.models.cof \n', '        pyod.models.combination \n', '        pyod.models.copod \n', '        pyod.models.feature_bagging \n', '        pyod.models.hbos \n', '        pyod.models.iforest \n', '        pyod.models.knn \n', '        pyod.models.lmdd \n', '        pyod.models.loda \n', '        pyod.models.lof \n', '        pyod.models.loci \n', '        pyod.models.lscp \n', '        pyod.models.mad \n', '        pyod.models.mcd \n', '        pyod.models.mo_gaal \n', '        pyod.models.ocsvm \n', '        pyod.models.pca \n', '        pyod.models.sod \n', '        pyod.models.so_gaal \n', '        pyod.models.sos \n', '        pyod.models.vae \n', '        pyod.models.xgbod \n', '         contents\n', '        Utility Functions\n', '        pyod.utils.data \n', '        pyod.utils.example \n', '        pyod.utils.stat_models \n', '        pyod.utils.utility \n', '\n', '        clf.fit(X)\n', '        scores_pred = clf.decision_function(X) * -1\n', '        y_pred = clf.predict(X)\n', '\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'float', 'pre_process_fun', 'global_pars_update']",5
example/click/outlier_predict.py:data_profile,data_profile,function,4,12,12,143,11.92,0,0,"['path_data_train', 'path_model', 'n_sample']","[None, None, None]","['""""', '""""', ' 5000']",194,[],['run_profile'],1
example/click/outlier_predict.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],218,[],[],0
example/click/test_online_shopping.py:global_pars_update,global_pars_update,function,21,53,38,1029,19.42,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",23,[],['print'],1
example/click/test_online_shopping.py:online_lightgbm,online_lightgbm,function,8,176,109,1840,10.45,0,0,[],[],[],84,"['    """"""\n', '       Contains all needed informations for Light GBM Classifier model,\n', '       used for titanic classification task\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/click/test_online_shopping.py:pd_col_myfun,pd_col_myfun,function,22,83,64,813,9.8,0,3,"['df', 'col', 'pars']","[None, None, None]","['None', 'None', '{}']",173,"['    """"""\n', '         Example of custom Processor\n', '    """"""\n']","['load', 'list', 'save', 'pars.get']",4
example/click/test_online_shopping.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],258,[],[],0
example/notebooks/core_allimport.py:save,save,function,8,17,17,121,7.12,1,0,"['path', 'name_list', 'glob']","[None, None, None]","[None, None, None]",75,[],"['os.makedirs', 'print', 'pickle.dump', 'open']",4
example/notebooks/core_allimport.py:load,load,function,4,9,9,61,6.78,0,0,['name'],[None],[None],83,[],"['pickle.load', 'open']",2
example/regress/regress_airbnb.py:global_pars_update,global_pars_update,function,22,62,47,1124,18.13,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",23,[],['print'],1
example/regress/regress_airbnb.py:y_norm,y_norm,function,15,70,41,311,4.44,1,4,"['y', 'inverse', 'mode']","[None, None, None]","[None, 'True', ""'boxcox'""]",70,[],[],0
example/regress/regress_airbnb.py:airbnb_lightgbm,airbnb_lightgbm,function,11,173,102,1772,10.24,0,0,['path_model_out'],[None],"['""""']",123,"['    """"""\n', '\n', '    """"""\n']","['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy', 'global_pars_update', 'os_get_function_name']",6
example/regress/regress_airbnb.py:airbnb_elasticnetcv,airbnb_elasticnetcv,function,6,95,74,903,9.51,0,0,['path_model_out'],[None],"['""""']",239,[],"['post_process_fun', 'y_norm', 'pre_process_fun']",3
example/regress/regress_boston.py:global_pars_update,global_pars_update,function,22,61,46,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",31,[],['print'],1
example/regress/regress_boston.py:y_norm,y_norm,function,15,70,41,312,4.46,1,4,"['y', 'inverse', 'mode']","[None, None, None]","[None, 'True', ""'boxcox'""]",90,[],[],0
example/regress/regress_boston.py:boston_lightgbm,boston_lightgbm,function,11,146,91,1546,10.59,0,0,['path_model_out'],[None],"['""""']",120,"['    """"""\n', '        Huber Loss includes L1  regurarlization\n', '        We test different features combinaison, default params is optimal\n', '    """"""\n']","['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy', 'global_pars_update', 'os_get_function_name']",6
example/regress/regress_boston.py:boston_causalnex,boston_causalnex,function,8,167,99,1549,9.28,0,0,['path_model_out'],[None],"['""""']",180,"['    """"""\n', '       Contains all needed informations for Light GBM Classifier model,\n', '       used for titanic classification task\n', '    """"""\n']","['post_process_fun', 'int', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/regress/regress_house.py:global_pars_update,global_pars_update,function,22,61,46,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",28,[],['print'],1
example/regress/regress_house.py:y_norm,y_norm,function,15,70,41,300,4.29,1,4,"['y', 'inverse', 'mode']","[None, None, None]","[None, 'True', ""'boxcox'""]",110,[],[],0
example/regress/regress_house.py:house_price_lightgbm,house_price_lightgbm,function,10,100,75,892,8.92,0,0,['path_model_out'],[None],"['""""']",140,"['    """"""\n', '        Huber Loss includes L1  regurarlization\n', '        We test different features combinaison, default params is optimal\n', '    """"""\n']","['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy', 'global_pars_update', 'config_name=os_get_function_name']",6
example/regress/regress_house.py:house_price_elasticnetcv,house_price_elasticnetcv,function,9,87,68,864,9.93,0,0,['path_model_out'],[None],"['""""']",197,[],"['post_process_fun', 'y_norm', 'pre_process_fun', 'global_pars_update', 'config_name=os_get_function_name']",5
example/regress/regress_house.py:data_profile,data_profile,function,4,12,12,139,11.58,0,0,[],[],[],254,[],['run_profile'],1
example/regress/regress_house.py:preprocess,preprocess,function,5,16,16,222,13.88,0,0,[],[],[],265,[],['run_preprocess_old.run_preprocess'],1
example/regress/regress_house.py:train,train,function,5,14,14,176,12.57,0,0,[],[],[],277,[],['run_train.run_train'],1
example/regress/regress_house.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],287,[],[],0
example/regress/regress_house.py:predict,predict,function,5,13,13,165,12.69,0,0,[],[],[],293,[],['run_inference.run_predict'],1
example/regress/regress_house.py:run_all,run_all,function,5,5,5,53,10.6,0,0,[],[],[],302,[],"['data_profile', 'preprocess', 'train', 'check', 'predict']",5
example/regress/regress_salary.py:global_pars_update,global_pars_update,function,22,61,46,1124,18.43,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",22,[],['print'],1
example/regress/regress_salary.py:y_norm,y_norm,function,15,70,41,312,4.46,1,4,"['y', 'inverse', 'mode']","[None, None, None]","[None, 'True', ""'boxcox'""]",85,[],[],0
example/regress/regress_salary.py:salary_lightgbm,salary_lightgbm,function,11,146,91,1548,10.6,0,0,['path_model_out'],[None],"['""""']",115,"['    """"""\n', '        Huber Loss includes L1  regurarlization\n', '        We test different features combinaison, default params is optimal\n', '    """"""\n']","['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy', 'global_pars_update', 'os_get_function_name']",6
example/regress/regress_salary.py:salary_elasticnetcv,salary_elasticnetcv,function,8,147,88,1554,10.57,0,0,['path_model_out'],[None],"['""""']",175,[],"['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy', 'global_pars_update', 'model_class=os_get_function_name']",6
example/regress/regress_salary.py:salary_bayesian_pyro,salary_bayesian_pyro,function,8,159,98,1629,10.25,0,0,['path_model_out'],[None],"['""""']",225,[],"['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy', 'global_pars_update', 'os_get_function_name']",6
example/regress/regress_salary.py:salary_glm,salary_glm,function,7,147,89,1483,10.09,0,0,['path_model_out'],[None],"['""""']",277,[],"['post_process_fun', 'y_norm', 'pre_process_fun', 'global_pars_update']",4
example/regress/regress_salary.py:check,check,function,0,1,1,4,4.0,0,0,[],[],[],342,[],[],0
example/svd/benchmark_mf.py:linear_surplus_confidence_matrix,linear_surplus_confidence_matrix,function,5,7,6,38,5.43,0,0,"['B', 'alpha']","[None, None]","[None, None]",47,[],['B.copy'],1
example/svd/benchmark_mf.py:log_surplus_confidence_matrix,log_surplus_confidence_matrix,function,6,9,7,56,6.22,0,0,"['B', 'alpha', 'epsilon']","[None, None, None]","[None, None, None]",54,[],"['B.copy', 'np.log']",2
example/svd/benchmark_mf.py:iter_rows,iter_rows,function,9,13,13,94,7.23,1,0,['S'],[None],[None],61,"['    """"""\n', '    Helper function to iterate quickly over the data and indices of the\n', '    rows of the S matrix. A naive implementation using indexing\n', '    on S is much, much slower.\n', '    """"""\n']",['range'],1
example/svd/benchmark_mf.py:recompute_factors,recompute_factors,function,50,198,110,1065,5.38,2,0,"['Y', 'S', 'lambda_reg', 'dtype']","[None, None, None, None]","[None, None, None, ""'float32'""]",71,"['    """"""\n', '    recompute matrix X from Y.\n', '    X = recompute_factors(Y, S, lambda_reg)\n', '    This can also be used for the reverse operation as follows:\n', '    Y = recompute_factors(X, ST, lambda_reg)\n', '\n', '    The comments are in terms of X being the users and Y being the items.\n', '    """"""\n']","['np.dot', 'np.eye', 'np.zeros', 'iter_rows', 's_u.reshape', 'recompute_factors_bias', 'Y.copy']",7
example/svd/benchmark_mf.py:recompute_factors_bias,recompute_factors_bias,function,47,123,94,613,4.98,1,0,"['Y', 'S', 'lambda_reg', 'dtype']","[None, None, None, None]","[None, None, None, ""'float32'""]",98,"['    """"""\n', '    Like recompute_factors, but the last column of X and Y is treated as\n', '    a bias vector.\n', '    """"""\n']","['Y.copy', 'np.dot', 'np.eye', 'np.zeros', 'iter_rows']",5
example/svd/benchmark_mf.py:factorize,factorize,function,22,43,35,313,7.28,1,0,"['S', 'num_factors', 'lambda_reg', 'num_iterations', 'init_std', 'verbose', 'dtype', 'recompute_factors', '*args', '**kwargs']","[None, None, None, None, None, None, None, None, None, None]","[None, None, '1e-5', '20', '0.01', 'False', ""'float32'"", 'recompute_factors', None, None]",138,"['    """"""\n', '    factorize a given sparse matrix using the Weighted Matrix Factorization algorithm by\n', '    Hu, Koren and Volinsky.\n', ""    S: 'surplus' confidence matrix, i.e. C - I where C is the matrix with confidence weights.\n"", '        S is sparse while C is not (and the sparsity pattern of S is the same as that of\n', ""        the preference matrix, so it doesn't need to be specified separately).\n"", '    num_factors: the number of factors.\n', '    lambda_reg: the value of the regularization constant.\n', '    num_iterations: the number of iterations to run the algorithm for. Each iteration consists\n', '        of two steps, one to recompute U given V, and one to recompute V given U.\n', '    init_std: the standard deviation of the Gaussian with which V is initialized.\n', '    verbose: print a bunch of stuff during training, including timing information.\n', '    dtype: the dtype of the resulting factor matrices. Using single precision is recommended,\n', '        it speeds things up a bit.\n', '    recompute_factors: helper function that implements the inner loop.\n', '    returns:\n', '        U, V: factor matrices. If bias=True, the last columns of the matrices contain the biases.\n', '    """"""\n']","['range', 'recompute_factors']",2
example/svd/benchmark_mf.py:time_reps,time_reps,function,7,14,14,109,7.79,1,0,"['func', 'params', 'reps']","[None, None, None]","[None, None, None]",217,[],"['time.time', 'range', 'func', 'print', 'str']",5
example/svd/benchmark_mf.py:scipy_svd,scipy_svd,function,2,4,4,45,11.25,0,0,"['A', 'K']","[None, None]","[None, None]",234,[],['svds'],1
example/svd/benchmark_mf.py:sklearn_randomized_svd,sklearn_randomized_svd,function,2,3,3,25,8.33,0,0,"['A', 'k']","[None, None]","[None, None]",237,[],['randomized_svd'],1
example/svd/benchmark_mf.py:sklearn_truncated_randomized_svd,sklearn_truncated_randomized_svd,function,2,5,5,61,12.2,0,0,"['A', 'k']","[None, None]","[None, None]",240,[],['TruncatedSVD'],1
example/svd/benchmark_mf.py:sklearn_truncated_arpack_svd,sklearn_truncated_arpack_svd,function,2,5,5,57,11.4,0,0,"['A', 'k']","[None, None]","[None, None]",243,[],['TruncatedSVD'],1
example/svd/benchmark_mf.py:sparsesvd_svd,sparsesvd_svd,function,2,3,3,20,6.67,0,0,"['A', 'k']","[None, None]","[None, None]",246,[],['sparsesvd'],1
example/svd/benchmark_mf.py:gensim_svd,gensim_svd,function,2,4,4,36,9.0,0,0,"['A', 'k']","[None, None]","[None, None]",249,[],['stochastic_svd'],1
example/svd/benchmark_mf.py:wmf,wmf,function,3,7,7,105,15.0,0,0,"['A', 'k']","[None, None]","[None, None]",253,[],['factorize'],1
example/svd/benchmark_mf.py:implicit_mf,implicit_mf,function,2,3,3,29,9.67,0,0,"['A', 'k']","[None, None]","[None, None]",257,[],['ImplicitMF'],1
example/svd/benchmark_mf.py:nmf_1,nmf_1,function,2,4,4,48,12.0,0,0,"['A', 'k']","[None, None]","[None, None]",260,[],['NMF_ANLS_BLOCKPIVOT'],1
example/svd/benchmark_mf.py:nmf_2,nmf_2,function,2,4,4,46,11.5,0,0,"['A', 'k']","[None, None]","[None, None]",263,[],['NMF_ANLS_AS_NUMPY'],1
example/svd/benchmark_mf.py:nmf_3,nmf_3,function,2,4,4,46,11.5,0,0,"['A', 'k']","[None, None]","[None, None]",266,[],['NMF_ANLS_AS_GROUP'],1
example/svd/benchmark_mf.py:nmf_4,nmf_4,function,2,4,4,38,9.5,0,0,"['A', 'k']","[None, None]","[None, None]",269,[],['NMF_HALS'],1
example/svd/benchmark_mf.py:nmf_5,nmf_5,function,2,4,4,36,9.0,0,0,"['A', 'k']","[None, None]","[None, None]",272,[],['NMF_MU'],1
example/svd/benchmark_mf.py:daal4py_svd,daal4py_svd,function,8,9,9,156,17.33,0,0,"['A', 'k)', 'k)']","[None, '  # only dense inputs)A)A', '']","[None, None, None]",276,[],"['d4p.implicit_als_training_init', 'algo1.compute', 'd4p.implicit_als_training', 'algo2.compute']",4
example/svd/benchmark_mf.py:daal4py_als,daal4py_als,function,8,9,9,156,17.33,0,0,"['A', 'k']","[None, None]","[None, None]",280,[],"['d4p.implicit_als_training_init', 'algo1.compute', 'd4p.implicit_als_training', 'algo2.compute']",4
example/svd/benchmark_mf.py:time_ns,time_ns,function,33,125,94,1213,9.7,1,0,[],[],[],287,[],"['np.arange', 'dict', 'enumerate', 'int', 'print', 'time_reps']",6
example/svd/benchmark_mf.py:ImplicitMF,ImplicitMF,class,49,94,79,1263,13.44,2,2,[],[],[],171,[],[],0
example/svd/benchmark_mf.py:ImplicitMF:__init__,ImplicitMF:__init__,method,12,12,12,169,14.08,0,0,"['self', 'counts', 'num_factors', 'num_iterations', 'reg_param']","[None, None, None, None, None]","[None, None, '40', '30', '0.8']",173,[],[],0
example/svd/benchmark_mf.py:ImplicitMF:train_model,ImplicitMF:train_model,method,6,16,13,338,21.12,1,0,['self'],[None],[None],182,[],"['range', 'self.iteration', 'sparse.csr_matrix']",3
example/svd/benchmark_mf.py:ImplicitMF:iteration,ImplicitMF:iteration,method,33,54,50,622,11.52,1,2,"['self', 'user', 'fixed_vecs']","[None, None, None]","[None, None, None]",192,[],"['sparse.eye', 'np.zeros', 'time.time', 'range', 'sparse.diags', 'counts_i.copy', 'spsolve']",7
example/svd/benchmark_mf0.py:linear_surplus_confidence_matrix,linear_surplus_confidence_matrix,function,5,7,6,38,5.43,0,0,"['B', 'alpha']","[None, None]","[None, None]",45,[],['B.copy'],1
example/svd/benchmark_mf0.py:log_surplus_confidence_matrix,log_surplus_confidence_matrix,function,6,9,7,56,6.22,0,0,"['B', 'alpha', 'epsilon']","[None, None, None]","[None, None, None]",52,[],"['B.copy', 'np.log']",2
example/svd/benchmark_mf0.py:iter_rows,iter_rows,function,9,13,13,94,7.23,1,0,['S'],[None],[None],59,"['    """"""\n', '    Helper function to iterate quickly over the data and indices of the\n', '    rows of the S matrix. A naive implementation using indexing\n', '    on S is much, much slower.\n', '    """"""\n']",['range'],1
example/svd/benchmark_mf0.py:recompute_factors,recompute_factors,function,50,198,110,1065,5.38,2,0,"['Y', 'S', 'lambda_reg', 'dtype']","[None, None, None, None]","[None, None, None, ""'float32'""]",69,"['    """"""\n', '    recompute matrix X from Y.\n', '    X = recompute_factors(Y, S, lambda_reg)\n', '    This can also be used for the reverse operation as follows:\n', '    Y = recompute_factors(X, ST, lambda_reg)\n', '\n', '    The comments are in terms of X being the users and Y being the items.\n', '    """"""\n']","['np.dot', 'np.eye', 'np.zeros', 'iter_rows', 's_u.reshape', 'recompute_factors_bias', 'Y.copy']",7
example/svd/benchmark_mf0.py:recompute_factors_bias,recompute_factors_bias,function,47,123,94,613,4.98,1,0,"['Y', 'S', 'lambda_reg', 'dtype']","[None, None, None, None]","[None, None, None, ""'float32'""]",96,"['    """"""\n', '    Like recompute_factors, but the last column of X and Y is treated as\n', '    a bias vector.\n', '    """"""\n']","['Y.copy', 'np.dot', 'np.eye', 'np.zeros', 'iter_rows']",5
example/svd/benchmark_mf0.py:factorize,factorize,function,22,43,35,313,7.28,1,0,"['S', 'num_factors', 'lambda_reg', 'num_iterations', 'init_std', 'verbose', 'dtype', 'recompute_factors', '*args', '**kwargs']","[None, None, None, None, None, None, None, None, None, None]","[None, None, '1e-5', '20', '0.01', 'False', ""'float32'"", 'recompute_factors', None, None]",136,"['    """"""\n', '    factorize a given sparse matrix using the Weighted Matrix Factorization algorithm by\n', '    Hu, Koren and Volinsky.\n', ""    S: 'surplus' confidence matrix, i.e. C - I where C is the matrix with confidence weights.\n"", '        S is sparse while C is not (and the sparsity pattern of S is the same as that of\n', ""        the preference matrix, so it doesn't need to be specified separately).\n"", '    num_factors: the number of factors.\n', '    lambda_reg: the value of the regularization constant.\n', '    num_iterations: the number of iterations to run the algorithm for. Each iteration consists\n', '        of two steps, one to recompute U given V, and one to recompute V given U.\n', '    init_std: the standard deviation of the Gaussian with which V is initialized.\n', '    verbose: print a bunch of stuff during training, including timing information.\n', '    dtype: the dtype of the resulting factor matrices. Using single precision is recommended,\n', '        it speeds things up a bit.\n', '    recompute_factors: helper function that implements the inner loop.\n', '    returns:\n', '        U, V: factor matrices. If bias=True, the last columns of the matrices contain the biases.\n', '    """"""\n']","['range', 'recompute_factors']",2
example/svd/benchmark_mf0.py:time_reps,time_reps,function,7,14,14,109,7.79,1,0,"['func', 'params', 'reps']","[None, None, None]","[None, None, None]",215,[],"['time.time', 'range', 'func', 'print', 'str']",5
example/svd/benchmark_mf0.py:scipy_svd,scipy_svd,function,2,4,4,45,11.25,0,0,"['A', 'K']","[None, None]","[None, None]",232,[],['svds'],1
example/svd/benchmark_mf0.py:sklearn_randomized_svd,sklearn_randomized_svd,function,2,3,3,25,8.33,0,0,"['A', 'k']","[None, None]","[None, None]",235,[],['randomized_svd'],1
example/svd/benchmark_mf0.py:sklearn_truncated_randomized_svd,sklearn_truncated_randomized_svd,function,2,5,5,61,12.2,0,0,"['A', 'k']","[None, None]","[None, None]",238,[],['TruncatedSVD'],1
example/svd/benchmark_mf0.py:sklearn_truncated_arpack_svd,sklearn_truncated_arpack_svd,function,2,5,5,57,11.4,0,0,"['A', 'k']","[None, None]","[None, None]",241,[],['TruncatedSVD'],1
example/svd/benchmark_mf0.py:sparsesvd_svd,sparsesvd_svd,function,2,3,3,20,6.67,0,0,"['A', 'k']","[None, None]","[None, None]",244,[],['sparsesvd'],1
example/svd/benchmark_mf0.py:gensim_svd,gensim_svd,function,2,4,4,36,9.0,0,0,"['A', 'k']","[None, None]","[None, None]",247,[],['stochastic_svd'],1
example/svd/benchmark_mf0.py:wmf,wmf,function,3,7,7,105,15.0,0,0,"['A', 'k']","[None, None]","[None, None]",251,[],['factorize'],1
example/svd/benchmark_mf0.py:implicit_mf,implicit_mf,function,2,3,3,29,9.67,0,0,"['A', 'k']","[None, None]","[None, None]",255,[],['ImplicitMF'],1
example/svd/benchmark_mf0.py:nmf_1,nmf_1,function,2,4,4,48,12.0,0,0,"['A', 'k']","[None, None]","[None, None]",258,[],['NMF_ANLS_BLOCKPIVOT'],1
example/svd/benchmark_mf0.py:nmf_2,nmf_2,function,2,4,4,46,11.5,0,0,"['A', 'k']","[None, None]","[None, None]",261,[],['NMF_ANLS_AS_NUMPY'],1
example/svd/benchmark_mf0.py:nmf_3,nmf_3,function,2,4,4,46,11.5,0,0,"['A', 'k']","[None, None]","[None, None]",264,[],['NMF_ANLS_AS_GROUP'],1
example/svd/benchmark_mf0.py:nmf_4,nmf_4,function,2,4,4,38,9.5,0,0,"['A', 'k']","[None, None]","[None, None]",267,[],['NMF_HALS'],1
example/svd/benchmark_mf0.py:nmf_5,nmf_5,function,2,4,4,36,9.0,0,0,"['A', 'k']","[None, None]","[None, None]",270,[],['NMF_MU'],1
example/svd/benchmark_mf0.py:time_ns,time_ns,function,32,116,88,1128,9.72,1,0,[],[],[],275,[],"['np.arange', 'dict', 'enumerate', 'int', 'print', 'time_reps']",6
example/svd/benchmark_mf0.py:ImplicitMF,ImplicitMF,class,49,94,79,1263,13.44,2,2,[],[],[],169,[],[],0
example/svd/benchmark_mf0.py:ImplicitMF:__init__,ImplicitMF:__init__,method,12,12,12,169,14.08,0,0,"['self', 'counts', 'num_factors', 'num_iterations', 'reg_param']","[None, None, None, None, None]","[None, None, '40', '30', '0.8']",171,[],[],0
example/svd/benchmark_mf0.py:ImplicitMF:train_model,ImplicitMF:train_model,method,6,16,13,338,21.12,1,0,['self'],[None],[None],180,[],"['range', 'self.iteration', 'sparse.csr_matrix']",3
example/svd/benchmark_mf0.py:ImplicitMF:iteration,ImplicitMF:iteration,method,33,54,50,622,11.52,1,2,"['self', 'user', 'fixed_vecs']","[None, None, None]","[None, None, None]",190,[],"['sparse.eye', 'np.zeros', 'time.time', 'range', 'sparse.diags', 'counts_i.copy', 'spsolve']",7
example/tseries/tseries_m5sales.py:pd_col_tocat,pd_col_tocat,function,12,21,18,249,11.86,2,0,"['df', 'nan_cols', 'colcat']","[None, None, None]","[None, None, None]",20,[],"['preprocessing.LabelEncoder', 'encoder.fit_transform']",2
example/tseries/tseries_m5sales.py:pd_merge,pd_merge,function,10,35,26,213,6.09,2,1,"['df_list', 'cols_join']","[None, None]","[None, None]",33,[],"['print', 'dfall.join']",2
example/tseries/tseries_m5sales.py:train,train,function,35,125,101,1371,10.97,1,0,"['input_path', 'n_experiments ', 'colid ', 'coly ']","[None, None, None, None]","[None, ' 3', ' None', ' None']",43,"['    """"""\n', '       Generic train\n', '    :param input_path:\n', '    :param max_rows:\n', '    :param n_experiments:\n', '    :param colid:\n', '    :param coly:\n', '    :return:\n', '    """"""\n']","['range', 'featurestore_filter_features', 'featurestore_get_feature_fromcolname', 'train_test_split', 'lgb.Dataset', 'lgb.train', 'clf.predict', 'np.sqrt', 'print', 'df_metrics.to_csv']",10
example/tseries/tseries_m5sales.py:featurestore_meta_update,featurestore_meta_update,function,17,52,40,661,12.71,1,2,"['featnames', 'filename', 'colcat']","[None, None, None]","[None, None, None]",103,[],"['pd.DataFrame', 'pd.read_csv', 'meta_csv.append', 'meta_csv.to_csv']",4
example/tseries/tseries_m5sales.py:featurestore_get_filelist_fromcolname,featurestore_get_filelist_fromcolname,function,12,23,19,365,15.87,2,0,"['selected_cols', 'colid']","[None, None]","[None, None]",121,[],"['pd.read_csv', 'print', 'file_feat_mapping.items']",3
example/tseries/tseries_m5sales.py:featurestore_generate_feature,featurestore_generate_feature,function,8,25,25,399,15.96,0,1,"['dir_in', 'dir_out', 'my_fun_features', 'features_group_name', 'input_raw_path ', 'auxiliary_csv_path ', 'coldrop ', 'index_cols ', 'merge_cols_mapping ', 'colcat ', 'colid', 'coly ', 'coldate ', 'max_rows ', 'step_wise_saving ']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, ' None', ' None', ' None', ' None', ' None', ' None', 'None', ' None', ' None', ' 5', ' False']",132,[],"['pd.read_parquet', 'my_fun_features', 'dfnew.to_parquet', 'featurestore_meta_update']",4
example/tseries/tseries_m5sales.py:featurestore_filter_features,featurestore_filter_features,function,11,51,32,469,9.2,2,3,"['mode ', 'colid ', 'coly ']","[None, None, None]","['""random""', ' None', ' None']",151,[],['custom_get_colsname'],1
example/tseries/tseries_m5sales.py:featurestore_get_filename,featurestore_get_filename,function,5,8,7,106,13.25,0,0,"['file_name', 'path']","[None, None]","[None, None]",177,[],"['file_name.split', 'glob.glob']",2
example/tseries/tseries_m5sales.py:featurestore_get_feature_fromcolname,featurestore_get_feature_fromcolname,function,21,55,43,663,12.05,3,1,"['path', 'selected_cols', 'colid']","[None, None, None]","[None, None, None]",183,[],"['print', 'featurestore_get_filelist_fromcolname', 'file_col_mapping.items', 'featurestore_get_filename', 'pd.read_parquet', 'pd.concat', 'feature_dfs.append', 'pd_merge', 'df_merged.sort_values', 'df_merged.drop']",10
example/tseries/tseries_m5sales.py:pd_tsfresh_m5data_sales,pd_tsfresh_m5data_sales,function,22,69,52,845,12.25,3,2,"['df_sales', 'dir_out', 'features_group_name', 'drop_cols', 'df_calendar', 'index_cols', 'merge_cols_mapping', 'id_cols']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",212,"['    """"""\n', '\n', '    :param df_sales:\n', '    :param dir_out:\n', '    :param features_group_name:\n', '    :param drop_cols:\n', '    :param df_calendar:\n', '    :param index_cols:\n', '    :param merge_cols_mapping:\n', '    :param id_cols:\n', '    :return:\n', '    """"""\n']","['df_calendar.drop', 'range', 'pd_tsfresh_features_single_row', 'pd.merge', 'str', 'merged_df_selected_cols.to_parquet', 'X_feat.append']",7
example/tseries/tseries_m5sales.py:pd_tsfresh_m5data,pd_tsfresh_m5data,function,47,137,103,1699,12.4,5,3,"['df_sales', 'dir_out', 'features_group_name', 'drop_cols', 'df_calendar', 'index_cols', 'merge_cols_mapping', 'id_cols']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",250,[],"['df_calendar.drop', 'range', 'pd_tsfresh_features_single_row', 'pd.merge', 'str', 'merged_df_selected_cols.to_parquet', 'X_feat.append', 'pd_tsfresh_m5data', 'roll_time_series', 'X.fillna', 'extract_relevant_features', 'filtered_col_name.replace', 'X_filtered.rename', 'pd.concat']",14
example/tseries/tseries_m5sales.py:pd_ts_tsfresh,pd_ts_tsfresh,function,10,27,25,333,12.33,1,0,"['df', 'input_raw_path', 'dir_out', 'features_group_name', 'auxiliary_csv_path', 'drop_cols', 'index_cols', 'merge_cols_mapping', 'cat_cols ', 'id_cols ', 'dep_col ', 'coldate ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, ' None', ' None', ' None', ' None', ' 10']",279,[],"['pd.read_csv', 'pd_tsfresh_m5data_sales']",2
example/tseries/tseries_m5sales.py:custom_get_colsname,custom_get_colsname,function,10,34,19,286,8.41,2,0,"['colid', 'coly']","[None, None]","[None, None]",299,[],['pd.read_csv'],1
example/tseries/tseries_m5sales.py:custom_rawdata_merge,custom_rawdata_merge,function,24,99,81,1310,13.23,0,1,"['out_path', 'max_rows']","[None, None]","[""'out/'"", '10']",307,[],"['pd.read_csv', 'pd.melt', 'df_calendar.drop', 'pd.merge', 'df_merged.merge', 'pd_col_tocat', 'os.makedirs', 'df_merged.to_parquet']",8
example/tseries/tseries_m5sales.py:custom_generate_feature_all,custom_generate_feature_all,function,1,82,32,609,7.43,0,0,"['input_path ', 'out_path', 'input_raw_path ', 'auxiliary_csv_path ', 'coldrop ', 'colindex ', 'merge_cols_mapping ', 'coldate ', 'colcat ', 'colid ', 'coly ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None, None]","[' data_path', '"".""', '"".""', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' 10']",344,[],['featurestore_generate_feature'],1
example/tseries/tseries_m5sales.py:run_train,run_train,function,6,72,54,868,12.06,0,3,"['input_path ', 'out_path', 'do_generate_raw', 'do_generate_feature', 'do_train', 'max_rows ']","[None, None, None, None, None, None]","['""data/input/tseries/tseries_m5/raw""', 'data_path', 'True', 'True', 'False', ' 10']",356,[],"['custom_rawdata_merge', 'custom_generate_feature_all', 'train']",3
example/tseries/tseries_m5sales.py:FeatureStore,FeatureStore,class,1,3,3,23,7.67,0,0,[],[],[],97,[],[],0
example/tseries/tseries_m5sales.py:FeatureStore:__init__,FeatureStore:__init__,method,0,1,1,4,4.0,0,0,['self'],[None],[None],98,[],[],0
example/tseries/tseries_retail.py:pd_col_tocat,pd_col_tocat,function,12,21,18,249,11.86,2,0,"['df', 'nan_cols', 'colcat']","[None, None, None]","[None, None, None]",20,[],"['preprocessing.LabelEncoder', 'encoder.fit_transform']",2
example/tseries/tseries_retail.py:pd_merge,pd_merge,function,10,35,26,213,6.09,2,1,"['df_list', 'cols_join']","[None, None]","[None, None]",33,[],"['print', 'dfall.join']",2
example/tseries/tseries_retail.py:train,train,function,35,125,101,1371,10.97,1,0,"['input_path', 'n_experiments ', 'colid ', 'coly ']","[None, None, None, None]","[None, ' 3', ' None', ' None']",43,"['    """"""\n', '       Generic train\n', '    :param input_path:\n', '    :param max_rows:\n', '    :param n_experiments:\n', '    :param colid:\n', '    :param coly:\n', '    :return:\n', '    """"""\n']","['range', 'featurestore_filter_features', 'featurestore_get_feature_fromcolname', 'train_test_split', 'lgb.Dataset', 'lgb.train', 'clf.predict', 'np.sqrt', 'print', 'df_metrics.to_csv']",10
example/tseries/tseries_retail.py:featurestore_meta_update,featurestore_meta_update,function,17,52,40,661,12.71,1,2,"['featnames', 'filename', 'colcat']","[None, None, None]","[None, None, None]",103,[],"['pd.DataFrame', 'pd.read_csv', 'meta_csv.append', 'meta_csv.to_csv']",4
example/tseries/tseries_retail.py:featurestore_get_filelist_fromcolname,featurestore_get_filelist_fromcolname,function,12,23,19,365,15.87,2,0,"['selected_cols', 'colid']","[None, None]","[None, None]",121,[],"['pd.read_csv', 'print', 'file_feat_mapping.items']",3
example/tseries/tseries_retail.py:featurestore_generate_feature,featurestore_generate_feature,function,8,25,25,399,15.96,0,1,"['dir_in', 'dir_out', 'my_fun_features', 'features_group_name', 'input_raw_path ', 'auxiliary_csv_path ', 'coldrop ', 'index_cols ', 'merge_cols_mapping ', 'colcat ', 'colid', 'coly ', 'coldate ', 'max_rows ', 'step_wise_saving ']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, ' None', ' None', ' None', ' None', ' None', ' None', 'None', ' None', ' None', ' 5', ' False']",132,[],"['pd.read_parquet', 'my_fun_features', 'dfnew.to_parquet', 'featurestore_meta_update']",4
example/tseries/tseries_retail.py:featurestore_filter_features,featurestore_filter_features,function,11,51,32,469,9.2,2,3,"['mode ', 'colid ', 'coly ']","[None, None, None]","['""random""', ' None', ' None']",151,[],['custom_get_colsname'],1
example/tseries/tseries_retail.py:featurestore_get_filename,featurestore_get_filename,function,5,8,7,106,13.25,0,0,"['file_name', 'path']","[None, None]","[None, None]",177,[],"['file_name.split', 'glob.glob']",2
example/tseries/tseries_retail.py:featurestore_get_feature_fromcolname,featurestore_get_feature_fromcolname,function,21,55,43,663,12.05,3,1,"['path', 'selected_cols', 'colid']","[None, None, None]","[None, None, None]",183,[],"['print', 'featurestore_get_filelist_fromcolname', 'file_col_mapping.items', 'featurestore_get_filename', 'pd.read_parquet', 'pd.concat', 'feature_dfs.append', 'pd_merge', 'df_merged.sort_values', 'df_merged.drop']",10
example/tseries/tseries_retail.py:custom_get_colsname,custom_get_colsname,function,10,34,19,286,8.41,2,0,"['colid', 'coly']","[None, None]","[None, None]",215,[],['pd.read_csv'],1
example/tseries/tseries_retail.py:custom_rawdata_merge,custom_rawdata_merge,function,20,54,42,714,13.22,0,1,"['out_path', 'max_rows']","[None, None]","[""'out/'"", '10']",223,[],"['pd.read_csv', 'pd.merge', 'pd_col_tocat', 'os.makedirs', 'df_merged.to_parquet']",5
example/tseries/tseries_retail.py:custom_generate_feature_all,custom_generate_feature_all,function,1,57,23,420,7.37,0,0,"['input_path ', 'out_path', 'input_raw_path ', 'auxiliary_csv_path ', 'coldrop ', 'colindex ', 'merge_cols_mapping ', 'coldate ', 'colcat ', 'colid ', 'coly ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None, None]","[' data_path', '"".""', '"".""', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' 10']",263,[],['featurestore_generate_feature'],1
example/tseries/tseries_retail.py:run_train,run_train,function,6,48,38,534,11.12,0,3,"['input_path ', 'out_path', 'do_generate_raw', 'do_generate_feature', 'do_train', 'max_rows ']","[None, None, None, None, None, None]","['""data/input/tseries/retail/raw""', 'data_path', 'True', 'True', 'False', ' 10']",275,[],"['custom_rawdata_merge', 'custom_generate_feature_all', 'train']",3
example/tseries/tseries_retail.py:FeatureStore,FeatureStore,class,1,3,3,23,7.67,0,0,[],[],[],97,[],[],0
example/tseries/tseries_retail.py:FeatureStore:__init__,FeatureStore:__init__,method,0,1,1,4,4.0,0,0,['self'],[None],[None],98,[],[],0
example/tseries/tseries_sales.py:pd_col_tocat,pd_col_tocat,function,12,21,18,249,11.86,2,0,"['df', 'nan_cols', 'colcat']","[None, None, None]","[None, None, None]",20,[],"['preprocessing.LabelEncoder', 'encoder.fit_transform']",2
example/tseries/tseries_sales.py:pd_merge,pd_merge,function,10,35,26,213,6.09,2,1,"['df_list', 'cols_join']","[None, None]","[None, None]",33,[],"['print', 'dfall.join']",2
example/tseries/tseries_sales.py:featurestore_meta_update,featurestore_meta_update,function,17,52,40,661,12.71,1,2,"['featnames', 'filename', 'colcat']","[None, None, None]","[None, None, None]",50,[],"['pd.DataFrame', 'pd.read_csv', 'meta_csv.append', 'meta_csv.to_csv']",4
example/tseries/tseries_sales.py:featurestore_get_filelist_fromcolname,featurestore_get_filelist_fromcolname,function,12,23,19,365,15.87,2,0,"['selected_cols', 'colid']","[None, None]","[None, None]",68,[],"['pd.read_csv', 'print', 'file_feat_mapping.items']",3
example/tseries/tseries_sales.py:featurestore_generate_feature,featurestore_generate_feature,function,8,25,25,399,15.96,0,1,"['dir_in', 'dir_out', 'my_fun_features', 'features_group_name', 'input_raw_path ', 'auxiliary_csv_path ', 'coldrop ', 'index_cols ', 'merge_cols_mapping ', 'colcat ', 'colid', 'coly ', 'coldate ', 'max_rows ', 'step_wise_saving ']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, ' None', ' None', ' None', ' None', ' None', ' None', 'None', ' None', ' None', ' 5', ' False']",79,[],"['pd.read_parquet', 'my_fun_features', 'dfnew.to_parquet', 'featurestore_meta_update']",4
example/tseries/tseries_sales.py:featurestore_filter_features,featurestore_filter_features,function,11,51,32,469,9.2,2,3,"['mode ', 'colid ', 'coly ']","[None, None, None]","['""random""', ' None', ' None']",98,[],['custom_get_colsname'],1
example/tseries/tseries_sales.py:featurestore_get_filename,featurestore_get_filename,function,5,8,7,106,13.25,0,0,"['file_name', 'path']","[None, None]","[None, None]",124,[],"['file_name.split', 'glob.glob']",2
example/tseries/tseries_sales.py:featurestore_get_feature_fromcolname,featurestore_get_feature_fromcolname,function,21,55,43,663,12.05,3,1,"['path', 'selected_cols', 'colid']","[None, None, None]","[None, None, None]",130,[],"['print', 'featurestore_get_filelist_fromcolname', 'file_col_mapping.items', 'featurestore_get_filename', 'pd.read_parquet', 'pd.concat', 'feature_dfs.append', 'pd_merge', 'df_merged.sort_values', 'df_merged.drop']",10
example/tseries/tseries_sales.py:custom_get_colsname,custom_get_colsname,function,10,34,19,286,8.41,2,0,"['colid', 'coly']","[None, None]","[None, None]",158,[],['pd.read_csv'],1
example/tseries/tseries_sales.py:custom_rawdata_merge,custom_rawdata_merge,function,20,54,42,714,13.22,0,1,"['out_path', 'max_rows']","[None, None]","[""'out/'"", '10']",166,[],"['pd.read_csv', 'pd.merge', 'pd_col_tocat', 'os.makedirs', 'df_merged.to_parquet']",5
example/tseries/tseries_sales.py:custom_generate_feature_all,custom_generate_feature_all,function,1,57,23,420,7.37,0,0,"['input_path ', 'out_path', 'input_raw_path ', 'auxiliary_csv_path ', 'coldrop ', 'colindex ', 'merge_cols_mapping ', 'coldate ', 'colcat ', 'colid ', 'coly ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None, None]","[' data_path', '"".""', '"".""', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' 10']",206,[],['featurestore_generate_feature'],1
example/tseries/tseries_sales.py:run_generate_train_data,run_generate_train_data,function,4,39,33,441,11.31,0,2,"['input_path ', 'out_path', 'do_generate_raw', 'do_generate_feature', 'do_train', 'max_rows ']","[None, None, None, None, None, None]","['""data/input/tseries/retail/raw""', 'data_path', 'True', 'True', 'False', ' 10']",218,[],"['custom_rawdata_merge', 'custom_generate_feature_all']",2
example/tseries/tseries_sales.py:FeatureStore,FeatureStore,class,1,3,3,23,7.67,0,0,[],[],[],44,[],[],0
example/tseries/tseries_sales.py:FeatureStore:__init__,FeatureStore:__init__,method,0,1,1,4,4.0,0,0,['self'],[None],[None],45,[],[],0
source/bin/column_encoder.py:OneHotEncoderRemoveOne,OneHotEncoderRemoveOne,class,18,32,31,397,12.41,0,0,[],[],[],20,[],[],0
source/bin/column_encoder.py:MinHashEncoder,MinHashEncoder,class,34,145,92,1256,8.66,6,3,[],[],[],43,[],[],0
source/bin/column_encoder.py:PasstroughEncoder,PasstroughEncoder,class,10,20,17,225,11.25,0,0,[],[],[],104,[],[],0
source/bin/column_encoder.py:OneHotEncoderRemoveOne:__init__,OneHotEncoderRemoveOne:__init__,method,13,13,13,186,14.31,0,0,"['self', 'n_values', 'categorical_features', 'categories', 'sparse', 'dtype', 'handle_unknown', '']","[None, None, None, None, None, None, None, None]","[None, 'None', 'None', '""auto""', 'True', 'np.float64', '""error""', None]",21,[],['super'],1
source/bin/column_encoder.py:OneHotEncoderRemoveOne:transform,OneHotEncoderRemoveOne:transform,method,4,5,5,43,8.6,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",38,[],['super'],1
source/bin/column_encoder.py:MinHashEncoder:__init__,MinHashEncoder:__init__,method,4,4,4,59,14.75,0,0,"['self', 'n_components', 'ngram_range', '4']","[None, None, None, None]","[None, None, '(2', None]",50,[],[],0
source/bin/column_encoder.py:MinHashEncoder:get_unique_ngrams,MinHashEncoder:get_unique_ngrams,method,8,34,26,231,6.79,2,0,"['self', 'string', 'ngram_range']","[None, None, None]","[None, None, None]",54,"['        """"""\n', '        Return a list of different n-grams in a string\n', '        """"""\n']","['range', 'list']",2
source/bin/column_encoder.py:MinHashEncoder:minhash,MinHashEncoder:minhash,method,9,37,31,353,9.54,1,1,"['self', 'string', 'n_components', 'ngram_range']","[None, None, None, None]","[None, None, None, None]",66,[],"['np.ones', 'self.get_unique_ngrams', 'len', 'np.array', 'range', 'np.minimum']",6
source/bin/column_encoder.py:MinHashEncoder:fit,MinHashEncoder:fit,method,7,20,18,170,8.5,1,1,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",78,[],"['enumerate', 'self.minhash']",2
source/bin/column_encoder.py:MinHashEncoder:transform,MinHashEncoder:transform,method,10,29,21,247,8.52,2,1,"['self', 'X']","[None, None]","[None, None]",88,[],"['np.zeros', 'enumerate', 'self.minhash']",3
source/bin/column_encoder.py:PasstroughEncoder:__init__,PasstroughEncoder:__init__,method,2,2,2,28,14.0,0,0,"['self', 'passthrough']","[None, None]","[None, 'True']",105,[],[],0
source/bin/column_encoder.py:PasstroughEncoder:fit,PasstroughEncoder:fit,method,4,6,6,83,13.83,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",78,[],['FunctionTransformer'],1
source/bin/column_encoder.py:PasstroughEncoder:transform,PasstroughEncoder:transform,method,2,2,2,31,15.5,0,0,"['self', 'X']","[None, None]","[None, None]",88,[],[],0
source/models/dataset.py:dictEval,dictEval,class,16,38,31,260,6.84,1,3,[],[],[],6,[],[],0
source/models/dataset.py:dictEval:eval_dict,dictEval:eval_dict,method,14,33,26,227,6.88,1,3,"['self', 'ddict']","[None, None]","[None, None]",8,[],"['ddict.items', 'isinstance', 'dst.setdefault', 'self.eval_dict', 'key.split', 'pd.read_csv']",6
source/models/keras_deepctr.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],45,[],"['print', 'log2', 'log3']",3
source/models/keras_deepctr.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],48,[],['print'],1
source/models/keras_deepctr.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],51,[],['print'],1
source/models/keras_deepctr.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],54,[],['os.makedirs'],1
source/models/keras_deepctr.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",60,[],['Model'],1
source/models/keras_deepctr.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],65,[],[],0
source/models/keras_deepctr.py:fit,fit,function,16,33,32,292,8.85,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",191,"['    """"""\n', '    """"""\n']","['get_dataset', 'compute_pars.get']",2
source/models/keras_deepctr.py:eval,eval,function,9,17,16,196,11.53,0,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",211,"['    """"""\n', '       Return metrics of the model when fitted.\n', '    """"""\n']",['get_dataset'],1
source/models/keras_deepctr.py:predict,predict,function,13,21,19,238,11.33,0,1,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",223,[],"['get_dataset', 'compute_pars.get']",2
source/models/keras_deepctr.py:save,save,function,17,28,26,261,9.32,0,1,"['path', 'save_weight']","[None, None]","['None', 'False']",235,[],['os.makedirs'],1
source/models/keras_deepctr.py:load_model,load_model,function,10,21,18,235,11.19,0,1,"['path', 'load_weight']","[None, None]","['""""', 'False']",249,[],[],0
source/models/keras_deepctr.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",262,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/keras_deepctr.py:preprocess,preprocess,function,21,68,38,740,10.88,0,2,['prepro_pars'],[None],[None],273,[],"['make_classification', 'train_test_split', 'pd.read_csv']",3
source/models/keras_deepctr.py:get_dataset,get_dataset,function,30,166,99,1551,9.34,0,8,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",304,"['    """"""\n', '      ""ram""  :\n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'get_xy_random2', 'get_xy_fd2', 'Exception']",4
source/models/keras_deepctr.py:get_xy_random2,get_xy_random2,function,38,101,73,874,8.65,2,0,"['X', 'y', 'cols_family']","[None, None, None]","[None, None, '{}']",376,[],"['range', 'pd.DataFrame', 'enumerate', 'get_feature_names']",4
source/models/keras_deepctr.py:get_xy_random,get_xy_random,function,62,248,110,2242,9.04,6,0,"['X', 'y', 'cols_family']","[None, None, None]","[None, None, '{}']",416,[],"['range', 'pd.DataFrame', 'enumerate', 'get_feature_names', 'get_xy_random', 'train_test_split']",6
source/models/keras_deepctr.py:get_xy_fd,get_xy_fd,function,32,311,132,2691,8.65,1,3,"['use_neg', 'hash_flag', 'use_session']","[None, None, None]","['False', 'False', 'False']",465,[],"['SparseFeat', 'DenseFeat', 'np.array', 'VarLenSparseFeat', 'get_feature_names']",5
source/models/keras_deepctr.py:get_xy_dataset,get_xy_dataset,function,60,248,159,3177,12.81,9,1,['data_sample'],[None],['None'],532,[],"['pd.read_csv', 'str', 'LabelEncoder', 'lbe.fit_transform', 'dict', 'SparseFeat', 'get_feature_names', 'range', 'MinMaxScaler', 'mms.fit_transform', 'enumerate', 'train_test_split']",12
source/models/keras_deepctr.py:test,test,function,50,339,198,3201,9.44,2,7,['config'],[None],"[""''""]",626,[],"['get_xy_random', 'get_xy_fd', 'get_xy_dataset', 'EarlyStopping', 'test_helper', 'Model', 'log', 'fit', 'predict', 'save', 'load_model']",11
source/models/keras_deepctr.py:test_helper,test_helper,function,12,86,55,1037,12.06,0,2,"['model_name', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, None, None, None]",721,[],"['Model', 'log', 'fit', 'predict', 'save', 'load_model']",6
source/models/keras_deepctr.py:Model,Model,class,18,93,61,1198,12.88,0,4,[],[],[],148,[],[],0
source/models/keras_deepctr.py:Model:__init__,Model:__init__,method,17,87,55,1121,12.89,0,4,"['self', 'model_pars', 'data_pars', 'compute_pars', '**kwargs']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",149,[],"['model_pars.get', 'list', 'ValueError', 'getattr', 'modeli']",5
source/models/keras_widedeep.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],20,[],"['print', 'log2', 'log3']",3
source/models/keras_widedeep.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],23,[],['print'],1
source/models/keras_widedeep.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],26,[],['print'],1
source/models/keras_widedeep.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],29,[],['os.makedirs'],1
source/models/keras_widedeep.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",35,[],['Model'],1
source/models/keras_widedeep.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],40,[],[],0
source/models/keras_widedeep.py:WideDeep_sparse,WideDeep_sparse,function,35,72,68,1295,17.99,1,1,['model_pars2'],[None],[None],59,"['    """""" Using TF feature column sparse tensor\n', '    """"""\n']","['model_pars2.get', 'tf_FeatureColumns', 'prepare.numeric_columns', 'prepare.categorical_columns', 'prepare.embeddings_columns', 'prepare.get_features', 'dnn_hidden_units.split', 'enumerate', 'model.compile', 'log2']",10
source/models/keras_widedeep.py:fit,fit,function,25,82,60,1417,17.28,0,3,"['data_pars', 'compute_pars']","[None, None]","[None, None]",137,"['    """"""\n', '    """"""\n']","['compute_pars.get', 'EarlyStopping', 'ModelCheckpoint', 'get_dataset', 'log']",5
source/models/keras_widedeep.py:predict,predict,function,14,86,36,599,6.97,2,7,"['Xpred', 'data_pars', 'compute_pars', 'out_pars']","[None, None, None, None]","['None', 'None', 'None', 'None']",182,[],['compute_pars.get'],1
source/models/keras_widedeep.py:save,save,function,18,29,29,348,12.0,0,0,"['path', 'info']","[None, None]","['None', 'None']",207,[],"['os.makedirs', 'Model', 'log']",3
source/models/keras_widedeep.py:load_model,load_model,function,17,29,27,315,10.86,0,0,['path'],[None],"['""""']",227,[],"['pickle.load', 'Model', 'log']",3
source/models/keras_widedeep.py:model_summary,model_summary,function,1,2,2,11,5.5,0,0,['path'],[None],"['""ztmp/""']",246,[],[],0
source/models/keras_widedeep.py:get_dataset_tuple,get_dataset_tuple,function,12,43,31,329,7.65,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",388,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/keras_widedeep.py:get_dataset,get_dataset,function,27,119,82,1165,9.79,1,6,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",411,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'get_dataset_tuple', 'Exception']",6
source/models/keras_widedeep.py:test_dataset_petfinder,test_dataset_petfinder,function,19,48,44,628,13.08,0,0,['nrows'],[None],['1000'],456,[],"['print', 'pd.read_csv', 'np.where', 'df.drop']",4
source/models/keras_widedeep.py:test,test,function,66,398,264,4388,11.03,0,0,['nrows'],[None],['1000'],482,[],"['print', 'pd.read_csv', 'np.where', 'df.drop', 'test', 'test_dataset_petfinder', 'post_process_fun', 'int', 'pre_process_fun', 'log', 'prepare.data_to_tensorflow', 'list', 'tf_FeatureColumns', 'test_helper', 'test2', 'prepare.numeric_columns', 'prepare.categorical_columns', 'prepare.bucketized_columns', 'prepare.get_features', 'tf_linear.values', 'tf_dnn.values', 'Model', 'fit', 'predict', 'save']",25
source/models/keras_widedeep.py:test2,test2,function,27,84,76,1117,13.3,0,0,['config'],[None],"[""''""]",595,"['    """"""\n', '    """"""\n']","['test_dataset_petfinder', 'tf_FeatureColumns', 'prepare.data_to_tensorflow', 'prepare.numeric_columns', 'prepare.categorical_columns', 'prepare.bucketized_columns', 'prepare.get_features', 'tf_linear.values', 'tf_dnn.values', 'test_helper']",10
source/models/keras_widedeep.py:test_helper,test_helper,function,10,31,28,430,13.87,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",647,[],"['Model', 'log', 'fit', 'predict', 'save']",5
source/models/keras_widedeep.py:WideDeep_dense,WideDeep_dense,function,34,83,67,1419,17.1,0,0,['model_pars2'],[None],[None],693,"['        """"""\n', '           Dense Model of DeepWide\n', '        :param n_wide_cross: \n', '        :param n_wide: \n', '        :param n_deep: \n', '        :param n_feat: \n', '        :param m_EMBEDDING: \n', '        :param loss: \n', '        :param metric: \n', '        :return: \n', '        """"""\n']","['m.get', 'wide_model.compile', 'deep_model.compile', 'log2', 'model.compile', 'log']",6
source/models/keras_widedeep.py:input_template_feed_keras_model,input_template_feed_keras_model,function,33,144,84,1491,10.35,5,5,"['Xtrain', 'cols_type_received', 'cols_ref', '**kw']","[None, None, None, None]","[None, None, None, None]",758,"['    """"""\n', '       Create sparse data struccture in KERAS  To plug with MODEL:\n', '       No data, just virtual data\n', '    https://github.com/GoogleCloudPlatform/data-science-on-gcp/blob/master/09_cloudml/flights_model_tf2.ipynb\n', '\n', '    :return:\n', '    """"""\n']","['len', 'min', 'int', 'categorical_column_with_hash_bucket', 'numeric_column', 'crossed_column', 'np.linspace', 'bucketized_column', 'indicator_column', 'dict_sparse.items', 'embedding_column']",11
source/models/keras_widedeep.py:get_dataset2,get_dataset2,function,22,89,61,958,10.76,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",825,"['    """"""\n', '      return tuple of Tensoflow\n', '    """"""\n']","['data_pars.get', 'get_dataset_tuple_keras', 'log2', 'Exception']",4
source/models/keras_widedeep.py:ModelCustom2,ModelCustom2,function,24,59,53,955,16.19,1,0,[],[],[],874,[],"['wide_and_deep_classifier', 'dnn_hidden_units.split', 'enumerate', 'model.compile', 'input_template_feed_keras', 'sparse.values', 'real.values']",7
source/models/keras_widedeep.py:get_dataset_tuple_keras,get_dataset_tuple_keras,function,35,96,76,1307,13.61,1,2,"['pattern', 'batch_size', 'mode', 'truncate']","[None, None, None, None]","[None, None, 'tf.estimator.ModeKeys.TRAIN', 'None']",905,"['    """"""  ACTUAL Data reading :\n', '           Dataframe ---> TF Dataset  --> feed Keras model\n', '\n', '    """"""\n']","['pandas_to_dataset', 'print', 'tf.cast', 'load_dataset', 'features_and_labels', 'features.pop', 'dataset.map', 'dataset.shuffle', 'dataset.repeat', 'dataset.prefetch', 'dataset.take']",11
source/models/keras_widedeep.py:Modelsparse2,Modelsparse2,function,54,184,129,2662,14.47,8,0,[],[],[],958,"['    """"""\n', '    https://github.com/GoogleCloudPlatform/data-science-on-gcp/blob/master/09_cloudml/flights_model_tf2.ipynb\n', '\n', '    :return:\n', '    """"""\n']","['real.keys', 'inputs.update', 'sparse.keys', 'np.linspace', 'disc.update', 'sparse.items', 'real.update', 'print', 'wide_and_deep_classifier', 'enumerate', 'model.compile', 'sparse.values', 'real.values']",13
source/models/keras_widedeep.py:Model,Model,class,18,43,35,475,11.05,0,2,[],[],[],115,[],[],0
source/models/keras_widedeep.py:tf_FeatureColumns,tf_FeatureColumns,class,106,225,161,4452,19.79,6,4,[],[],[],257,[],[],0
source/models/keras_widedeep.py:Model:__init__,Model:__init__,method,16,36,28,386,10.72,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars', '']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",117,[],"['model_pars.get', 'cpars.update', 'WideDeep_sparse', 'WideDeep_dense']",4
source/models/keras_widedeep.py:tf_FeatureColumns:__init__,tf_FeatureColumns:__init__,method,3,6,5,72,12.0,0,0,"['self', 'dataframe']","[None, None]","[None, 'None']",261,[],[],0
source/models/keras_widedeep.py:tf_FeatureColumns:df_to_dataset,tf_FeatureColumns:df_to_dataset,method,23,48,34,708,14.75,0,2,"['self', 'dataframe', 'target', 'shuffle', 'batch_size']","[None, None, None, None, None]","[None, None, None, 'True', '32']",267,[],"['dataframe.copy', 'dataframe.pop', 'ds.shuffle', 'ds.batch', 'df_to_dataset_dense', 'dataframe.apply']",6
source/models/keras_widedeep.py:tf_FeatureColumns:df_to_dataset_dense,tf_FeatureColumns:df_to_dataset_dense,method,18,31,25,429,13.84,0,1,"['self', 'dataframe', 'target', 'shuffle', 'batch_size']","[None, None, None, None, None]","[None, None, None, 'True', '32']",275,[],"['dataframe.copy', 'dataframe.pop', 'dataframe.apply']",3
source/models/keras_widedeep.py:tf_FeatureColumns:data_to_tensorflow,tf_FeatureColumns:data_to_tensorflow,method,26,53,48,1224,23.09,1,2,"['self', 'df', 'target', 'model', 'shuffle_train', 'shuffle_test', 'shuffle_val', 'batch_size', 'test_split', 'colnum', 'colcat']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, ""'sparse'"", 'False', 'False', 'False', '32', '0.2', '[]', '[]']",293,[],"['train_test_split', 'log', 'self.df_to_dataset', 'self.df_to_dataset_dense']",4
source/models/keras_widedeep.py:tf_FeatureColumns:numeric_columns,tf_FeatureColumns:numeric_columns,method,8,13,12,190,14.62,1,0,"['self', 'columnsName']","[None, None]","[None, None]",319,[],['feature_column.numeric_column'],1
source/models/keras_widedeep.py:tf_FeatureColumns:bucketized_columns,tf_FeatureColumns:bucketized_columns,method,8,12,11,201,16.75,1,0,"['self', 'columnsBoundaries']","[None, None]","[None, None]",327,[],"['columnsBoundaries.items', 'feature_column.numeric_column', 'feature_column.bucketized_column']",3
source/models/keras_widedeep.py:tf_FeatureColumns:categorical_columns,tf_FeatureColumns:categorical_columns,method,12,19,17,395,20.79,1,0,"['self', 'indicator_column_names', 'colcat_nunique', 'output']","[None, None, None, None]","[None, None, 'None', 'False']",335,[],"['feature_column.categorical_column_with_vocabulary_list', 'feature_column.indicator_column']",2
source/models/keras_widedeep.py:tf_FeatureColumns:hashed_columns,tf_FeatureColumns:hashed_columns,method,8,13,12,274,21.08,1,0,"['self', 'hashed_columns_dict']","[None, None]","[None, None]",351,[],"['hashed_columns_dict.items', 'feature_column.categorical_column_with_hash_bucket', 'feature_column.indicator_column']",3
source/models/keras_widedeep.py:tf_FeatureColumns:crossed_feature_columns,tf_FeatureColumns:crossed_feature_columns,method,5,9,7,227,25.22,0,0,"['self', 'columns_crossed', 'nameOfLayer', 'bucket_size']","[None, None, None, None]","[None, None, None, '10']",360,[],"['feature_column.crossed_column', 'feature_column.indicator_column']",2
source/models/keras_widedeep.py:tf_FeatureColumns:embeddings_columns,tf_FeatureColumns:embeddings_columns,method,10,16,15,296,18.5,1,0,"['self', 'coldim_dict']","[None, None]","[None, None]",367,[],"['coldim_dict.items', 'feature_column.categorical_column_with_hash_bucket', 'feature_column.embedding_column']",3
source/models/keras_widedeep.py:tf_FeatureColumns:transform_output,tf_FeatureColumns:transform_output,method,5,5,5,125,25.0,0,0,"['self', 'featureColumn']","[None, None]","[None, None]",377,[],"['layers.DenseFeatures', 'next', 'log']",3
source/models/keras_widedeep.py:tf_FeatureColumns:get_features,tf_FeatureColumns:get_features,method,2,2,2,69,34.5,0,0,['self'],[None],[None],383,[],[],0
source/models/keras_widedeep_dense.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],16,[],"['print', 'log2', 'log3']",3
source/models/keras_widedeep_dense.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],19,[],['print'],1
source/models/keras_widedeep_dense.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],22,[],['print'],1
source/models/keras_widedeep_dense.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],25,[],['os.makedirs'],1
source/models/keras_widedeep_dense.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",31,[],['Model'],1
source/models/keras_widedeep_dense.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],36,[],[],0
source/models/keras_widedeep_dense.py:Modelcustom,Modelcustom,function,22,64,49,1100,17.19,0,0,"['n_wide_cross', 'n_wide', 'n_deep', 'n_feat', 'm_EMBEDDING', 'loss', 'metric ']","[None, None, None, None, None, None, None]","[None, None, None, '8', '10', ""'mse'"", "" 'mean_squared_error'""]",58,[],"['layers.Input', 'layers.concatenate', 'layers.Dense', 'keras.Model', 'wide_model.compile', 'log2', 'layers.Embedding', 'layers.Flatten', 'deep_model.compile', 'model.compile']",10
source/models/keras_widedeep_dense.py:get_dataset_tuple,get_dataset_tuple,function,12,43,31,329,7.65,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",94,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/keras_widedeep_dense.py:get_dataset,get_dataset,function,31,136,91,1321,9.71,1,6,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",116,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",7
source/models/keras_widedeep_dense.py:fit,fit,function,22,49,46,554,11.31,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",186,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'ModelCheckpoint']",5
source/models/keras_widedeep_dense.py:evaluate,evaluate,function,0,1,1,4,4.0,0,0,"['Xy_pred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",204,[],[],0
source/models/keras_widedeep_dense.py:predict,predict,function,18,37,33,403,10.89,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",208,[],"['get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log2', 'compute_pars.get']",5
source/models/keras_widedeep_dense.py:save,save,function,17,33,31,377,11.42,0,0,"['path', 'info']","[None, None]","['None', 'None']",225,[],"['os.makedirs', 'Model', 'pickle.dump', 'open']",4
source/models/keras_widedeep_dense.py:load_model,load_model,function,17,30,28,310,10.33,0,0,['path'],[None],"['""""']",243,[],"['pickle.load', 'Model']",2
source/models/keras_widedeep_dense.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",258,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/keras_widedeep_dense.py:test,test,function,41,190,147,1894,9.97,2,0,['config'],[None],"[""''""]",271,"['    """"""\n', '        Group of columns for the input model\n', '           cols_input_group = [ ]\n', '          for cols in cols_input_group,\n', '    :param config:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 'range', 'train_test_split', 'log', 'colg_input.items', 'test_helper', 'Model', 'fit', 'predict', 'save', 'load_model']",11
source/models/keras_widedeep_dense.py:test_helper,test_helper,function,12,44,36,565,12.84,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",343,[],"['Model', 'log', 'fit', 'predict', 'save', 'load_model']",6
source/models/keras_widedeep_dense.py:get_dataset2,get_dataset2,function,22,90,61,960,10.67,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",383,"['    """"""\n', '      return tuple of Tensoflow\n', '    """"""\n']","['data_pars.get', 'get_dataset_tuple_keras', 'log2', 'Exception']",4
source/models/keras_widedeep_dense.py:get_dataset_tuple_keras,get_dataset_tuple_keras,function,83,334,204,4073,12.19,13,5,"['Xtrain', 'cols_type_received', 'cols_ref', '**kw']","[None, None, None, None]","[None, None, None, None]",433,"['    """"""\n', '       Create sparse data struccture from dataframe data  to Feed Keras\n', '    https://github.com/GoogleCloudPlatform/data-science-on-gcp/blob/master/09_cloudml/flights_model_tf2.ipynb\n', '    :return:\n', '    """"""\n']","['len', 'min', 'int', 'categorical_column_with_hash_bucket', 'numeric_column', 'crossed_column', 'np.linspace', 'bucketized_column', 'indicator_column', 'dict_sparse.items', 'embedding_column', 'real.keys', 'inputs.update', 'sparse.keys', 'disc.update', 'sparse.items', 'real.update', 'wide_and_deep_classifier', 'dnn_hidden_units.split', 'enumerate', 'model.compile', 'sparse.values', 'real.values']",23
source/models/keras_widedeep_dense.py:Model,Model,class,19,40,37,734,18.35,0,1,[],[],[],161,[],[],0
source/models/keras_widedeep_dense.py:Model:__init__,Model:__init__,method,18,35,32,666,19.03,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",162,[],"['log2', 'len', 'Modelcustom']",3
source/models/model_bayesian_numpyro.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],13,[],"['print', 'log2', 'log3']",3
source/models/model_bayesian_numpyro.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],16,[],['print'],1
source/models/model_bayesian_numpyro.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],19,[],['print'],1
source/models/model_bayesian_numpyro.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],22,[],['os.makedirs'],1
source/models/model_bayesian_numpyro.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",28,[],['Model'],1
source/models/model_bayesian_numpyro.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],33,[],[],0
source/models/model_bayesian_numpyro.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],13,[],"['print', 'log2', 'log3']",3
source/models/model_bayesian_numpyro.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",28,[],['Model'],1
source/models/model_bayesian_numpyro.py:fit,fit,function,30,55,50,678,12.33,1,1,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",114,"['    """"""\n', '    """"""\n']","['get_dataset', 'torch.tensor', 'log', 'compute_pars.get', 'compute_pars2.get', 'np.array', 'pd.DataFrame']",7
source/models/model_bayesian_numpyro.py:predict,predict,function,46,114,97,1289,11.31,2,3,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', 'None', '{}', None]",145,[],"['compute_pars2.get', 'get_dataset', 'list', 'len', 'torch.tensor', 'post_process_fun', 'summary', 'samples.items', 'torch.mean', 'torch.std', 'Predictive', 'predictive', 'enumerate', 'pd.DataFrame', 'print']",15
source/models/model_bayesian_numpyro.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],33,[],[],0
source/models/model_bayesian_numpyro.py:save,save,function,8,26,22,299,11.5,0,0,"['path', 'info']","[None, None]","['None', 'None']",212,[],"['os.makedirs', 'pickle.dump', 'open']",3
source/models/model_bayesian_numpyro.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",224,[],"['pickle.load', 'Model']",2
source/models/model_bayesian_numpyro.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",237,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_bayesian_numpyro.py:preprocess,preprocess,function,21,67,37,705,10.52,0,2,['prepro_pars'],[None],[None],248,[],"['make_classification', 'train_test_split', 'pd.read_csv']",3
source/models/model_bayesian_numpyro.py:get_dataset,get_dataset,function,7,49,33,415,8.47,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",278,"['    """"""\n', '      ""ram""  : \n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/model_bayesian_numpyro.py:get_params,get_params,function,13,30,27,302,10.07,0,1,"['param_pars', '**kw']","[None, None]","['{}', None]",304,[],"['json.load', 'Exception']",2
source/models/model_bayesian_numpyro.py:Model,Model,class,28,61,54,675,11.07,0,2,[],[],[],85,[],[],0
source/models/model_bayesian_numpyro.py:Model:__init__,Model:__init__,method,28,56,51,607,10.84,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",86,[],"['BayesianRegression', '__init__', 'dict', 'const=dict', 'x=dict', 'log']",6
source/models/model_bayesian_pyro.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],16,[],"['print', 'log2', 'log3']",3
source/models/model_bayesian_pyro.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],19,[],['print'],1
source/models/model_bayesian_pyro.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],22,[],['print'],1
source/models/model_bayesian_pyro.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],25,[],['os.makedirs'],1
source/models/model_bayesian_pyro.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",31,[],['Model'],1
source/models/model_bayesian_pyro.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],36,[],[],0
source/models/model_bayesian_pyro.py:model_class_loader,model_class_loader,function,8,14,13,136,9.71,1,0,"['m_name', 'class_list']","[None, 'list']","[""'BayesianRegression'"", 'None']",76,[],"['m_name.split', 'class_list_dict.get']",2
source/models/model_bayesian_pyro.py:fit,fit,function,39,87,79,939,10.79,2,1,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",105,"['    """"""\n', '    """"""\n']","['get_dataset', 'torch.tensor', 'log', 'compute_pars.get', 'compute_pars2.get', 'AutoDiagonalNormal', 'SVI', 'loss=Trace_ELBO', 'pyro.clear_param_store', 'range', 'svi.step', 'losses.append', 'len', 'pd.DataFrame']",14
source/models/model_bayesian_pyro.py:predict,predict,function,53,129,107,1442,11.18,2,4,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', 'None', '{}', None]",147,[],"['compute_pars2.get', 'get_dataset', 'list', 'len', 'torch.tensor', 'post_process_fun', 'summary', 'samples.items', 'torch.mean', 'torch.std', 'getattr', 'Predictive', 'predictive', 'enumerate', 'pd.DataFrame', 'print', 'compute_pars.get']",17
source/models/model_bayesian_pyro.py:save,save,function,8,19,17,229,12.05,0,0,"['path', 'info']","[None, None]","['None', 'None']",205,[],"['os.makedirs', 'pickle.dump', 'open']",3
source/models/model_bayesian_pyro.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",217,[],"['pickle.load', 'Model']",2
source/models/model_bayesian_pyro.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",230,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_bayesian_pyro.py:get_dataset,get_dataset,function,7,49,33,415,8.47,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",241,"['    """"""\n', '      ""ram""  : \n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/model_bayesian_pyro.py:y_norm,y_norm,function,17,73,45,339,4.64,1,4,"['y', 'inverse', 'mode']","[None, None, None]","[None, 'True', ""'boxcox'""]",268,[],"['np.sign', 'np.abs']",2
source/models/model_bayesian_pyro.py:test_dataset_regress_fake,test_dataset_regress_fake,function,23,56,50,457,8.16,1,0,['nrows'],[None],['500'],297,[],"['range', 'sklearn_datasets.make_regression', 'pd.DataFrame', 'y.reshape']",4
source/models/model_bayesian_pyro.py:test,test,function,58,237,189,2376,10.03,2,0,['nrows'],[None],['500'],318,[],"['range', 'sklearn_datasets.make_regression', 'pd.DataFrame', 'y.reshape', 'test', 'test_dataset_regress_fake', 'train_test_split', 'log', 'post_process_fun', 'y_norm', 'pre_process_fun', 'Model', 'fit', 'predict', 'save', 'load_model', 'reset']",17
source/models/model_bayesian_pyro.py:BayesianRegression,BayesianRegression,class,15,35,32,476,13.6,0,0,[],[],[],54,[],[],0
source/models/model_bayesian_pyro.py:Model,Model,class,20,47,44,453,9.64,0,1,[],[],[],83,[],[],0
source/models/model_bayesian_pyro.py:BayesianRegression:__init__,BayesianRegression:__init__,method,6,11,10,226,20.55,0,0,"['self', 'X_dim', 'y_dim']","[None, 'int', 'int']","[None, '17', '1']",55,[],"['super', 'PyroSample']",2
source/models/model_bayesian_pyro.py:BayesianRegression:forward,BayesianRegression:forward,method,8,16,15,178,11.12,0,0,"['self', 'x', 'y']","[None, None, None]","[None, None, 'None']",64,[],"['pyro.sample', 'dist.Uniform', 'self.linear', 'pyro.plate', 'dist.Normal']",5
source/models/model_bayesian_pyro.py:Model:__init__,Model:__init__,method,19,42,39,385,9.17,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",84,[],"['model_class_loader', 'model_pars.get', 'model_class', 'log']",4
source/models/model_encoder.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],15,[],"['print', 'log2', 'log3']",3
source/models/model_encoder.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],18,[],['print'],1
source/models/model_encoder.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],21,[],['print'],1
source/models/model_encoder.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],24,[],['os.makedirs'],1
source/models/model_encoder.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",30,[],['Model'],1
source/models/model_encoder.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],35,[],[],0
source/models/model_encoder.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],15,[],"['print', 'log2', 'log3']",3
source/models/model_encoder.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],18,[],['print'],1
source/models/model_encoder.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],21,[],['print'],1
source/models/model_encoder.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",30,[],['Model'],1
source/models/model_encoder.py:fit,fit,function,13,21,21,228,10.86,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[' dict', ' dict', ' dict', None]","['None', 'None', 'None', None]",102,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy']",2
source/models/model_encoder.py:transform,transform,function,22,50,41,519,10.38,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",116,"['    """""" Geenrate Xtrain  ----> Xtrain_new  (ie transformed)\n', '    :param Xpred:\n', '              ==> dataframe       if you want to transorm by sklearn models like TruncatedSVD\n', '    :param data_pars:\n', '    :param compute_pars:\n', '    :param out_pars:\n', '    :param kw:\n', '    :return:\n', '    """"""\n']","['get_dataset', 'kw.get', 'get_dataset_tuple', 'compute_pars.get', 'log3']",5
source/models/model_encoder.py:decode,decode,function,0,1,1,4,4.0,0,0,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",155,"['    """"""\n', '       Embedding --> Actual values\n', '    :param Xpred:\n', '    :param data_pars:\n', '    :param compute_pars:\n', '    :param out_pars:\n', '    :param kw:\n', '    :return:\n', '    """"""\n']",[],0
source/models/model_encoder.py:predict,predict,function,7,17,12,133,7.82,0,0,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",169,"['    """"""\n', '       Encode + Decode\n', '\n', '    :param Xpred:\n', '    :param data_pars:\n', '    :param compute_pars:\n', '    :param out_pars:\n', '    :param kw:\n', '    :return:\n', '    """"""\n']","['encode', 'decode']",2
source/models/model_encoder.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],35,[],[],0
source/models/model_encoder.py:save,save,function,8,26,22,299,11.5,0,0,"['path', 'info']","[None, None]","['None', 'None']",192,[],"['os.makedirs', 'pickle.dump', 'open']",3
source/models/model_encoder.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",204,[],"['pickle.load', 'Model']",2
source/models/model_encoder.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",218,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_encoder.py:get_dataset_tuple,get_dataset_tuple,function,9,34,28,282,8.29,1,1,"['Xtrain', 'cols_type_received', 'cols_ref', 'split']","[None, None, None, None]","[None, None, None, 'False']",231,"['    """"""  Split into Tuples = (df1, df2, df3) to feed model, (ie Keras)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :param split: \n', '        True :  split data to list of dataframe \n', '        False:  return same input of data\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/model_encoder.py:get_dataset,get_dataset,function,29,136,88,1376,10.12,1,5,"['Xtrain', 'cols_type_received', 'cols_ref', 'split']","[None, None, None, None]","[None, None, None, 'False']",253,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'list', 'cols_type_received.keys', 'kw.get', 'get_dataset_tuple', 'log2', 'Exception']",10
source/models/model_encoder.py:test_dataset_classi_fake,test_dataset_classi_fake,function,23,48,44,377,7.85,1,0,['nrows'],[None],['500'],303,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'y.reshape', 'len']",5
source/models/model_encoder.py:test,test,function,60,326,218,3191,9.79,1,0,['nrows'],[None],['500'],325,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'y.reshape', 'len', 'test', 'test_dataset_classi_fake', 'np.arange', 'train_test_split', 'post_process_fun', 'int', 'pre_process_fun', 'log', 'test_helper', 'Model', 'fit', 'transform', 'save', 'load_model']",19
source/models/model_encoder.py:test_helper,test_helper,function,11,36,30,449,12.47,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",435,[],"['Model', 'log', 'fit', 'transform', 'save', 'load_model']",6
source/models/model_encoder.py:pd_export,pd_export,function,11,20,16,244,12.2,0,0,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",472,"['    """"""\n', '       Export in train folder for next training\n', '       colsall\n', '    :param df:\n', '    :param col:\n', '    :param pars:\n', '    :return:\n', '    """"""\n']","['dfX.set_index', 'dfX.to_parquet', 'dfy.set_index']",3
source/models/model_encoder.py:pd_autoencoder,pd_autoencoder,function,49,126,98,1690,13.41,0,2,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",493,"['    """"""""\n', '    (4) Autoencoder\n', '    An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.\n', '    The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction,\n', '    by training the network to ignore noise.\n', '    (i) Feed Forward\n', '    The simplest form of an autoencoder is a feedforward, non-recurrent\n', '    neural network similar to single layer perceptrons that participate in multilayer perceptrons\n', '    """"""\n']","['encoder_dataset', 'df.select_dtypes', 'print', 'encode_objects', 'OrdinalEncoder', 'oe.fit', 'oe.transform', 'minmax_scale', 'pars.get', 'pd.DataFrame', 'encoded_train.add_prefix', 'pd.concat']",12
source/models/model_encoder.py:pd_covariate_shift_adjustment,pd_covariate_shift_adjustment,function,61,116,101,971,8.37,1,1,[],[],[],561,"['    """"""\n', '    https://towardsdatascience.com/understanding-dataset-shift-f2a5a262a766\n', '     Covariate shift has been extensively studied in the literature, and a number of proposals to work under it have been published. Some of the most important ones include:\n', '        Weighting the log-likelihood function (Shimodaira, 2000)\n', '        Importance weighted cross-validation (Sugiyama et al, 2007 JMLR)\n', '        Integrated optimization problem. Discriminative learning. (Bickel et al, 2009 JMRL)\n', '        Kernel mean matching (Gretton et al., 2009)\n', '        Adversarial search (Globerson et al, 2009)\n', '        Frank-Wolfe algorithm (Wen et al., 2015)\n', '    """"""\n']","['datasets.make_regression', 'FW', 'sparse.dok_matrix', 'range', 'np.argmax', 'np.sign', 'trace.append', 'min', 'q_t.dot', 'np.array', 'plt.plot', 'plt.yscale', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.grid', 'plt.show', 'np.mean', 'print']",19
source/models/model_encoder.py:Model,Model,class,12,24,23,321,13.38,0,1,[],[],[],89,[],[],0
source/models/model_encoder.py:Model:__init__,Model:__init__,method,11,19,18,253,13.32,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",90,[],"['globals', 'model_class', 'log2']",3
source/models/model_gefs.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],16,[],"['print', 'log2', 'log3']",3
source/models/model_gefs.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],19,[],['print'],1
source/models/model_gefs.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],22,[],['print'],1
source/models/model_gefs.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],25,[],['os.makedirs'],1
source/models/model_gefs.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",31,[],['Model'],1
source/models/model_gefs.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],36,[],[],0
source/models/model_gefs.py:fit,fit,function,27,91,81,900,9.89,1,1,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",81,"['    """"""\n', '    """"""\n']","['get_dataset', 'log', 'pd.concat', 'pd_colcat_get_catcount', 'np.array', 'RandomForest']",6
source/models/model_gefs.py:eval,eval,function,17,41,40,461,11.24,0,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",126,"['    """"""\n', '       Return metrics of the model when fitted.\n', '    """"""\n']","['get_dataset', 'predict', 'compute_pars.get', 'mpars.get', 'scorer']",5
source/models/model_gefs.py:predict,predict,function,20,47,36,568,12.09,0,3,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",145,[],"['post_process_fun', 'y.astype', 'get_dataset', 'np.max', 'compute_pars.get']",5
source/models/model_gefs.py:save,save,function,8,26,22,299,11.5,0,0,"['path', 'info']","[None, None]","['None', 'None']",169,[],"['os.makedirs', 'pickle.dump', 'open']",3
source/models/model_gefs.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",181,[],"['pickle.load', 'Model']",2
source/models/model_gefs.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",194,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_gefs.py:get_dataset,get_dataset,function,7,49,33,415,8.47,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",205,"['    """"""\n', '      ""ram""  : \n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/model_gefs.py:pd_colcat_get_catcount,pd_colcat_get_catcount,function,19,34,29,290,8.53,2,1,"['df', 'colcat', 'classcol', 'continuous_ids']","[None, None, None, None]","[None, None, None, None]",235,"['    """"""  Learns the number of categories in each variable and standardizes the df.\n', '        ncat: numpy m The number of categories of each variable. One if the variable is continuous.\n', '    """"""\n']","['df.copy', 'enumerate']",2
source/models/model_gefs.py:test_dataset_classi_fake,test_dataset_classi_fake,function,23,49,45,380,7.76,1,0,['nrows'],[None],['500'],262,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'y.reshape', 'len']",5
source/models/model_gefs.py:test,test,function,54,249,184,2383,9.57,1,0,['nrows'],[None],['500'],290,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'y.reshape', 'len', 'test', 'test_dataset_classi_fake', 'train_test_split', 'log', 'post_process_fun', 'y.astype', 'pre_process_fun', 'int', 'test_helper', 'Model', 'fit', 'predict', 'print']",18
source/models/model_gefs.py:test_helper,test_helper,function,11,57,41,752,13.19,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",343,[],"['Model', 'log', 'fit', 'predict', 'print']",5
source/models/model_gefs.py:is_continuous,is_continuous,function,5,29,27,232,8.0,0,1,['v_array'],[None],[None],378,"['    """""" Returns true if df was sampled from a continuous variables, and false\n', '    """"""\n']","['np.sum', 'np.round', 'len', 'min', 'any']",5
source/models/model_gefs.py:test2,test2,function,69,207,137,1956,9.45,3,5,[],[],[],391,[],"['get_stats', 'np.ones', 'np.zeros', 'range', 'np.max', 'np.min', 'np.mean', 'np.std', 'is_continuous', 'standardize_data', 'data.copy', 'np.clip', 'train_test', 'pd.read_csv', 'pd_colcat_get_catcount', 'np.where', 'Model', 'log']",18
source/models/model_gefs.py:Model,Model,class,32,71,57,567,7.99,0,2,[],[],[],59,[],[],0
source/models/model_gefs.py:Model:__init__,Model:__init__,method,31,66,52,499,7.56,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",60,[],"['model_pars.get', 'log', 'RandomForest']",3
source/models/model_numpyro.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],79,[],"['print', 'log2', 'log3']",3
source/models/model_numpyro.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],82,[],['print'],1
source/models/model_numpyro.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],85,[],['print'],1
source/models/model_numpyro.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],88,[],['os.makedirs'],1
source/models/model_numpyro.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",94,[],['Model'],1
source/models/model_numpyro.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],99,[],[],0
source/models/model_numpyro.py:require_fitted,require_fitted,function,7,18,16,150,8.33,0,1,['f'],[None],[None],143,"['    """"""Decorate a function to require the model to be fitted for usage.""""""\n']","['wrapper', 'getattr', 'exceptions.NotFittedError', 'f']",4
source/models/model_numpyro.py:metrics,metrics,function,10,21,18,173,8.24,0,1,"['y', 'yhat']","[' pd.Series', ' pd.Series']","[None, None]",155,"['    """"""Return general fit metrics of one series against another.""""""\n']","['dict', 'onp.corrcoef', 'onp.mean']",3
source/models/model_numpyro.py:Model,Model,class,0,1,1,4,4.0,0,0,[],[],[],105,[],[],0
source/models/model_numpyro.py:NumpyEncoder,NumpyEncoder,class,5,25,19,271,10.84,0,1,[],[],[],128,[],[],0
source/models/model_numpyro.py:BaseModel,BaseModel,class,149,665,398,6439,9.68,6,18,[],[],[],166,[],[],0
source/models/model_numpyro.py:Normal,Normal,class,9,18,16,211,11.72,0,0,[],[],[],673,[],[],0
source/models/model_numpyro.py:Poisson,Poisson,class,6,12,10,131,10.92,0,0,[],[],[],690,[],[],0
source/models/model_numpyro.py:Bernoulli,Bernoulli,class,6,12,10,145,12.08,0,0,[],[],[],705,[],[],0
source/models/model_numpyro.py:ShabadooException,ShabadooException,class,8,10,10,98,9.8,0,0,[],[],[],731,[],[],0
source/models/model_numpyro.py:NotFittedError,NotFittedError,class,5,17,17,121,7.12,0,1,[],[],[],741,[],[],0
source/models/model_numpyro.py:AlreadyFittedError,AlreadyFittedError,class,4,12,12,103,8.58,0,0,[],[],[],750,[],[],0
source/models/model_numpyro.py:IncompleteModel,IncompleteModel,class,4,14,14,128,9.14,0,0,[],[],[],759,[],[],0
source/models/model_numpyro.py:IncompleteFeature,IncompleteFeature,class,2,12,12,77,6.42,0,0,[],[],[],768,[],[],0
source/models/model_numpyro.py:IncompleteSamples,IncompleteSamples,class,2,12,12,77,6.42,0,0,[],[],[],776,[],[],0
source/models/model_numpyro.py:NumpyEncoder:default,NumpyEncoder:default,method,4,22,16,249,11.32,0,1,"['self', 'obj']","[None, None]","[None, None]",131,"['        """"""Encode numpy types or pass to default.""""""\n']","['isinstance', 'int', 'float', 'onp.array', 'super']",5
source/models/model_numpyro.py:BaseModel:link,BaseModel:link,method,0,1,1,4,4.0,0,0,['x'],[None],[None],181,"['        """"""Implement the link function for the model.""""""\n']",[],0
source/models/model_numpyro.py:BaseModel:likelihood_func,BaseModel:likelihood_func,method,0,1,1,4,4.0,0,0,"['self', 'yhat']","[None, None]","[None, None]",186,"['        """"""Return the likelihood distribution given predictions yhat.""""""\n']",[],0
source/models/model_numpyro.py:BaseModel:__init__,BaseModel:__init__,method,11,43,31,356,8.28,2,3,"['self', 'rng_seed']","[None, ' int ']","[None, ' None']",190,"['        """"""Initialize the model object. Set some flags and runs some validations.\n', '        \n', '        Optionally set the rng seed.\n', '        """"""\n']","['exceptions.IncompleteModel', 'exceptions.IncompleteFeature', 'random.PRNGKey', 'randint']",4
source/models/model_numpyro.py:BaseModel:__repr__,BaseModel:__repr__,method,8,27,22,273,10.11,0,1,['self'],[None],[None],213,"['        """"""Print out the model name and the model family.""""""\n']","['inspect.getmro', 'next', 'zip']",3
source/models/model_numpyro.py:BaseModel:split_rand_key,BaseModel:split_rand_key,method,7,14,12,101,7.21,0,1,"['self', 'n']","[None, ' int ']","[None, ' 1']",225,"['        """"""Split the random key, assign a new key and return the subkeys.\n', '        \n', '        Parameters\n', '        ----------\n', '        n : int\n', '            Number of subkeys to generate. Default 1.\n', '\n', '        Returns\n', '        -------\n', '        random.PRNGKey\n', '            An array of PRNG keys or just a single key (if n=1).\n', '\n', '        """"""\n']",['random.split'],1
source/models/model_numpyro.py:BaseModel:transform,BaseModel:transform,method,2,12,12,121,10.08,0,0,"['cls', 'df']","[None, ' pd.DataFrame']","[None, None]",247,"['        """"""Transform a dataframe for model input.\n', '        \n', '        Parameters\n', '        ----------\n', '        df : pd.DataFrame\n', '            Source dataframe to transform.\n', '\n', '        Returns\n', '        -------\n', '        pd.DataFrame\n', '            Dataframe containing transformed inputs.\n', '\n', '        """"""\n']",['df.assign'],1
source/models/model_numpyro.py:BaseModel:model,BaseModel:model,method,19,65,56,568,8.74,1,2,"['self', 'df']","[None, ' pd.DataFrame']","[None, None]",268,"['        """"""Define and return samples from the model.\n', '        \n', '        Parameters\n', '        ----------\n', '        df : pd.DataFrame\n', '            Input data for the model.\n', '\n', '        """"""\n']","['self.transform', 'numpyro.sample', 'np.sum', 'self.link', 'RuntimeError', 'self.likelihood_func']",6
source/models/model_numpyro.py:BaseModel:fit,BaseModel:fit,method,19,52,48,598,11.5,0,2,"['self', 'df', 'sampler', 'rng_key', 'sampler_kwargs', 'typing.Any] ', '**mcmc_kwargs', '']","[None, ' pd.DataFrame', ' str ', ' np.ndarray ', ' typing.Dict[str', None, None, None]","[None, None, ' ""NUTS""', ' None', None, ' None', None, None]",312,"['        """"""Fit the model to a DataFrame.\n', '        \n', '        Parameters\n', '        ----------\n', '        df : pd.DataFrame\n', '            Source dataframe.\n', '        sampler : str\n', '            Numpyro sampler name. Default NUTS\n', '        rng_key : two-element ndarray.\n', '            Optional rng key, will be randomly splitted if not provided.\n', '        sampler_kwargs :\n', '            Passed to the numpyro sampler selected.\n', '        **mcmc_kwargs :\n', '            Passed to numpyro.infer.MCMC\n', '\n', '        Returns\n', '        -------\n', '        Model\n', '            The fitted model.\n', '\n', '        """"""\n']","['exceptions.AlreadyFittedError', 'sampler.upper', 'ValueError', 'getattr', 'dict', '_mcmc_kwargs.update', '_sampler_kwargs.update', 'infer.MCMC', 'self.split_rand_key', 'rng_key.astype', 'mcmc.run', 'mcmc.get_samples']",12
source/models/model_numpyro.py:BaseModel:predict,BaseModel:predict,method,12,40,36,408,10.2,0,2,"['self', 'df', 'ci', 'ci_interval', 'aggfunc', 'typing.Callable] ', '']","[None, ' pd.DataFrame', ' bool ', ' float ', ' typing.Union[str', None, None]","[None, None, ' False', ' 0.9', None, ' ""mean""', None]",371,"['        """"""Return the average posterior prediction across all samples.\n', '        \n', '        Parameters\n', '        ----------\n', '        df : pd.DataFrame\n', '            Source dataframe.\n', '        ci : float\n', '            Option to include a credible interval around the predictions. Returns a \n', '            dataframe if true, a series if false. Default False.\n', '        ci_interval : float\n', '            Credible interval width. Default 0.9.\n', '        aggfunc : string or callable\n', '            Aggregation function called over predictions across posterior samples. \n', '            Applies only to the point prediction (not the CI).\n', '\n', '        Returns\n', '        -------\n', '        pd.Series or pd.DataFrame\n', '            Forecasts. Will be a series with the name of the dv if no ci. Will be a\n', '            dataframe if ci is included.\n', '\n', '        """"""\n']","['callable', 'getattr', 'self.link', 'self.transform', 'aggfunc', 'pd.Series', 'onp.quantile', 'pd.DataFrame']",8
source/models/model_numpyro.py:BaseModel:sample_posterior_predictive,BaseModel:sample_posterior_predictive,method,9,40,37,418,10.45,0,1,"['self', 'df', 'hdpi', 'hdpi_interval', 'rng_key', '']","[None, ' pd.DataFrame', ' bool ', ' float ', ' np.ndarray ', None]","[None, None, ' False', ' 0.9', ' None', None]",420,"['        """"""Obtain samples from the posterior predictive.\n', '        \n', '        Parameters\n', '        ----------\n', '        df : pd.DataFrame\n', '            Source dataframe.\n', '        hdpi : bool\n', '            Option to include lower/upper bound of the highest posterior density \n', '            interval. Returns a dataframe if true, a series if false. Default False.\n', '        hdpi_interval : float\n', '            HDPI width. Default 0.9.\n', '        rng_key : two-element ndarray.\n', '            Optional rng key, will be randomly splitted if not provided.\n', '\n', '        Returns\n', '        -------\n', '        pd.Series or pd.DataFrame\n', '            Forecasts. Will be a series with the name of the dv if no HDPI. Will be a\n', '            dataframe if HDPI is included.\n', '\n', '        """"""\n']","['self.split_rand_key', 'rng_key.astype', 'infer.Predictive', 'pd.Series', 'diagnostics.hpdi', 'pd.DataFrame', 'predictions.mean']",7
source/models/model_numpyro.py:BaseModel:num_samples,BaseModel:num_samples,method,4,4,4,69,17.25,0,0,['self'],[None],[None],474,"['        """"""Return the number of samples per variable.\n', '        \n', '        Assumes samples from all variables have same shape. Counts samples across all\n', '        chains.\n', '        """"""\n']","['next', 'np.prod']",2
source/models/model_numpyro.py:BaseModel:num_chains,BaseModel:num_chains,method,4,4,4,63,15.75,0,0,['self'],[None],[None],485,"['        """"""Return the number of chains per variable in the model.\n', '        \n', '        Assumes samples from all variables have same shape.\n', '        """"""\n']",['next'],1
source/models/model_numpyro.py:BaseModel:samples_flat,BaseModel:samples_flat,method,2,8,8,49,6.12,0,0,['self'],[None],[None],495,"['        """"""Provide a 1D view of the model\'s samples.""""""\n']",['v.flatten'],1
source/models/model_numpyro.py:BaseModel:samples_df,BaseModel:samples_df,method,12,21,19,318,15.14,0,0,['self'],[None],[None],501,"['        """"""Return a DataFrame of the model\'s MCMC samples.""""""\n']","['np.arange', 'pd.DataFrame']",2
source/models/model_numpyro.py:BaseModel:from_dict,BaseModel:from_dict,method,10,16,14,128,8.0,1,0,"['cls', 'data', 'typing.Any]', '**model_kw']","[None, ' typing.Dict[str', None, None]","[None, None, None, None]",515,"['        """"""Return a pre-fitted model given a dictionary of config.\n', '\n', '        The dictionary MUST contain the following:\n', '\n', '        - samples. A dictionary of variables to MCMC samples. Must contain all feature \n', ""        names and additional model variables. Each variable's data must be the same \n"", '        shape.\n', '\n', '        Any other dict keys will be added as model attributes.\n', '        \n', '        Parameters\n', '        ----------\n', '        data : dict.\n', '            Model configuration, including requirements listed above.\n', '        kwargs\n', '            passed to Model() init.\n', '    \n', '        Returns\n', '        -------\n', '        Model\n', '            A ready-to-use model.\n', '        """"""\n']","['cls.preprocess_config_dict', 'cls', 'data.items', 'setattr']",4
source/models/model_numpyro.py:BaseModel:to_json,BaseModel:to_json,method,2,4,4,59,14.75,0,0,['self'],[None],[None],549,"['        """"""Return a JSON payload of the model\'s config.""""""\n']",['json.dumps'],1
source/models/model_numpyro.py:BaseModel:preprocess_config_dict,BaseModel:preprocess_config_dict,method,13,49,35,389,7.94,2,3,"['cls', 'config']","[None, ' dict']","[None, None]",554,"['        """"""Run checks and transformations on dicts for use in ``from_dict()``.""""""\n']","['device_put', 'list', 'exceptions.IncompleteSamples', 'samples.items']",4
source/models/model_numpyro.py:BaseModel:metrics,BaseModel:metrics,method,13,22,19,193,8.77,0,1,"['self', 'df', 'aggerrs']","[None, ' pd.DataFrame', ' bool ']","[None, None, ' True']",577,"['        """"""Get prediction accuracy metrics of the model against data.\n', '        \n', '        Parameters\n', '        ----------\n', '        df : pd.DataFrame\n', '            Input data for the model.\n', '        aggerrs : bool\n', '            Option to aggregate errors across observations (default True). If true, \n', '            a dictionary of summary statistics are returned. If False, pointwise errors\n', '            are returned as a DataFrame.\n', '        \n', '        Returns\n', '        -------\n', '        dict or pd.DataFrame\n', '            If aggerrs, a dictionary of summary statistics are returned. If False, \n', '            pointwise errors are returned as a DataFrame.\n', '        """"""\n']","['self.predict', 'metrics', 'pd.DataFrame', 'onp.abs']",4
source/models/model_numpyro.py:BaseModel:grouped_metrics,BaseModel:grouped_metrics,method,5,35,31,293,8.37,0,1,"['self', 'df', 'groupby', 'typing.List[str]]', 'aggfunc', 'aggerrs', '']","[None, ' pd.DataFrame', ' typing.Union[str', None, ' typing.Callable ', ' bool ', None]","[None, None, None, None, ' onp.sum', ' True', None]",607,"['        """"""Return grouped accuracy metrics.\n', '        \n', '        Parameters\n', '        ----------\n', '        df : pd.DataFrame\n', '            Input data for the model.\n', '        groupby : str or list of str\n', '            Groupby clause for pandas.\n', '        aggfunc : callable\n', '            How to aggregate actuals and predictions wihtin a group. Default sum.\n', '        aggerrs : bool\n', '            Option to aggregate errors across groups (default True). If true, \n', '            a dictionary of summary statistics are returned. If False, groupwise errors\n', '            are returned as a DataFrame.\n', '        \n', '        Returns\n', '        -------\n', '        dict or pd.DataFrame\n', '            If aggerrs, a dictionary of summary statistics are returned. If False, \n', '            groupwise errors are returned as a DataFrame.\n', '        """"""\n']","['df.assign', 'onp.abs', 'metrics']",3
source/models/model_numpyro.py:BaseModel:formula,BaseModel:formula,method,14,31,25,349,11.26,0,1,['self'],[None],[None],653,"['        """"""Return a formula string describing the model.""""""\n']","['get_str', 'formula_template.format']",2
source/models/model_numpyro.py:Normal:link,Normal:link,method,2,2,2,7,3.5,0,0,['x'],[None],[None],181,"['        """"""Implement the link function for the model.""""""\n']",[],0
source/models/model_numpyro.py:Normal:likelihood_func,Normal:likelihood_func,method,4,6,6,97,16.17,0,0,"['self', 'yhat']","[None, None]","[None, None]",186,"['        """"""Return the likelihood distribution given predictions yhat.""""""\n']","['numpyro.sample', 'dist.Exponential', 'dist.Normal']",3
source/models/model_numpyro.py:Poisson:link,Poisson:link,method,2,2,2,15,7.5,0,0,['x'],[None],[None],181,"['        """"""Implement the link function for the model.""""""\n']",['np.exp'],1
source/models/model_numpyro.py:Poisson:likelihood_func,Poisson:likelihood_func,method,2,2,2,24,12.0,0,0,"['self', 'yhat']","[None, None]","[None, None]",186,"['        """"""Return the likelihood distribution given predictions yhat.""""""\n']",['dist.Poisson'],1
source/models/model_numpyro.py:Bernoulli:link,Bernoulli:link,method,2,2,2,14,7.0,0,0,['x'],[None],[None],181,"['        """"""Implement the link function for the model.""""""\n']",['expit'],1
source/models/model_numpyro.py:Bernoulli:likelihood_func,Bernoulli:likelihood_func,method,2,2,2,33,16.5,0,0,"['self', 'probs']","[None, None]","[None, None]",715,"['        """"""Return a Bernoulli likelihood.""""""\n']",['dist.Bernoulli'],1
source/models/model_numpyro.py:ShabadooException:__str__,ShabadooException:__str__,method,7,8,8,80,10.0,0,0,['self'],[None],[None],734,"['        """"""Return the exception string.""""""\n']",[],0
source/models/model_numpyro.py:NotFittedError:__init__,NotFittedError:__init__,method,4,14,14,92,6.57,0,1,"['self', 'func']","[None, None]","[None, 'None']",744,"['        """"""Set the message.""""""\n']",[],0
source/models/model_numpyro.py:AlreadyFittedError:__init__,AlreadyFittedError:__init__,method,3,9,9,78,8.67,0,0,"['self', 'model']","[None, None]","[None, None]",753,"['        """"""Set the message.""""""\n']",[],0
source/models/model_numpyro.py:IncompleteModel:__init__,IncompleteModel:__init__,method,3,10,10,93,9.3,0,0,"['self', 'model', 'attribute']","[None, None, None]","[None, None, None]",762,"['        """"""Set the message.""""""\n']",[],0
source/models/model_numpyro.py:IncompleteFeature:__init__,IncompleteFeature:__init__,method,1,8,8,49,6.12,0,0,"['self', 'name', 'key']","[None, None, None]","[None, None, None]",771,"['        """"""Set the message.""""""\n']",[],0
source/models/model_numpyro.py:IncompleteSamples:__init__,IncompleteSamples:__init__,method,1,9,9,53,5.89,0,0,"['self', 'name']","[None, None]","[None, None]",779,"['        """"""Set the message.""""""\n']",[],0
source/models/model_outlier.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],14,[],"['print', 'log2', 'log3']",3
source/models/model_outlier.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],17,[],['print'],1
source/models/model_outlier.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],20,[],['print'],1
source/models/model_outlier.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],23,[],['os.makedirs'],1
source/models/model_outlier.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",29,[],['Model'],1
source/models/model_outlier.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],34,[],[],0
source/models/model_outlier.py:fit,fit,function,17,47,41,493,10.49,1,2,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",95,"['    """"""\n', '    """"""\n']","['get_dataset', 'log2', 'Xtrain.astype', 'ytrain.astype']",4
source/models/model_outlier.py:predict,predict,function,17,48,35,452,9.42,0,4,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",114,[],"['get_dataset', 'Xpred.astype', 'compute_pars.get']",3
source/models/model_outlier.py:save,save,function,15,54,39,536,9.93,0,0,"['path', 'info']","[None, None]","['None', 'None']",144,[],"['os.makedirs', 'pickle.dump', 'open', 'mkeras.save', 'log']",5
source/models/model_outlier.py:load_model,load_model,function,20,50,41,417,8.34,0,1,['path'],[None],"['""""']",166,[],"['pickle.load', 'Model', 'log']",3
source/models/model_outlier.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",189,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_outlier.py:get_dataset,get_dataset,function,7,49,33,416,8.49,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",201,"['    """"""\n', '      ""ram""  : \n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/model_outlier.py:Model,Model,class,15,37,35,512,13.84,0,1,[],[],[],79,[],[],0
source/models/model_outlier.py:Model:__init__,Model:__init__,method,14,32,30,444,13.88,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",80,[],"['globals', 'model_class', 'log2']",3
source/models/model_rec.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],16,[],"['print', 'log2', 'log3']",3
source/models/model_rec.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],19,[],['print'],1
source/models/model_rec.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],22,[],['print'],1
source/models/model_rec.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],25,[],['os.makedirs'],1
source/models/model_rec.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",31,[],['Model'],1
source/models/model_rec.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],36,[],[],0
source/models/model_rec.py:get_dataset,get_dataset,function,26,122,51,1366,11.2,0,4,"['data_pars', 'task_type']","[None, None]","[None, '""train""']",82,"['    """"""\n', '    :param data_pars:\n', '    :param task_type:\n', '    :return:\n', '    """"""\n']",['utils.load_data'],1
source/models/model_rec.py:fit,fit,function,11,20,20,180,9.0,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",147,"['    """"""\n', '    """"""\n']",['get_dataset'],1
source/models/model_rec.py:predict,predict,function,11,18,16,177,9.83,0,1,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",161,[],"['get_dataset', 'next', 'model.model']",3
source/models/model_rec.py:eval,eval,function,7,24,19,291,12.12,0,0,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, ' dict', ' dict', ' dict', None]","['None', '{}', '{}', '{}', None]",178,[],"['encode', 'log', 'decode']",3
source/models/model_rec.py:save,save,function,11,32,29,360,11.25,0,0,"['path', 'info']","[None, None]","['None', 'None']",191,"['    """""" Custom saving\n', '    """"""\n']","['os.makedirs', 'pickle.dump', 'open']",3
source/models/model_rec.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",208,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_rec.py:get_dataset_tuple,get_dataset_tuple,function,12,42,30,330,7.86,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",221,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3) OR single dataframe\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/model_rec.py:get_dataset2,get_dataset2,function,22,90,61,936,10.4,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",247,"['    """"""  Return tuple of dataframes\n', '    """"""\n']","['data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",4
source/models/model_rec.py:train_test_split2,train_test_split2,function,21,39,32,411,10.54,0,0,"['df', 'coly']","[None, None]","[None, None]",294,[],"['log3', 'df.drop', 'np.sum', 'X.head', 'train_test_split', 'len']",6
source/models/model_rec.py:test,test,function,28,215,149,2011,9.35,0,0,['n_sample          '],[None],[' 1000'],311,[],"['test_dataset_goodbooks', 'train_test_split2', 'post_process_fun', 'int', 'pre_process_fun', 'len', 'test_helper', 'Model', 'log', 'fit', 'predict', 'save']",12
source/models/model_rec.py:test_helper,test_helper,function,10,31,28,430,13.87,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",377,[],"['Model', 'log', 'fit', 'predict', 'save']",5
source/models/model_rec.py:Model,Model,class,14,26,25,292,11.23,0,1,[],[],[],67,[],[],0
source/models/model_rec.py:Model:__init__,Model:__init__,method,13,20,19,207,10.35,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars', 'global_pars']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",68,[],"['TorchEASE', 'log2']",2
source/models/model_sampler.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],22,[],"['print', 'log2', 'log3']",3
source/models/model_sampler.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],25,[],['print'],1
source/models/model_sampler.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],28,[],['print'],1
source/models/model_sampler.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],31,[],['os.makedirs'],1
source/models/model_sampler.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",37,[],['Model'],1
source/models/model_sampler.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],42,[],[],0
source/models/model_sampler.py:fit,fit,function,16,37,33,353,9.54,1,1,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[' dict', ' dict', ' dict', None]","['None', 'None', 'None', None]",105,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy']",2
source/models/model_sampler.py:eval,eval,function,18,45,37,440,9.78,0,2,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",120,"['    """"""\n', '       Return metrics of the model when fitted.\n', '    """"""\n']","['get_dataset', 'transform', 'compute_pars.get', 'evaluate']",4
source/models/model_sampler.py:transform,transform,function,32,255,85,2230,8.75,0,10,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",145,"['    """""" Geenrate Xtrain  ----> Xtrain_new\n', '    :param Xpred:\n', '        Xpred ==> None            if you want to get generated samples by by SDV models\n', '              ==> tuple of (x, y) if you want to resample dataset with IMBLEARN models\n', '              ==> dataframe       if you want to transorm by sklearn models like TruncatedSVD\n', '    :param data_pars:\n', '    :param compute_pars:\n', '    :param out_pars:\n', '    :param kw:\n', '    :return:\n', '    """"""\n']","['get_dataset', 'kw.get', 'isinstance', 'len', 'get_dataset_tuple', 'Exception', 'log3', 'transform2']",8
source/models/model_sampler.py:transform2,transform2,function,29,131,73,1146,8.75,0,5,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",202,"['    """""" Geenrate Xtrain  ----> Xtrain_new\n', '    :param Xpred:\n', '        Xpred ==> None            if you want to get generated samples by by SDV models\n', '              ==> tuple of (x, y) if you want to resample dataset with IMBLEARN models\n', '              ==> dataframe       if you want to transorm by sklearn models like TruncatedSVD\n', '    :param data_pars:\n', '    :param compute_pars:\n', '    :param out_pars:\n', '    :param kw:\n', '    :return:\n', '    """"""\n']","['get_dataset', 'kw.get', 'isinstance', 'len', 'get_dataset_tuple', 'Exception', 'log3']",7
source/models/model_sampler.py:predict,predict,function,2,4,4,24,6.0,0,0,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",257,[],[],0
source/models/model_sampler.py:save,save,function,8,26,22,299,11.5,0,0,"['path', 'info']","[None, None]","['None', 'None']",263,[],"['os.makedirs', 'pickle.dump', 'open']",3
source/models/model_sampler.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",275,[],"['pickle.load', 'Model']",2
source/models/model_sampler.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",288,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_sampler.py:get_dataset_tuple,get_dataset_tuple,function,9,34,28,282,8.29,1,1,"['Xtrain', 'cols_type_received', 'cols_ref', 'split']","[None, None, None, None]","[None, None, None, 'False']",301,"['    """"""  Split into Tuples = (df1, df2, df3) to feed model, (ie Keras)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :param split: \n', '        True :  split data to list of dataframe \n', '        False:  return same input of data\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/model_sampler.py:get_dataset,get_dataset,function,29,136,88,1376,10.12,1,5,"['Xtrain', 'cols_type_received', 'cols_ref', 'split']","[None, None, None, None]","[None, None, None, 'False']",323,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'list', 'cols_type_received.keys', 'kw.get', 'get_dataset_tuple', 'log2', 'Exception']",10
source/models/model_sampler.py:test,test,function,58,292,201,2378,8.14,4,0,[],[],[],372,[],"['make_classification', 'pd.DataFrame', 'range', 'np.arange', 'len', 'train_test_split', 'colg_input.items', 'log', 'models.items', 'test_helper', 'test_dataset_classi_fake', 'sklearn_datasets.make_classification', 'y.reshape']",13
source/models/model_sampler.py:test_dataset_classi_fake,test_dataset_classi_fake,function,23,50,46,385,7.7,1,0,['nrows'],[None],['500'],487,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'len', 'y.reshape']",5
source/models/model_sampler.py:train_test_split2,train_test_split2,function,24,43,36,461,10.72,0,0,"['df', 'coly']","[None, None]","[None, None]",503,[],"['log3', 'df.drop', 'np.sum', 'X.head', 'train_test_split', 'len']",6
source/models/model_sampler.py:test2,test2,function,18,173,112,1472,8.51,0,0,['n_sample          '],[None],[' 1000'],521,[],"['test_dataset_classi_fake', 'train_test_split2', 'post_process_fun', 'int', 'pre_process_fun', 'test_helper']",6
source/models/model_sampler.py:test_helper,test_helper,function,11,41,33,534,13.02,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",582,[],"['Model', 'log', 'fit', 'transform', 'save', 'load_model']",6
source/models/model_sampler.py:pd_sample_imblearn,pd_sample_imblearn,function,45,151,107,1310,8.68,2,3,"['df', 'col', 'pars']","[None, None, None]","['None', 'None', 'None']",625,"['    """"""\n', '        Over-sample\n', '    """"""\n']","['params_check', 'locals', 'pars.get', 'model_resample', 'gp.fit_resample', 'pd.DataFrame', 'save_features', 'prefix.replace', 'save']",9
source/models/model_sampler.py:pd_augmentation_sdv,pd_augmentation_sdv,function,18,123,100,1066,8.67,0,2,"['df', 'col', 'pars']","[None, None, None]","[None, 'None', '{}']",677,"[""    '''\n"", '    Using SDV Variation Autoencoders, the function augments more data into the dataset\n', '    params:\n', '            df          : (pandas dataframe) original dataframe\n', '            col : column name for data enancement\n', '            pars        : (dict - optional) contains:\n', '                n_samples     : (int - optional) number of samples you would like to add, defaul is 10%\n', '                primary_key   : (String - optional) the primary key of dataframe\n', '                aggregate  : (boolean - optional) if False, prints SVD metrics, else it averages them\n', '                path_model_save: saving location if save_model is set to True\n', '                path_model_load: saved model location to skip training\n', '                path_data_new  : new data where saved\n', '    returns:\n', '            df_new      : (pandas dataframe) df with more augmented data\n', '            col         : (list of strings) same columns\n', ""    '''\n""]","['pars.get', 'max', 'int', 'model', 'log', 'model.sample', 'log_pd', 'evaluate', 'df.append', 'len', 'new_data.to_parquet']",11
source/models/model_sampler.py:logs,logs,function,2,4,4,30,7.5,0,1,['*s'],[None],[None],774,[],['print'],1
source/models/model_sampler.py:log_pd,log_pd,function,2,6,6,47,7.83,0,0,"['df', '*s', 'n', 'm']","[None, None, None, None]","[None, None, '0', '1']",779,[],"['print', 'df.head']",2
source/models/model_sampler.py:pd_export,pd_export,function,11,20,16,244,12.2,0,0,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",788,"['    """"""\n', '       Export in train folder for next training\n', '       colsall\n', '    :param df:\n', '    :param col:\n', '    :param pars:\n', '    :return:\n', '    """"""\n']","['dfX.set_index', 'dfX.to_parquet', 'dfy.set_index']",3
source/models/model_sampler.py:pd_filter_rows,pd_filter_rows,function,20,50,41,320,6.4,0,0,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",810,"['    """"""\n', '       Remove rows based on criteria\n', '    :param df:\n', '    :param col:\n', '    :param pars:\n', '    :return:\n', '    """"""\n']","['isfloat', 'float', 'pars.get', 'filter_pars.get']",4
source/models/model_sampler.py:pd_autoencoder,pd_autoencoder,function,49,126,98,1690,13.41,0,2,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",841,"['    """"""""\n', '    (4) Autoencoder\n', '    An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.\n', '    The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction,\n', '    by training the network to ignore noise.\n', '    (i) Feed Forward\n', '    The simplest form of an autoencoder is a feedforward, non-recurrent\n', '    neural network similar to single layer perceptrons that participate in multilayer perceptrons\n', '    """"""\n']","['encoder_dataset', 'df.select_dtypes', 'print', 'encode_objects', 'OrdinalEncoder', 'oe.fit', 'oe.transform', 'minmax_scale', 'pars.get', 'pd.DataFrame', 'encoded_train.add_prefix', 'pd.concat']",12
source/models/model_sampler.py:test_pd_augmentation_sdv,test_pd_augmentation_sdv,function,21,111,65,1243,11.2,0,0,[],[],[],910,[],"['load_boston', 'pd.DataFrame', 'log_pd', 'os.makedirs', 'log', 'pd_augmentation_sdv', 'load_timeseries_demo']",7
source/models/model_sampler.py:pd_covariate_shift_adjustment,pd_covariate_shift_adjustment,function,61,114,99,965,8.46,1,1,[],[],[],958,"['    """"""\n', '    https://towardsdatascience.com/understanding-dataset-shift-f2a5a262a766\n', '     Covariate shift has been extensively studied in the literature, and a number of proposals to work under it have been published. Some of the most important ones include:\n', '        Weighting the log-likelihood function (Shimodaira, 2000)\n', '        Importance weighted cross-validation (Sugiyama et al, 2007 JMLR)\n', '        Integrated optimization problem. Discriminative learning. (Bickel et al, 2009 JMRL)\n', '        Kernel mean matching (Gretton et al., 2009)\n', '        Adversarial search (Globerson et al, 2009)\n', '        Frank-Wolfe algorithm (Wen et al., 2015)\n', '    """"""\n']","['datasets.make_regression', 'FW', 'sparse.dok_matrix', 'range', 'np.argmax', 'np.sign', 'trace.append', 'min', 'q_t.dot', 'np.array', 'plt.plot', 'plt.yscale', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.grid', 'plt.show', 'np.mean', 'print']",19
source/models/model_sampler.py:test,test,function,58,292,201,2378,8.14,4,0,[],[],[],372,[],"['make_classification', 'pd.DataFrame', 'range', 'np.arange', 'len', 'train_test_split', 'colg_input.items', 'log', 'models.items', 'test_helper', 'test_dataset_classi_fake', 'sklearn_datasets.make_classification', 'y.reshape']",13
source/models/model_sampler.py:Model,Model,class,13,34,32,392,11.53,0,1,[],[],[],90,[],[],0
source/models/model_sampler.py:Model:__init__,Model:__init__,method,12,29,27,324,11.17,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",91,[],"['KeyError', 'model_class', 'log2']",3
source/models/model_sklearn.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],15,[],"['print', 'log2', 'log3']",3
source/models/model_sklearn.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],18,[],['print'],1
source/models/model_sklearn.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],21,[],['print'],1
source/models/model_sklearn.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],24,[],['os.makedirs'],1
source/models/model_sklearn.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",30,[],['Model'],1
source/models/model_sklearn.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],35,[],[],0
source/models/model_sklearn.py:model_automl,model_automl,function,7,32,26,367,11.47,0,0,[],[],[],80,[],['os.system'],1
source/models/model_sklearn.py:fit,fit,function,12,33,29,363,11.0,1,1,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",108,"['    """"""\n', '    """"""\n']","['get_dataset', 'log']",2
source/models/model_sklearn.py:eval,eval,function,16,42,41,495,11.79,0,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",122,"['    """"""\n', '       Return metrics of the model when fitted.\n', '    """"""\n']","['get_dataset', 'predict', 'compute_pars.get', 'mpars.get', 'scorer']",5
source/models/model_sklearn.py:predict,predict,function,14,29,24,278,9.59,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",146,[],"['get_dataset', 'compute_pars.get']",2
source/models/model_sklearn.py:save,save,function,8,26,22,299,11.5,0,0,"['path', 'info']","[None, None]","['None', 'None']",162,[],"['os.makedirs', 'pickle.dump', 'open']",3
source/models/model_sklearn.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",174,[],"['pickle.load', 'Model']",2
source/models/model_sklearn.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",187,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_sklearn.py:preprocess,preprocess,function,21,68,38,740,10.88,0,2,['prepro_pars'],[None],[None],198,[],"['make_classification', 'train_test_split', 'pd.read_csv']",3
source/models/model_sklearn.py:get_dataset,get_dataset,function,12,65,49,580,8.92,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",229,"['    """"""\n', '      ""ram""  : \n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'log3', 'Exception']",3
source/models/model_sklearn.py:get_params_sklearn,get_params_sklearn,function,2,2,2,39,19.5,0,0,['deep'],[None],['False'],261,[],[],0
source/models/model_sklearn.py:get_params,get_params,function,15,34,30,377,11.09,0,1,['deep'],[None],['False'],265,[],"['get_params', 'json.load', 'Exception']",3
source/models/model_sklearn.py:test_dataset_classi_fake,test_dataset_classi_fake,function,23,50,46,385,7.7,1,0,['nrows'],[None],['500'],285,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'len', 'y.reshape']",5
source/models/model_sklearn.py:train_test_split2,train_test_split2,function,21,39,32,411,10.54,0,0,"['df', 'coly']","[None, None]","[None, None]",301,[],"['log3', 'df.drop', 'np.sum', 'X.head', 'train_test_split', 'len']",6
source/models/model_sklearn.py:test,test,function,23,50,46,385,7.7,1,0,['nrows'],[None],['500'],317,"['    """"""https://github.com/manujosephv/pytorch_tabular/blob/main/tests/test_mdn.py\n', '    \n', '\n', '    """"""\n']","['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'len', 'y.reshape']",5
source/models/model_sklearn.py:test2,test2,function,39,137,58,1041,7.6,2,0,[],[],[],418,"['    """"""\n', '        from pyod.models.abod  import *\n', '    from pyod.models.auto_encoder import *\n', '    from pyod.models.cblof import *\n', '    from pyod.models.cof import *\n', '    from pyod.models.combination import *\n', '    from pyod.models.copod import *\n', '    from pyod.models.feature_bagging import *\n', '    from pyod.models.hbos import *\n', '    from pyod.models.iforest import *\n', '    from pyod.models.knn import *\n', '    from pyod.models.lmdd import *\n', '    from pyod.models.loda import *\n', '    from pyod.models.lof import *\n', '    from pyod.models.loci import *\n', '    from pyod.models.lscp import *\n', '    from pyod.models.mad import *\n', '    from pyod.models.mcd import *\n', '    from pyod.models.mo_gaal import *\n', '    from pyod.models.ocsvm import *\n', '    from pyod.models.pca import *\n', '    from pyod.models.sod import *\n', '    from pyod.models.so_gaal import *\n', '    from pyod.models.sos import *\n', '    from pyod.models.vae import *\n', '    from pyod.models.xgbod import *\n', '\n', '    https://pyod.readthedocs.io/en/latest/pyod.html\n', '\n', '    :return:\n', '    """"""\n']","['log', 'os.system', 'template_dict']",3
source/models/model_sklearn.py:Model,Model,class,14,32,29,393,12.28,0,2,[],[],[],91,[],[],0
source/models/model_sklearn.py:Model:__init__,Model:__init__,method,13,26,23,324,12.46,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",92,[],"['model_automl', 'globals', 'model_class', 'log']",4
source/models/model_tseries.py:log,log,function,2,5,4,54,10.8,0,0,['*s'],[None],[None],34,[],"['print', 'log3']",2
source/models/model_tseries.py:log3,log3,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],38,[],['print'],1
source/models/model_tseries.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",44,[],['Model'],1
source/models/model_tseries.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],49,[],[],0
source/models/model_tseries.py:fit,fit,function,13,35,30,373,10.66,1,2,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",73,"['    """"""\n', '    """"""\n']","['get_dataset', 'log']",2
source/models/model_tseries.py:predict,predict,function,14,25,23,254,10.16,0,1,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",88,[],"['get_dataset', 'ForecastingHorizon']",2
source/models/model_tseries.py:save,save,function,8,26,22,299,11.5,0,0,"['path', 'info']","[None, None]","['None', 'None']",104,[],"['os.makedirs', 'pickle.dump', 'open']",3
source/models/model_tseries.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",116,[],"['pickle.load', 'Model']",2
source/models/model_tseries.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",129,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_tseries.py:get_dataset,get_dataset,function,12,65,49,580,8.92,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",142,"['    """"""\n', '      ""ram""  :\n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'log3', 'Exception']",3
source/models/model_tseries.py:test_dataset_tseries,test_dataset_tseries,function,15,23,19,233,10.13,0,0,['nrows'],[None],['10000'],177,[],"['pd.read_csv', 'df.groupby', 'df.set_index', 'pd.to_datetime']",4
source/models/model_tseries.py:LighGBM_forecaster,LighGBM_forecaster,function,5,14,13,176,12.57,0,0,"[""lightgbm_pars= {'objective'"", ""'alpha'"", ""forecaster_pars = {'window_length'""]","['', ' 0.5}', '']","["" {'objective':'quantile'"", None, "" {'window_length': 4}""]",190,"['    """"""\n', '    #1.Separate the Seasonal Component.\n', '    #2.Fit a forecaster for the trend.\n', '    #3.Fit a Autoregressor to the resdiual(autoregressing on four historic values).\n', '\n', '    """"""\n']","['LGBMRegressor', 'RecursiveRegressionForecaster']",2
source/models/model_tseries.py:test0,test0,function,29,71,54,970,13.66,1,0,"['nrows', 'file_path', 'coly', 'coldate', 'colcat']","[None, None, None, None, None]","['1000', 'None', 'None', 'None', 'None']",206,"['    """"""\n', '        nrows : take first nrows from dataset\n', '    """"""\n']","['test_dataset_tseries', 'df.drop', 'temporal_train_test_split', 'LighGBM_forecaster', 'forecaster.fit', 'ForecastingHorizon', 'forecaster.predict', 'pd.DataFrame', 'log']",9
source/models/model_tseries.py:test2,test2,function,58,287,216,3116,10.86,2,0,['nrows'],[None],['1000'],272,"['    """"""\n', '        nrows : take first nrows from dataset\n', '    """"""\n']","['test_dataset', 'log', 'np.sum', 'train_test_split', 'temporal_train_test_split', 'get_transformed_target_forecaster', 'lgb.LGBMRegressor', 'RecursiveRegressionForecaster', 'ForecastingHorizon', 'forecaster.fit', 'forecaster.predict', 'forecasts.append', 'reset', 'len', 'post_process_fun', 'int', 'pre_process_fun', 'Model', 'fit', 'predict']",20
source/models/model_tseries.py:myModel,myModel,class,0,1,1,4,4.0,0,0,[],[],[],54,[],[],0
source/models/model_tseries.py:Model,Model,class,13,26,24,330,12.69,0,2,[],[],[],61,[],[],0
source/models/model_tseries.py:Model:__init__,Model:__init__,method,12,21,19,262,12.48,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",62,[],"['globals', 'model_class', 'log']",3
source/models/model_vaem.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],17,[],"['print', 'log2', 'log3']",3
source/models/model_vaem.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],20,[],['print'],1
source/models/model_vaem.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],23,[],['print'],1
source/models/model_vaem.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],26,[],['os.makedirs'],1
source/models/model_vaem.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",32,[],['Model'],1
source/models/model_vaem.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],37,[],[],0
source/models/model_vaem.py:save,save,function,18,34,32,436,12.82,0,0,"['path', 'info']","[None, None]","[""''"", 'None']",87,[],"['os.makedirs', 'Model', 'pickle.dump', 'open']",4
source/models/model_vaem.py:load_model,load_model,function,18,30,28,353,11.77,0,0,['path'],[None],"['""""']",105,[],"['pickle.load', 'Model', 'get_model', 'log']",4
source/models/model_vaem.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",126,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_vaem.py:load_data,load_data,function,68,132,119,2456,18.61,1,2,"['filePath', 'categories', 'cat_col', 'num_cols', 'discrete_cols', 'targetCol', 'nsample', 'delimiter']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",225,[],"['pd.read_csv', 'print', 'dataframe.copy', 'process.encode_catrtogrial_column', 'np.array', 'len', 'np.concatenate', 'np.zeros', 'np.ones', 'Data_sub.min', 'process.data_preprocess', 'train_test_split', 'Data_train_decompressed.copy', 'Data_test_decompressed.copy', 'process.noisy_transform']",15
source/models/model_vaem.py:encode2,encode2,function,32,134,73,2298,17.15,0,2,"['data_decode', 'list_discrete', 'records_d', 'fast_plot']","[None, None, None, None]","[None, None, None, None]",307,[],"['p_vae_active_learning', 'process.compress_data', 'tf.reset_default_graph', 'vae.get_imputation', 'x_real.min', 'pd.DataFrame', 'sns.pairplot', 'g.map_diag', 'g.set', 'g.map_upper', 'g.map_lower', 'process.invert_noise']",12
source/models/model_vaem.py:decode2,decode2,function,41,73,71,1389,19.03,0,1,"['data_decode', 'scaling_factor', 'list_discrete', 'records_d', 'plot']","[None, None, None, None, None]","[None, None, None, None, 'False']",366,[],"['params.Params', 'print', 'p_vae_active_learning', 'np.load', 'plt.figure', 'plt.subplots', 'ax1.plot', 'ax1.errorbar', 'plt.xlabel', 'plt.ylabel', 'plt.xticks', 'plt.yticks', 'ax1.legend', 'ax1.ticklabel_format', 'plt.show']",15
source/models/model_vaem.py:save_model2,save_model2,function,1,1,1,22,22.0,0,0,"['model', 'output_dir']","[None, None]","[None, None]",405,[],['model.save'],1
source/models/model_vaem.py:p_vae_active_learning,p_vae_active_learning,function,137,540,271,4948,9.16,8,6,"['Data_train_compressed', 'Data_train', 'mask_train', 'Data_test', 'mask_test_compressed', 'mask_test', 'cat_dims', 'dim_flt', 'dic_var_type', 'args', 'list_discrete', 'records_d', 'estimation_method']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None, '1']",408,[],"['len', 'np.zeros', 'range', 'reward.lindley', 'tf.reset_default_graph', 'train_p_vae', 'np.tile', 'random.shuffle', 'vae.predictive_loss', 'print', 'np.eye', 'np.savez', 'Data_train_compressed.reshape', 'np.ones', 'np.where', 'reward_estimation.R_lindley_chain', 'reward_estimation.completion']",17
source/models/model_vaem.py:train_p_vae,train_p_vae,function,87,362,189,4013,11.09,10,8,"['stage', 'x_train', 'Data_train', 'mask_train', 'epochs', 'latent_dim', 'cat_dims', 'dim_flt', 'batch_size', 'p', 'K', 'iteration', 'list_discrete', 'records_d', 'args']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]",585,[],"['np.arange', 'np.minimum', 'encoders.vaem_encoders', 'decoders.vaem_decoders', 'vae_model.partial_vaem', 'int', 'np.zeros', 'range', 'sample', 'bernoulli.rvs', 'np.sum', 'mask_drop.reshape', 'vae.update', 'vae.full_batch_loss', 'print', 'vae.save_generator', 'vae.save_encoder']",17
source/models/model_vaem.py:test,test,function,9,13,13,352,27.08,0,0,[],[],[],738,[],"['Model_custom', 'model.fit', 'model.encode', 'model.decode']",4
source/models/model_vaem.py:Model,Model,class,0,1,1,4,4.0,0,0,[],[],[],83,[],[],0
source/models/model_vaem.py:Model_custom,Model_custom,class,57,103,83,1633,15.85,3,0,[],[],[],140,[],[],0
source/models/model_vaem.py:Model_custom:__init__,Model_custom:__init__,method,1,5,5,48,9.6,0,0,['self'],[None],[None],141,[],['print'],1
source/models/model_vaem.py:Model_custom:fit,Model_custom:fit,method,37,47,44,764,16.26,1,0,"['self,filePath, categories,cat_cols,num_cols,discrete_cols,targetCol,nsample ']",[None],"["" -1,delimiter=',',plot=False""]",150,"['        """"""\n', '        This Function will load the data and fit the preprocessing technique into it\n', '        params:\n', '            filePath: CSV File path relative to file\n', '            categories: Categorical feature list\n', '            cat_cols: category_columns\n', '            nums_cols: Numerical Columns list\n', '            discrete_cols: discrete cols list\n', '            target_cols: Column name of Target Variable\n', '            nsample: No of Sample for Encoding-Decoding\n', ""            delimiter: Delimiter in CSV File,Default=','\n"", '\n', '        """"""\n']","['load_data', 'encode2', 'decode2']",3
source/models/model_vaem.py:Model_custom:encode,Model_custom:encode,method,14,23,23,392,17.04,1,0,['self'],[None],[None],187,[],"['range', 'print', 'log2', 'pa.table', 'pq.write_table']",5
source/models/model_vaem.py:Model_custom:decode,Model_custom:decode,method,11,18,18,261,14.5,1,0,['self'],[None],[None],206,[],"['range', 'pa.table', 'pq.write_table', 'print']",4
source/models/model_vaemdn.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],15,[],"['print', 'log2', 'log3']",3
source/models/model_vaemdn.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],18,[],['print'],1
source/models/model_vaemdn.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],21,[],['print'],1
source/models/model_vaemdn.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],24,[],['os.makedirs'],1
source/models/model_vaemdn.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",30,[],['Model'],1
source/models/model_vaemdn.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],35,[],[],0
source/models/model_vaemdn.py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],59,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/model_vaemdn.py:VAEMDN,VAEMDN,function,69,217,168,3250,14.98,1,0,['model_pars'],[None],[None],70,[],"['Input', 'Dense', 'Reshape', 'Lambda', 'encoder.compile', 'decoder.compile', 'decoder', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log', 'vae.add_loss', 'vae.compile']",16
source/models/model_vaemdn.py:AUTOENCODER_BASIC,AUTOENCODER_BASIC,function,30,124,100,1720,13.87,0,2,"['X_input_dim', 'loss_type', 'lr', 'epsilon', 'decay', 'optimizer', 'encodingdim ', 'dim_list']","[None, None, None, None, None, None, None, None]","[None, '""CosineSimilarity""', '0.01', '1e-3', '1e-4', ""'adam'"", ' 50', '""50,25,10""']",185,[],"['print', 'custom_loss_func_dice', 'K.sum', 'dim_list.split', 'int', 'encoder.compile', 'decoder.compile', 'decoder', 'autoencoder.compile']",9
source/models/model_vaemdn.py:AUTOENCODER_MULTIMODAL,AUTOENCODER_MULTIMODAL,function,8,13,12,225,17.31,0,0,"['input_shapes', 'hidden_dims', '64', '8]', 'output_activations', ""'relu']"", 'loss ', ""'poisson_divergence']"", 'optimizer']","[None, None, None, None, None, None, None, None, None]","['[10]', '[128', None, None, ""['sigmoid'"", None, "" ['bernoulli_divergence'"", None, ""'adam'""]",234,"['    """"""\n', '    pip install mmae[keras]\n', '    from tensorflow.keras.datasets import mnist\n', '    (x_train, y_train), (x_validation, y_validation) = mnist.load_data()\n', ""    x_train = x_train.astype('float32') / 255.0\n"", ""    y_train = y_train.astype('float32') / 255.0\n"", ""    x_validation = x_validation.astype('float32') / 255.0\n"", ""    y_validation = y_validation.astype('float32') / 255.0\n"", '\n', '    data = [x_train, y_train]\n', '    validation_data = [x_validation, y_validation]\n', '\n', '\n', '\n', '    # Train model where input and output are the same\n', '    autoencoder.fit(data, epochs=100, batch_size=256,\n', '                    validation_data=validation_data)\n', '\n', '    #To obtain a latent representation of the training data:\n', '    latent_data = autoencoder.encode(data)\n', '\n', '    #To decode the latent representation:\n', '    reconstructed_data = autoencoder.decode(latent_data)\n', '\n', '    #Encoding and decoding can also be merged into the following single statement:\n', '    reconstructed_data = autoencoder.predict(data)\n', '\n', '    :return:\n', '    """"""\n']","['MultimodalAutoencoder', 'autoencoder.compile', 'autoencoder.summary']",3
source/models/model_vaemdn.py:fit,fit,function,23,62,56,747,12.05,1,1,"['data_pars', 'compute_pars', 'out_pars', 'model_class', '**kw']","[None, None, None, None, None]","['None', 'None', 'None', ""'VAEMDN'"", None]",335,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'np.ones']",5
source/models/model_vaemdn.py:encode,encode,function,32,111,71,1381,12.44,3,4,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', 'model_class', '**kw']","[None, None, None, None, None, None]","['None', 'None', '{}', '{}', ""'VAEMDN'"", None]",369,[],"['get_dataset', 'data_pars.get', 'get_dataset_tuple', 'np.ones', 'compute_pars.get', 'os_makedirs', 'enumerate', 'range', 'log2', 'pa.table', 'pq.write_table', 'log']",12
source/models/model_vaemdn.py:decode,decode,function,25,62,56,768,12.39,1,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', 'index ', '**kw']","[None, None, None, None, None, None]","['None', 'None', '{}', '{}', ' 0', None]",417,[],"['get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log', 'compute_pars.get', 'range', 'log2', 'pa.table', 'pq.write_table']",9
source/models/model_vaemdn.py:predict,predict,function,13,35,31,377,10.77,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', 'model_class', '**kw']","[None, None, None, None, None, None]","['None', 'None', '{}', '{}', ""'VAEMDN'"", None]",450,[],"['get_dataset', 'data_pars.get', 'get_dataset_tuple', 'np.ones']",4
source/models/model_vaemdn.py:get_dataset_tuple,get_dataset_tuple,function,9,31,25,272,8.77,1,1,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",471,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/model_vaemdn.py:get_dataset,get_dataset,function,28,121,85,1134,9.37,1,6,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",491,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'test_dataset_correlation', 'get_dataset_tuple', 'Exception']",7
source/models/model_vaemdn.py:get_label,get_label,function,18,40,34,351,8.78,1,1,"['encoder', 'x_train', 'dummy_train', 'class_num', 'batch_size']","[None, None, None, None, None]","[None, None, None, '5', '256']",539,[],"['encoder.predict', 'np.zeros', 'range', 'np.max', 'np.argmax']",5
source/models/model_vaemdn.py:save,save,function,18,34,32,436,12.82,0,0,"['path', 'info']","[None, None]","['None', 'None']",555,[],"['os.makedirs', 'Model', 'pickle.dump', 'open']",4
source/models/model_vaemdn.py:load_model,load_model,function,21,42,37,530,12.62,0,1,"['path', 'model_class']","[None, None]","['""""', ""'VAEMDN'""]",573,[],"['pickle.load', 'Model', 'VAEMDN', 'AUTOENCODER_BASIC', 'log']",5
source/models/model_vaemdn.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",597,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/model_vaemdn.py:test_dataset_correlation,test_dataset_correlation,function,55,170,128,1974,11.61,7,2,['n_rows'],[None],['100'],610,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/model_vaemdn.py:test,test,function,107,354,275,3846,10.86,8,2,['n_rows'],[None],['100'],681,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze', 'test', 'test_dataset_correlation', 'test_helper', 'test_dataset_classi_fake', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'len', 'y.reshape', 'test_dataset_petfinder', 'log3', 'pd.read_csv', 'np.where', 'df.drop', 'df.apply']",23
source/models/model_vaemdn.py:test_dataset_classi_fake,test_dataset_classi_fake,function,23,50,46,385,7.7,1,0,['nrows'],[None],['500'],725,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'len', 'y.reshape']",5
source/models/model_vaemdn.py:test_dataset_petfinder,test_dataset_petfinder,function,24,54,49,712,13.19,0,0,['nrows'],[None],['1000'],741,[],"['log3', 'pd.read_csv', 'np.where', 'df.drop', 'df.apply']",5
source/models/model_vaemdn.py:train_test_split2,train_test_split2,function,21,39,32,411,10.54,0,0,"['df', 'coly']","[None, None]","[None, None]",768,[],"['log3', 'df.drop', 'np.sum', 'X.head', 'train_test_split', 'len']",6
source/models/model_vaemdn.py:test2,test2,function,18,200,132,1655,8.28,0,0,['n_sample          '],[None],[' 1000'],785,[],"['test_dataset_classi_fake', 'train_test_split2', 'post_process_fun', 'int', 'pre_process_fun', 'len', 'test_helper']",7
source/models/model_vaemdn.py:test3,test3,function,18,204,136,1708,8.37,0,0,['n_sample '],[None],[' 1000'],858,[],"['test_dataset_petfinder', 'train_test_split2', 'post_process_fun', 'int', 'pre_process_fun', 'len', 'test_helper']",7
source/models/model_vaemdn.py:test_helper,test_helper,function,24,102,74,1314,12.88,1,1,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",932,[],"['init', 'Model', 'log', 'fit', 'predict', 'encode', 'print', 'enumerate', 'decode', 'decoded_array.append', 'save', 'load_model']",12
source/models/model_vaemdn.py:benchmark,benchmark,function,46,175,133,2054,11.74,2,0,"['config', 'dmin', 'dmax']","[None, None, None]","[""''"", '5', '6']",990,[],"['fetch_data', 'train_test_split', 'post_process_fun', 'int', 'pre_process_fun', 'len', 'print', 'log', 'range', 'pd.DataFrame', 'VAEMDN', 'vae.fit', 'vae.predict', 'evaluate', 'AUTOENCODER_BASIC', 'basic_ae.fit', 'basic_ae.predict']",17
source/models/model_vaemdn.py:Model,Model,class,26,70,64,774,11.06,0,2,[],[],[],296,[],[],0
source/models/model_vaemdn.py:Model:__init__,Model:__init__,method,25,65,59,706,10.86,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",297,[],"['model_pars.get', 'VAEMDN', 'AUTOENCODER_BASIC', 'log2']",4
source/models/optuna_lightgbm.py:log,log,function,4,20,11,133,6.65,0,3,['*s'],[None],[None],16,[],"['print', 'log2', 'log3']",3
source/models/optuna_lightgbm.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],19,[],['print'],1
source/models/optuna_lightgbm.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],22,[],['print'],1
source/models/optuna_lightgbm.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],25,[],['os.makedirs'],1
source/models/optuna_lightgbm.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",31,[],['Model'],1
source/models/optuna_lightgbm.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],36,[],[],0
source/models/optuna_lightgbm.py:fit,fit,function,26,81,66,1028,12.69,1,1,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",75,"['    """"""\n', '    """"""\n']","['get_dataset', 'log2', 'compute_pars.get', 'print']",4
source/models/optuna_lightgbm.py:eval,eval,function,0,1,1,4,4.0,0,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",119,"['    """"""\n', '       Return metrics of the model when fitted.\n', '    """"""\n']",[],0
source/models/optuna_lightgbm.py:predict,predict,function,13,29,24,296,10.21,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",126,[],"['get_dataset', 'compute_pars.get']",2
source/models/optuna_lightgbm.py:save,save,function,8,35,24,427,12.2,0,0,"['path', 'info']","[None, None]","['None', 'None']",141,[],"['os.makedirs', 'pickle.dump', 'open']",3
source/models/optuna_lightgbm.py:load_model,load_model,function,18,28,26,293,10.46,0,0,['path'],[None],"['""""']",156,[],"['pickle.load', 'Model']",2
source/models/optuna_lightgbm.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",171,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/optuna_lightgbm.py:get_dataset,get_dataset,function,7,49,33,415,8.47,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",182,"['    """"""\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/optuna_lightgbm.py:test_dataset_classi_fake,test_dataset_classi_fake,function,23,47,43,376,8.0,1,0,['nrows'],[None],['500'],207,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'y.reshape', 'len']",5
source/models/optuna_lightgbm.py:test,test,function,50,296,205,2781,9.4,1,0,['nrows'],[None],['500'],225,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'y.reshape', 'len', 'test', 'test_dataset_classi_fake', 'post_process_fun', 'int', 'pre_process_fun', 'log', 'train_test_split', 'test_helper', 'Model', 'fit', 'predict', 'save']",17
source/models/optuna_lightgbm.py:test_helper,test_helper,function,10,29,27,399,13.76,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",315,[],"['Model', 'log', 'fit', 'predict', 'save']",5
source/models/optuna_lightgbm.py:benchmark,benchmark,function,30,252,164,2571,10.2,1,0,[],[],[],336,[],"['log', 'os.system', 'fetch_data', 'train_test_split', 'benchmark_helper', 'post_process_fun', 'int', 'pre_process_fun', 'test_helper']",9
source/models/optuna_lightgbm.py:benchmark_helper,benchmark_helper,function,16,210,132,1992,9.49,0,0,"['train_df', 'test_df']","[None, None]","[None, None]",356,[],"['post_process_fun', 'int', 'pre_process_fun', 'log', 'test_helper']",5
source/models/optuna_lightgbm.py:Model,Model,class,25,57,49,682,11.96,0,2,[],[],[],48,[],[],0
source/models/optuna_lightgbm.py:Model:__init__,Model:__init__,method,24,52,44,614,11.81,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",49,[],"['globals', 'log2']",2
source/models/torch_ease.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],16,[],"['print', 'log2', 'log3']",3
source/models/torch_ease.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],19,[],['print'],1
source/models/torch_ease.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],22,[],['print'],1
source/models/torch_ease.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],25,[],['os.makedirs'],1
source/models/torch_ease.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",31,[],['Model'],1
source/models/torch_ease.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],36,[],[],0
source/models/torch_ease.py:get_dataset,get_dataset,function,26,122,51,1366,11.2,0,4,"['data_pars', 'task_type']","[None, None]","[None, '""train""']",87,"['    """"""\n', '    :param data_pars:\n', '    :param task_type:\n', '    :return:\n', '    """"""\n']",['utils.load_data'],1
source/models/torch_ease.py:fit,fit,function,11,20,20,180,9.0,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",152,"['    """"""\n', '    """"""\n']",['get_dataset'],1
source/models/torch_ease.py:predict,predict,function,11,18,16,177,9.83,0,1,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",166,[],"['get_dataset', 'next', 'model.model']",3
source/models/torch_ease.py:eval,eval,function,7,24,19,291,12.12,0,0,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, ' dict', ' dict', ' dict', None]","['None', '{}', '{}', '{}', None]",183,[],"['encode', 'log', 'decode']",3
source/models/torch_ease.py:save,save,function,11,32,29,360,11.25,0,0,"['path', 'info']","[None, None]","['None', 'None']",196,"['    """""" Custom saving\n', '    """"""\n']","['os.makedirs', 'pickle.dump', 'open']",3
source/models/torch_ease.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",213,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/torch_ease.py:get_dataset_tuple,get_dataset_tuple,function,12,42,30,330,7.86,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",226,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3) OR single dataframe\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/torch_ease.py:get_dataset2,get_dataset2,function,22,90,61,936,10.4,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",252,"['    """"""  Return tuple of dataframes\n', '    """"""\n']","['data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",4
source/models/torch_ease.py:test_dataset_goodbooks,test_dataset_goodbooks,function,10,16,14,117,7.31,0,0,['nrows'],[None],['1000'],299,[],[],0
source/models/torch_ease.py:train_test_split2,train_test_split2,function,21,39,32,411,10.54,0,0,"['df', 'coly']","[None, None]","[None, None]",310,[],"['log3', 'df.drop', 'np.sum', 'X.head', 'train_test_split', 'len']",6
source/models/torch_ease.py:test,test,function,10,16,14,117,7.31,0,0,['nrows'],[None],['1000'],327,[],[],0
source/models/torch_ease.py:test_helper,test_helper,function,10,31,28,430,13.87,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",393,[],"['Model', 'log', 'fit', 'predict', 'save']",5
source/models/torch_ease.py:Model,Model,class,17,32,31,530,16.56,0,1,[],[],[],67,[],[],0
source/models/torch_ease.py:Model:__init__,Model:__init__,method,16,26,25,445,17.12,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars', 'global_pars']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",68,[],"['copy.deepcopy', 'model_pars2.update', 'TorchEASE', 'log2']",4
source/models/torch_rvae.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],16,[],"['print', 'log2', 'log3']",3
source/models/torch_rvae.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],19,[],['print'],1
source/models/torch_rvae.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],22,[],['print'],1
source/models/torch_rvae.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],25,[],['os.makedirs'],1
source/models/torch_rvae.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",31,[],['Model'],1
source/models/torch_rvae.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],36,[],[],0
source/models/torch_rvae.py:get_dataset,get_dataset,function,26,122,51,1366,11.2,0,4,"['data_pars', 'task_type']","[None, None]","[None, '""train""']",93,"['    """"""\n', '    :param data_pars:\n', '    :param task_type:\n', '    :return:\n', '    """"""\n']",['utils.load_data'],1
source/models/torch_rvae.py:fit,fit,function,16,24,24,253,10.54,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",158,"['    """"""\n', '    """"""\n']",['get_dataset'],1
source/models/torch_rvae.py:encode,encode,function,7,15,13,145,9.67,0,1,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, ' dict', ' dict', ' dict', None]","['None', '{}', '{}', '{}', None]",173,[],['get_dataset'],1
source/models/torch_rvae.py:decode,decode,function,9,30,20,193,6.43,0,3,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, ' dict', ' dict', ' dict', None]","['None', '{}', '{}', '{}', None]",188,"['    """""" Specify the format required   due to sampling\n', '    :param Xpred:\n', '    :param data_pars:\n', '    :param compute_pars:\n', '    :param out_pars:\n', '    :param kw:\n', '    :return:\n', '    """"""\n']","['log', 'type']",2
source/models/torch_rvae.py:compute_metrics,compute_metrics,function,31,429,163,5463,12.73,0,11,"['model', 'X', 'dataset_obj', 'args', 'epoch', 'losses_save', 'logit_pi_prev', 'X_clean', 'target_errors', 'mode']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None]",215,[],"['evaluation_phase', 'repair_phase', 'log', 'p_recon', 'utils.cell_metrics', 'utils.row_metrics', 'loss_ret.update', 'clean_loss_ret.update']",8
source/models/torch_rvae.py:predict,predict,function,11,18,16,177,9.83,0,1,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",338,[],"['get_dataset', 'next', 'model.model']",3
source/models/torch_rvae.py:eval,eval,function,7,24,19,291,12.12,0,0,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, ' dict', ' dict', ' dict', None]","['None', '{}', '{}', '{}', None]",355,[],"['encode', 'log', 'decode']",3
source/models/torch_rvae.py:save,save,function,11,32,29,360,11.25,0,0,"['path', 'info']","[None, None]","['None', 'None']",368,"['    """""" Custom saving\n', '    """"""\n']","['os.makedirs', 'pickle.dump', 'open']",3
source/models/torch_rvae.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",385,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/torch_rvae.py:get_dataset_tuple,get_dataset_tuple,function,12,42,30,330,7.86,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",398,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3) OR single dataframe\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/torch_rvae.py:get_dataset2,get_dataset2,function,22,90,61,936,10.4,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",424,"['    """"""  Return tuple of dataframes\n', '    """"""\n']","['data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",4
source/models/torch_rvae.py:test,test,function,24,238,126,3543,14.89,0,0,['nrows'],[None],['1000'],470,"['    """"""nrows : take first nrows from dataset\n', '    """"""\n']","['test_helper', 'test2', 'print', 'os.system', 'reset', 'log', 'Model', 'fit', 'encode', 'decode', 'predict', 'eval']",12
source/models/torch_rvae.py:test2,test2,function,9,95,73,1370,14.42,0,0,['nrows'],[None],['1000'],518,"['    """"""\n', '    """"""\n']","['print', 'os.system', 'test_helper']",3
source/models/torch_rvae.py:test_helper,test_helper,function,14,63,46,1027,16.3,0,0,['m'],[None],[None],579,[],"['reset', 'log', 'Model', 'fit', 'encode', 'decode', 'predict', 'eval']",8
source/models/torch_rvae.py:test_rvae,test_rvae,function,13,109,78,2236,20.51,0,0,[],[],[],1071,[],"['namedtuple', 'args.keys', 'm.keys', 'log', 'RVAE', 'model.fit', 'model.save']",7
source/models/torch_rvae.py:Model,Model,class,19,35,34,597,17.06,0,1,[],[],[],72,[],[],0
source/models/torch_rvae.py:RVAE,RVAE,class,235,1050,528,13087,12.46,4,31,[],[],[],613,[],[],0
source/models/torch_rvae.py:Model:__init__,Model:__init__,method,18,29,28,512,17.66,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars', 'global_pars']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",73,[],"['copy.deepcopy', 'model_pars2.update', 'namedtuple', 'model_pars2.keys', 'RVAE', 'log2']",6
source/models/torch_rvae.py:RVAE:__init__,RVAE:__init__,method,49,145,100,2278,15.71,0,9,"['self', 'args']","[None, None]","[None, None]",614,[],"['super', 'self._get_dataset_obj', 'len', 'nn.ModuleList', 'nn.Linear', 'nn.Parameter', 'nn.ReLU', 'nn.Hardtanh', 'nn.LogSoftmax', 'nn.Sigmoid', 'nn.ParameterList']",11
source/models/torch_rvae.py:RVAE:_get_dataset_obj,RVAE:_get_dataset_obj,method,15,57,29,611,10.72,0,0,['self'],[None],[None],696,[],['utils.load_data'],1
source/models/torch_rvae.py:RVAE:fit,RVAE:fit,method,21,63,55,803,12.75,1,0,['self'],[None],[None],729,[],"['optim.Adam', 'filter', 'self.parameters', 'torch.tensor', 'range', 'training_phase', 'compute_metrics']",7
source/models/torch_rvae.py:RVAE:save,RVAE:save,method,9,46,37,669,14.54,0,0,['self'],[None],[None],762,[],"['self._save_to_csv', 'self.cpu', 'torch.save', 'open', 'json.dump']",5
source/models/torch_rvae.py:RVAE:_save_to_csv,RVAE:_save_to_csv,method,42,321,125,2922,9.1,0,2,"['self', 'X_data', 'X_data_clean', 'target_errors', 'attributes', 'losses_save', 'dataset_obj', 'path_output', 'args', 'epoch', 'mode']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, ""'train'""]",789,"['        """""" This method performs all operations needed to save the data to csv """"""\n']","['os.makedirs', 'evaluation_phase', 'repair_phase', 'utils.cell_metrics', 'np.zeros', 'pd.DataFrame', 'df_avpr_feat_cell.to_csv', 'df_auc_feat_cell.to_csv', 'p', 'df_out.to_csv', 'df_errors_repair.to_csv']",11
source/models/torch_rvae.py:RVAE:get_inputs,RVAE:get_inputs,method,29,112,72,1668,14.89,1,8,"['self', 'x_data', 'one_hot_categ', 'masking', 'drop_mask', 'in_aux_samples']","[None, None, None, None, None, None]","[None, None, 'False', 'False', '[]', '[]']",855,"['        """"""\n', '            drop_mask: (N,D) defines which entries are to be zeroed-out\n', '        """"""\n']","['torch.ones', 'isinstance', 'enumerate', 'EmbeddingMul', 'func_embedd', 'input_list.append', 'torch.cat']",7
source/models/torch_rvae.py:RVAE:encode,RVAE:encode,method,16,28,27,364,13.0,0,1,"['self', 'x_data', 'one_hot_categ', 'masking', 'drop_mask', 'in_aux_samples']","[None, None, None, None, None, None]","[None, None, 'False', 'False', '[]', '[]']",917,[],"['dict', 'self.get_inputs', 'self.fc1', 'self.activ', 'self.fc21', 'self.fc22', 'self.qw_fc1', 'self.qw_fc2']",8
source/models/torch_rvae.py:RVAE:sample_normal,RVAE:sample_normal,method,8,15,12,180,12.0,0,2,"['self', 'q_params_z', 'eps']","[None, None, None]","[None, None, 'None']",934,[],"['torch.randn_like', 'eps.mul']",2
source/models/torch_rvae.py:RVAE:reparameterize,RVAE:reparameterize,method,4,7,6,93,13.29,0,0,"['self', 'q_params', 'eps_samples']","[None, None, None]","[None, None, 'None']",948,[],"['dict', 'self.sample_normal']",2
source/models/torch_rvae.py:RVAE:decode,RVAE:decode,method,18,42,35,629,14.98,1,3,"['self', 'z']","[None, None]","[None, None]",957,[],"['dict', 'self.activ', 'self.out_cat_linears', 'enumerate', 'out_cat_list.append', 'torch.cat']",6
source/models/torch_rvae.py:RVAE:predict,RVAE:predict,method,2,7,7,104,14.86,0,0,"['self', 'x_data', 'n_epoch', 'one_hot_categ', 'masking', 'drop_mask', 'in_aux_samples']","[None, None, None, None, None, None, None]","[None, None, 'None', 'False', 'False', '[]', '[]']",989,[],['self.forward'],1
source/models/torch_rvae.py:RVAE:forward,RVAE:forward,method,6,12,11,168,14.0,0,0,"['self', 'x_data', 'n_epoch', 'one_hot_categ', 'masking', 'drop_mask', 'in_aux_samples']","[None, None, None, None, None, None, None]","[None, None, 'None', 'False', 'False', '[]', '[]']",992,[],"['self.encode', 'self.reparameterize', 'self.decode']",3
source/models/torch_rvae.py:RVAE:loss_function,RVAE:loss_function,method,38,123,84,1766,14.36,1,6,"['self', 'input_data', 'p_params', 'q_params', 'q_samples', 'clean_comp_only', 'data_eval_clean']","[None, None, None, None, None, None, None]","[None, None, None, None, None, 'False', 'False']",1000,"['        """""" ELBO: reconstruction loss for each variable + KL div losses summed over elements of a batch """"""\n']","['torch.zeros', 'torch.sigmoid', 'torch.ones_like', 'nll_gauss_global', 'enumerate', 'nll_categ_global', 'torch.sum', 'torch.tensor', 'torch.log']",9
source/models/torch_tabular.py:log,log,function,4,16,10,118,7.38,0,2,['*s'],[None],[None],40,[],"['print', 'log2', 'log3']",3
source/models/torch_tabular.py:log2,log2,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],43,[],['print'],1
source/models/torch_tabular.py:log3,log3,function,2,6,6,35,5.83,0,1,['*s'],[None],[None],46,[],['print'],1
source/models/torch_tabular.py:os_makedirs,os_makedirs,function,2,8,8,163,20.38,0,1,['dir_or_file'],[None],[None],49,[],['os.makedirs'],1
source/models/torch_tabular.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",55,[],['Model'],1
source/models/torch_tabular.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],60,[],[],0
source/models/torch_tabular.py:fit,fit,function,16,43,38,444,10.33,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",153,"['    """"""\n', '    """"""\n']","['copy.deepcopy', 'compute_pars.get', 'get_dataset', 'pd.concat']",4
source/models/torch_tabular.py:predict,predict,function,19,38,34,443,11.66,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, ' dict', ' dict', ' dict', None]","['None', '{}', '{}', '{}', None]",174,[],"['get_dataset', 'data_pars.get', 'get_dataset_tuple', 'pd.concat', 'compute_pars.get']",5
source/models/torch_tabular.py:save,save,function,11,32,29,360,11.25,0,0,"['path', 'info']","[None, None]","['None', 'None']",193,"['    """""" Custom saving\n', '    """"""\n']","['os.makedirs', 'pickle.dump', 'open']",3
source/models/torch_tabular.py:load_model,load_model,function,18,31,29,348,11.23,0,0,['path'],[None],"['""""']",209,[],"['pickle.load', 'Model', 'TabularModel.load_from_checkpoint']",3
source/models/torch_tabular.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",227,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/torch_tabular.py:get_dataset_tuple,get_dataset_tuple,function,14,40,32,343,8.57,2,1,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, 'None']",240,"['    """"""  Split into Tuples:  Xtuple = (df1, df2, df3) OR single dataframe  to Feed model\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/torch_tabular.py:get_dataset,get_dataset,function,33,129,93,1299,10.07,2,5,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, 'None']",264,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",7
source/models/torch_tabular.py:test_dataset_covtype,test_dataset_covtype,function,22,94,91,1392,14.81,0,1,['nrows'],[None],['1000'],310,[],"['Path.home', 'BASE_DIR.joinpath', 'datafile.exists', 'wget.download', 'datafile.as_posix', 'pd.read_csv']",6
source/models/torch_tabular.py:train_test_split2,train_test_split2,function,17,35,28,392,11.2,0,0,"['df', 'coly']","[None, None]","[None, None]",337,[],"['log3', 'df.drop', 'np.sum', 'X.head', 'train_test_split', 'len']",6
source/models/torch_tabular.py:test,test,function,22,94,91,1392,14.81,0,1,['nrows'],[None],['1000'],347,"['    """"""\n', '        nrows : take first nrows from dataset\n', '    """"""\n']","['Path.home', 'BASE_DIR.joinpath', 'datafile.exists', 'wget.download', 'datafile.as_posix', 'pd.read_csv']",6
source/models/torch_tabular.py:test3,test3,function,25,274,154,2715,9.91,1,0,['n_sample '],[None],[' 100'],509,"['    """"""\n', '        nrows : take first nrows from dataset\n', '    """"""\n']","['test_dataset_covtype', 'train_test_split2', 'post_process_fun', 'int', 'pre_process_fun', 'log', 'test_helper']",7
source/models/torch_tabular.py:test_helper,test_helper,function,13,62,54,830,13.39,0,1,"['m', 'X_valid']","[None, None]","[None, None]",625,[],"['reset', 'log', 'Model', 'fit', 'predict', 'save', 'load_model']",7
source/models/torch_tabular.py:test2,test2,function,41,143,136,2414,16.88,0,1,['nrows'],[None],['10000'],659,"['    """"""\n', '       python source/models/torch_tabular.py test\n', '\n', '    """"""\n']","['Path.home', 'BASE_DIR.joinpath', 'datafile.exists', 'wget.download', 'datafile.as_posix', 'pd.read_csv', 'df.head', 'train_test_split', 'len', 'DataConfig', 'CategoryEmbeddingModelConfig', 'TrainerConfig', 'ExperimentConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'log', 'test.drop', 'tabular_model.predict']",20
source/models/torch_tabular.py:get_dataset2,get_dataset2,function,7,49,33,415,8.47,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",750,"['    """"""\n', '      ""ram""  :\n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/torch_tabular.py:Model,Model,class,36,135,114,1468,10.87,1,2,[],[],[],100,[],[],0
source/models/torch_tabular.py:Model:__init__,Model:__init__,method,35,130,109,1400,10.77,1,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",101,[],"['DataConfig', 'model_pars.get', 'MixtureDensityHeadConfig', 'model_class', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'log']",8
source/models/util_models.py:log,log,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],13,[],['print'],1
source/models/util_models.py:test_dataset_classifier_covtype,test_dataset_classifier_covtype,function,25,60,57,818,13.63,0,1,['nrows'],[None],['500'],19,[],"['log', 'Path.home', 'BASE_DIR.joinpath', 'datafile.exists', 'wget.download', 'datafile.as_posix', 'pd.read_csv']",7
source/models/util_models.py:test_dataset_regress_fake,test_dataset_regress_fake,function,22,46,42,355,7.72,1,0,['nrows'],[None],['500'],55,[],"['range', 'sklearn_datasets.make_regression', 'pd.DataFrame', 'y.reshape', 'len']",5
source/models/util_models.py:test_dataset_classi_fake,test_dataset_classi_fake,function,23,48,44,374,7.79,1,0,['nrows'],[None],['500'],72,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'y.reshape', 'len']",5
source/models/util_models.py:test_dataset_petfinder,test_dataset_petfinder,function,22,52,48,649,12.48,0,0,['nrows'],[None],['1000'],89,[],"['print', 'pd.read_csv', 'np.where', 'df.drop']",4
source/models/util_models.py:tf_data_create_sparse,tf_data_create_sparse,function,37,150,84,1555,10.37,6,5,"['cols_type_received', ""'col2']"", ""'cols_num'    "", ""'colb']}"", 'cols_ref', ""'col_num'  ]"", 'Xtrain', '**kw']","['dict', None, "" ['cola'"", None, 'list', None, 'pd.DataFrame', None]","["" {'cols_sparse' : ['col1'"", None, None, None, ""  [ 'col_sparse'"", None, 'None', None]",122,"['    """"""\n', '\n', '       Create sparse data struccture in KERAS  To plug with MODEL:\n', '       No data, just virtual data\n', '    https://github.com/GoogleCloudPlatform/data-science-on-gcp/blob/master/09_cloudml/flights_model_tf2.ipynb\n', '\n', '    :return:\n', '    """"""\n']","['int', 'min', 'col_unique.get', 'categorical_column_with_hash_bucket', 'numeric_column', 'crossed_column', 'np.linspace', 'bucketized_column', 'indicator_column', 'dict_cat_sparse.items', 'embedding_column']",11
source/models/util_models.py:tf_data_pandas_to_dataset,tf_data_pandas_to_dataset,function,9,25,21,326,13.04,1,0,"['training_df', 'colsX', 'coly']","[None, None, None]","[None, None, None]",192,[],"['print', 'tf.cast']",2
source/models/util_models.py:tf_data_file_to_dataset,tf_data_file_to_dataset,function,29,72,57,958,13.31,0,2,"['pattern', 'batch_size', 'mode', 'truncate']","[None, None, None, None]","[None, None, 'tf.estimator.ModeKeys.TRAIN', 'None']",212,"['    """"""  ACTUAL Data reading :\n', '           Dataframe ---> TF Dataset  --> feed Keras model\n', '\n', '    """"""\n']","['load_dataset', 'features_and_labels', 'features.pop', 'dataset.map', 'dataset.shuffle', 'dataset.repeat', 'dataset.prefetch', 'dataset.take']",8
source/utils/util.py:os_make_dirs,os_make_dirs,function,8,26,22,224,8.62,1,3,['filename'],[None],[None],40,[],"['isinstance', 'os.makedirs', 'print']",3
source/utils/util.py:save_all,save_all,function,5,16,16,144,9.0,1,0,"['variable_list', 'folder', 'globals_main']","[None, None, None]","[None, None, 'None']",56,"['    """""" Pickle saving batch\n', '    :param variable_list:\n', '    :param folder:\n', '    :param globals_main:\n', '    :return:\n', '    """"""\n']","['save', 'print']",2
source/utils/util.py:save,save,function,11,38,31,360,9.47,1,0,"['variable_list', 'folder', 'globals_main']","[None, None, None]","[None, None, 'None']",71,[],"['save', 'print', 'os_make_dirs', 'open', 'pickle.dump']",5
source/utils/util.py:load,load,function,7,16,15,120,7.5,0,0,"['filename', 'isabsolutpath', 'encoding1']","[None, None, None]","['""/folder1/keyname""', '0', '""utf-8""']",88,"['    """""" pickle load\n', '    :param filename:\n', '    :param isabsolutpath:\n', '    :param encoding1:\n', '    :return:\n', '    """"""\n']","['os_make_dirs', 'open', 'pickle.load', 'print']",4
source/utils/util.py:create_appid,create_appid,function,4,6,5,47,7.83,0,0,['filename'],[None],[None],104,[],['str'],1
source/utils/util.py:create_logfilename,create_logfilename,function,2,3,3,50,16.67,0,0,['filename'],[None],[None],110,[],['filename.split'],1
source/utils/util.py:create_uniqueid,create_uniqueid,function,2,4,4,87,21.75,0,0,[],[],[],114,[],['str'],1
source/utils/util.py:logger_setup,logger_setup,function,9,42,34,409,9.74,0,3,"['logger_name', 'log_file', 'formatter', 'isrotate', 'isconsole_output', 'logging_level', '']","[None, None, None, None, None, None, None]","['None', 'None', 'FORMATTER_1', 'False', 'True', 'logging.DEBUG', None]",120,"['    """"""\n', '    my_logger = util_log.logger_setup(""my module name"", log_file="""")\n', '    APP_ID    = util_log.create_appid(__file__ )\n', '    def log(*argv):\n', '      my_logger.info("","".join([str(x) for x in argv]))\n', '  \n', '   """"""\n']","['logging.getLogger', 'logger.setLevel', 'logger.addHandler', 'logger_handler_file']",4
source/utils/util.py:logger_handler_console,logger_handler_console,function,6,13,11,163,12.54,0,1,['formatter'],[None],['None'],156,[],"['logging.StreamHandler', 'console_handler.setFormatter']",2
source/utils/util.py:logger_handler_file,logger_handler_file,function,11,33,22,331,10.03,0,3,"['isrotate', 'rotate_time', 'formatter', 'log_file_used']","[None, None, None, None]","['False', '""midnight""', 'None', 'None']",163,[],"['print', 'TimedRotatingFileHandler', 'fh.setFormatter', 'logging.FileHandler']",4
source/utils/util.py:logger_setup2,logger_setup2,function,13,14,13,215,15.36,0,0,"['name', 'level']","[None, None]","['__name__', 'None']",177,[],"['logging.getLogger', 'logger.setLevel', 'logging.StreamHandler', 'ch.setLevel', 'logging.Formatter', 'ch.setFormatter', 'logger.addHandler']",7
source/utils/util.py:printlog,printlog,function,9,44,38,396,9.0,0,3,"['s', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 'app_id', 'logfile', 'iswritelog', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","['""""', '""""', '""""', '""""', '""""', '""""', '""""', '""""', '""""', '""""', '""""', '""""', 'None', 'True', None]",193,[],"['str', 'print', 'writelog']",3
source/utils/util.py:writelog,writelog,function,5,15,14,66,4.4,0,1,"['m', 'f']","[None, None]","['""""', 'None']",240,[],"['open', '_log.write']",2
source/utils/util.py:load_arguments,load_arguments,function,32,78,64,756,9.69,2,2,"['config_file', 'arg_list']","[None, None]","['None', 'None']",247,"['    """"""\n', '      Load CLI input, load config.toml , overwrite config.toml by CLI Input\n', '      [{}, {}]\n', '    """"""\n']","['argparse.ArgumentParser', 'p.add_argument', 'p.parse_args', 'to_name', '__init__', 'print', 'toml.load', 'vars']",8
source/utils/util.py:sk_tree_get_ifthen,sk_tree_get_ifthen,function,28,94,67,781,8.31,2,3,"['tree', 'feature_names', 'target_names', 'spacer_base']","[None, None, None, None]","[None, None, None, '"" ""']",295,"['    """"""Produce psuedo-code for decision tree.\n', '    tree -- scikit-leant DescisionTree.\n', '    feature_names -- list of feature names.\n', '    target_names -- list of target (output) names.\n', '    spacer_base -- used for spacing code (default: ""    "").\n', '    """"""\n']","['recurse', 'print', 'str', 'zip', 'int']",5
source/utils/util_autofeature.py:create_model_name,create_model_name,function,0,1,1,4,4.0,0,0,"['save_folder', 'model_name']","[None, None]","[None, None]",56,[],[],0
source/utils/util_autofeature.py:optim_,optim_,function,5,32,26,297,9.28,0,2,"['modelname', 'pars', 'df ', 'optim_engine', 'optim_method', 'save_folder', 'log_folder', 'ntrials']","[None, None, None, None, None, None, None, None]","['""model_dl.1_lstm.py""', ' {}', ' None', '""optuna""', '""normal/prune""', '""model_save/""', '""logs/""', '2']",63,[],"['print', 'optim_optuna']",2
source/utils/util_autofeature.py:optim_optuna,optim_optuna,function,44,130,99,1730,13.31,1,3,"['modelname', 'pars', 'df ', 'optim_method', 'save_folder', 'log_folder', 'ntrials']","[None, None, None, None, None, None, None]","['""model_dl.1_lstm.py""', ' {}', ' None', '""normal/prune""', '""/mymodel/""', '""""', '2']",81,"['    """"""\n', '       Interface layer to Optuna  for hyperparameter optimization\n', '       return Best Parameters \n', '\n', ""    weight_decay = trial.suggest_loguniform('weight_decay', 1e-10, 1e-3)\n"", ""    optimizer = trial.suggest_categorical('optimizer', ['MomentumSGD', 'Adam']) # Categorical parameter\n"", ""    num_layers = trial.suggest_int('num_layers', 1, 3)      # Int parameter\n"", ""    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 1.0)      # Uniform parameter\n"", ""    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)      # Loguniform parameter\n"", ""    drop_path_rate = trial.suggest_discrete_uniform('drop_path_rate', 0.0, 1.0, 0.1) # Discrete-uniform parameter\n"", '    """"""\n']","['module_load', 'objective', 'module.get_params', 'pars.items', 'trial.suggest_loguniform', 'trial.suggest_int', 'trial.suggest_categorical', 'trial.suggest_discrete_uniform', 'trial.suggest_uniform', 'Exception', 'module.Model', 'module.fit', 'tf.reset_default_graph', 'optuna.create_study', 'study.optimize', 'param_dict.update', 'modelname.replace', 'not', 'os.makedirs', 'save', 'study.trials_dataframe', 'study_trials.to_csv', 'json.dump']",23
source/utils/util_autofeature.py:load_arguments,load_arguments,function,12,102,79,1102,10.8,0,1,['config_file'],[None],[' None'],183,"['    """"""\n', '        Load CLI input, load config.toml , overwrite config.toml by CLI Input\n', '    """"""\n']","['print', 'argparse.ArgumentParser', 'p.add_argument', 'p.parse_args', 'load_config']",5
source/utils/util_autofeature.py:data_loader,data_loader,function,10,16,13,229,14.31,0,0,['file_name'],[None],"[""'dataset/GOOG-year.csv'""]",210,[],"['pd.read_csv', 'pd.to_datetime', 'MinMaxScaler', 'minmax.transform', 'pd.DataFrame']",5
source/utils/util_autofeature.py:test_all,test_all,function,6,55,38,369,6.71,0,0,[],[],[],223,[],"['data_loader', 'optim', 'print']",3
source/utils/util_autofeature.py:test_fast,test_fast,function,6,61,44,481,7.89,0,0,[],[],[],236,[],"['data_loader', 'optim', 'print']",3
source/utils/util_automl.py:import_,import_,function,7,23,22,243,10.57,0,1,"['abs_module_path', 'class_name']","[None, None]","[None, 'None']",30,[],"['import_module', 'print', 'getattr']",3
source/utils/util_automl.py:model_auto_tpot,model_auto_tpot,function,24,49,42,606,12.37,0,1,"['df', 'colX', 'coly', 'outfolder', 'model_type', 'train_size', 'generation', 'population_size', 'verbosity', '']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, '""aaserialize/""', '""regressor/classifier""', '0.5', '1', '5', '2', None]",66,"['    """""" Automatic training of Xmat--->Y, Generate SKlearn code in outfile\n', '      Very Slow Process, use lower number of Sample\n', '  :param Xmat:\n', '  :param y:\n', '  :param outfolder:\n', '  :param model_type:\n', '  :param train_size:\n', '  :param generation:\n', '  :param population_size:\n', '  :param verbosity:\n', '  :return:\n', '    """"""\n']","['import_', 'train_test_split', 'tpot.TPOTRegressor', 'tpot.TPOTClassifier', 'print', 'clf.fit', 'tpot.score', 'str', 'tpot.export']",9
source/utils/util_automl.py:model_auto_mlbox,model_auto_mlbox,function,26,60,54,623,10.38,0,1,"['filepath= [ ""train.csv"", ""test.csv"" ],colX=None, coly=None,do=""predict"",outfolder=""aaserialize/"",model_type=""regressor/classifier"",params={ ""csv_seprator"" ']",[''],"[' [ ""train.csv"", ""test.csv"" ],colX=None, coly=None,do=""predict"",outfolder=""aaserialize/"",model_type=""regressor/classifier"",params={ ""csv_seprator"" : "","", ""train_size"" : 0.5, ""score_metric"" : ""accuracy"",""n_folds"": 3, ""n_step"": 10},param_space =  {\'est__strategy\':{""search"":""choice"",                         ""space"":[""LightGBM""]},\'est__n_estimators\':{""search"":""choice"",                     ""space"":[150]},\'est__colsample_bytree\':{""search"":""uniform"",                ""space"":[0.8,0.95]},\'est__subsample\':{""search"":""uniform"",                       ""space"":[0.8,0.95]},\'est__max_depth\':{""search"":""choice"",                        ""space"":[5,6,7,8,9]},\'est__learning_rate\':{""search"":""choice"",                    ""space"":[0.07]}},generation=1,population_size=5,verbosity=2,']",119,"['    """"""\n', '      Using mlbox\n', '      https://www.analyticsvidhya.com/blog/2017/07/mlbox-library-automated-machine-learning/\n', '\n', '\n', '    Parameters\n', '    ----------\n', '    df : TYPE\n', '        DESCRIPTION.\n', '    colX : TYPE\n', '        DESCRIPTION.\n', '    coly : TYPE\n', '        DESCRIPTION.\n', '    outfolder : TYPE, optional\n', '        DESCRIPTION. The default is ""aaserialize/"".\n', '    model_type : TYPE, optional\n', '        DESCRIPTION. The default is ""regressor/classifier"".\n', '    params : TYPE, optional\n', '        DESCRIPTION. The default is {""train_size"" : 0.5}.\n', '    generation : TYPE, optional\n', '        DESCRIPTION. The default is 1.\n', '    population_size : TYPE, optional\n', '        DESCRIPTION. The default is 5.\n', '    verbosity : TYPE, optional\n', '        DESCRIPTION. The default is 2.\n', '\n', '    Returns\n', '    -------\n', '    None.\n', '\n', '    """"""\n']","['dict2', 'Reader', 'rd.train_test_split', 'Drift_thresholder', 'dft.fit_transform', 'Optimiser', 'opt.optimise', 'Predictor', 'clf.fit_predict', 'pd.read_csv', 'print', 'preds.head']",12
source/utils/util_automl.py:model_auto_automlgs,model_auto_automlgs,function,6,16,16,191,11.94,0,0,"['filepath= [ ""train.csv"", ""test.csv"" ],colX=None, coly=None,do=""predict"",outfolder=""aaserialize/"",model_type=""regressor/classifier"",params={ ""csv_seprator"" ']",[''],"[' [ ""train.csv"", ""test.csv"" ],colX=None, coly=None,do=""predict"",outfolder=""aaserialize/"",model_type=""regressor/classifier"",params={ ""csv_seprator"" : "","", ""train_size"" : 0.5, ""score_metric"" : ""accuracy"",""n_folds"": 3, ""n_step"": 10},param_space =  {\'est__strategy\':{""search"":""choice"",                         ""space"":[""LightGBM""]},\'est__n_estimators\':{""search"":""choice"",                     ""space"":[150]},\'est__colsample_bytree\':{""search"":""uniform"",                ""space"":[0.8,0.95]},\'est__subsample\':{""search"":""uniform"",                       ""space"":[0.8,0.95]},\'est__max_depth\':{""search"":""choice"",                        ""space"":[5,6,7,8,9]},\'est__learning_rate\':{""search"":""choice"",                    ""space"":[0.07]}},generation=1,population_size=5,verbosity=2,']",223,"['    """"""\n', '     https://github.com/minimaxir/automl-gs\n', '\n', '     https://github.com/minimaxir/automl-gs\n', '\n', '     automl_gs titanic.csv Survived --framework xgboost --num_trials 1000\n', '\n', '\n', 'csv_path: Path to the CSV file (must be in the current directory) [Required]\n', 'target_field: Target field to predict [Required]\n', 'target_metric: Target metric to optimize [Default: Automatically determined depending on problem type]\n', ""framework: Machine learning framework to use [Default: 'tensorflow']\n"", ""config_name: Name of the model (if you want to train models with different names) [Default: 'automl']\n"", 'num_trials: Number of trials / different hyperparameter combos to test. [Default: 100]\n', 'split: Train-validation split when training the models [Default: 0.7]\n', 'num_epochs: Number of epochs / passes through the data when training the models. [Default: 20]\n', ""col_types: Dictionary of fields:data types to use to override automl-gs's guesses. (only when using in Python) [Default: {}]\n"", 'gpu: For non-Tensorflow frameworks and Pascal-or-later GPUs, boolean to determine whether to use GPU-optimized training methods (TensorFlow can detect it automatically) [Default: False]\n', 'tpu_address: For TensorFlow, hardware address of the TPU on the system. [Default: None]   \n', '\n', '   The output of the automl-gs training is:\n', '\n', 'A timestamped folder (e.g. automl_tensorflow_20190317_020434) with contains:\n', 'model.py: The generated model file.\n', 'pipeline.py: The generated pipeline file.\n', 'requirements.txt: The generated requirements file.\n', '/encoders: A folder containing JSON-serialized encoder files\n', '/metadata: A folder containing training statistics + other cool stuff not yet implemented!\n', 'The model itself (format depends on framework)\n', 'automl_results.csv: A CSV containing the training results after each epoch and the hyperparameters used to train at that time.\n', 'Once the training is done, you can run the generated files from the command line within the generated folder above.\n', '\n', '\n', '    Parameters\n', '    ----------\n', '    filepath : TYPE, optional\n', '        DESCRIPTION. The default is [ ""train.csv"", ""test.csv"" ].\n', '    colX : TYPE, optional\n', '        DESCRIPTION. The default is None.\n', '    coly : TYPE, optional\n', '        DESCRIPTION. The default is None.\n', '    do : TYPE, optional\n', '        DESCRIPTION. The default is ""predict"".\n', '    outfolder : TYPE, optional\n', '        DESCRIPTION. The default is ""aaserialize/"".\n', '    model_type : TYPE, optional\n', '        DESCRIPTION. The default is ""regressor/classifier"".\n', '    params : TYPE, optional\n', '        DESCRIPTION. The default is { ""csv_seprator"" : "","", ""train_size"" : 0.5, ""score_metric"" : ""accuracy"",             ""n_folds"": 3, ""n_step"": 10}.\n', '    param_space : TYPE, optional\n', '        DESCRIPTION. The default is {        \'est__strategy\':{""search"":""choice"",                         ""space"":[""LightGBM""]},        \'est__n_estimators\':{""search"":""choice"",                     ""space"":[150]},        \'est__colsample_bytree\':{""search"":""uniform"",                ""space"":[0.8,0.95]},        \'est__subsample\':{""search"":""uniform"",                       ""space"":[0.8,0.95]},        \'est__max_depth\':{""search"":""choice"",                        ""space"":[5,6,7,8,9]},        \'est__learning_rate\':{""search"":""choice"",                    ""space"":[0.07]}    }.\n', '    generation : TYPE, optional\n', '        DESCRIPTION. The default is 1.\n', '    population_size : TYPE, optional\n', '        DESCRIPTION. The default is 5.\n', '    verbosity : TYPE, optional\n', '        DESCRIPTION. The default is 2.\n', '\n', '    Returns\n', '    -------\n', '    None.\n', '\n', '    """"""\n']","['dict2', 'automl_grid_search']",2
source/utils/util_automl.py:dict2,dict2,class,3,5,5,36,7.2,0,0,[],[],[],57,[],[],0
source/utils/util_automl.py:dict2:__init__,dict2:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",58,[],[],0
source/utils/util_credit.py:ztest,ztest,function,4,5,5,27,5.4,0,0,[],[],[],24,[],['print'],1
source/utils/util_credit.py:pd_num_segment_limit,pd_num_segment_limit,function,25,102,68,725,7.11,1,4,"['df', 'col_score', 'coldefault', 'ntotal_default', 'def_list', 'nblock']","[None, None, None, None, None, None]","[None, '""scoress""', '""y""', '491', 'None', '20.0']",30,"['    """"""\n', '    Calculate Segmentation of colum using rule based.\n', '    :param df:\n', '    :param col_score:\n', '    :param coldefault:\n', '    :param ntotal_default:\n', '    :param def_list:\n', '    :param nblock:\n', '    :return:\n', '    """"""\n']","['np.ones', 'np.floor', 'df.groupby', 'dfs5.sort_values', 'dfs5.iterrows', 'l2.append', 'pd.DataFrame']",7
source/utils/util_credit.py:fun_get_segmentlimit,fun_get_segmentlimit,function,5,14,11,55,3.93,1,1,"['x', 'l1']","[None, None]","[None, None]",77,"['    """"""\n', '    ##### Get Kaiso limit ###############################################################\n', '    :param x:\n', '    :param l1:\n', '    :return :\n', '    """"""\n']","['range', 'len']",2
source/utils/util_credit.py:np_drop_duplicates,np_drop_duplicates,function,3,11,10,64,5.82,0,0,['l1'],[None],[None],94,"['    """"""\n', '    :param l1:\n', '    :return :\n', '    """"""\n']","['np.array', 'list']",2
source/utils/util_credit.py:model_logistic_score,model_logistic_score,function,16,47,41,479,10.19,0,2,"['clf', 'df1', 'cols', 'coltarget', 'outype']","[None, None, None, None, None]","[None, None, None, None, '""score""']",103,"['    """"""\n', '\n', '    :param clf:\n', '    :param df1:\n', '    :param cols:\n', '    :param outype:\n', '    :return:\n', '    """"""\n']","['score_calc', 'np.log', 'clf.predict_proba', 'clf.predict', 'sk_showmetrics', 'np.min']",6
source/utils/util_credit.py:split_train_test,split_train_test,function,7,28,21,277,9.89,0,0,"['X', 'y', 'split_ratio']","[None, None, None]","[None, None, '0.8']",143,[],"['train_test_split', 'print']",2
source/utils/util_credit.py:split_train,split_train,function,28,171,83,1743,10.19,0,0,"['X', 'y', 'split_ratio']","[None, None, None]","[None, None, '0.8']",156,[],"['train_test_split', 'print', 'split_train', 'len', 'pd.concat', 'sum', 'split_train2', 'int']",8
source/utils/util_credit.py:split_train2,split_train2,function,21,66,46,667,10.11,0,0,"['df1', 'ntrain', 'ntest', 'colused', 'coltarget', 'nratio']","[None, None, None, None, None, None]","[None, '10000', '100000', 'None', 'None', '0.4']",185,[],"['len', 'int', 'pd.concat', 'print', 'sum']",5
source/utils/util_csv.py:xl_setstyle,xl_setstyle,function,33,73,48,697,9.55,2,0,['file1'],[None],[None],34,"['    """"""\n', '   http://openpyxl.readthedocs.io/en/default/styles.html#cell-styles-and-named-styles\n', '  import openpyxl.styles.builtins  as bi\n', '  import openpyxl.styles.builtins\n', '\n', ""  col = ws.column_dimensions['A']\n"", '  col.font = Font(bold=True)\n', '\n', ""  for cell in ws['A'] + ws[1]:\n"", ""    cell.style = 'data01'\n"", '\n', '  bd = Side(style=\'thick\', color=""000000"")\n', '  highlight.border = BORDER_NONE\n', '  from openpyxl.styles import\n', ' """"""\n']","['load_workbook', 'print', 'named_styles.NamedStyle', 'Font', 'wb.get_sheet_names', 'wb.get_sheet_by_name', 'range', 'wb.save']",8
source/utils/util_csv.py:xl_val,xl_val,function,7,13,12,108,8.31,0,0,"['ws', 'colj', 'rowi']","[None, None, None]","[None, None, None]",82,[],"['ws[gcol', 'str']",2
source/utils/util_csv.py:xl_get_rowcol,xl_get_rowcol,function,18,53,46,270,5.09,1,1,"['ws', 'i0', 'j0', 'imax', 'jmax']","[None, None, None, None, None]","[None, None, None, None, None]",91,[],"['range', 'isnull', 'xl_val', 'rmat.append']",4
source/utils/util_csv.py:csv_dtypes_getdict,csv_dtypes_getdict,function,5,17,12,123,7.24,0,1,"['df', 'csvfile']","[None, None]","['None', 'None']",195,[],['pd.read_csv'],1
source/utils/util_csv.py:csv_fast_processing,csv_fast_processing,function,0,0,0,0,0.0,0,0,[],[],[],203,"['    """"""\n', '   http://word.bitly.com/post/74069870671/optimizing-text-processing\n', '\n', 'import sys\n', 'from collections import defaultdict\n', 'OUT_FILES = defaultdict(dict)\n', '\n', 'open_outfiles()  # open all files I could possibly need\n', '\n', 'for line in sys.stdin:\n', '    # 1. parse line for account_id and metric_type\n', ""    key = line.split(',')\n"", '    account_id = key[ACCOUNT_ID_INDEX][1:] # strip leading quote\n', '\n', '    # 2. write to appropriate file for account_id and metric_type\n', '    OUT_FILES[account_id][key[METRIC_TYPE_INDEX]].write(line)\n', '\n', '   close_outfiles()  # close all the files we opened\n', '\n', '   """"""\n']",[],0
source/utils/util_csv.py:csv_col_schema_toexcel,csv_col_schema_toexcel,function,73,318,202,2643,8.31,4,8,"['dircsv', 'filepattern', 'outfile', 'returntable', 'maxrow', 'maxcol_pertable', 'maxstrlen', '']","[None, None, None, None, None, None, None, None]","['""""', '""*.csv""', '"".xlsx""', '1', '5000000', '90', '""U80""', None]",226,"['    """"""Take All csv in a folder and provide Table, Column Schema, type\n', ' str(df[col].dtype)  USE str always, otherwise BIG Issue\n', '\n', 'METHOD FOR Unicode / ASCII issue\n', ""1. Decode early:  Decode to <type 'unicode'> ASAP\n"", ""    df['PREF_NAME']=       df['PREF_NAME'].apply(to_unicode)\n"", '2. Unicode everywhere\n', ""3. Encode late :f = open('/tmp/ivan_out.txt','w')\n"", ""                f.write(ivan_uni.encode('utf-8'))\n"", ' """"""\n']","['util.os_file_listall', 'len', 'np.array', 'enumerate', 'print', 'pd.read_csv', 'str', 'util.pd_toexcel', 'df.apply', 'np.zeros', 'range', 'float', 'gc.collect', 'pd.DataFrame', 'util.save', 'outfile.replace']",16
source/utils/util_csv.py:csv_col_get_dict_categoryfreq,csv_col_get_dict_categoryfreq,function,35,84,61,555,6.61,4,4,"['dircsv', 'filepattern', 'category_cols', 'maxline', 'fileencoding']","[None, None, None, None, None]","[None, '""*.csv""', '[]', '-1', '""utf-8""']",355,"['    """""" Find Category Freq in large CSV Transaction Column   """"""\n']","['datetime.now', 'defaultdict', 'util.os_file_listall', 'enumerate', 'line.split', 'print']",6
source/utils/util_csv.py:csv_row_reduce_line,csv_row_reduce_line,function,18,78,59,479,6.14,1,5,"['fromfile', 'tofile', 'condfilter', 'catval_tokeep', 'header', 'maxline']","[None, None, None, None, None, None]","[None, None, None, None, 'True', '-1']",386,"['    """""" Reduce Data Row by filtering on some Category\n', '    file_category=  in1+ ""offers.csv""\n', '    ncol= 8\n', '    catval_tokeep=[ {} for i in xrange(0, ncol)]\n', '    for i, line in enumerate(open(file_category)):\n', '      ll=  line.split("","")\n', '      catval_tokeep[3][  ll[1] ]  = 1  # Offer_file_col1 --> Transact_file_col_4\n', '      catval_tokeep[4][  ll[3] ] =  1  # Offer_file_col3 --> Transact_file_col_4\n', '\n', '  def condfilter(colk, catval_tokeep) :\n', '    if colk[3] in catval_tokeep[3] or colk[4] in catval_tokeep[4]: return True\n', '    else: return False\n', '  """"""\n']","['datetime.now', 'open', 'next', 'outfile.write', 'enumerate', 'line.split', 'condfilter', 'print']",8
source/utils/util_csv.py:csv_analysis,csv_analysis,function,0,0,0,0,0.0,0,0,[],[],[],430,"['    """"""\n', '   https://csvkit.readthedocs.io/en/540/tutorial/1_getting_started.html\n', '\n', '   sudo pip install csvkit\n', '\n', '   :return:\n', '   """"""\n']",[],0
source/utils/util_csv.py:csv_row_reduce_line_manual,csv_row_reduce_line_manual,function,17,75,54,505,6.73,2,3,"['file_category', 'file_transact', 'file_reduced']","[None, None, None]","[None, None, None]",440,"['    """""" Reduce Data by filtering on some Category """"""\n']","['datetime.now', 'enumerate', 'line.split', 'open', 'outfile.write', 'print']",6
source/utils/util_csv.py:csv_row_mapreduce,csv_row_mapreduce,function,16,33,29,282,8.55,2,0,"['dircsv', 'outfile', 'type_mapreduce', 'nrow', 'chunk']","[None, None, None, None, None]","['""""', '""""', '""sum""', '1000000', '5000000']",472,"['    """"""Take All csv in a folder and provide Table, Column Schema""""""\n']","['util.os_file_listall', 'int', 'pd.DataFrame', 'enumerate', 'range', 'pd.read_csv']",6
source/utils/util_csv.py:csv_pivotable,csv_pivotable,function,12,54,37,587,10.87,1,4,"['dircsv', 'filepattern', 'fileh5', 'leftX', 'topY', 'centerZ', 'mapreduce', 'chunksize', 'tablename', '']","[None, None, None, None, None, None, None, None, None, None]","['""""', '""*.csv""', '"".h5""', '""col0""', '""col2""', '""coli""', '""sum""', '500000', '""df""', None]",488,"['    """""" return df Pivot Table from series of csv file (transfer to d5 temporary)\n', '\n', 'Edit: you can groupby/sum from the store iteratively since this ""map-reduces"" over the chunks:\n', '\n', 'reduce(lambda x, y: x.add(y, fill_value=0),\n', ""       (df.groupby().sum() for df in store.select('df', chunksize=50000)))\n"", '\n', ' """"""\n']","['util.pd_h5_fromcsv_tohdfs', 'pd.HDFStore', 'store.select', 'pd.concat']",4
source/utils/util_csv.py:csv_bigcompute,csv_bigcompute,function,0,1,1,4,4.0,0,0,[],[],[],537,[],[],0
source/utils/util_csv.py:db_getdata,db_getdata,function,0,1,1,4,4.0,0,0,[],[],[],542,[],[],0
source/utils/util_csv.py:db_sql,db_sql,function,0,1,1,4,4.0,0,0,[],[],[],546,[],[],0
source/utils/util_csv.py:db_meta_add,db_meta_add,function,24,73,56,765,10.48,1,4,"['""""', '[])', 'schema', 'df_table_uri', 'df_table_columns']","[None, None, None, None, None]","[None, None, 'None', 'None', 'None']",550,"['    """""" Create Meta database to store infos on the tables : csv, zip, HFS, Postgres\n', ""ALL_DB['japancoupon']= {}\n"", ""ALL_DB['japancoupon']['schema']=    df_schema\n"", ""ALL_DB['japancoupon']['df_table_uri']= df_schema_dictionnary\n"", ""ALL_DB['japancoupon']['df_table_columns']= df_schema_dict\n"", '        DBname, db_schema, db_table_uri, db_table_columns(dict_table->colum_list),\n', '   """"""\n']","['pd_df_todict', 'df.drop_duplicates', 'range', 'dict0.setdefault']",4
source/utils/util_csv.py:db_meta_find,db_meta_find,function,20,88,65,626,7.11,3,4,"['ALLDB', 'query', 'filter_db', 'filter_table', 'filter_column']","[None, None, None, None, None]","[None, '""""', '[]', '[]', '[]']",594,"['    """""" Find string in all the meta table name, column\n', ""  db_meta_find(ALLDB, query='bottler', filter_db=['cokeon'],   filter_table=['table'], filter_column=['table'] )\n"", '  dbname: should be exact name\n', '  fitler_table: partial match is ok\n', '  fitler_column : partial name is ok\n', '  return   (dbname, meta_table_name,  meta_table_filtered_by_row_containing query)\n', '  """"""\n']","['list', 'len', 'isinstance', 'util.str_match_fuzzy', 'util.find_fuzzy', 'util.pd_find', 'rs.append']",7
source/utils/util_csv.py:str_to_unicode,str_to_unicode,function,4,12,10,63,5.25,0,1,"['x', 'encoding']","[None, None]","[None, '""utf-8""']",635,[],"['isinstance', 'str']",2
source/utils/util_csv.py:isnull,isnull,function,3,7,6,20,2.86,0,0,['x'],[None],[None],643,[],[],0
source/utils/util_date.py:pd_datestring_split,pd_datestring_split,function,13,59,47,515,8.73,0,3,"['dfref', 'coldate', 'fmt=""%Y-%m-%d %H', 'return_val']","[None, None, '', None]","[None, None, '""%Y-%m-%d %H:%M:%S""', '""split""']",30,"['    """"""\n', '      Parsing date\n', ""      'Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'\n"", '    :param datelist:\n', '    :param fmt:\n', '    :return:\n', '    """"""\n']","['isinstance', 'Exception', 'pd.DataFrame', 'pd.to_datetime']",4
source/utils/util_date.py:datestring_todatetime,datestring_todatetime,function,9,39,26,343,8.79,1,3,"['datelist', 'fmt=""%Y-%m-%d %H']","[None, '']","[None, '""%Y-%m-%d %H:%M:%S""']",59,"['    """"""\n', '      Parsing date\n', ""      'Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p'\n"", '    :param datelist:\n', '    :param fmt:\n', '    :return:\n', '    """"""\n']","['isinstance', 'datenew.append', 'datetime.strptime']",3
source/utils/util_date.py:datetime_tostring,datetime_tostring,function,4,16,14,125,7.81,0,1,"['datelist', 'fmt=""%Y-%m-%d %H']","[None, '']","[None, '""%Y-%m-%d %H:%M:%S""']",86,"['    """"""\n', '  https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n', '  :param x:\n', '  :param fmt:\n', '  :return:\n', '  """"""\n']","['isinstance', 'datetime.strftime']",2
source/utils/util_date.py:datetime_tointhour,datetime_tointhour,function,8,51,28,311,6.1,1,1,['datelist'],[None],[None],100,[],"['isinstance', 'yy2.append', 'np.array']",3
source/utils/util_date.py:datetime_toint,datetime_toint,function,12,77,33,506,6.57,2,2,['datelist'],[None],[None],125,[],"['isinstance', 'yy2.append', 'np.array', 'datetime_toint']",4
source/utils/util_date.py:datetime_to_milisec,datetime_to_milisec,function,3,21,18,165,7.86,0,1,['datelist'],[None],[None],135,[],"['isinstance', 'datetime', 'datetime.datetime']",3
source/utils/util_date.py:datetime_weekday,datetime_weekday,function,2,13,12,116,8.92,0,1,['datelist'],[None],[None],143,[],"['isinstance', 'int']",2
source/utils/util_date.py:datetime_weekday_fast,datetime_weekday_fast,function,4,10,8,108,10.8,0,0,['dateval'],[None],[None],153,"['    """"""\n', '      date values\n', '    :param dateval:\n', '    :return:\n', '    """"""\n']",['datetime_weekday'],1
source/utils/util_date.py:datetime_quarter,datetime_quarter,function,3,7,7,35,5.0,0,0,['datetimex'],[None],[None],167,[],['int'],1
source/utils/util_date.py:dateime_daytime,dateime_daytime,function,7,37,21,129,3.49,0,1,['datetimex'],[None],[None],172,[],[],0
source/utils/util_date.py:datenumpy_todatetime,datenumpy_todatetime,function,9,40,24,348,8.7,0,3,"['tt', 'islocaltime']","[None, None]","[None, 'True']",186,[],"['type', 'datetime.fromtimestamp', 'datetime.utcfromtimestamp']",3
source/utils/util_date.py:datetime_tonumpydate,datetime_tonumpydate,function,2,2,2,22,11.0,0,0,"['t', 'islocaltime']","[None, None]","[None, 'True']",203,[],['np.datetime64'],1
source/utils/util_date.py:np_dict_tolist,np_dict_tolist,function,3,7,7,37,5.29,1,0,['dd'],[None],[None],215,[],['list'],1
source/utils/util_date.py:np_dict_tostr_val,np_dict_tostr_val,function,1,7,7,52,7.43,0,0,['dd'],[None],[None],219,[],['list'],1
source/utils/util_date.py:np_dict_tostr_key,np_dict_tostr_key,function,1,7,7,52,7.43,0,0,['dd'],[None],[None],223,[],['list'],1
source/utils/util_deep.py:tf_to_dot,tf_to_dot,function,9,22,19,165,7.5,2,0,['graph'],[None],[None],7,[],"['Digraph', 'graph.as_graph_def', 'dot.node', 'dot.edge']",4
source/utils/util_import.py:dict2,dict2,class,3,5,5,36,7.2,0,0,[],[],[],53,[],[],0
source/utils/util_import.py:dict2:__init__,dict2:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",54,[],[],0
source/utils/util_metric.py:mean_reciprocal_rank,mean_reciprocal_rank,function,3,18,15,90,5.0,0,0,['rs'],[None],[None],13,"['    """"""Score is reciprocal of the rank of the first relevant item\n', ""    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n"", '    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n', '    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n', '    >>> mean_reciprocal_rank(rs)\n', '    0.61111111111111105\n', '    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n', '    >>> mean_reciprocal_rank(rs)\n', '    0.5\n', '    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n', '    >>> mean_reciprocal_rank(rs)\n', '    0.75\n', '    Args:\n', '        rs: Iterator of relevance scores (list or numpy) in rank order\n', '            (first element is the first item)\n', '    Returns:\n', '        Mean reciprocal rank\n', '    """"""\n']",['np.mean'],1
source/utils/util_metric.py:r_precision,r_precision,function,7,13,12,84,6.46,0,1,['r'],[None],[None],36,"['    """"""Score is precision after all relevant documents have been retrieved\n', '    Relevance is binary (nonzero is relevant).\n', '    >>> r = [0, 0, 1]\n', '    >>> r_precision(r)\n', '    0.33333333333333331\n', '    >>> r = [0, 1, 0]\n', '    >>> r_precision(r)\n', '    0.5\n', '    >>> r = [1, 0, 0]\n', '    >>> r_precision(r)\n', '    1.0\n', '    Args:\n', '        r: Relevance scores (list or numpy) in rank order\n', '            (first element is the first item)\n', '    Returns:\n', '        R Precision\n', '    """"""\n']","['np.asarray', 'r.nonzero', 'np.mean']",3
source/utils/util_metric.py:precision_at_k,precision_at_k,function,7,17,17,105,6.18,0,1,"['r', 'k']","[None, None]","[None, None]",61,"['    """"""Score is precision @ k\n', '    Relevance is binary (nonzero is relevant).\n', '    >>> r = [0, 0, 1]\n', '    >>> precision_at_k(r, 1)\n', '    0.0\n', '    >>> precision_at_k(r, 2)\n', '    0.0\n', '    >>> precision_at_k(r, 3)\n', '    0.33333333333333331\n', '    >>> precision_at_k(r, 4)\n', '    Traceback (most recent call last):\n', '        File ""<stdin>"", line 1, in ?\n', '    ValueError: Relevance score length < k\n', '    Args:\n', '        r: Relevance scores (list or numpy) in rank order\n', '            (first element is the first item)\n', '    Returns:\n', '        Precision @ k\n', '    Raises:\n', '        ValueError: len(r) must be >= k\n', '    """"""\n']","['np.asarray', 'ValueError', 'np.mean']",3
source/utils/util_metric.py:average_precision,average_precision,function,5,20,17,109,5.45,0,1,['r'],[None],[None],90,"['    """"""Score is average precision (area under PR curve)\n', '    Relevance is binary (nonzero is relevant).\n', '    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n', '    >>> delta_r = 1. / sum(r)\n', '    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n', '    0.7833333333333333\n', '    >>> average_precision(r)\n', '    0.78333333333333333\n', '    Args:\n', '        r: Relevance scores (list or numpy) in rank order\n', '            (first element is the first item)\n', '    Returns:\n', '        Average precision\n', '    """"""\n']","['np.asarray', 'range', 'np.mean']",3
source/utils/util_metric.py:mean_average_precision,mean_average_precision,function,2,6,6,45,7.5,0,0,['rs'],[None],[None],112,"['    """"""Score is mean average precision\n', '    Relevance is binary (nonzero is relevant).\n', '    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n', '    >>> mean_average_precision(rs)\n', '    0.78333333333333333\n', '    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n', '    >>> mean_average_precision(rs)\n', '    0.39166666666666666\n', '    Args:\n', '        rs: Iterator of relevance scores (list or numpy) in rank order\n', '            (first element is the first item)\n', '    Returns:\n', '        Mean average precision\n', '    """"""\n']",['np.mean'],1
source/utils/util_metric.py:dcg_at_k,dcg_at_k,function,8,31,25,213,6.87,0,2,"['r', 'k', 'method']","[None, None, None]","[None, None, '0']",130,"['    """"""Score is discounted cumulative gain (dcg)\n', '    Relevance is positive real values.  Can use binary\n', '    as the previous methods.\n', '    Example from\n', '    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n', '    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n', '    >>> dcg_at_k(r, 1)\n', '    3.0\n', '    >>> dcg_at_k(r, 1, method=1)\n', '    3.0\n', '    >>> dcg_at_k(r, 2)\n', '    5.0\n', '    >>> dcg_at_k(r, 2, method=1)\n', '    4.2618595071429155\n', '    >>> dcg_at_k(r, 10)\n', '    9.6051177391888114\n', '    >>> dcg_at_k(r, 11)\n', '    9.6051177391888114\n', '    Args:\n', '        r: Relevance scores (list or numpy) in rank order\n', '            (first element is the first item)\n', '        k: Number of results to consider\n', '        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n', '                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n', '    Returns:\n', '        Discounted cumulative gain\n', '    """"""\n']","['np.asfarray', 'np.sum', 'np.log2', 'ValueError']",4
source/utils/util_metric.py:ndcg_at_k,ndcg_at_k,function,3,15,12,107,7.13,0,1,"['r', 'k', 'method']","[None, None, None]","[None, None, '0']",169,"['    """"""Score is normalized discounted cumulative gain (ndcg)\n', '    Relevance is positive real values.  Can use binary\n', '    as the previous methods.\n', '    Example from\n', '    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n', '    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n', '    >>> ndcg_at_k(r, 1)\n', '    1.0\n', '    >>> r = [2, 1, 2, 0]\n', '    >>> ndcg_at_k(r, 4)\n', '    0.9203032077642922\n', '    >>> ndcg_at_k(r, 4, method=1)\n', '    0.96519546960144276\n', '    >>> ndcg_at_k([0], 1)\n', '    0.0\n', '    >>> ndcg_at_k([1], 2)\n', '    1.0\n', '    Args:\n', '        r: Relevance scores (list or numpy) in rank order\n', '            (first element is the first item)\n', '        k: Number of results to consider\n', '        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n', '                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n', '    Returns:\n', '        Normalized discounted cumulative gain\n', '    """"""\n']",['dcg_at_k'],1
source/utils/util_model.py:import_,import_,function,7,23,22,243,10.57,0,1,"['abs_module_path', 'class_name']","[None, None]","[None, 'None']",79,[],"['import_module', 'print', 'getattr']",3
source/utils/util_model.py:pd_dim_reduction,pd_dim_reduction,function,16,45,34,410,9.11,0,4,"['df', 'colname', 'colprefix', 'method', 'dimpca', 'model_pretrain', 'return_val', '']","[None, None, None, None, None, None, None, None]","[None, None, '""colsvd""', '""svd""', '2', 'None', '""dataframe,param""', None]",117,"['    """"""\n', '       Dimension reduction technics\n', '       dftext_svd, svd = pd_dim_reduction(dfcat_test, None,colprefix=""colsvd"",\n', '                         method=""svd"", dimpca=2, return_val=""dataframe,param"")\n', '    :param df:\n', '    :param colname:\n', '    :param colprefix:\n', '    :param method:\n', '    :param dimpca:\n', '    :param return_val:\n', '    :return:\n', '    """"""\n']","['list', 'TruncatedSVD', 'svd.fit', 'copy.deepcopy', 'svd.transform', 'pd.DataFrame', 'str']",7
source/utils/util_model.py:model_lightgbm_kfold,model_lightgbm_kfold,function,25,75,69,940,12.53,1,1,"['df', 'colname', 'num_folds', 'stratified', 'colexclude', 'debug']","[None, None, None, None, None, None]","[None, 'None', '2', 'False', 'None', 'False']",158,[],"['StratifiedKFold', 'KFold', 'np.zeros', 'pd.DataFrame', 'enumerate', 'lgb.Dataset', 'lgb.train', 'regs.append']",8
source/utils/util_model.py:model_catboost_classifier,model_catboost_classifier,function,21,50,47,585,11.7,0,2,"['Xtrain', 'Ytrain', 'Xcolname', 'pars={""learning_rate""', '""iterations""', '""random_seed""', '""loss_function""', '}', 'isprint', '']","[None, None, None, '', ' 1000', ' 0', ' ""MultiClass""', None, None, None]","[None, None, 'None', '{""learning_rate"": 0.1', None, None, None, None, '0', None]",208,"['    """"""\n', '  from catboost import Pool, CatBoostClassifier\n', '\n', ""TRAIN_FILE = '../data/cloudness_small/train_small'\n"", ""TEST_FILE = '../data/cloudness_small/test_small'\n"", ""CD_FILE = '../data/cloudness_small/train.cd'\n"", '# Load data from files to Pool\n', 'train_pool = Pool(TRAIN_FILE, column_description=CD_FILE)\n', 'test_pool = Pool(TEST_FILE, column_description=CD_FILE)\n', '# Initialize CatBoostClassifier\n', ""model = CatBoostClassifier(iterations=2, learning_rate=1, depth=2, loss_function='MultiClass')\n"", '# Fit model\n', 'model.fit(train_pool)\n', '# Get predicted classes\n', 'preds_class = model.predict(test_pool)\n', '# Get predicted probabilities for each class\n', 'preds_proba = model.predict_proba(test_pool)\n', '# Get predicted RawFormulaVal\n', ""  preds_raw = model.predict(test_pool, prediction_type='RawFormulaVal')\n"", '\n', '\n', '  https://tech.yandex.com/catboost/doc/dg/concepts/python-usages-examples-docpage/\n', '\n', '  """"""\n']","['dict2', 'range', 'pd.DataFrame', 'catboost.CatBoostClassifier', 'clf.fit', 'clf.predict', 'cm.astype', 'cm.sum', 'print']",9
source/utils/util_model.py:sk_score_get,sk_score_get,function,10,19,15,223,11.74,0,2,['name'],[None],"['""r2""']",273,[],[],0
source/utils/util_model.py:sk_params_search_best,sk_params_search_best,function,27,50,41,690,13.8,0,2,"['clf', 'X', 'y', '0', '1', '5)}', 'method', 'param_search={""scorename""', '""cv""', '""population_size""', '""generations_number""', '']","[None, None, None, None, None, None, None, '', ' 5', ' 5', ' 3}', None]","[None, None, None, None, None, None, '""gridsearch""', '{""scorename"": ""r2""', None, None, None, None]",283,"['    """"""\n', '   Genetic: population_size=5, ngene_mutation_prob=0.10,,gene_crossover_prob=0.5, tournament_size=3,  generations_number=3\n', '\n', '    :param X:\n', '    :param y:\n', '    :param clf:\n', '    :param param_grid:\n', '    :param method:\n', '    :param param_search:\n', '    :return:\n', '  """"""\n']","['sk_score_get', 'GridSearchCV', 'grid.fit', 'EvolutionaryAlgorithmSearchCV', 'cv=StratifiedKFold', 'cv.fit']",6
source/utils/util_model.py:sk_error,sk_error,function,13,39,31,327,8.38,0,2,"['ypred', 'ytrue', 'method', 'sample_weight', 'multioutput']","[None, None, None, None, None]","[None, None, '""r2""', 'None', 'None']",334,[],"['np.sqrt', 'len', 'print', 'np.std', 'r2_score', 'np.sign']",6
source/utils/util_model.py:sk_cluster,sk_cluster,function,43,130,101,1055,8.12,2,8,"['Xmat', 'method', ')', 'kwds={""metric""', '""min_cluster_size""', '""min_samples""', 'isprint', 'preprocess={""norm""', '']","[None, None, None, '', ' 150', ' 3}', None, '', None]","[None, '""kmode""', None, '{""metric"": ""euclidean""', None, None, '1', '{""norm"": False}', None]",348,"['    """"""\n', ""   'hdbscan',(), kwds={'metric':'euclidean', 'min_cluster_size':150, 'min_samples':3 }\n"", ""   'kmodes',(), kwds={ n_clusters=2, n_init=5, init='Huang', verbose=1 }\n"", ""   'kmeans',    kwds={ n_clusters= nbcluster }\n"", '\n', '   Xmat[ Xcluster== 5 ]\n', '   # HDBSCAN Clustering\n', '   Xcluster_hdbscan= da.sk_cluster_algo_custom(Xtrain_d, hdbscan.HDBSCAN, (),\n', ""                  {'metric':'euclidean', 'min_cluster_size':150, 'min_samples':3})\n"", '\n', '   print len(np.unique(Xcluster_hdbscan))\n', '\n', '   Xcluster_use =  Xcluster_hdbscan\n', '\n', '# Calculate Distribution for each cluster\n', ""kde= da.plot_distribution_density(Y[Xcluster_use== 2], kernel='gaussian', N=200, bandwith=1 / 500.)\n"", 'kde.sample(5)\n', '\n', '   """"""\n']","['km.fit_predict', 'hdbscan.HDBSCAN', 'print', 'len', 'np.std', 'np.mean', 'Xmat.reshape', 'KMeans', 'kmeans.fit', 'range', 'plt.plot', 'plt.show']",12
source/utils/util_model.py:sk_model_ensemble_weight,sk_model_ensemble_weight,function,14,26,24,237,9.12,1,0,"['model_list', 'acclevel', 'maxlevel']","[None, None, None]","[None, None, '0.88']",477,[],"['min', 'len', 'np.empty', 'range', 'estww.append', 'np.log', 'np.array']",7
source/utils/util_model.py:sk_model_votingpredict,sk_model_votingpredict,function,17,38,30,269,7.08,2,1,"['estimators', 'voting', 'ww', 'X_test']","[None, None, None, None]","[None, None, None, None]",494,[],"['np.sum', 'np.zeros', 'enumerate', 'clf.predict_proba', 'range', 'len']",6
source/utils/util_model.py:sk_showconfusion,sk_showconfusion,function,11,22,20,209,9.5,0,1,"['Y', 'Ypred', 'isprint']","[None, None, None]","[None, None, 'True']",514,[],"['cm.astype', 'cm.sum', 'print']",3
source/utils/util_model.py:sk_showmetrics,sk_showmetrics,function,21,81,70,799,9.86,0,1,"['y_test', 'ytest_pred', 'ytest_proba', 'target_names', '""1""]', 'return_stat']","[None, None, None, None, None, None]","[None, None, None, '[""0""', None, '0']",524,[],"['sk_showconfusion', 'roc_auc_score', 'accuracy_score', 'print', 'str', 'roc_curve', 'plt.plot', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.show']",11
source/utils/util_model.py:sk_metric_roc_optimal_cutoff,sk_metric_roc_optimal_cutoff,function,13,21,21,236,11.24,0,0,"['ytest', 'ytest_proba']","[None, None]","[None, None]",562,"['    """""" Find the optimal probability cutoff point for a classification model related to event rate\n', '    Parameters\n', '    ----------\n', '    ytest : Matrix with dependent or target data, where rows are observations\n', '    ytest_proba : Matrix with predicted data, where rows are observations\n', '\n', '    # Find prediction to the dataframe applying threshold\n', ""    data['pred'] = data['pred_proba'].map(lambda x: 1 if x > threshold else 0)\n"", '    # Print confusion Matrix\n', '    from sklearn.metrics import confusion_matrix\n', ""    confusion_matrix(data['admit'], data['pred'])\n"", '    # array([[175,  98],\n', '    #        [ 46,  81]])\n', '    Returns: with optimal cutoff value\n', '    """"""\n']","['roc_curve', 'np.arange', 'pd.DataFrame', 'pd.Series']",4
source/utils/util_model.py:sk_metric_roc_auc,sk_metric_roc_auc,function,45,188,132,1791,9.53,2,2,"['y_test', 'ytest_pred', 'ytest_proba']","[None, None, None]","[None, None, None]",587,[],"['print', 'roc_auc_score', 'roc_curve', 'classification_report', 'sk_metric_roc_auc_multiclass', 'dict', 'np.array', 'range', 'list', 'np.concatenate', 'plt.figure', 'plt.plot', 'plt.xlim', 'plt.ylim', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.legend', 'plt.show']",19
source/utils/util_model.py:sk_metric_roc_auc_multiclass,sk_metric_roc_auc_multiclass,function,37,140,112,1333,9.52,2,1,"['n_classes', 'y_test', 'y_test_pred', 'y_predict_proba']","[None, None, None, None]","['3', 'None', 'None', 'None']",604,[],"['print', 'dict', 'np.array', 'range', 'list', 'np.concatenate', 'roc_curve', 'plt.figure', 'plt.plot', 'plt.xlim', 'plt.ylim', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.legend', 'plt.show']",16
source/utils/util_model.py:sk_model_eval_regression,sk_model_eval_regression,function,10,44,36,538,12.23,0,1,"['clf', 'istrain', 'Xtrain', 'ytrain', 'Xval', 'yval']","[None, None, None, None, None, None]","[None, '1', 'None', 'None', 'None', 'None']",663,[],"['clf.fit', 'print', 'CV_score.mean', 'CV_score.std', 'clf.predict', 'mean_absolute_error']",6
source/utils/util_model.py:sk_model_eval_classification,sk_model_eval_classification,function,12,32,29,456,14.25,0,1,"['clf', 'istrain', 'Xtrain', 'ytrain', 'Xtest', 'ytest']","[None, None, None, None, None, None]","[None, '1', 'None', 'None', 'None', 'None']",683,[],"['print', 'clf.fit', 'clf.predict_proba', 'clf.predict', 'sk_showmetrics']",5
source/utils/util_model.py:sk_metrics_eval,sk_metrics_eval,function,14,34,28,318,9.35,2,0,"['clf', 'Xtest', 'ytest', 'cv', 'metrics', '""accuracy""', '""precision_macro""', '""recall_macro""]']","[None, None, None, None, None, None, None, None]","[None, None, None, '1', '[""f1_macro""', None, None, None]",699,[],"['cross_val_score', 'enumerate', 'entries.append', 'pd.DataFrame']",4
source/utils/util_model.py:sk_model_eval,sk_model_eval,function,17,82,65,1089,13.28,0,2,"['clf', 'istrain', 'Xtrain', 'ytrain', 'Xval', 'yval']","[None, None, None, None, None, None]","[None, '1', 'None', 'None', 'None', 'None']",711,"['    """"""\n', '       Feature importance with colname\n', '    :param clf:  model or colnum with weights\n', '    :param colname:\n', '    :return:\n', '    """"""\n']","['clf.fit', 'print', 'CV_score.mean', 'CV_score.std', 'clf.predict', 'mean_absolute_error', 'sk_model_eval_classification', 'clf.predict_proba', 'sk_showmetrics']",9
source/utils/util_model.py:sk_feature_impt,sk_feature_impt,function,18,50,43,583,11.66,1,2,"['clf', 'colname', 'model_type']","[None, None, None]","[None, None, '""logistic""']",723,"['    """"""\n', '       Feature importance with colname\n', '    :param clf:  model or colnum with weights\n', '    :param colname:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 'np.abs', 'np.arange', 'len', 'isinstance', 'np.argsort', 'range']",7
source/utils/util_model.py:sk_feature_selection,sk_feature_selection,function,25,48,38,422,8.79,1,3,"['clf', 'method', 'colname', 'kbest', 'Xtrain', 'ytrain']","[None, None, None, None, None, None]","[None, '""f_classif""', 'None', '50', 'None', 'None']",753,[],"['SelectKBest', 'clf_best.get_support', 'zip', 'new_features.append']",4
source/utils/util_model.py:sk_feature_evaluation,sk_feature_evaluation,function,28,63,58,696,11.05,2,1,"['clf', 'df', 'kbest', 'colname_best', 'dfy']","[None, None, None, None, None]","[None, None, '30', 'None', 'None']",771,[],"['copy.deepcopy', 'train_test_split', 'print', 'range', 'len', 'clf.fit', 'clf.predict_proba', 'clf.predict', 'sk_showmetrics', 'pd.DataFrame']",10
source/utils/util_model.py:sk_feature_prior_shift,sk_feature_prior_shift,function,0,0,0,0,0.0,0,0,[],[],[],804,"['    """"""\n', '     Label is drifting\n', '    https://dkopczyk.quantee.co.uk/covariate_shift/\n', '\n', '    Parameters\n', '    ----------\n', '    df : TYPE\n', '        DESCRIPTION.\n', '\n', '    Returns\n', '    -------\n', '    None.\n', '\n', '    """"""\n']",[],0
source/utils/util_model.py:sk_feature_concept_shift,sk_feature_concept_shift,function,0,1,1,4,4.0,0,0,['df'],[None],[None],822,"['    """"""\n', '\n', '       (X,y) distribution relation is shifting.\n', '    https://dkopczyk.quantee.co.uk/covariate_shift/\n', '\n', '    Parameters\n', '    ----------\n', '    df : TYPE\n', '        DESCRIPTION.\n', '\n', '    Returns\n', '    -------\n', '    None.\n', '\n', '    """"""\n']",[],0
source/utils/util_model.py:sk_feature_covariate_shift,sk_feature_covariate_shift,function,25,58,49,582,10.03,1,3,"['dftrain', 'dftest', 'colname', 'nsample']","[None, None, None, None]","[None, None, None, '10000']",841,"['    """"""\n', '      X is drifting\n', '    Parameters\n', '    ----------\n', '    dftrain : TYPE\n', '        DESCRIPTION.\n', '    dftest : TYPE\n', '        DESCRIPTION.\n', '    colname : TYPE\n', '        DESCRIPTION.\n', '    nsample : TYPE, optional\n', '        DESCRIPTION. The default is 10000.\n', '\n', '    Returns\n', '    -------\n', '    drop_list : TYPE\n', '        DESCRIPTION.\n', '\n', '    """"""\n']","['len', 'train.append', 'combi.drop', 'RandomForestClassifier', 'cross_val_score', 'pd.DataFrame', 'np.mean', 'drop_list.append', 'print']",9
source/utils/util_model.py:sk_model_eval_classification_cv,sk_model_eval_classification_cv,function,25,62,43,570,9.19,2,1,"['clf', 'X', 'y', 'test_size', 'ncv', 'method']","[None, None, None, None, None, None]","[None, None, None, '0.5', '1', '""random""']",889,"['    """"""\n', '    :param clf:\n', '    :param X:\n', '    :param y:\n', '    :param test_size:\n', '    :param ncv:\n', '    :param method:\n', '    :return:\n', '    """"""\n']","['StratifiedKFold', 'enumerate', 'print', 'sk_model_eval_classification', 'range', 'train_test_split']",6
source/utils/util_model.py:dict2,dict2,class,3,5,5,36,7.2,0,0,[],[],[],109,[],[],0
source/utils/util_model.py:model_template1,model_template1,class,41,135,81,1190,8.81,0,3,[],[],[],424,[],[],0
source/utils/util_model.py:dict2:__init__,dict2:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",110,[],[],0
source/utils/util_model.py:model_template1:__init__,model_template1:__init__,method,13,16,15,166,10.38,0,0,"['self', 'alpha', 'low_y_cut', 'high_y_cut', 'ww0']","[None, None, None, None, None]","[None, '0.5', '-0.09', '0.09', '0.95']",425,[],['Ridge'],1
source/utils/util_model.py:model_template1:fit,model_template1:fit,method,19,44,37,466,10.59,0,1,"['self', 'X', 'Y']","[None, None, None]","[None, None, 'None']",432,[],"['len', 'print', 'np.median']",3
source/utils/util_model.py:model_template1:predict,model_template1:predict,method,8,25,17,165,6.6,0,1,"['self', 'X', 'y', 'ymedian']","[None, None, None, None]","[None, None, 'None', 'None']",451,[],['Y.clip'],1
source/utils/util_model.py:model_template1:score,model_template1:score,method,12,30,23,215,7.17,0,1,"['self', 'X', 'Ytrue', 'ymedian']","[None, None, None, None]","[None, None, 'None', 'None']",463,[],"['Y.clip', 'r2_score']",2
source/utils/util_optim.py:create_model_name,create_model_name,function,0,1,1,4,4.0,0,0,"['save_folder', 'model_name']","[None, None]","[None, None]",53,[],[],0
source/utils/util_optim.py:optim,optim,function,5,32,26,297,9.28,0,2,"['modelname', 'pars', 'df ', 'optim_engine', 'optim_method', 'save_folder', 'log_folder', 'ntrials']","[None, None, None, None, None, None, None, None]","['""model_dl.1_lstm.py""', ' {}', ' None', '""optuna""', '""normal/prune""', '""model_save/""', '""logs/""', '2']",60,[],"['print', 'optim_optuna']",2
source/utils/util_optim.py:optim_optuna,optim_optuna,function,44,130,99,1730,13.31,1,3,"['modelname', 'pars', 'df ', 'optim_method', 'save_folder', 'log_folder', 'ntrials']","[None, None, None, None, None, None, None]","['""model_dl.1_lstm.py""', ' {}', ' None', '""normal/prune""', '""/mymodel/""', '""""', '2']",78,"['    """"""\n', '       Interface layer to Optuna  for hyperparameter optimization\n', '       return Best Parameters \n', '\n', ""    weight_decay = trial.suggest_loguniform('weight_decay', 1e-10, 1e-3)\n"", ""    optimizer = trial.suggest_categorical('optimizer', ['MomentumSGD', 'Adam']) # Categorical parameter\n"", ""    num_layers = trial.suggest_int('num_layers', 1, 3)      # Int parameter\n"", ""    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 1.0)      # Uniform parameter\n"", ""    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)      # Loguniform parameter\n"", ""    drop_path_rate = trial.suggest_discrete_uniform('drop_path_rate', 0.0, 1.0, 0.1) # Discrete-uniform parameter\n"", '    """"""\n']","['module_load', 'objective', 'module.get_params', 'pars.items', 'trial.suggest_loguniform', 'trial.suggest_int', 'trial.suggest_categorical', 'trial.suggest_discrete_uniform', 'trial.suggest_uniform', 'Exception', 'module.Model', 'module.fit', 'tf.reset_default_graph', 'optuna.create_study', 'study.optimize', 'param_dict.update', 'modelname.replace', 'not', 'os.makedirs', 'save', 'study.trials_dataframe', 'study_trials.to_csv', 'json.dump']",23
source/utils/util_optim.py:load_arguments,load_arguments,function,12,102,79,1102,10.8,0,1,['config_file'],[None],[' None'],180,"['    """"""\n', '        Load CLI input, load config.toml , overwrite config.toml by CLI Input\n', '    """"""\n']","['print', 'argparse.ArgumentParser', 'p.add_argument', 'p.parse_args', 'load_config']",5
source/utils/util_optim.py:data_loader,data_loader,function,10,16,13,229,14.31,0,0,['file_name'],[None],"[""'dataset/GOOG-year.csv'""]",207,[],"['pd.read_csv', 'pd.to_datetime', 'MinMaxScaler', 'minmax.transform', 'pd.DataFrame']",5
source/utils/util_optim.py:test_all,test_all,function,6,55,38,369,6.71,0,0,[],[],[],220,[],"['data_loader', 'optim', 'print']",3
source/utils/util_optim.py:test_fast,test_fast,function,6,61,44,481,7.89,0,0,[],[],[],233,[],"['data_loader', 'optim', 'print']",3
source/utils/util_pipeline.py:pd_pipeline,pd_pipeline,function,11,40,35,507,12.68,0,0,"['bin_cols', 'text_col', 'X', 'y']","[None, None, None, None]","[None, None, None, None]",17,[],"['Pipeline', 'MySelector', 'MyBinarizer', 'CountVectorizer', 'TfidfVectorizer', 'FeatureUnion', 'PCA', 'LinearSVC', 'train_test_split', 'full_pipeline.fit']",10
source/utils/util_pipeline.py:pd_grid_search,pd_grid_search,function,10,25,22,368,14.72,0,0,"['full_pipeline', 'X', 'y']","[None, None, None]","[None, None, None]",62,[],"['train_test_split', 'GridSearchCV', 'full_pipeline.predict', 'print']",4
source/utils/util_plot.py:pd_colnum_tocat_stat,pd_colnum_tocat_stat,function,53,151,105,2000,13.25,1,5,"['input_data', 'feature', 'target_col', 'bins', 'cuts']","[None, None, None, None, None]","[None, None, None, None, '0']",27,"['    """"""\n', '    Bins continuous features into equal sample size buckets and returns the target mean in each bucket. Separates out\n', '    nulls into another bucket.\n', '    :param input_data: dataframe containg features and target column\n', '    :param feature: feature column name\n', '    :param target_col: target column\n', '    :param bins: Number bins required\n', '    :param cuts: if buckets of certain specific cuts are required. Used on test data to use cuts from train.\n', '    :return: If cuts are passed only grouped data is returned, else cuts and grouped data is returned\n', '    """"""\n']","['pd.isnull', 'input_data.reset_index', 'min', 'range', 'np.percentile', 'cuts.append', 'pd.cut', 'input_data.groupby', 'grouped.reset_index', 'list', 'grouped.rename', 'str', 'len', 'pd.concat']",14
source/utils/util_plot.py:plot_univariate_plots,plot_univariate_plots,function,9,36,29,373,10.36,1,2,"['data', 'target_col', 'features_list', 'bins', 'data_test']","[None, None, None, None, None]","[None, None, '0', '10', '0']",97,"['    """"""\n', '    Creates univariate dependence plots for features in the dataset\n', '    :param data: dataframe containing features and target columns\n', '    :param target_col: target column name\n', '    :param features_list: by default creates plots for all features. If list passed, creates plots of only those features.\n', '    :param bins: number of bins to be created from continuous feature\n', '    :param data_test: test data which has to be compared with input data for correlation\n', '    :return: Draws univariate plots for all columns in data\n', '    """"""\n']","['type', 'list', 'features_list.remove', 'print', 'plot_univariate_histogram']",5
source/utils/util_plot.py:plot_univariate_histogram,plot_univariate_histogram,function,15,68,49,1060,15.59,0,3,"['feature', 'data', 'target_col', 'bins', 'data_test']","[None, None, None, None, None]","[None, None, None, '10', '0']",120,"['    """"""\n', '    Calls the draw plot function and editing around the plots\n', '    :param feature: feature column name\n', '    :param data: dataframe containing features and target columns\n', '    :param target_col: target column name\n', '    :param bins: number of bins to be created from continuous feature\n', '    :param data_test: test data which has to be compared with input data for correlation\n', '    :return: grouped data if only train passed, else (grouped train data, grouped test data)\n', '    """"""\n']","['print', 'pd_colnum_tocat_stat', 'type', 'pd_stat_distribution_trend_correlation', 'plot_col_univariate']",5
source/utils/util_plot.py:pd_stat_distribution_trend_correlation,pd_stat_distribution_trend_correlation,function,19,63,55,902,14.32,0,2,"['grouped', 'grouped_test', 'feature', 'target_col']","[None, None, None, None]","[None, None, None, None]",158,"['    """"""\n', '    Calculates correlation between train and test trend of feature wrt target.\n', '    :param grouped: train grouped data\n', '    :param grouped_test: test grouped data\n', '    :param feature: feature column name\n', '    :param target_col: target column name\n', '    :return: trend correlation between train and test\n', '    """"""\n']","['grouped.merge', 'pd.isnull', 'len', 'np.corrcoef', 'print']",5
source/utils/util_plot.py:plot_col_univariate,plot_col_univariate,function,29,100,72,1218,12.18,0,1,"['input_data', 'feature', 'target_col', 'trend_correlation']","[None, None, None, None]","[None, None, None, 'None']",190,"['    """"""\n', '    Draws univariate dependence plots for a feature\n', '    :param input_data: grouped data contained bins of feature and target mean.\n', '    :param feature: feature column name\n', '    :param target_col: target column\n', '    :param trend_correlation: correlation between train and test trends of feature wrt target\n', '    :return: Draws trend plots for feature\n', '    """"""\n']","['pd_stat_distribution_trend_correlation', 'plt.figure', 'plt.subplot', 'ax1.plot', 'ax1.set_xticks', 'ax1.set_xticklabels', 'plt.xticks', 'ax1.set_xlabel', 'ax1.set_ylabel', 'str', 'dict', 'ax1.text', 'plt.title', 'ax2.bar', 'ax2.set_xticks', 'ax2.set_xticklabels', 'ax2.set_xlabel', 'ax2.set_ylabel', 'plt.tight_layout', 'plt.show']",20
source/utils/util_plot.py:plotbar,plotbar,function,6,9,9,152,16.89,0,0,"['df', 'colname', 'figsize', '10']","[None, None, None, None]","[None, None, '(20', None]",235,[],"['plt.figure', 'sns.barplot', 'plt.title', 'plt.tight_layout', 'plt.show', 'plt.savefig']",6
source/utils/util_plot.py:plotxy,plotxy,function,9,19,19,196,10.32,0,0,"['12', '10)', 'title', 'savefile']","[None, None, None, None]","[None, None, '""feature importance""', '""myfile.png""']",244,"['    """"""\n', '    :param x:\n', '    :param y:\n', '    :param color:\n', '    :param size:\n', '    :param title:\n', '    """"""\n']","['np.zeros', 'type', 'plt.subplots', 'plt.scatter', 'plt.title', 'plt.show', 'plt.savefig']",7
source/utils/util_plot.py:plot_col_distribution,plot_col_distribution,function,27,128,85,1105,8.63,4,3,"['df', 'col_include', 'col_exclude', 'pars={""binsize""']","[None, None, None, '']","[None, 'None', 'None', '{""binsize"": 20}']",263,"['    """"""  Retrives all the information of the column\n', '    :param df:\n', '    :param col_include:\n', '    :param col_exclude:\n', '    :param pars:\n', '    """"""\n']","['list', 'np.count_nonzero', 'sorted', 'print', 'len', 'np.max', 'np.min', 'np.median', 'np.mean', 'np.std', 'plot_Y', 'plt.title', 'plt.plot', 'plt.hist', 'plt.show', 'plt.figure', 'np.percentile']",17
source/utils/util_plot.py:plot_pair,plot_pair,function,12,24,23,211,8.79,1,0,"['df', 'Xcolname', 'Ycoltarget']","[None, None, None]","[None, 'None', 'None']",305,"['    """"""\n', '    :param df:\n', '    :param Xcolname:\n', '    :param Ycoltarget:\n', ' \n', '    """"""\n']","['str', 'plt.scatter', 'plt.autoscale', 'plt.title', 'plt.show']",5
source/utils/util_plot.py:plot_distance_heatmap,plot_distance_heatmap,function,19,29,28,363,12.52,0,0,"['Xmat_dist', 'Xcolname']","[None, None]","[None, None]",324,"['    """"""\n', '\n', '    :param Xmat_dist:\n', '    :param Xcolname:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 'plt.figure', 'fig.add_subplot', 'ax.imshow', 'ax.set_xlabel', 'ax.set_ylabel', 'ax.set_title', 'plt.colorbar']",8
source/utils/util_plot.py:plot_cluster_2D,plot_cluster_2D,function,12,24,22,241,10.04,1,0,"['X_2dim', 'target_class', 'target_names']","[None, None, None]","[None, None, None]",352,"['    """""" \n', '    :param X_2dim:\n', '    :param target_class:\n', '    :param target_names:\n', '    :return: \n', '    Plot 2d of Clustering Class,\n', '    X2d: Nbsample x 2 dim  (projection on 2D sub-space)\n', '   """"""\n']","['itertools.cycle', 'range', 'len', 'plt.figure', 'zip', 'plt.scatter', 'plt.legend', 'plt.show']",8
source/utils/util_plot.py:plot_cluster_tsne,plot_cluster_tsne,function,22,54,46,599,11.09,0,3,"['Xmat', 'Xcluster_label', 'metric', 'perplexity', 'ncomponent', 'savefile', 'isprecompute', 'returnval', '']","[None, None, None, None, None, None, None, None, None]","[None, 'None', '""euclidean""', '50', '2', '""""', 'False', 'True', None]",370,"['    """"""\n', '    :return:\n', '    \n', '    Plot High dimemnsionnal State using TSNE method\n', ""   'euclidean, 'minkowski', 'cityblock', 'seuclidean', 'sqeuclidean, 'cosine, 'correlation, 'hamming, 'jaccard, 'chebyshev,\n"", ""   'canberra, 'braycurtis, 'mahalanobis', VI=None) 'yule, 'matching, 'dice, 'kulsinski, 'rogerstanimoto, 'russellrao, 'sokalmichener, 'sokalsneath,\n"", '\n', '   Xtsne= da.plot_cluster_tsne(Xtrain_dist, Xcluster_label=None, perplexity=40, ncomponent=2, isprecompute=True)\n', '\n', '   Xtrain_dist= sci.spatial.distance.squareform(sci.spatial.distance.pdist(Xtrain_d,\n', ""               metric='cityblock', p=2, w=None, V=None, VI=None))\n"", '   """"""\n']","['np.set_printoptions', 'model.fit_transform', 'np.arange', 'plot_XY']",4
source/utils/util_plot.py:plot_cluster_pca,plot_cluster_pca,function,20,48,41,487,10.15,0,3,"['Xmat', 'Xcluster_label', 'metric', 'dimpca', 'whiten', 'isprecompute', 'savefile', 'doreturn', '']","[None, None, None, None, None, None, None, None, None]","[None, 'None', '""euclidean""', '2', 'True', 'False', '""""', '1', None]",420,"['    """"""\n', '    :return:\n', '    """"""\n']","['pca', 'model.fit_transform', 'np.zeros', 'plot_XY']",4
source/utils/util_plot.py:plot_cluster_hiearchy,plot_cluster_hiearchy,function,32,90,80,980,10.89,1,3,"['Xmat_dist', 'p', 'truncate_mode', 'color_threshold', 'get_leaves', 'orientation', 'labels', 'count_sort', 'distance_sort', 'show_leaf_counts', 'do_plot', 'no_labels', 'leaf_font_size', 'leaf_rotation', 'leaf_label_func', 'show_contracted', 'link_color_func', 'ax', 'above_threshold_color', 'annotate_above', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, '30', 'None', 'None', 'True', '""top""', 'None', 'False', 'False', 'True', '1', 'False', 'None', 'None', 'None', 'False', 'None', 'None', '""b""', '0', None]",459,"['    """"""\n', '    :return:\n', '    """"""\n']","['dendrogram', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'zip', 'sum', 'plt.plot', 'plt.annotate', 'plt.axhline']",9
source/utils/util_plot.py:plot_distribution_density,plot_distribution_density,function,6,8,7,66,8.25,0,0,"['Xsample', 'kernel', 'N', 'bandwith']","[None, None, None, None]","[None, '""gaussian""', '10', '1 / 10.0']",533,[],[],0
source/utils/util_plot.py:plot_Y,plot_Y,function,4,7,7,88,12.57,0,0,"['Yval', 'typeplot', 'tsize', 'labels', 'title', 'xlabel', 'ylabel', 'zcolor_label', '8', '6)', 'dpi', 'savefile', 'color_dot', 'doreturn', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, '"".b""', 'None', 'None', '""""', '""""', '""""', '""""', None, None, '75', '""""', '""Blues""', '0', None]",576,"['    """"""\n', '     Return plot values\n', '    """"""\n']","['plt.figure', 'plt.title', 'plt.plot', 'plt.show']",4
source/utils/util_plot.py:plot_XY,plot_XY,function,72,179,144,1786,9.98,0,7,"['xx', 'yy', 'zcolor', 'tsize', 'labels', 'title', 'xlabel', 'ylabel', 'zcolor_label', '8', '6)', 'dpi', 'savefile', 'color_dot', 'doreturn', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', '""""', '""""', '""""', '""""', None, None, '75', '""""', '""Blues""', '0', None]",600,"['    """"""\n', '      labels= numpy array, ---> Generate HTML File with the labels interactives\n', '      Color: Plasma\n', '    """"""\n']","['isinstance', 'np.array', 'np.abs', 'plt.get_cmap', 'np.min', 'plt.subplots', 'fig.set_size_inches', 'fig.set_dpi', 'fig.tight_layout', 'ax1.scatter', 'ax1.set_xlabel', 'ax1.set_ylabel', 'ax1.set_title', 'ax1.grid', 'np.max', 'scatter.set_clim', 'fig.colorbar', 'cb.set_label', 'list', 'mpld3.save_html', 'plt.show', 'os_folder_create', 'plt.savefig', 'plot_XY_plotly', 'go.Scatter', 'py.iplot', 'py.plot', 'plot_XY_seaborn', 'sns.set_context', 'sns.set_color_codes', 'sns.color_palette', 'np.unique', 'plt.scatter', 'plt.gca', 'plt.title']",35
source/utils/util_plot.py:plot_XY_plotly,plot_XY_plotly,function,15,28,24,261,9.32,0,1,"['xx', 'yy', 'towhere']","[None, None, None]","[None, None, '""url""']",687,"['    """"""\n', '     Create Interactive Plotly\n', '    :param xx:\n', '    :param yy:\n', '    :param towhere:\n', '    :return:\n', '    """"""\n']","['go.Scatter', 'py.iplot', 'py.plot']",3
source/utils/util_plot.py:plot_XY_seaborn,plot_XY_seaborn,function,13,39,38,381,9.77,0,1,"['X', 'Y', 'Zcolor']","[None, None, None]","[None, None, 'None']",716,"['    """"""\n', '    :param X:\n', '    :param Y:\n', '    :param Zcolor:\n', '    :return:\n', '    """"""\n']","['sns.set_context', 'sns.set_color_codes', 'sns.color_palette', 'np.unique', 'plt.scatter', 'plt.gca', 'plt.title']",7
source/utils/util_plot.py:plot_cols_with_NaNs,plot_cols_with_NaNs,function,1,14,14,134,9.57,0,0,"['df', 'nb_to_show']","[None, None]","[None, None]",739,"['    """"""\n', '    Function to plot highest missing value columns\n', '    Arguments:\n', '        df:         dataframe\n', '        nb_to_show: number of columns to show\n', '    Prints:\n', '        nb_to_show columns with most missing values\n', '    """"""\n']",['print'],1
source/utils/util_plot.py:plot_col_correl_matrix,plot_col_correl_matrix,function,1,7,7,99,14.14,0,0,"['df', 'cols', 'annot', 'size']","[None, None, None, None]","[None, None, 'True', '30']",753,"['    """"""\n', '    Function to plot correlation matrix\n', '    Arguments:\n', '        df:    dataframe\n', '        cols:  columns to correlate\n', '        annot: annotate or not (default = True)\n', '        size:  size of correlation matrix (default = 30)\n', '    Prints:\n', '        correlation matrix of columns to each other\n', '    """"""\n']",['sns.heatmap'],1
source/utils/util_plot.py:plot_col_correl_target,plot_col_correl_target,function,7,20,19,247,12.35,0,1,"['df', 'cols', 'coltarget', 'nb_to_show', 'ascending']","[None, None, None, None, None]","[None, None, None, '10', 'False']",769,"['    """"""\n', '    Function to plot correlated columns to target\n', '    Arguments:\n', '        df:          dataframe\n', '        cols:        columns to correlate to target\n', '        coltarget:   target column\n', '        nb_to_show:  number of columns to show. Default = 10\n', '        ascending:   show most correlated (False) or least correlated (True). Default=False\n', '    Prints:\n', '        correlation columns to target\n', '    """"""\n']",['print'],1
source/utils/util_plot.py:plot_plotly,plot_plotly,function,0,0,0,0,0.0,0,0,[],[],[],792,"['    """"""\n', '    pip install plotly # Plotly is a pre-requisite before installing cufflinks\n', 'pip install cufflinks\n', '\n', '    #importing Pandas\n', 'import pandas as pd\n', '#importing plotly and cufflinks in offline mode\n', 'import cufflinks as cf\n', 'import plotly.offline\n', 'cf.go_offline()\n', 'cf.set_config_file(offline=False, world_readable=True)\n', '\n', '\n', '    :param df:\n', '    :return:\n', '    """"""\n']",[],0
source/utils/util_sql.py:sql_create_dbengine,sql_create_dbengine,function,5,48,29,378,7.88,0,1,"['type1', 'dbname', 'login', 'password', 'url', 'port']","[None, None, None, None, None, None]","['""""', '""""', '""""', '""""', '""localhost""', '5432']",40,"['    """""" Return SQL Alchemy Connector\n', '\n', '   ## psycopg2\n', ""   # engine = create_engine('postgresql+psycopg2://scott:tiger@localhost/mydatabase')\n"", '\n', '   ## MySQL-connector-python  Official one\n', ""   # engine = create_engine('mysql+mysqlconnector://scott:tiger@localhost/foo')\n"", '   # conda install -c anaconda mysql-connector-python=2.0.4\n', ""   # engine = create_engine('postgresql://%s:%s@localhost:5432/%s' %(myusername, mypassword, mydatabase))\n"", '\n', ""   # engine = create_engine('sqlite:///  folder/foo.db')\n"", '\n', '   """"""\n']","['sql.create_engine', 'str']",2
source/utils/util_sql.py:sql_query,sql_query,function,8,19,17,158,8.32,0,2,"['sqlr', 'dbengine', 'output', 'dburl=""sqlite', '']","[None, None, None, '', None]","['""SELECT ticker,shortratio,sector1_id, FROM stockfundamental""', 'None', '""df""', '""sqlite:///aaserialize/store/finviz.db""', None]",88,"['    """"""\n', ""   :param sqlr:       'SELECT ticker,shortratio,sector1_id, FROM stockfundamental'\n"", '   :param output:     df   /   file1.csv\n', ""   :param dburl:      'sqlite:///aaserialize/store/finviz.db'\n"", ""   :param dbengine:   dbengine = sql.create_engine('postgresql+psycopg2://postgres:postgres@localhost/coke')\n"", '   :return:\n', '   """"""\n']","['sql.create_engine', 'pd.read_sql_query', 'output.find', 'df.to_csv']",4
source/utils/util_sql.py:sql_get_dbschema,sql_get_dbschema,function,15,37,28,319,8.62,2,2,"['dburl=""sqlite', 'dbengine', 'isprint']","['', None, None]","['""sqlite:///aapackage/store/yahoo.db""', 'None', '0']",111,[],"['sql.create_engine', 'sql.inspect', 'inspector.get_table_names', 'inspector.get_columns', 'l1.append', 'print', 'np.array']",7
source/utils/util_sql.py:sql_delete_table,sql_delete_table,function,1,9,9,89,9.89,0,0,"['name', 'dbengine']","[None, None]","[None, None]",126,[],[],0
source/utils/util_sql.py:sql_insert_excel,sql_insert_excel,function,51,134,99,996,7.43,5,2,"['file1', 'dbengine', 'dbtype']","[None, None, None]","['"".xls""', 'None', '""""']",131,"['    """"""\n', ' # http://flask-excel.readthedocs.io/en/latest/\n', ' # https://pythonhosted.org/pyexcel/tutorial_data_conversion.html#import-excel-sheet-into-a-database-table\n', ' # from sqlalchemy import create_engine\n', ' # from sqlalchemy.ext.declarative import declarative_base\n', ' # from sqlalchemy import Column , Integer, String, Float, Date\n', ' # from sqlalchemy.orm import sessionmaker\n', ' # engine = create_engine(""sqlite:///birth.db"")\n', ' # Base = declarative_base()\n', ' # Session = sessionmaker(bind=engine)\n', '\n', ' # class BirthRegister(Base):\n', "" # ...     __tablename__='birth'\n"", ' # ...     id=Column(Integer, primary_key=True)\n', ' # ...     name=Column(String)\n', ' # ...     weight=Column(Float)\n', ' # ...     birth=Column(Date)\n', '\n', ' # Base.metadata.create_all(engine)\n', '\n', ' # https://www.digitalocean.com/community/tutorials/how-to-use-celery-with-rabbitmq-to-queue-tasks-on-an-ubuntu-vps\n', ' # import os\n', ' # import pyexcel\n', ' # import datetime\n', '\n', ' # from sqlalchemy import create_engine\n', ' # from sqlalchemy.ext.declarative import declarative_base\n', ' # from sqlalchemy import Column, Integer, String, Float, Date\n', ' # from sqlalchemy.orm import sessionmaker\n', '\n', '\n', ' # engine = create_engine(""sqlite:///birth.db"")\n', ' # Base = declarative_base()\n', ' # Session = sessionmaker(bind=engine)\n', '\n', '\n', ' # here is the destination table\n', ' # class BirthRegister(Base):\n', "" #    __tablename__ = 'birth'\n"", ' #    id = Column(Integer, primary_key=True)\n', ' #    name = Column(String)\n', ' #    weight = Column(Float)\n', ' #    birth = Column(Date)\n', '\n', '\n', ' # Base.metadata.create_all(engine)\n', '\n', '\n', ' ## create fixture\n', ' # data = [\n', '    [""name"", ""weight"", ""birth""],\n', '    [""Adam"", 3.4, datetime.date(2015, 2, 3)],\n', '    [""Smith"", 4.2, datetime.date(2014, 11, 12)]\n', ' # ]\n', ' # pyexcel.save_as(array=data,\n', ' #               dest_file_name=""birth.xls"")\n', '\n', ' # # import the xls file\n', ' # session = Session()  # obtain a sql session\n', ' # pyexcel.save_as(file_name=""birth.xls"",\n', ' #                name_columns_by_row=0,\n', ' #                dest_session=session,\n', ' #                dest_table=BirthRegister)\n', '\n', ' # # verify results\n', ' # sheet = pyexcel.get_sheet(session=session, table=BirthRegister)\n', ' # print(sheet)\n', '\n', ' # session.close()\n', "" # os.unlink('birth.db')\n"", ' # os.unlink(""birth.xls"")\n', '\n', ' # This code uses the openpyxl package for playing around with excel using Python code\n', ' # to convert complete excel workbook (all sheets) to an SQLite database\n', ' # The code assumes that the first row of every sheet is the column name\n', ' # Every sheet is stored in a separate table\n', ' # The sheet name is assigned as the table name for every sheet\n', ' """"""\n']","['slugify', 'text.strip', 're.sub', 'load_workbook', 'wb.get_sheet_names', 'str', 'columns.append', 'dbengine.execute', 'enumerate', 'tuprow.append', 'tup.append', 'VALUES', 'dbengine.executemany', 'dbengine.commit', 'dbengine.close']",15
source/utils/util_sql.py:sql_insert_df,sql_insert_df,function,23,62,52,485,7.82,1,2,"['df', 'dbtable', 'dbengine', 'col_drop', 'verbose']","[None, None, None, None, None]","[None, None, None, '[""id""]', '1']",260,[],"['df.drop', 'df.to_dict', 'print', 'sql.Table', 'Session', 'dbengine.execute', 'session.commit', 'session.close']",8
source/utils/util_sql.py:sql_insert_csv,sql_insert_csv,function,43,177,134,1394,7.88,3,3,"['csvfile', 'dbtable', 'dbengine', 'col_drop']","[None, None, None, None]","[None, None, None, '[]']",296,[],"['datetime.now', 'pd.read_csv', 'df.rename', 'c.replace', 'df.drop', 'df.to_dict', 'sql.Table', 'Session', 'dbengine.execute', 'session.commit', 'session.close', 'print', 'sql_insert_csv2', 'os.listdir', 'i.endswith', 'i.startswith', 'df.to_sql']",17
source/utils/util_sql.py:sql_insert_csv2,sql_insert_csv2,function,16,66,59,492,7.45,1,2,"['csvfile', 'dbtable', 'columns', 'dbengine', 'nrows']","[None, None, None, None, None]","['""""', '""""', '[]', 'None', '10000']",370,"['    """"""\n', '    Upload data to a temporary table first using PANDAs to identify optimal data-types for columns\n', '    PANDAS is not speed-efficient as it uses INSERT commands rather than COPY e.g. it took COPY 16mins average\n', '    to get a 15GB CSV into the database (door-to-door) whereas pandas.to_sql took 50mins\n', '    """"""\n']","['os.listdir', 'i.endswith', 'i.startswith', 'print', 'pd.read_csv', 'df.to_sql']",6
source/utils/util_sql.py:sql_postgres_create_table,sql_postgres_create_table,function,29,115,90,950,8.26,1,3,"['mytable', 'database', 'username', 'password']","[None, None, None, None]","['""""', '""""', '""""', '""""']",403,"['    """""" Create table copying the structure of the temp table created using pandas  Timer to benchmark """"""\n']","['psycopg2.connect', 'con.cursor', 'print', 'sys.exit', 'cur.execute', 'time.time', 'os.listdir', 'i.endswith', 'i.startswith', 'open', 'cur.copy_expert', 'con.commit', 'con.close']",13
source/utils/util_sql.py:sql_postgres_query_to_csv,sql_postgres_query_to_csv,function,13,42,40,365,8.69,0,1,"['sqlr', 'csv_out']","[None, None]","['""SELECT ticker,shortratio,sector1_id, FROM stockfundamental""', '""""']",488,"['    """""" Submit query to created PostgreSQL database and output results to a CSV  """"""\n']","['psycopg2.connect', 'con.cursor', 'print', 'open', 'cur.copy_expert', 'con.close']",6
source/utils/util_sql.py:sql_postgres_pivot,sql_postgres_pivot,function,0,1,1,4,4.0,0,0,[],[],[],509,"['    """"""\n', '   # Enabling the Crosstab Function\n', '   # As we previously mentioned, the crosstab function is part of a PostgreSQL extension called tablefunc. To call the crosstab function,\n', '   # you must first enable the tablefunc extension by executing the following SQL command:\n', '   # CREATE extension tablefunc;\n', '\n', '   # SELECT *\n', ""   # FROM crosstab( 'select student, subject, evaluation_result from evaluations order by 1,2')\n"", '   # AS final_result(Student TEXT, Geography NUMERIC,History NUMERIC,Language NUMERIC,Maths NUMERIC,Music NUMERIC);\n', '\n', '\n', '   ##### Correct Even iF there are missing values :\n', '   # http://www.vertabelo.com/blog/technical-articles/creating-pivot-tables-in-postgresql-using-the-crosstab-function\n', '\n', '   # SELECT *\n', ""   # FROM crosstab( 'select student, subject, evaluation_result from evaluations\n"", ""   #                 where extract (month from evaluation_day) = 7 order by 1,2',\n"", ""   #                 'select name from subject order by 1')\n"", '   #      AS final_result(Student TEXT, Geography NUMERIC,History NUMERIC,Language NUMERIC,Maths NUMERIC,Music NUMERIC);\n', '\n', '\n', '   """"""\n']",[],0
source/utils/util_sql.py:sql_mysql_insert_excel,sql_mysql_insert_excel,function,29,84,72,711,8.46,1,0,[],[],[],535,[],"['xlrd.open_workbook', 'list.sheet_by_index', 'MySQLdb.connect', 'database.cursor', 'range', 'sheet.cell', 'cursor.execute', 'cursor.close', 'database.commit', 'database.close', 'print', 'str']",12
source/utils/util_sql.py:sql_pivotable,sql_pivotable,function,1,3,3,10,3.33,0,0,"['dbcon', 'ss']","[None, None]","[None, '""select  ""']",568,"['    """"""\n', '\n', '  # 1) get the category\n', '\n', '  # 2) Build the Pivot From category\n', '  # SELECT *\n', ""  # FROM crosstab( 'select student, subject, evaluation_result from evaluations\n"", ""  #               where extract (month from evaluation_day) = 7 order by 1,2',\n"", ""  #               'select name from subject order by 1')\n"", '  #  AS final_result(Student TEXT, Geography NUMERIC,History NUMERIC,Language NUMERIC,Maths NUMERIC,Music NUMERIC);\n', '\n', '  # https://www.amazon.com/PostgreSQL-High-Performance-Gregory-Smith/dp/184951030X/ref=as_li_ss_tl?s=books&ie=UTF8&qid=1458352081&sr=1-6&keywords=postgres&linkCode=sl1&tag=postgres-bottom-20&linkId=c981783121cbd5542dc2b44a2297df57\n', '\n', '\n', '  # http://blog.brakmic.com/data-science-for-losers-part-2/\n', '\n', '  Here we instruct Pandas to merge two tables by using certain primary keys from both when combining their rows into a new table. The parameter how instructs Pandas to use the inner-join which means it will only combine such rows which belong to both of the tables. Therefore well not receive any NaN-rows. But in some cases this could be desirable. Then use the alternative options like left, right or outer.\n', '\n', '  Pivots with Tables from SQLAlchemy\n', '\n', '  And of course its possible to generate the same pivot tables with data that came from SQLAlchemy.\n', '  Theyre nothing else but DataFrames all the way down. OK, not absolutely all the way down,\n', '  because there are also Series and NumPy arrays etc.,\n', '  but this is a little bit too much of knowledge for Losers like us. Maybe in some later articles.\n', '\n', '\n', '  :return:\n', '  """"""\n']",[],0
source/utils/util_stat.py:np_conditional_entropy,np_conditional_entropy,function,14,26,22,259,9.96,1,0,"['x', 'y']","[None, None]","[None, None]",51,"['    """"""\n', '    Calculates the conditional entropy of x given y: S(x|y)\n', '    Wikipedia: https://en.wikipedia.org/wiki/Conditional_entropy\n', '    **Returns:** float\n', '    Parameters\n', '    ----------\n', '    x : list / NumPy ndarray / Pandas Series\n', '        A sequence of measurements\n', '    y : list / NumPy ndarray / Pandas Series\n', '        A sequence of measurements\n', '    """"""\n']","['Counter', 'sum', 'xy_counter.keys', 'math.log']",4
source/utils/util_stat.py:np_correl_cat_cat_cramers_v,np_correl_cat_cat_cramers_v,function,17,44,33,296,6.73,0,0,"['x', 'y']","[None, None]","[None, None]",74,"['    """"""\n', ""    Calculates Cramer's V statistic for categorical-categorical association.\n"", '    Uses correction from Bergsma and Wicher, Journal of the Korean Statistical Society 42 (2013):\n', '    This is a symmetric coefficient: V(x,y) = V(y,x)\n', '    Original function taken from: https://stackoverflow.com/a/46498792/5863503\n', '    Wikipedia: https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n', '    **Returns:** float in the range of [0,1]\n', '    Parameters\n', '    ----------\n', '    x : list / NumPy ndarray / Pandas Series\n', '        A sequence of categorical measurements\n', '    y : list / NumPy ndarray / Pandas Series\n', '        A sequence of categorical measurements\n', '    """"""\n']","['pd.crosstab', 'confusion_matrix.sum', 'max', 'np.sqrt', 'min']",5
source/utils/util_stat.py:np_correl_cat_cat_theils_u,np_correl_cat_cat_theils_u,function,10,24,21,223,9.29,0,1,"['x', 'y']","[None, None]","[None, None]",100,"['    """"""\n', ""    Calculates Theil's U statistic (Uncertainty coefficient) for categorical-categorical association.\n"", '    This is the uncertainty of x given y: value is on the range of [0,1] - where 0 means y provides no information about\n', '    x, and 1 means y provides full information about x.\n', '    This is an asymmetric coefficient: U(x,y) != U(y,x)\n', '    Wikipedia: https://en.wikipedia.org/wiki/Uncertainty_coefficient\n', '    **Returns:** float in the range of [0,1]\n', '    Parameters\n', '    ----------\n', '    x : list / NumPy ndarray / Pandas Series\n', '        A sequence of categorical measurements\n', '    y : list / NumPy ndarray / Pandas Series\n', '        A sequence of categorical measurements\n', '    """"""\n']","['np_conditional_entropy', 'Counter', 'sum', 'list', 'x_counter.values']",5
source/utils/util_stat.py:np_correl_cat_num_ratio,np_correl_cat_num_ratio,function,25,52,46,630,12.12,1,1,"['cat_array', 'num_array']","[None, None]","[None, None]",125,"['    """"""\n', '    Calculates the Correlation Ratio (sometimes marked by the greek letter Eta) for categorical-continuous association.\n', '    Answers the question - given a continuous value of a measurement, is it possible to know which category is it\n', '    associated with?\n', '    Value is in the range [0,1], where 0 means a category cannot be determined by a continuous measurement, and 1 means\n', '    a category can be determined with absolute certainty.\n', '    Wikipedia: https://en.wikipedia.org/wiki/Correlation_ratio\n', '    **Returns:** float in the range of [0,1]\n', '    Parameters\n', '    ----------\n', '    cat_array : list / NumPy ndarray / Pandas Series   A sequence of categorical measurements\n', '    num_array : list / NumPy ndarray / Pandas Series  A sequence of continuous measurements\n', '    """"""\n']","['convert', 'pd.factorize', 'np.max', 'np.zeros', 'range', 'len', 'np.average', 'np.sum', 'np.power', 'np.sqrt']",10
source/utils/util_stat.py:pd_num_correl_associations,pd_num_correl_associations,function,28,120,62,1097,9.14,2,9,"['df', 'colcat', 'mark_columns', 'theil_u', 'plot', 'return_results', '**kwargs']","[None, None, None, None, None, None, None]","[None, 'None', 'False', 'False', 'True', 'False', None]",159,"['    """"""\n', '    Calculate the correlation/strength-of-association of features in data-set with both categorical (eda_tools) and\n', '    continuous features using:\n', ""     * Pearson's R for continuous-continuous cases\n"", '     * Correlation Ratio for categorical-continuous cases\n', ""     * Cramer's V or Theil's U for categorical-categorical cases\n"", '    **Returns:** a DataFrame of the correlation/strength-of-association between all features\n', '    **Example:** see `associations_example` under `dython.examples`\n', '    Parameters\n', '    ----------\n', '    df : NumPy ndarray / Pandas DataFrame\n', ""        The data-set for which the features' correlation is computed\n"", '    colcat : string / list / NumPy ndarray\n', ""        Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all\n"", '        columns are categorical, or None (default) to state none are categorical\n', '    mark_columns : Boolean, default = False\n', ""        if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or\n"", '        continuous), as provided by colcat\n', '    theil_u : Boolean, default = False\n', ""        In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V\n"", '    plot : Boolean, default = True\n', '        If True, plot a heat-map of the correlation matrix\n', '    return_results : Boolean, default = False\n', '        If True, the function will return a Pandas DataFrame of the computed associations\n', '    kwargs : any key-value pairs\n', '        Arguments to be passed to used function and methods\n', '    """"""\n']","['list', 'pd.DataFrame', 'range', 'len', 'np_correl_cat_cat_theils_u', 'np_correl_cat_cat_cramers_v', 'np_correl_cat_num_ratio', 'corr.fillna']",8
source/utils/util_stat.py:stat_hypothesis_test_permutation,stat_hypothesis_test_permutation,function,24,47,46,696,14.81,1,0,"['df', 'variable', 'classes', 'repetitions']","[None, None, None, None]","[None, None, None, None]",248,"['    """"""Test whether two numerical samples\n', '    come from the same underlying distribution,\n', '    using the absolute difference between the means.\n', '    table: name of table containing the sample\n', '    variable: label of column containing the numerical variable\n', '    classes: label of column containing names of the two samples\n', '    repetitions: number of random permutations""""""\n']","['t.groupby', 'abs', 'means_table.column', 'make_array', 'np.arange', 't.select', 'shuffled.group', 'm_tbl.column', 'np.append', 'np.count_nonzero', 'Table', 'plots.title', 'print']",13
source/utils/util_stat.py:np_transform_pca,np_transform_pca,function,4,5,5,72,14.4,0,0,"['X', 'dimpca', 'whiten']","[None, None, None]","[None, '2', 'True']",294,"['    """"""Project ndim data into dimpca sub-space  """"""\n']","['PCA', 'pca.transform']",2
source/utils/util_stat.py:sk_distribution_kernel_bestbandwidth,sk_distribution_kernel_bestbandwidth,function,9,20,20,182,9.1,0,0,"['X', 'kde']","[None, None]","[None, None]",300,"['    """"""Find best Bandwidht for a  given kernel\n', '  :param kde:\n', '  :return:\n', ' """"""\n']","['GridSearchCV', 'np.linspace', 'grid.fit']",3
source/utils/util_stat.py:sk_distribution_kernel_sample,sk_distribution_kernel_sample,function,14,32,29,211,6.59,1,0,"['kde', 'n']","[None, None]","['None', '1']",314,"['    """"""\n', '  kde = sm.nonparametric.KDEUnivariate(np.array(Y[Y_cluster==0],dtype=np.float64))\n', '  kde = sm.nonparametric.KDEMultivariate()  # ... you already did this\n', ' """"""\n']","['np.zeros', 'func', 'kde.cdf', 'range', 'brentq']",5
source/utils/util_stat.py:dict2,dict2,class,3,5,5,36,7.2,0,0,[],[],[],42,[],[],0
source/utils/util_stat.py:dict2:__init__,dict2:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",44,[],[],0
source/utils/util_text.py:get_stopwords,get_stopwords,function,3,5,5,64,12.8,0,1,['lang'],[None],[None],67,[],['json.load'],1
source/utils/util_text.py:coltext_stemporter,coltext_stemporter,function,4,12,12,103,8.58,0,0,['text'],[None],[None],75,[],['text.split'],1
source/utils/util_text.py:coltext_lemmatizer,coltext_lemmatizer,function,4,11,11,102,9.27,0,0,['text'],[None],[None],85,[],['text.split'],1
source/utils/util_text.py:coltext_stemmer,coltext_stemmer,function,4,11,11,106,9.64,0,0,"['text', 'sep']","[None, None]","[None, '"" ""']",95,[],['text.split'],1
source/utils/util_text.py:coltext_stopwords,coltext_stopwords,function,3,16,13,100,6.25,0,0,"['text', 'stopwords', 'sep']","[None, None, None]","[None, 'None', '"" ""']",101,[],"['text.split', 't.strip']",2
source/utils/util_text.py:pd_coltext_fillna,pd_coltext_fillna,function,2,2,2,29,14.5,0,0,"['df', 'colname', 'val']","[None, None, None]","[None, None, '""""']",107,[],[],0
source/utils/util_text.py:pd_coltext_clean,pd_coltext_clean,function,13,64,40,662,10.34,2,1,"['dfref', 'colname', 'stopwords']","[None, None, None]","[None, None, None]",111,[],"['isinstance', 'Exception', 'x.translate', 're.sub', 'coltext_stopwords', 'pd_coltext_clean_advanced', 'set']",7
source/utils/util_text.py:pd_coltext_clean_advanced,pd_coltext_clean_advanced,function,10,18,15,173,9.61,1,0,"['dfref', 'colname', 'fromword', 'toword']","[None, None, None, None]","[None, None, None, None]",129,[],['set'],1
source/utils/util_text.py:pd_coltext_wordfreq,pd_coltext_wordfreq,function,5,12,10,174,14.5,0,0,"['df', 'coltext', 'sep']","[None, None, None]","[None, None, '"" ""']",141,"['    """"""\n', '    :param df:\n', '    :param coltext:  text where word frequency should be extracted\n', '    :param nb_to_show:\n', '    :return:\n', '    """"""\n']","['pd.value_counts', 'dfres.sort_values']",2
source/utils/util_text.py:pd_fromdict,pd_fromdict,function,12,30,26,229,7.63,1,0,"['ddict', 'colname']","[None, None]","[None, None]",154,"['    """"""\n', '    :param ddict:\n', '    :param colname:\n', '    :return:\n', '    """"""\n']","['ddict.items', 'klist.append', 'xlist.append', 'pd.DataFrame', 'df.sort_values']",5
source/utils/util_text.py:pd_coltext_encoder,pd_coltext_encoder,function,0,1,1,4,4.0,0,0,['df'],[None],[None],170,"['    """"""\n', '    https://dirty-cat.github.io/stable/auto_examples/02_fit_predict_plot_employee_salaries.html#sphx-glr-auto-examples-02-fit-predict-plot-employee-salaries-py\n', '\n', '    :param df:\n', '    :return:\n', '    """"""\n']",[],0
source/utils/util_text.py:pd_coltext_countvect,pd_coltext_countvect,function,26,53,46,616,11.62,0,3,"['df', 'coltext', 'word_tokeep', 'word_minfreq', 'return_val']","[None, None, None, None, None]","[None, None, 'None', '1', '""dataframe,param""']",180,"['    """"""\n', '    Function that adds count of a given column for words in a text corpus.\n', '    Arguments:\n', '        df:             original dataframe\n', '        word_tokeep: corpus of words to look into\n', '        coltext:   column of df to apply tf-idf to\n', '    Returns:\n', '        concat_df:      dataframe with a new column for each word\n', '        https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n', '    """"""\n']","['isinstance', 'Exception', 'CountVectorizer', 'vect.fit_transform', 'vect.fit', 'vect.transform', 'v.toarray', 'vect.get_feature_names', 'np.asarray', 'dict', 'pd.DataFrame']",11
source/utils/util_text.py:pd_coltext_tdidf,pd_coltext_tdidf,function,33,62,56,746,12.03,1,3,"['df', 'coltext', 'word_tokeep', 'word_minfreq', 'return_val']","[None, None, None, None, None]","[None, None, 'None', '1', '""dataframe,param""']",227,"['    """"""\n', '    Function that adds tf-idf of a given column for words in a text corpus.\n', '    Arguments:\n', '        df:             original dataframe\n', '        word_tokeep: corpus of words to look into\n', '        col_tofilter:   column of df to apply tf-idf to\n', '    Returns:\n', '        concat_df:      dataframe with a new column for each word\n', '        https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n', '    """"""\n']","['isinstance', 'Exception', 'CountVectorizer', 'cv.fit_transform', 'cv.get_feature_names', 'np.asarray', 'dict', 'TfidfVectorizer', 'vectorizer.fit', 'vectorizer.transform', 'v.toarray', 'pd.DataFrame']",12
source/utils/util_text.py:pd_coltext_minhash,pd_coltext_minhash,function,27,70,53,647,9.24,1,3,"['dfref', 'colname', 'n_component', 'model_pretrain_dict', 'return_val']","[None, None, None, None, None]","[None, None, '2', 'None', '""dataframe,param""']",277,"['    """"""\n', '    dfhash, colcat_hash_param = pd_colcat_minhash(df, colcat, n_component=[2] * len(colcat),\n', '                                              return_val=""dataframe,param"")\n', '    :param dfref:\n', '    :param colname:\n', '    :param n_component:\n', '    :param return_val:\n', '    :return:\n', '    """"""\n']","['enumerate', 'model_pretrain_dict.get', 'MinHashEncoder', 'clf.fit', 'copy.deepcopy', 'clf.transform', 'pd.DataFrame', 'range', 'pd.concat']",9
source/utils/util_text.py:pd_coltext_hashing,pd_coltext_hashing,function,13,22,21,290,13.18,0,0,"['df', 'coltext', 'n_features']","[None, None, None]","[None, None, '20']",319,"['    """"""\n', '    Function that adds Hash a given column for words in a text corpus.\n', '    Arguments:\n', '        df:             original dataframe\n', '        word_tokeep: corpus of words to look into\n', '        col_tofilter:   column of df to apply tf-idf to\n', '\n', '    Returns:\n', '        concat_df:      dataframe with a new column for each word\n', '    """"""\n']","['HashingVectorizer', 'vectorizer.transform', 'print', 'str', 'range', 'pd.DataFrame']",6
source/utils/util_text.py:pd_coltext_tdidf_multi,pd_coltext_tdidf_multi,function,16,46,33,463,10.07,2,3,"['df', 'coltext', 'coltext_freq', 'ntoken', 'word_tokeep_dict', 'stopwords', 'return_val', '']","[None, None, None, None, None, None, None, None]","[None, None, None, '100', 'None', 'None', '""dataframe,param""', None]",341,[],['pd_coltext_tdidf'],1
source/utils/util_text_embedding.py:test_MDVEncoder,test_MDVEncoder,function,9,51,29,354,6.94,0,0,[],[],[],642,[],"['np.array', 'MDVEncoder', 'encoder.fit', 'np.array_equal']",4
source/utils/util_text_embedding.py:NgramNaiveFisherKernel,NgramNaiveFisherKernel,class,111,399,243,4030,10.1,11,9,[],[],[],63,[],[],0
source/utils/util_text_embedding.py:PretrainedWord2Vec,PretrainedWord2Vec,class,12,35,30,363,10.37,0,0,[],[],[],252,[],[],0
source/utils/util_text_embedding.py:PretrainedBert,PretrainedBert,class,7,44,29,536,12.18,0,2,[],[],[],272,[],[],0
source/utils/util_text_embedding.py:PretrainedGensim,PretrainedGensim,class,41,106,70,1018,9.6,4,4,[],[],[],295,[],[],0
source/utils/util_text_embedding.py:PretrainedFastText,PretrainedFastText,class,33,82,68,779,9.5,3,2,[],[],[],350,[],[],0
source/utils/util_text_embedding.py:AdHocIndependentPDF,AdHocIndependentPDF,class,49,102,82,1048,10.27,4,0,[],[],[],391,[],[],0
source/utils/util_text_embedding.py:NgramsMultinomialMixture,NgramsMultinomialMixture,class,105,288,192,3456,12.0,4,7,[],[],[],427,[],[],0
source/utils/util_text_embedding.py:AdHocNgramsMultinomialMixture,AdHocNgramsMultinomialMixture,class,70,195,129,2132,10.93,3,3,[],[],[],539,[],[],0
source/utils/util_text_embedding.py:MDVEncoder,MDVEncoder,class,27,96,56,715,7.45,4,5,[],[],[],609,[],[],0
source/utils/util_text_embedding.py:PasstroughEncoder,PasstroughEncoder,class,10,20,17,225,11.25,0,0,[],[],[],652,[],[],0
source/utils/util_text_embedding.py:ColumnEncoder,ColumnEncoder,class,110,604,347,8393,13.9,1,15,[],[],[],669,[],[],0
source/utils/util_text_embedding.py:DimensionalityReduction,DimensionalityReduction,class,26,118,93,1180,10.0,0,5,[],[],[],993,[],[],0
source/utils/util_text_embedding.py:NgramNaiveFisherKernel:__init__,NgramNaiveFisherKernel:__init__,method,17,17,17,241,14.18,0,0,"['self', '2', '4)', 'categories', 'dtype', 'handle_unknown', 'hashing_dim', 'n_prototypes', 'random_state', 'n_jobs', '']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, '""auto""', 'np.float64', '""ignore""', 'None', 'None', 'None', 'None', None]",71,[],['super'],1
source/utils/util_text_embedding.py:NgramNaiveFisherKernel:fit,NgramNaiveFisherKernel:fit,method,33,145,111,1372,9.46,2,7,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['self._check_X', 'ValueError', 'isinstance', 'np.all', 'np.array', 'list', 'check_random_state', 'range', 'np.unique', 'get_kmeans_prototypes', 'np.in1d']",11
source/utils/util_text_embedding.py:NgramNaiveFisherKernel:transform,NgramNaiveFisherKernel:transform,method,38,75,65,658,8.77,2,2,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['self._check_X', 'range', 'np.in1d', 'np.all', 'np.unique', 'ValueError', 'sum', 'np.empty', 'enumerate', 'self._ngram_presence_fisher_kernel', 'len']",11
source/utils/util_text_embedding.py:NgramNaiveFisherKernel:_ngram_presence_fisher_kernel,NgramNaiveFisherKernel:_ngram_presence_fisher_kernel,method,46,138,75,1486,10.77,7,0,"['self', 'strings', 'cats']","[None, None, None]","[None, None, None]",189,"['        """""" given to arrays of strings, returns the\n', '        encoding matrix of size\n', '        len(strings) x len(cats)\n', '        kernel fisher with p\n', '        where p is the presence vector\n', '        """"""\n']","['np.unique', 'sum', 'theta.sum', 'CountVectorizer', 'vectorizer.fit_transform', 'vectorizer.transform', 'enumerate', 'np.ones', 'similarity.append', 'gamma_inv.sum', 'sparse.vstack', 'similarity.reshape', 'np.empty', 'len', 'np.nan_to_num', '_ngram_presence_fisher_kernel2', 'np.zeros', 'np.array']",18
source/utils/util_text_embedding.py:NgramNaiveFisherKernel:_ngram_presence_fisher_kernel2,NgramNaiveFisherKernel:_ngram_presence_fisher_kernel2,method,34,65,51,672,10.34,4,0,"['self', 'strings', 'cats']","[None, None, None]","[None, None, None]",221,"['        """""" given to arrays of strings, returns the\n', '        encoding matrix of size\n', '        len(strings) x len(cats)\n', '        kernel fisher with p\n', '        where p is the presence vector\n', '        """"""\n']","['np.unique', 'sum', 'CountVectorizer', 'vectorizer.fit_transform', 'vectorizer.transform', 'enumerate', 'np.zeros', 'similarity.append', 'np.array', 'np.empty', 'len', 'np.nan_to_num']",12
source/utils/util_text_embedding.py:PretrainedWord2Vec:__init__,PretrainedWord2Vec:__init__,method,9,9,9,124,13.78,0,0,"['self', 'n_components', 'language', 'model_path', ""bert_args={'bert_model'"", ""'bert_dataset_name'"", ""'oov'"", ""'ctx'""]","[None, None, None, None, '', ' None', "" 'sum'"", ' None}']","[None, 'None', '""english""', 'None', ""{'bert_model': None"", None, None, None]",254,[],['super'],1
source/utils/util_text_embedding.py:PretrainedWord2Vec:fit,PretrainedWord2Vec:fit,method,0,1,1,4,4.0,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']",[],0
source/utils/util_text_embedding.py:PretrainedWord2Vec:transform,PretrainedWord2Vec:transform,method,0,1,1,4,4.0,0,0,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']",[],0
source/utils/util_text_embedding.py:PretrainedBert:fit,PretrainedBert:fit,method,4,31,19,429,13.84,0,2,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']",['BertEmbedding'],1
source/utils/util_text_embedding.py:PretrainedBert:transform,PretrainedBert:transform,method,3,5,4,56,11.2,0,0,"['self', 'X']","[None, ' list']","[None, None]",289,[],['self.ft_model'],1
source/utils/util_text_embedding.py:PretrainedGensim:fit,PretrainedGensim:fit,method,9,12,12,201,16.75,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['KeyedVectors.load', 'PorterStemmer', 'LancasterStemmer', 'SnowballStemmer']",4
source/utils/util_text_embedding.py:PretrainedGensim:transform,PretrainedGensim:transform,method,17,41,32,347,8.46,2,2,"['self', 'X']","[None, ' dict']","[None, None]",310,[],"['type', 'doc.split', 'corpora.Dictionary', 'np.zeros', 'X.items', 'self.__get_word_embedding']",6
source/utils/util_text_embedding.py:PretrainedGensim:__word_forms,PretrainedGensim:__word_forms,method,7,14,8,181,12.93,0,0,"['self', 'word']","[None, None]","[None, None]",328,[],"['word.lower', 'word.upper', 'word.capitalize']",3
source/utils/util_text_embedding.py:PretrainedGensim:__get_word_embedding,PretrainedGensim:__get_word_embedding,method,7,24,11,166,6.92,2,2,"['self', 'word', 'model']","[None, None, None]","[None, None, None]",337,[],"['self.__word_forms', 'word.strip']",2
source/utils/util_text_embedding.py:PretrainedFastText:__init__,PretrainedFastText:__init__,method,4,4,4,53,13.25,0,0,"['self', 'n_components', 'language']","[None, None, None]","[None, None, '""english""']",355,[],[],0
source/utils/util_text_embedding.py:PretrainedFastText:fit,PretrainedFastText:fit,method,7,29,27,316,10.9,0,1,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['dict', 'path_dict.keys', 'AttributeError', 'load_model']",4
source/utils/util_text_embedding.py:PretrainedFastText:transform,PretrainedFastText:transform,method,21,38,32,312,8.21,3,1,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['X.ravel', 'np.unique', 'dict', 'enumerate', 'x.find', 'np.empty', 'zip']",7
source/utils/util_text_embedding.py:AdHocIndependentPDF:__init__,AdHocIndependentPDF:__init__,method,8,9,9,162,18.0,0,0,"['self', 'fisher_kernel', 'dtype', 'ngram_range', '4']","[None, None, None, None, None]","[None, 'True', 'np.float64', '(2', None]",392,[],['CountVectorizer'],1
source/utils/util_text_embedding.py:AdHocIndependentPDF:fit,AdHocIndependentPDF:fit,method,11,16,15,201,12.56,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['np.unique', 'sum']",2
source/utils/util_text_embedding.py:AdHocIndependentPDF:transform,AdHocIndependentPDF:transform,method,29,64,50,565,8.83,4,0,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['np.unique', 'len', 'enumerate', 'np.ones', 'inv_beta.transpose', 'inv_beta.sum', 'np.zeros', 'np.nan_to_num']",8
source/utils/util_text_embedding.py:NgramsMultinomialMixture:__init__,NgramsMultinomialMixture:__init__,method,16,18,18,280,15.56,0,0,"['self', 'n_topics', 'max_iters', 'fisher_kernel', 'beta_init_type', 'max_mean_change_tol', '2', '4)', '']","[None, None, None, None, None, None, None, None, None]","[None, '10', '100', 'True', 'None', '1e-5', None, None, None]",71,[],['CountVectorizer'],1
source/utils/util_text_embedding.py:NgramsMultinomialMixture:_get_most_frequent,NgramsMultinomialMixture:_get_most_frequent,method,12,15,13,235,15.67,0,0,"['self', 'X']","[None, None]","[None, None]",452,[],"['np.unique', 'np.argsort']",2
source/utils/util_text_embedding.py:NgramsMultinomialMixture:_max_mean_change,NgramsMultinomialMixture:_max_mean_change,method,3,5,4,76,15.2,0,0,"['self', 'last_beta', 'beta']","[None, None, None]","[None, None, None]",460,[],['max'],1
source/utils/util_text_embedding.py:NgramsMultinomialMixture:_e_step,NgramsMultinomialMixture:_e_step,method,24,52,40,734,14.12,2,0,"['self', 'D', 'unqD', 'X', 'unqX', 'theta', 'beta']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",464,[],"['enumerate', 'np.log', 'np.array', 'range', 'logsumexp', 'np.zeros', 'np.exp']",7
source/utils/util_text_embedding.py:NgramsMultinomialMixture:_m_step,NgramsMultinomialMixture:_m_step,method,9,21,20,232,11.05,0,0,"['self', 'D', '_doc_topic_posterior']","[None, None, None]","[None, None, None]",484,[],"['np.dot', 'D.toarray', 'np.divide', 'np.sum']",4
source/utils/util_text_embedding.py:NgramsMultinomialMixture:fit,NgramsMultinomialMixture:fit,method,43,97,71,1194,12.31,2,4,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['np.unique', 'self._get_most_frequent', 'protoD.sum', 'np.ones', 'np.zeros', 'range', 'print', 'self._e_step', 'self._m_step', 'self._max_mean_change']",10
source/utils/util_text_embedding.py:NgramsMultinomialMixture:transform,NgramsMultinomialMixture:transform,method,14,43,31,361,8.4,0,3,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['np.unique', 'type', 'TypeError', 'self._e_step']",4
source/utils/util_text_embedding.py:AdHocNgramsMultinomialMixture:__init__,AdHocNgramsMultinomialMixture:__init__,method,8,9,9,162,18.0,0,0,"['self', 'n_iters', 'fisher_kernel', 'ngram_range', '4']","[None, None, None, None, None]","[None, '10', 'True', '(2', None]",547,[],['CountVectorizer'],1
source/utils/util_text_embedding.py:AdHocNgramsMultinomialMixture:_e_step,AdHocNgramsMultinomialMixture:_e_step,method,24,52,38,638,12.27,2,0,"['self', 'D', 'unqD', 'X', 'unqX', 'theta', 'beta']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",464,[],"['enumerate', 'np.array', 'range', 'P_dz_thetabeta.sum', 'np.zeros']",5
source/utils/util_text_embedding.py:AdHocNgramsMultinomialMixture:_m_step,AdHocNgramsMultinomialMixture:_m_step,method,9,21,20,232,11.05,0,0,"['self', 'D', '_doc_topic_posterior']","[None, None, None]","[None, None, None]",484,[],"['np.dot', 'D.toarray', 'np.divide', 'np.sum']",4
source/utils/util_text_embedding.py:AdHocNgramsMultinomialMixture:fit,AdHocNgramsMultinomialMixture:fit,method,26,45,38,539,11.98,1,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['np.unique', 'len', 'sparse.csr_matrix', 'unqD.sum', 'range', 'self._e_step', 'self._m_step']",7
source/utils/util_text_embedding.py:AdHocNgramsMultinomialMixture:transform,AdHocNgramsMultinomialMixture:transform,method,14,43,31,361,8.4,0,3,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['np.unique', 'type', 'TypeError', 'self._e_step']",4
source/utils/util_text_embedding.py:MDVEncoder:__init__,MDVEncoder:__init__,method,2,2,2,22,11.0,0,0,"['self', 'clf_type']","[None, None]","[None, None]",610,[],[],0
source/utils/util_text_embedding.py:MDVEncoder:fit,MDVEncoder:fit,method,18,53,36,408,7.7,3,2,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['np.unique', 'enumerate']",2
source/utils/util_text_embedding.py:MDVEncoder:transform,MDVEncoder:transform,method,10,31,21,210,6.77,1,3,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['np.zeros', 'len', 'enumerate']",3
source/utils/util_text_embedding.py:PasstroughEncoder:__init__,PasstroughEncoder:__init__,method,2,2,2,28,14.0,0,0,"['self', 'passthrough']","[None, None]","[None, 'True']",653,[],[],0
source/utils/util_text_embedding.py:PasstroughEncoder:fit,PasstroughEncoder:fit,method,4,6,6,83,13.83,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']",['FunctionTransformer'],1
source/utils/util_text_embedding.py:PasstroughEncoder:transform,PasstroughEncoder:transform,method,2,2,2,31,15.5,0,0,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']",[],0
source/utils/util_text_embedding.py:ColumnEncoder:__init__,ColumnEncoder:__init__,method,52,246,162,4816,19.58,0,0,"['self', 'encoder_name', 'reduction_method', '2', '4)', 'categories', 'dtype', 'handle_unknown', 'clf_type', 'n_components', '']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'None', None, None, '""auto""', 'np.float64', '""ignore""', 'None', 'None', None]",71,[],"['OneHotEncoder', 'OneHotEncoderRemoveOne', 'SimilarityEncoder', 'NgramNaiveFisherKernel', 'CountVectorizer', 'TfidfVectorizer', 'TargetEncoder', 'MDVEncoder', 'cat_enc.BackwardDifferenceEncoder', 'cat_enc.BinaryEncoder', 'cat_enc.HashingEncoder', 'cat_enc.HelmertEncoder', 'cat_enc.SumEncoder', 'cat_enc.PolynomialEncoder', 'cat_enc.BaseNEncoder', 'cat_enc.LeaveOneOutEncoder', 'Pipeline', 'LatentDirichletAllocation', 'NMF', 'NgramsMultinomialMixture', 'AdHocNgramsMultinomialMixture', 'AdHocIndependentPDF', 'gamma_poisson_factorization.OnlineGammaPoissonFactorization', 'MinHashEncoder', 'PretrainedFastText', 'FunctionTransformer', 'PasstroughEncoder']",27
source/utils/util_text_embedding.py:ColumnEncoder:_get_most_frequent,ColumnEncoder:_get_most_frequent,method,13,42,41,404,9.62,0,1,"['self', 'X']","[None, None]","[None, None]",452,[],"['np.unique', 'len', 'warnings.warn', 'unqX.ravel', 'np.argsort', 'np.sort']",6
source/utils/util_text_embedding.py:ColumnEncoder:get_feature_names,ColumnEncoder:get_feature_names,method,5,9,7,120,13.33,0,0,['self'],[None],[None],868,[],[],0
source/utils/util_text_embedding.py:ColumnEncoder:fit,ColumnEncoder:fit,method,40,267,151,2602,9.75,1,13,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['ValueError', 'np.all', 'np.array', 'LabelEncoder', 'np.in1d', 'X.reshape', 'len', 'warnings.warn', 'Pipeline', 'DimensionalityReduction']",10
source/utils/util_text_embedding.py:ColumnEncoder:transform,ColumnEncoder:transform,method,9,15,14,169,11.27,0,1,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']",[],0
source/utils/util_text_embedding.py:DimensionalityReduction:__init__,DimensionalityReduction:__init__,method,9,26,25,417,16.04,0,0,"['self', 'method_name', 'n_components', 'column_names']","[None, None, None, None]","[None, 'None', 'None', 'None']",994,[],"['FunctionTransformer', 'GaussianRandomProjection', 'TruncatedSVD', 'PCA']",4
source/utils/util_text_embedding.py:DimensionalityReduction:fit,DimensionalityReduction:fit,method,13,69,53,557,8.07,0,4,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",92,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['ValueError', 'warnings.warn']",2
source/utils/util_text_embedding.py:DimensionalityReduction:transform,DimensionalityReduction:transform,method,5,11,9,87,7.91,0,1,"['self', 'X']","[None, None]","[None, None]",151,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']",['Xout.reshape'],1
source/utils/ztest.py:dict2,dict2,class,3,5,5,36,7.2,0,0,[],[],[],53,[],[],0
source/utils/ztest.py:dict2:__init__,dict2:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",54,[],[],0
source/zold/config_model.py:y_norm,y_norm,function,15,70,41,312,4.46,1,4,"['y', 'inverse', 'mode']","[None, None, None]","[None, 'True', ""'boxcox'""]",6,[],[],0
source/zold/config_model.py:elasticnetcv,elasticnetcv,function,5,77,60,710,9.22,0,0,['path_model_out'],[None],[None],49,[],"['post_process_fun', 'y_norm', 'pre_process_fun']",3
source/zold/config_model.py:lightgbm,lightgbm,function,6,76,59,714,9.39,0,0,['path_model_out'],[None],[None],75,"['    """"""\n', '      Huber Loss includes L1  regurarlization         \n', '      We test different features combinaison, default params is optimal\n', '    """"""\n']","['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy']",4
source/zold/config_model.py:salary_lightgbm,salary_lightgbm,function,6,76,59,714,9.39,0,0,['path_model_out'],[None],[None],109,"['    """"""\n', '      Huber Loss includes L1  regurarlization         \n', '      We test different features combinaison, default params is optimal\n', '    """"""\n']","['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy']",4
source/zold/config_model.py:titanic_lightgbm,titanic_lightgbm,function,5,73,57,644,8.82,0,0,['path_model_out'],[None],[None],143,"['    """"""\n', '\n', '       titanic \n', '    """"""\n']","['post_process_fun', 'y.astype', 'pre_process_fun']",3
source/zold/config_model.py:bayesian_pyro_salary,bayesian_pyro_salary,function,6,89,70,820,9.21,0,0,['path_model_out'],[None],[None],177,[],"['post_process_fun', 'y_norm', 'pre_process_fun', 'copy.deepcopy']",4
source/zold/config_model.py:airbnb_lightgbm,airbnb_lightgbm,function,2,2,2,21,10.5,0,0,"['path_model_out) ', 'because run_preprocess is calling it from this filey)']","['      #bnb model added here', '']","[None, None]",208,"['    """"""\n', '       airbnb\n', '\t\t\t   \n', '    """"""\n']",['y.astype'],1
source/zold/config_model.py:glm_salary,glm_salary,function,5,76,59,687,9.04,0,0,['path_model_out'],[None],[None],254,[],"['post_process_fun', 'y_norm', 'pre_process_fun']",3
source/zold/config_model.py:cardif_lightgbm,cardif_lightgbm,function,2,2,2,21,10.5,0,0,"['path_model_out) ', 'because run_preprocess is calling it from this filey)']","['      #Cardif model added here', '']","[None, None]",281,"['    """"""\n', '       cardif\n', '    """"""\n']",['y.astype'],1
source/zold/run_inference_old.py:log,log,function,3,11,10,65,5.91,0,0,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '1']",28,[],['print'],1
source/zold/run_inference_old.py:preprocess,preprocess,function,52,254,181,3337,13.14,3,8,"['df', 'path_pipeline', 'preprocess_pars']","[None, None, None]","[None, '""data/pipeline/pipe_01/""', '{}']",41,"['    """"""\n', '      FUNCTIONNAL approach is used for pre-processing, so the code can be EASILY extensible to PYSPPARK.\n', '      PYSPARK  supports better UDF, lambda function\n', '    """"""\n']","['log', 'load', 'preprocess_pars.get', 'pd_colcat_toint', 'list', 'pd_col_to_onehot', 'pd_colnum_tocat', 'print', 'pd.DataFrame', 'dfcat_hot.join', 'pd_feature_generate_cross', 'gc.collect', 'locals', 'pd.concat']",14
source/zold/run_inference_old.py:map_model,map_model,function,4,16,10,151,9.44,0,0,['model_name'],[None],[None],148,[],['importlib.import_module'],1
source/zold/run_inference_old.py:predict,predict,function,12,44,34,383,8.7,0,0,"['model_name', 'path_model', 'dfX', 'cols_family']","[None, None, None, None]","[None, None, None, None]",163,"['    """"""\n', ""    if config_name in ['ElasticNet', 'ElasticNetCV', 'LGBMRegressor', 'LGBMModel', 'TweedieRegressor', 'Ridge']:\n"", '        from models import model_sklearn as modelx\n', '\n', ""    elif config_name == 'model_bayesian_pyro':\n"", '        from models import model_bayesian_pyro as modelx\n', '\n', ""    elif config_name == 'model_widedeep':\n"", '        from models import model_widedeep as modelx\n', '    """"""\n']","['map_model', 'modelx.reset', 'log', 'load', 'modelx.predict']",5
source/zold/run_inference_old.py:run_predict,run_predict,function,21,65,54,810,12.46,0,1,"['model_name', 'path_model', 'path_data', 'path_output', 'n_sample']","[None, None, None, None, None]","[None, None, None, None, '-1']",201,[],"['log', 'load', 'load_dataset', 'preprocess', 'predict', 'os.makedirs', 'df.to_csv']",7
source/zold/run_inference_old.py:run_check,run_check,function,28,49,45,718,14.65,0,0,"['path_data', 'path_data_ref', 'path_model', 'path_output', 'sample_ratio']","[None, None, None, None, None]","[None, None, None, None, '0.5']",232,"['    """"""\n', '     Calcualata Dataset Shift before prediction.\n', '    """"""\n']","['os.makedirs', 'load', 'load_dataset', 'preprocess', 'int', 'len', 'util_feature.pd_stat_dataset_shift', 'metrics_psi.to_csv', 'log']",9
source/zold/run_preprocess_old.py:log,log,function,4,21,16,139,6.62,0,0,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '1']",30,[],"['print', 'log_pd', 'df.head']",3
source/zold/run_preprocess_old.py:log_pd,log_pd,function,2,6,6,47,7.83,0,0,"['df', '*s', 'n', 'm']","[None, None, None, None]","[None, None, '0', '1']",37,[],"['print', 'df.head']",2
source/zold/run_preprocess_old.py:save_features,save_features,function,6,18,17,169,9.39,0,2,"['df', 'name', 'path']","[None, None, None]","[None, None, None]",52,"['    """"""\n', '    :param df:\n', '    :param name:\n', '    :param path:\n', '    :return:\n', '    """"""\n']","['os.makedirs', 'isinstance', 'df0.to_parquet']",3
source/zold/run_preprocess_old.py:coltext_stopwords,coltext_stopwords,function,3,16,13,100,6.25,0,0,"['text', 'stopwords', 'sep']","[None, None, None]","[None, 'None', '"" ""']",69,[],"['text.split', 't.strip']",2
source/zold/run_preprocess_old.py:pd_coltext_clean,pd_coltext_clean,function,17,46,34,531,11.54,1,0,"['df', 'col', 'stopwords', 'pars']","[None, None, None, None]","[None, None, ' None', 'None']",75,[],"['pars.get', 'df.fillna', 'log', 'list1.append', 'x.translate', 're.sub', 'coltext_stopwords']",7
source/zold/run_preprocess_old.py:pd_coltext_wordfreq,pd_coltext_wordfreq,function,14,31,25,402,12.97,1,1,"['df', 'col', 'stopwords', 'ntoken']","[None, None, None, None]","[None, None, None, '100']",96,"['    """"""\n', '    :param df:\n', '    :param coltext:  text where word frequency should be extracted\n', '    :param nb_to_show:\n', '    :return:\n', '    """"""\n']","['print', 'coltext_freq.sort_values', 'log']",3
source/zold/run_preprocess_old.py:nlp_get_stopwords,nlp_get_stopwords,function,10,42,30,301,7.17,1,0,[],[],[],119,[],"['json.load', 'stopwords.sort', 'print', 'set']",4
source/zold/run_preprocess_old.py:pipe_text,pipe_text,function,22,60,54,640,10.67,0,0,"['df', 'col', 'pars']","[None, None, None]","[None, None, '{}']",132,[],"['pd_coltext_clean', 'print', 'pd_coltext_wordfreq', 'util_text.pd_coltext_tdidf', 'log', 'util_model.pd_dim_reduction']",6
source/zold/run_preprocess_old.py:preprocess,preprocess,function,105,592,383,6300,10.64,7,15,"['path_train_X', 'path_train_y', 'path_pipeline_export', 'cols_group', 'n_sample', 'preprocess_pars', 'filter_pars', 'path_features_store']","[None, None, None, None, None, None, None, None]","['""""', '""""', '""""', 'None', '5000', '{}', '{}', 'None']",156,[],"['log', 'cols_group.get', 'preprocess_pars.get', 'pipe_list.append', 'load_dataset', 'isfloat', 'float', 'filter_pars.get', 'print', 'y_norm_fun', 'save', 'save_features', 'pd_colnum_normalize', 'pd_colnum_tocat', 'list', 'pd_col_to_onehot', 'pd_colcat_mapping', 'pd_colcat_toint', 'dfcat_hot.join', 'copy.deepcopy', 'colcross_single_onehot_select.append', 'pd_feature_generate_cross', 'nlp_get_stopwords', 'pipe_text', 'pd.concat', 'util_date.pd_datestring_split', 'os.makedirs', 'locals', 'colXy.remove', 'preprocess_load', 'pd.read_parquet', 'dfXy.join', 'load']",33
source/zold/run_preprocess_old.py:preprocess_load,preprocess_load,function,12,32,27,378,11.81,0,0,"['path_train_X', 'path_train_y', 'path_pipeline_export', 'cols_group', 'n_sample', 'preprocess_pars', 'filter_pars', 'path_features_store']","[None, None, None, None, None, None, None, None]","['""""', '""""', '""""', 'None', '5000', '{}', '{}', 'None']",385,[],"['pd.read_parquet', 'dfXy.join', 'log', 'load']",4
source/zold/run_preprocess_old.py:run_preprocess,run_preprocess,function,3,10,10,134,13.4,0,0,"['model_name', 'path_data', 'path_output', 'path_config_model', 'n_sample', 'mode', 'path_output         ', 'mode=\'r\'))cols_group)""#### Model parameters Dynamic loading  ############################################"")uri_name= path_config_model + ""']","[None, None, None, None, None, None, None, '']","[None, None, None, '""source/config_model.py""', '5000', ""'run_preprocess'"", ' root + path_outputpath_data           = root + path_datapath_features_store = path_output + ""/features_store/""path_pipeline_out   = path_output + ""/pipeline/""path_model_out      = path_output + ""/model/""path_check_out      = path_output + ""/check/""path_train_X        = path_data   + ""/features*""    ### Can be a list of zip or parquet filespath_train_y        = path_data   + ""/target*""      ### Can be a list of zip or parquet filespath_output)""#### load input column family  ###################################################"")open(path_data + ""/cols_group.json""', '\'r\'))cols_group)""#### Model parameters Dynamic loading  ############################################"")uri_name= path_config_model + ""::"" + model_name)path_model_out)   ### params""#### Preprocess  #################################################################"")preprocess_pars = model_dict[\'model_pars\'][\'pre_process_pars\']filter_pars     = model_dict[\'data_pars\'][\'filter_pars\']if mode == ""run_preprocess"" :']",405,"['    """"""\n', '      Configuration of the model is in config_model.py file\n', '    """"""\n']",['preprocess'],1
source/zold/zold_prepro_sampler.py:log,log,function,6,26,20,183,7.04,0,1,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '1']",45,[],"['print', 'logs', 'log_pd', 'df.head']",4
source/zold/zold_prepro_sampler.py:logs,logs,function,2,4,4,30,7.5,0,1,['*s'],[None],[None],51,[],['print'],1
source/zold/zold_prepro_sampler.py:log_pd,log_pd,function,2,6,6,47,7.83,0,0,"['df', '*s', 'n', 'm']","[None, None, None, None]","[None, None, '0', '1']",56,[],"['print', 'df.head']",2
source/zold/zold_prepro_sampler.py:pd_export,pd_export,function,11,20,16,244,12.2,0,0,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",65,"['    """"""\n', '       Export in train folder for next training\n', '       colsall\n', '    :param df:\n', '    :param col:\n', '    :param pars:\n', '    :return:\n', '    """"""\n']","['dfX.set_index', 'dfX.to_parquet', 'dfy.set_index']",3
source/zold/zold_prepro_sampler.py:pd_filter_rows,pd_filter_rows,function,20,50,41,320,6.4,0,0,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",90,"['    """"""\n', '       Remove rows based on criteria\n', '    :param df:\n', '    :param col:\n', '    :param pars:\n', '    :return:\n', '    """"""\n']","['isfloat', 'float', 'pars.get', 'filter_pars.get']",4
source/zold/zold_prepro_sampler.py:pd_sample_imblearn,pd_sample_imblearn,function,45,152,109,1310,8.62,2,3,"['df', 'col', 'pars']","[None, None, None]","['None', 'None', 'None']",119,"['    """"""\n', '        Over-sample\n', '    """"""\n']","['params_check', 'locals', 'pars.get', 'model_resample', 'gp.fit_resample', 'pd.DataFrame', 'save_features', 'prefix.replace', 'save']",9
source/zold/zold_prepro_sampler.py:pd_autoencoder,pd_autoencoder,function,49,126,98,1690,13.41,0,2,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",168,"['    """"""""\n', '    (4) Autoencoder\n', '    An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.\n', '    The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction,\n', '    by training the network to ignore noise.\n', '    (i) Feed Forward\n', '    The simplest form of an autoencoder is a feedforward, non-recurrent\n', '    neural network similar to single layer perceptrons that participate in multilayer perceptrons\n', '    """"""\n']","['encoder_dataset', 'df.select_dtypes', 'print', 'encode_objects', 'OrdinalEncoder', 'oe.fit', 'oe.transform', 'minmax_scale', 'pars.get', 'pd.DataFrame', 'encoded_train.add_prefix', 'pd.concat']",12
source/zold/zold_prepro_sampler.py:pd_augmentation_sdv,pd_augmentation_sdv,function,28,171,122,1393,8.15,0,3,"['df', 'col', 'pars']","[None, None, None]","[None, 'None', '{}']",234,"[""    '''\n"", '    Using SDV Variation Autoencoders, the function augments more data into the dataset\n', '    params:\n', '            df          : (pandas dataframe) original dataframe\n', '            col : column name for data enancement\n', '            pars        : (dict - optional) contains:                                          \n', '                n_samples     : (int - optional) number of samples you would like to add, defaul is 10%\n', '                primary_key   : (String - optional) the primary key of dataframe\n', '                aggregate  : (boolean - optional) if False, prints SVD metrics, else it averages them\n', '                path_model_save: saving location if save_model is set to True\n', '                path_model_load: saved model location to skip training\n', '                path_data_new  : new data where saved\n', '    returns:\n', '            df_new      : (pandas dataframe) df with more augmented data\n', '            col         : (list of strings) same columns\n', ""    '''\n""]","['pars.get', 'max', 'int', 'Exception', 'os.system', 'model', 'log', 'model.sample', 'log_pd', 'evaluate', 'df.append', 'len']",12
source/zold/zold_prepro_sampler.py:test_pd_augmentation_sdv,test_pd_augmentation_sdv,function,21,113,66,1243,11.0,0,0,[],[],[],325,[],"['load_boston', 'pd.DataFrame', 'log_pd', 'os.makedirs', 'log', 'pd_augmentation_sdv', 'load_timeseries_demo']",7
source/zold/zold_prepro_sampler.py:pd_covariate_shift_adjustment,pd_covariate_shift_adjustment,function,61,114,99,965,8.46,1,1,[],[],[],373,"['    """"""\n', '    https://towardsdatascience.com/understanding-dataset-shift-f2a5a262a766\n', '     Covariate shift has been extensively studied in the literature, and a number of proposals to work under it have been published. Some of the most important ones include:\n', '        Weighting the log-likelihood function (Shimodaira, 2000)\n', '        Importance weighted cross-validation (Sugiyama et al, 2007 JMLR)\n', '        Integrated optimization problem. Discriminative learning. (Bickel et al, 2009 JMRL)\n', '        Kernel mean matching (Gretton et al., 2009)\n', '        Adversarial search (Globerson et al, 2009)\n', '        Frank-Wolfe algorithm (Wen et al., 2015)\n', '    """"""\n']","['datasets.make_regression', 'FW', 'sparse.dok_matrix', 'range', 'np.argmax', 'np.sign', 'trace.append', 'min', 'q_t.dot', 'np.array', 'plt.plot', 'plt.yscale', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.grid', 'plt.show', 'np.mean', 'print']",19
source/zold/zold_prepro_sampler.py:test,test,function,21,113,66,1243,11.0,0,0,[],[],[],434,"['""""""\n', '\n', '\n', 'def pd_generic_transform(df, col=None, pars={}, model=None)  :\n', ' \n', '     Transform or Samples using  model.fit()   model.sample()  or model.transform()\n', '    params:\n', '            df    : (pandas dataframe) original dataframe\n', '            col   : column name for data enancement\n', '            pars  : (dict - optional) contains:                                          \n', '                path_model_save: saving location if save_model is set to True\n', '                path_model_load: saved model location to skip training\n', '                path_data_new  : new data where saved \n', '    returns:\n', '            model, df_new, col, pars\n', '   \n', ""    path_model_save = pars.get('path_model_save', 'data/output/ztmp/')\n"", ""    pars_model      = pars.get('pars_model', {} )\n"", ""    model_method    = pars.get('method', 'transform')\n"", '    \n', '    # model fitting \n', ""    if 'path_model_load' in pars:\n"", ""            model = load(pars['path_model_load'])\n"", '    else:\n', ""            log('##### Training Started #####')\n"", '            model = model( **pars_model)\n', '            model.fit(df)\n', ""            log('##### Training Finshed #####')\n"", '            try:\n', '                 save(model, path_model_save )\n', ""                 log('model saved at: ' + path_model_save  )\n"", '            except:\n', ""                 log('saving model failed: ', path_model_save)\n"", '\n', ""    log('##### Generating Samples/transform #############')    \n"", ""    if model_method == 'sample' :\n"", ""        n_samples =pars.get('n_samples', max(1, 0.10 * len(df) ) )\n"", '        new_data  = model.sample(n_samples)\n', '        \n', ""    elif model_method == 'transform' :\n"", '        new_data = model.transform(df.values)\n', '    else :\n', '        raise Exception(""Unknown"", model_method)\n', '        \n', '    log_pd( new_data, n=7)    \n', ""    if 'path_newdata' in pars :\n"", ""        new_data.to_parquet( pars['path_newdata'] + '/features.parquet' ) \n"", ""        log('###### df transform save on disk', pars['path_newdata'] )    \n"", '    \n', '    return model, df_new, col, pars\n', '\n', '\n', '\n', '""""""\n']","['load_boston', 'pd.DataFrame', 'log_pd', 'os.makedirs', 'log', 'pd_augmentation_sdv', 'load_timeseries_demo']",7
data/docs/data_generator/gen_utils.py:text1_transform,text1_transform,function,9,21,19,150,7.14,1,0,['text'],[None],[None],24,[],"['text.split', 'len', 'enumerate', 'np.power']",4
data/docs/data_generator/gen_utils.py:text2_transform,text2_transform,function,4,9,9,62,6.89,0,0,['text'],[None],[None],35,[],"['Counter', 'sum']",2
data/docs/data_generator/gen_utils.py:cat1_transform,cat1_transform,function,2,2,2,20,10.0,0,0,['key'],[None],[None],40,[],[],0
data/docs/data_generator/gen_utils.py:cat2_transform,cat2_transform,function,2,2,2,20,10.0,0,0,['key'],[None],[None],44,[],[],0
data/docs/data_generator/gen_utils.py:datetime2_transform,datetime2_transform,function,11,46,27,283,6.15,0,0,['datetime_col'],[None],[None],48,[],['np.array'],1
data/docs/encoder/codility1.py:solution,solution,function,6,19,16,116,6.11,1,0,"['predicted', 'observed']","[None, None]","[None, None]",3,[],"['len', 'range', 'math.sqrt', 'float']",4
data/docs/hunga_bunga/classification.py:run_all_classifiers,run_all_classifiers,function,3,44,30,562,12.77,0,0,"['x', 'y', 'small ', 'normalize_x ', 'n_jobs']","[None, None, None, None, None]","[None, None, ' True', ' True', 'cpu_count(']",177,[],"['main_loop', 'StandardScaler']",2
data/docs/hunga_bunga/classification.py:HungaBungaClassifier,HungaBungaClassifier,class,27,64,59,787,12.3,0,0,[],[],[],182,[],[],0
data/docs/hunga_bunga/classification.py:HungaBungaClassifier:__init__,HungaBungaClassifier:__init__,method,21,24,23,294,12.25,0,0,"['self', 'brain', 'test_size ', 'n_splits ', 'random_state', 'upsample', 'scoring', 'verbose', 'normalize_x ', 'n_jobs ']","[None, None, None, None, None, None, None, None, None, None]","[None, 'False', ' 0.2', ' 5', 'None', 'True', 'None', 'False', ' True', 'cpu_count(']",183,[],['super'],1
data/docs/hunga_bunga/classification.py:HungaBungaClassifier:fit,HungaBungaClassifier:fit,method,3,14,14,256,18.29,0,0,"['self', 'x', 'y']","[None, None, None]","[None, None, None]",197,[],['run_all_classifiers'],1
data/docs/hunga_bunga/classification.py:HungaBungaClassifier:predict,HungaBungaClassifier:predict,method,2,2,2,27,13.5,0,0,"['self', 'x']","[None, None]","[None, None]",201,[],[],0
data/docs/hunga_bunga/core.py:upsample_indices_clf,upsample_indices_clf,function,15,36,32,373,10.36,1,1,"['inds', 'y']","[None, None]","[None, None]",62,[],"['len', 'dict', 'max', 'countByClass.items', 'int', 'extras.append', 'np.concatenate']",7
data/docs/hunga_bunga/core.py:cv_clf,cv_clf,function,5,19,18,198,10.42,1,1,"['x', 'y', 'test_size ', 'n_splits ', 'random_state', 'doesUpsample ']","[None, None, None, None, None, None]","[None, None, ' 0.2', ' 5', 'None', ' True']",75,[],['sss'],1
data/docs/hunga_bunga/core.py:cv_reg,cv_reg,function,6,9,8,64,7.11,0,0,"['x', 'test_size ', 'n_splits ', 'random_state=None)', 'test_size', 'random_state', 'params', 'x', 'y)']","[None, None, None, '', None, None, None, None, '']","[None, ' 0.2', ' 5', 'None): return ss(n_splits', None, 'random_state).split(x)klass', None, None, None]",81,[],"['time', 'klass', 'clf.fit']",3
data/docs/hunga_bunga/core.py:timeit,timeit,function,6,9,8,64,7.11,0,0,"['klass', 'params', 'x', 'y']","[None, None, None, None]","[None, None, None, None]",84,[],"['time', 'klass', 'clf.fit']",3
data/docs/hunga_bunga/core.py:main_loop,main_loop,function,33,179,120,1746,9.75,2,15,"['models_n_params', 'x', 'y', 'isClassification', 'test_size ', 'n_splits ', 'random_state', 'upsample', 'scoring', 'verbose', 'n_jobs ']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, ' 0.2', ' 5', 'None', 'True', 'None', 'True', 'cpu_count(']",91,[],"['cv_', 'cv_clf', 'cv_reg', 'print', 'enumerate', 'len', 'type', 'GridSearchCVProgressBar', 'cv=cv_', 'RandomizedSearchCVProgressBar', 'clf_search.fit', 'timeit', 'pprint', 'res.append', 'traceback.print_exc', 'np.argmax']",16
data/docs/hunga_bunga/core.py:GridSearchCVProgressBar,GridSearchCVProgressBar,class,16,35,30,580,16.57,0,0,[],[],[],28,[],[],0
data/docs/hunga_bunga/core.py:RandomizedSearchCVProgressBar,RandomizedSearchCVProgressBar,class,16,35,30,592,16.91,0,0,[],[],[],45,[],[],0
data/docs/hunga_bunga/core.py:GridSearchCVProgressBar:_get_param_iterator,GridSearchCVProgressBar:_get_param_iterator,method,15,33,29,550,16.67,0,0,['self'],[None],[None],29,[],"['super', 'list', 'len', 'getattr', 'ParallelProgressBar', '__call__', 'tqdm', 'iterable.set_description']",8
data/docs/hunga_bunga/core.py:RandomizedSearchCVProgressBar:_get_param_iterator,RandomizedSearchCVProgressBar:_get_param_iterator,method,15,33,29,562,17.03,0,0,['self'],[None],[None],29,[],"['super', 'list', 'len', 'getattr', 'ParallelProgressBar', '__call__', 'tqdm', 'iterable.set_description']",8
data/docs/hunga_bunga/regression.py:gen_reg_data,gen_reg_data,function,6,15,14,149,9.93,0,0,"['x_mu', 'x_sigma', 'num_samples', 'num_features', 'y_formula', 'y_sigma']","[None, None, None, None, None, None]","['10.', '1.', '100', '3', 'sum', '1.']",287,[],['np.apply_along_axis'],1
data/docs/hunga_bunga/regression.py:run_all_regressors,run_all_regressors,function,3,44,30,563,12.8,0,0,"['x', 'y', 'small ', 'normalize_x ', 'n_jobs']","[None, None, None, None, None]","[None, None, ' True', ' True', 'cpu_count(']",292,[],"['main_loop', 'StandardScaler']",2
data/docs/hunga_bunga/regression.py:HungaBungaRegressor,HungaBungaRegressor,class,26,63,58,785,12.46,0,0,[],[],[],297,[],[],0
data/docs/hunga_bunga/regression.py:HungaBungaRegressor:__init__,HungaBungaRegressor:__init__,method,20,23,22,293,12.74,0,0,"['self', 'brain', 'test_size ', 'n_splits ', 'random_state', 'upsample', 'scoring', 'verbose', 'normalize_x ', 'n_jobs ']","[None, None, None, None, None, None, None, None, None, None]","[None, 'False', ' 0.2', ' 5', 'None', 'True', 'None', 'False', ' True', 'cpu_count(']",298,[],['super'],1
data/docs/hunga_bunga/regression.py:HungaBungaRegressor:fit,HungaBungaRegressor:fit,method,3,14,14,255,18.21,0,0,"['self', 'x', 'y']","[None, None, None]","[None, None, None]",312,[],['run_all_regressors'],1
data/docs/hunga_bunga/regression.py:HungaBungaRegressor:predict,HungaBungaRegressor:predict,method,2,2,2,27,13.5,0,0,"['self', 'x']","[None, None]","[None, None]",316,[],[],0
data/docs/hunga_bunga/__init__.py:HungaBungaZeroKnowledge,HungaBungaZeroKnowledge,class,30,84,66,1169,13.92,0,0,[],[],[],10,[],[],0
data/docs/hunga_bunga/__init__.py:HungaBungaZeroKnowledge:__init__,HungaBungaZeroKnowledge:__init__,method,22,26,25,325,12.5,0,0,"['self', 'brain', 'test_size ', 'n_splits ', 'random_state', 'upsample', 'scoring', 'verbose', 'normalize_x ', 'n_jobs ']","[None, None, None, None, None, None, None, None, None, None]","[None, 'False', ' 0.2', ' 5', 'None', 'True', 'None', 'True', ' True', 'cpu_count(']",11,[],['super'],1
data/docs/hunga_bunga/__init__.py:HungaBungaZeroKnowledge:fit,HungaBungaZeroKnowledge:fit,method,6,32,20,608,19.0,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, None]",26,[],"['HungaBungaClassifier', 'HungaBungaRegressor']",2
data/docs/hunga_bunga/__init__.py:HungaBungaZeroKnowledge:predict,HungaBungaZeroKnowledge:predict,method,2,2,2,27,13.5,0,0,"['self', 'x']","[None, None]","[None, None]",37,[],[],0
data/input/adfraud/generate_train.py:generate_train,generate_train,function,17,202,56,2000,9.9,0,0,"['df', 'col', 'pars']","[None, None, None]","[None, 'None', 'None']",6,"['   """"""\n', '     By IP Apress\n', '           channel                                  1011\n', '      os                                        544\n', '      hour                                      472\n', '      app                                       468\n', '      ip_app_os_device_day_click_time_next_1     320\n', '      app_channel_os_mean_is_attributed         189\n', '      ip_app_mean_is_attributed                 124\n', '      ip_app_os_device_day_click_time_next_2     120\n', '      ip_os_device_count_click_id               113\n', '      ip_var_hour                                94\n', '      ip_day_hour_count_click_id                 91\n', '      ip_mean_is_attributed                      74\n', '      ip_count_click_id                          73\n', '      ip_app_os_device_day_click_time_lag1       67\n', '      app_mean_is_attributed                     67\n', '      ip_nunique_os_device                       65\n', '      ip_nunique_app                             63\n', '      ip_nunique_os                              51\n', '      ip_nunique_app_channel                     49\n', '      ip_os_device_mean_is_attributed            46\n', '      device                                     41\n', '      app_channel_os_count_click_id              37\n', '      ip_hour_mean_is_attributed                 21\n', '\n', '\n', '   """"""\n']","['copy.deepcopy', 'df.drop', 'print', 'pd.to_datetime', 'df.groupby', 'float', 'len']",7
data/input/adfraud/generate_train.py:generatePastClickFeatures,generatePastClickFeatures,function,8,28,24,357,12.75,1,0,['df'],[None],[None],130,[],"['pd.DataFrame', 'x.diff']",2
data/input/adfraud/generate_train.py:count_past_events,count_past_events,function,6,10,9,180,18.0,0,0,['series'],[None],[None],150,[],"['pd.Series', 'print', 'new_series.rolling']",3
data/input/adfraud/generate_train.py:time_diff,time_diff,function,3,4,3,82,20.5,0,0,['series'],[None],[None],181,"['    """"""Returns a series with the time since the last timestamp in seconds.""""""\n']",['series.diff'],1
data/input/adfraud/generate_train.py:a10m,a10m,function,13,18,18,242,13.44,0,0,[],[],[],272,[],"['pd.read_csv', 'generate_train', 'df.drop', 'os.makedirs', 'df_X.to_parquet', 'df_y.to_parquet']",6
data/input/adfraud/generate_train.py:a100k,a100k,function,16,28,27,373,13.32,0,0,[],[],[],287,[],"['pd.read_csv', 'generate_train2', 'df.drop', 'print', 'np.sum', 'os.makedirs', 'df_X.to_parquet', 'df_y.to_parquet', 'df_X.to_csv', 'df_y.to_csv']",10
data/input/adfraud/generate_train.py:a200m,a200m,function,13,18,18,246,13.67,0,0,[],[],[],308,[],"['pd.read_csv', 'generate_train', 'df.drop', 'os.makedirs', 'df_X.to_parquet', 'df_y.to_parquet']",6
data/input/adfraud/generate_train.py:generate_train2,generate_train2,function,34,300,96,3842,12.81,2,0,"['df', 'col', 'pars']","[None, None, None]","[None, 'None', 'None']",360,[],"['copy.deepcopy', 'df.drop', 'pd.to_datetime', 'zip', 'print', 'float', 'len']",7
data/input/adfraud/generate_train.py:generateAggregateFeatures,generateAggregateFeatures,function,13,75,59,740,9.87,1,0,['df'],[None],[None],514,[],"['pd.DataFrame', 'float', 'len', 'print', 'df2.merge', 'gc.collect']",6
data/input/adfraud/train.py:train_lgb_class_imbalance,train_lgb_class_imbalance,function,44,320,205,2578,8.06,0,0,[],[],[],54,[],"['lgb.Dataset', 'lgb_modelfit_nocv', 'child', 'weight', 'lgb_params.update', 'print', 'lgb.train', 'time.time', 'bst.predict', 'metrics.f1_score']",10
data/input/adfraud/train.py:train_model_with_smote_oversampling,train_model_with_smote_oversampling,function,21,68,63,739,10.87,0,0,[],[],[],143,[],"['SMOTE', 'lgb.Dataset', 'lgb.train', 'bst.predict', 'metrics.f1_score', 'print']",6
data/input/adfraud/train.py:plot_model_information,plot_model_information,function,15,56,48,670,11.96,1,0,"['bst', 'validation_metrics', 'my_own_metrics']","[None, None, None]","[None, None, None]",177,[],"['print', 'bst.num_trees', 'lgb.plot_metric', 'plt.show', 'lgb.plot_importance', 'plot_my_own_metrics', 'x=list', 'y=list', 'plt.barh', 'enumerate', 'plt.text', 'str', 'lgb.plot_tree']",13
data/input/adfraud/train.py:train_RF,train_RF,function,14,26,23,295,11.35,0,0,[],[],[],211,[],"['RandomForestClassifier', 'clf.fit', 'clf.predict', 'clf.predict_proba', 'f1_score', 'print']",6
data/input/adfraud/train.py:get_dummies,get_dummies,function,10,37,17,301,8.14,2,2,['df'],[None],[None],235,[],"['isinstance', 'pd.factorize', 'get_dummies', 'df.copy']",4
data/input/adfraud/train.py:get_dummies,get_dummies,function,10,37,17,301,8.14,2,2,['df'],[None],[None],235,[],"['isinstance', 'pd.factorize', 'get_dummies', 'df.copy']",4
data/input/adfraud/train.py:train_test_split_gefs,train_test_split_gefs,function,10,26,24,333,12.81,0,0,"['df', 'ncat', 'train_ratio', 'prep']","[None, None, None, None]","[None, None, '0.7', ""'std'""]",341,[],['pd_colcat_get_catcount'],1
data/input/adfraud/train.py:train_gefs_model,train_gefs_model,function,30,89,69,767,8.62,0,0,[],[],[],372,[],"['print', 'load_dataset', 'train_test_split_gefs', 'RandomForest', 'rf.fit', 'rf.topc', 'gef.classify_avg', 'gef.classify', 'np.max', 'metrics.f1_score']",10
data/input/adfraud/train.py:train_baseline_model,train_baseline_model,function,16,73,70,653,8.95,0,0,[],[],[],471,[],"['print', 'lgb.Dataset', 'lgb.train', 'bst.predict', 'metrics.f1_score']",5
data/input/adfraud/train.py:pd_colcat_get_catcount,pd_colcat_get_catcount,function,16,36,29,240,6.67,1,2,"['df', 'classcol', 'continuous_ids']","[None, None, None]","[None, 'None', '[]']",258,"['    """"""\n', '        Learns the number of categories in each variable and standardizes the df.\n', '        Parameters\n', '        -------\n', '        ncat: numpy m\n', '            The number of categories of each variable. One if the variable is\n', '            continuous.\n', '    """"""\n']","['df.copy', 'np.ones', 'range', 'is_continuous', 'max']",5
data/input/adfraud/train.py:pd_colnum_stats_univariate,pd_colnum_stats_univariate,function,35,96,51,679,7.07,2,3,"['df', 'ncat']","[None, None]","[None, 'None']",280,"['    """"""\n', '        mean, std: numpy m\n', '            The mean and standard deviation of the variable. Zero and one, resp.\n', '            if the variable is categorical.\n', '    """"""\n']","['df.copy', 'np.ones', 'np.zeros', 'range', 'np.max', 'np.min', 'np.mean', 'np.std', 'is_continuous']",9
data/input/adfraud/train.py:normalize_data,normalize_data,function,9,18,16,111,6.17,1,1,"['df', 'maxv', 'minv']","[None, None, None]","[None, None, None]",312,[],"['df.copy', 'range']",2
data/input/adfraud/train.py:standardize_data,standardize_data,function,9,24,20,124,5.17,1,1,"['df', 'mean', 'std']","[None, None, None]","[None, None, None]",320,[],"['df.copy', 'range', 'np.clip']",3
data/input/adfraud/train.py:is_continuous,is_continuous,function,24,80,57,681,8.51,0,2,['df'],[None],[None],330,[],"['np.sum', 'np.round', 'len', 'min', 'any', 'pd_colnum_stats_univariate', 'normalize_data', 'standardize_data']",8
data/input/adfraud/train.py:load_dataset,load_dataset,function,10,34,32,316,9.29,1,1,['df'],[None],[None],361,[],"['df.insert', 'df.pop', 'get_dummies']",3
data/input/airbnb/clean.py:clean_prices,clean_prices,function,14,29,26,325,11.21,1,2,"['df', 'colnum']","[None, None]","[None, None]",34,[],"['clean', 'isinstance', 'np.dtype', 'df.fillna']",4
data/input/airbnb/clean.py:log,log,function,3,11,10,65,5.91,0,0,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '1']",130,[],['print'],1
data/input/airbnb/clean.py:coltext_stopwords,coltext_stopwords,function,3,16,13,100,6.25,0,0,"['text', 'stopwords', 'sep']","[None, None, None]","[None, 'None', '"" ""']",137,[],"['text.split', 't.strip']",2
data/input/airbnb/clean.py:clean_prices,clean_prices,function,14,29,26,325,11.21,1,2,"['df', 'colnum']","[None, None]","[None, None]",34,[],"['clean', 'isinstance', 'np.dtype', 'df.fillna']",4
data/input/airbnb/clean.py:save_features,save_features,function,6,18,17,169,9.39,0,2,"['df', 'name', 'path']","[None, None, None]","[None, None, None]",161,"['    """"""\n', '\n', '    :param df:\n', '    :param name:\n', '    :param path:\n', '    :return:\n', '    """"""\n']","['os.makedirs', 'isinstance', 'df0.to_parquet']",3
data/input/airbnb/clean.py:text_preprocess,text_preprocess,function,72,283,201,2912,10.29,5,2,"['path_train_X', 'path_train_y', 'path_pipeline_export', 'cols_group', 'n_sample', 'preprocess_pars', 'filter_pars', 'path_features_store']","[None, None, None, None, None, None, None, None]","['""""', '""""', '""""', 'None', '5000', '{}', '{}', 'None']",179,"['    """"""\n', '\n', '    :param path_train_X:\n', '    :param path_train_y:\n', '    :param path_pipeline_export:\n', '    :param cols_group:\n', '    :param n_sample:\n', '    :param preprocess_pars:\n', '    :param filter_pars:\n', '    :param path_features_store:\n', '    :return:\n', '    """"""\n']","['log', 'cols_group.get', 'load_dataset', 'json.load', 'stopwords.sort', 'print', 'set', 'pipe_text', 'df.fillna', 'list1.append', 'x.translate', 're.sub', 'coltext_stopwords', 'pd.value_counts', 'coltext_freq.sort_values', 'util_text.pd_coltext_tdidf', 'util_model.pd_dim_reduction', 'save_features', 'pd.concat', 'dftext1.to_csv', 'os.makedirs', 'locals', 'save']",23
data/input/airbnb/clean.py:run_text_preprocess,run_text_preprocess,function,3,10,10,139,13.9,0,0,"['model_name', 'path_data', 'path_output', 'path_config_model', 'n_sample', 'mode', 'path_output         ', 'mode=\'r\'))cols_group)""#### Model parameters Dynamic loading  ############################################"")uri_name= path_config_model + ""']","[None, None, None, None, None, None, None, '']","[None, None, None, '""source/config_model.py""', '5000', ""'run_preprocess'"", ' root + path_outputpath_data           = root + path_datapath_features_store = path_output + ""/features_store/""path_pipeline_out   = path_output + ""/pipeline/""path_model_out      = path_output + ""/model/""path_check_out      = path_output + ""/check/""path_train_X        = path_data   + ""/features*""    ### Can be a list of zip or parquet filespath_train_y        = path_data   + ""/target*""      ### Can be a list of zip or parquet filespath_output)""#### load input column family  ###################################################"")open(path_data + ""/cols_group.json""', '\'r\'))cols_group)""#### Model parameters Dynamic loading  ############################################"")uri_name= path_config_model + ""::"" + model_name)path_model_out)   ### params""#### Preprocess  #################################################################"")preprocess_pars = model_dict[\'model_pars\'][\'pre_process_pars\']filter_pars     = model_dict[\'data_pars\'][\'filter_pars\']if mode == ""run_preprocess"" :']",309,"['    """"""\n', '      Configuration of the model is in config_model.py file\n', '\n', '    """"""\n']",['text_preprocess'],1
data/input/bank/clean.py:categorize_cols,categorize_cols,function,16,36,28,379,10.53,1,2,[],[],[],24,[],"['len', 'str', 'colnum.append', 'coltext.append', 'colcat.append', 'print']",6
data/input/bank/clean.py:profile,profile,function,8,11,11,176,16.0,1,0,[],[],[],74,[],"['os.makedirs', 'pandas_profiling.ProfileReport', 'profile.to_file']",3
data/input/bank/clean.py:create_features,create_features,function,2,2,2,8,4.0,0,0,['df'],[None],[None],89,[],[],0
data/input/bank/clean.py:train_test_split,train_test_split,function,17,35,30,637,18.2,0,0,[],[],[],93,[],"['os.makedirs', 'df.dropna', 'pd.DataFrame', 'int', 'len', 'df1_train.to_csv', 'df1_test.to_csv']",7
data/input/bank/clean.py:save_features,save_features,function,6,18,17,169,9.39,0,2,"['df', 'name', 'path']","[None, None, None]","[None, None, None]",113,[],"['os.makedirs', 'isinstance', 'df0.to_parquet']",3
data/input/income_status/preprocess.py:pd_cleanup,pd_cleanup,function,9,43,27,252,5.86,1,3,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",29,[],"['df.drop', 'df.replace', 'range']",3
data/input/income_status/preprocess.py:pd_normalize_quantile,pd_normalize_quantile,function,41,202,125,2054,10.17,1,6,"['df', 'col', ""'final_weight'"", ""'capital-gain'"", ""'capital-loss'"", ""'hours-per-week']"", 'pars']","[None, None, None, None, None, None, None]","[None, ""['age'"", None, None, None, None, '{}']",45,"['  """"""\n', '     Processor for DSA@\n', '  """"""\n']","['pars.get', 'df.quantile', 'pd.DataFrame', 'len', 'np.where', 'save_features', 'save']",7
data/input/mydata/clean.py:cols_group,cols_group,function,23,79,56,647,8.19,1,2,[],[],[],33,[],"['len', 'print', 'str', 'colcat.append', 'colnum.append', 'coltext.append', 'open', 'json.dump']",8
data/input/mydata/clean.py:profile,profile,function,8,10,10,184,18.4,1,0,[],[],[],99,[],"['os.makedirs', 'df.profile_report']",2
data/input/mydata/clean.py:create_features,create_features,function,3,4,4,95,23.75,0,0,['df)return df) '],[''],[None],118,[],"['os.makedirs', 'create_features']",2
data/input/mydata/clean.py:train_test_split,train_test_split,function,3,4,4,95,23.75,0,0,[],[],[],125,[],"['os.makedirs', 'create_features']",2
data/input/online_shopping/clean.py:cols_group,cols_group,function,22,64,50,661,10.33,1,2,[],[],[],25,[],"['len', 'print', 'str', 'colnum.append', 'coltext.append', 'colcat.append', 'open', 'json.dump']",8
data/input/online_shopping/clean.py:down_sample,down_sample,function,12,21,19,242,11.52,0,0,['df'],[None],[None],66,[],"['print', 'df_class_0.sample', 'pd.concat']",3
data/input/online_shopping/clean.py:profile,profile,function,8,11,11,176,16.0,1,0,[],[],[],78,[],"['os.makedirs', 'pandas_profiling.ProfileReport', 'profile.to_file']",3
data/input/online_shopping/clean.py:create_features,create_features,function,2,2,2,8,4.0,0,0,['df'],[None],[None],93,[],[],0
data/input/online_shopping/clean.py:train_test_split,train_test_split,function,25,55,48,819,14.89,1,0,[],[],[],97,[],"['cols_group', 'print', 'os.makedirs', 'df.dropna', 'down_sample', 'pd.DataFrame', 'int', 'len', 'df1_train.to_csv', 'df1_test.to_csv']",10
data/input/online_shopping/clean.py:save_features,save_features,function,6,18,17,169,9.39,0,2,"['df', 'name', 'path']","[None, None, None]","[None, None, None]",127,[],"['os.makedirs', 'isinstance', 'df0.to_parquet']",3
data/input/rldata/cafem.py:generate_trajectories,generate_trajectories,function,42,89,79,1052,11.82,3,2,['task'],[None],[None],70,[],"['load', 'Env', 'Model', 'tf.ConfigProto', 'tf.Session', 'localsess.run', 'range', 'print', 'env.reset', 'np.copy', 'ma.masked_array', 'env.step', 'tmp_buffer.append']",13
data/input/rldata/cafem.py:main,main,function,83,201,153,2695,13.41,7,1,[],[],[],120,[],"['Model', 'Tasks', 'tf.Session', 'sess.run', 'saver.save', 'tf.trainable_variables', 'updateTargetGraph', 'tqdm', 'tasks.sample', 'Pool', 'np.array', 'pool.close', 'pool.join', 'enumerate', 'generate_trajectories', 'task_buff.sample', 'np.split', 'np.reshape', 'np.clip', 'inputsa.append', 'labela.append', 'inputsb.append', 'labelb.append', 'actiona.append', 'actionb.append', 'print', 'updateTarget', 'open', 'f.write', 'f.close']",30
data/input/rldata/cafem.py:Tasks,Tasks,class,27,51,43,600,11.76,3,2,[],[],[],46,[],[],0
data/input/rldata/cafem.py:Tasks:__init__,Tasks:__init__,method,22,42,36,489,11.64,3,2,"['self', 'datasetids', 'buffer_size']","[None, None, None]","[None, None, '100']",47,[],"['pd.read_csv', 'open', 'tqdm', 'load', 'enumerate', 'f.write', 'f.close']",7
data/input/rldata/cafem.py:Tasks:sample,Tasks:sample,method,3,4,3,45,11.25,0,0,"['self', 'n']","[None, None]","[None, None]",65,[],['random.sample'],1
data/input/rldata/MLFE.py:one_mse_func,one_mse_func,function,9,15,13,230,15.33,0,0,[],[],[],29,[],"['one_relative_abs', 'mean_absolute_error', 'np.mean', 'np.abs', 'make_scorer']",5
data/input/rldata/MLFE.py:load,load,function,24,38,33,411,10.82,1,3,['f_path'],[None],[None],115,[],"['LabelEncoder', 'loadarff', 'np.array', 'meta.names', 'meta.types', 'enumerate', 'le.fit_transform', 'dataset.astype']",8
data/input/rldata/MLFE.py:updateTargetGraph,updateTargetGraph,function,7,25,20,215,8.6,1,0,"['tfVars', 'tau']","[None, None]","[None, None]",805,[],"['len', 'enumerate', 'op_holder.append']",3
data/input/rldata/MLFE.py:updateTarget,updateTarget,function,10,32,26,279,8.72,2,0,"['tfVars', 'tau']","[None, None]","[None, None]",813,[],"['len', 'enumerate', 'op_holder.append', 'updateTarget', 'sess.run']",5
data/input/rldata/MLFE.py:performance,performance,function,20,43,36,459,10.67,3,0,"['did', 'maxfeat', 'step']","[None, None, None]","[None, None, None]",983,[],"['load', 'Env', 'range', 'pd.read_csv', 'actions.append', 'trfs.append', 'env.batch_perform']",7
data/input/rldata/MLFE.py:Evaluater,Evaluater,class,63,147,105,1785,12.14,1,6,[],[],[],43,[],[],0
data/input/rldata/MLFE.py:Env,Env,class,191,931,396,10896,11.7,19,47,[],[],[],138,[],[],0
data/input/rldata/MLFE.py:Buffer,Buffer,class,12,32,25,438,13.69,0,2,[],[],[],585,[],[],0
data/input/rldata/MLFE.py:Model,Model,class,136,469,287,6412,13.67,7,3,[],[],[],605,[],[],0
data/input/rldata/MLFE.py:Evaluater:__init__,Evaluater:__init__,method,19,52,33,727,13.98,0,4,"['self', 'cv', 'stratified', 'n_jobs', 'tasktype', 'evaluatertype', 'n_estimators', '100000))']","[None, None, None, None, None, None, None, '']","[None, '5', 'True', '1', '""C""', '""rf""', '20', None]",45,[],"['StratifiedKFold', 'KFold', 'RandomForestClassifier', 'RandomForestRegressor', 'LogisticRegression', 'Lasso']",6
data/input/rldata/MLFE.py:Evaluater:CV,Evaluater:CV,method,29,46,42,524,11.39,1,1,"['self', 'X', 'y']","[None, None, None]","[None, None, None]",73,[],"['np.nan_to_num', 'np.clip', 'one_mse_func', 'cross_val_score', 'abs', 'CV2', 'res.append', 'np.array']",8
data/input/rldata/MLFE.py:Evaluater:CV2,Evaluater:CV2,method,19,26,25,306,11.77,1,0,"['self', 'X', 'y']","[None, None, None]","[None, None, None]",85,[],"['res.append', 'np.array']",2
data/input/rldata/MLFE.py:Evaluater:metrics,Evaluater:metrics,method,15,32,26,346,10.81,0,1,"['self', 'y_true', 'y_pred']","[None, None, None]","[None, None, None]",102,[],"['f1_score', 'roc_auc_score', 'log_loss', 'mean_absolute_error', 'np.mean', 'mean_squared_error']",6
data/input/rldata/MLFE.py:Env:__init__,Env:__init__,method,50,86,75,1211,14.08,3,4,"['self', 'dataset', 'feature', 'globalreward', 'maxdepth', 'evalcount', 'binsize', 'opt_type', 'tasktype', 'evaluatertype', '\\100000)', 'historysize', 'pretransform', 'n_jobs=1)']","[None, None, None, None, None, None, None, None, None, None, None, None, None, '']","[None, None, None, 'True', '5', '10', '100', ""'o1'"", '""C""', ""'rf'"", None, '5', 'None', '1):']",139,[],"['np.array', 'len', 'range', 'print', 'self.fe', 'value.append', 'Evaluater', 'np.copy', 'self._init']",9
data/input/rldata/MLFE.py:Env:_init,Env:_init,method,24,40,37,735,18.38,0,0,['self'],[None],[None],192,[],"['np.copy', 'self._QSA', 'np.concatenate', 'np.array']",4
data/input/rldata/MLFE.py:Env:node2root,Env:node2root,method,8,17,15,188,11.06,2,0,"['self', 'adict', 'node']","[None, None, None]","[None, None, None]",222,[],"['apath.append', 'range']",2
data/input/rldata/MLFE.py:Env:step,Env:step,method,100,399,218,4709,11.8,7,23,"['self', 'action']","[None, None]","[None, None]",231,[],"['set', 'getattr', 'feature.min', 'np.log', 'np.sqrt', 'MinMaxScaler', 'mmn.fit_transform', 'np.var', 'stats.zscore', 'np.nan_to_num', 'np.clip', 'math.sqrt', 'np.insert', 'self.node2root', 'np.concatenate', 'len', 'np.copy', 'self._QSA', 'range', 'abs', 'np.array', 'allperf.argmax', 'allperf.max']",23
data/input/rldata/MLFE.py:Env:_QSA,Env:_QSA,method,27,146,60,1570,10.75,1,7,['self'],[None],[None],432,[],"['np.median', 'feat_0.min', 'abs', 'np.arange', 'np.bincount', 'len', 'feat_1.min', 'np.concatenate', 'range', 'feat_0.max', 'feat_1.max', 'QSA.append']",12
data/input/rldata/MLFE.py:Env:reset,Env:reset,method,5,5,5,75,15.0,0,0,['self'],[None],[None],486,[],['self._init'],1
data/input/rldata/MLFE.py:Env:fe,Env:fe,method,39,221,104,2054,9.29,6,13,"['self', 'operators', 'feat_id']","[None, None, None]","[None, None, None]",492,[],"['type', 'set', 'getattr', 'feature.min', 'np.log', 'np.sqrt', 'MinMaxScaler', 'mmn.fit_transform', 'np.var', 'stats.zscore', 'len', 'np.nan_to_num', 'np.clip', 'math.sqrt', 'np.insert', 'np.delete', 'range']",17
data/input/rldata/MLFE.py:Buffer:__init__,Buffer:__init__,method,3,4,4,43,10.75,0,0,"['self', 'buffer_size ']","[None, None]","[None, ' 50000']",586,[],[],0
data/input/rldata/MLFE.py:Buffer:add,Buffer:add,method,3,8,8,129,16.12,0,1,"['self', 'experience']","[None, None]","[None, None]",590,[],['len'],1
data/input/rldata/MLFE.py:Buffer:sample,Buffer:sample,method,5,11,10,180,16.36,0,1,"['self', 'size']","[None, None]","[None, None]",595,[],"['len', 'np.copy']",2
data/input/rldata/MLFE.py:Model:__init__,Model:__init__,method,35,61,44,1002,16.43,0,1,"['self', 'opt_size', 'input_size', 'name', 'meta', 'update_lr', 'meta_lr', 'num_updates', 'maml', 'qsasize']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, 'False', '1e-3', '0.001', '1', 'True', '200']",606,[],"['tf.placeholder', 'self.construct_fc_weights', 'self.network', 'self.construct_model', 'tf.global_variables_initializer']",5
data/input/rldata/MLFE.py:Model:mse,Model:mse,method,2,3,3,45,15.0,0,0,"['self', 'y_pred', 'y_true']","[None, None, None]","[None, None, None]",635,[],['tf.reduce_sum'],1
data/input/rldata/MLFE.py:Model:construct_fc_weights,Model:construct_fc_weights,method,14,83,55,1161,13.99,1,1,['self'],[None],[None],638,[],"['tf.Variable', 'range', 'len', 'str', 'tf.truncated_normal']",5
data/input/rldata/MLFE.py:Model:forward,Model:forward,method,14,114,47,1237,10.85,3,1,"['self', 'inp', 'weights', 'reuse']","[None, None, None, None]","[None, None, None, 'False']",668,[],"['normalize', 'range', 'len', 'str', 'scope=str', 'tf.matmul']",6
data/input/rldata/MLFE.py:Model:L2loss,Model:L2loss,method,5,11,9,97,8.82,1,0,"['self', 'weights', 'reg']","[None, None, None]","[None, None, None]",702,[],[],0
data/input/rldata/MLFE.py:Model:network,Model:network,method,12,19,19,376,19.79,0,0,['self'],[None],[None],708,[],"['self.forward', 'tf.one_hot', 'tf.reduce_sum', 'self.loss_func']",4
data/input/rldata/MLFE.py:Model:construct_model,Model:construct_model,method,67,155,129,2194,14.15,2,0,['self'],[None],[None],719,[],"['tf.variable_scope', 'training_scope.reuse_variables', 'forward_Q', 'tf.one_hot', 'tf.reduce_sum', 'task_metalearn', 'self.loss_func', 'tf.gradients', 'list', 'dict', 'zip', 'weights.keys', 'task_outputbs.append', 'task_lossesb.append', 'range', 'fast_weights.keys', 'tf.map_fn', 'tf.to_float', 'optimizer.compute_gradients', 'optimizer.apply_gradients']",20
data/input/rldata/scripted_runner.py:run_cafem,run_cafem,function,3,4,3,134,33.5,0,0,[],[],[],2,[],['subprocess.Popen'],1
data/input/rldata/scripted_runner.py:run_single_afem,run_single_afem,function,3,4,3,209,52.25,0,0,[],[],[],6,[],['subprocess.Popen'],1
data/input/rldata/single_afem.py:simulate,simulate,function,35,60,54,956,15.93,1,4,['inp'],[None],[None],8,[],"['Model', 'tf.ConfigProto', 'tf.Session', 'saver.restore', 'env.reset', 'range', 'np.copy', 'localsess.run', 'ma.masked_array', 'env.step', 'tmp_buffer.append', 'localsess.close']",12
data/input/rldata/single_afem.py:main,main,function,149,493,332,5848,11.86,7,15,[],[],[],55,[],"['os.mkdir', 'load', 'Model', 'tf.ConfigProto', 'tf.Session', 'tqdm', 'Buffer', 'len', 'print', 'saver.restore', 'saver.save', 'sess.run', 'Env', 'open', 'f.write', 'f.close', 'tf.trainable_variables', 'updateTargetGraph', 'Pool', 'pool.map', 'range', 'pool.close', 'pool.join', 'buff.add', 'best_seq.append', 'best_pfm.append', 'simulate', 'buff.sample', 'np.split', 'np.array', 'np.reshape', 'enumerate', 'np.clip', 'mean_loss.append', 'updateTarget', 'np.around', 'ma.masked_array', 'pretransform.append', 'max', 'np.copy', 'env.step', 'pretransform_test.append', 'test_pfm.append']",43
data/input/rldata/utils.py:normalize,normalize,function,2,2,2,21,10.5,0,0,"['inp', 'activation', 'reuse', 'scope', 'norm']","[None, None, None, None, None]","[None, None, None, None, None]",33,[],['activation'],1
data/input/rldata/utils.py:load_pretransform,load_pretransform,function,12,19,16,445,23.42,0,0,['fdir'],[None],[None],47,[],"['pd.read_csv', 'transform.fillna', 'np.argmax', 'print', 'int']",5
data/input/rldata/utils.py:get_result,get_result,function,17,36,31,742,20.61,1,1,"['mark', 'did', 'plot']","[None, None, None]","[None, None, 'True']",63,[],"['pd.read_csv', 'score_te.drop_duplicates', 'res.append', 'plt.plot', 'plt.show', 'print', 'pd.DataFrame']",7
data/input/rldata/utils.py:plot,plot,function,42,133,106,1586,11.92,3,1,"['fpath1', 'fpath2', 'size', 'name']","[None, None, None, None]","[None, None, '30', ""''""]",90,[],"['pd.read_csv', 'plt.figure', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'plt.xticks', 'plt.plot', 'len', 'range', 'plt.legend', 'plt.show', 'plot2', 'np.arange', 'plt.subplots', 'ax.bar', 'plt.yticks', 'zip', 'plt.text', 'ax.set_ylabel', 'ax.set_title', 'ax.legend']",21
data/input/rldata/utils.py:plot2,plot2,function,29,96,77,959,9.99,2,0,[],[],[],110,[],"['np.arange', 'plt.subplots', 'ax.bar', 'plt.yticks', 'zip', 'plt.text', 'ax.set_ylabel', 'ax.set_title', 'plt.xticks', 'ax.legend', 'plt.show']",11
data/input/tseries/test.py:data_copy,data_copy,function,9,12,9,209,17.42,0,0,[],[],[],47,[],"['pd.read_csv', 'df.dropna', 'pd.to_datetime', 'df.set_index']",4
data/input/tseries/test.py:robust_scaler,robust_scaler,function,14,27,21,224,8.3,0,2,"['df', 'drop', 'quantile_range', '75']","[None, None, None, None]","[None, 'None', '(25', None]",270,[],"['df.drop', 'np.median', 'np.percentile', 'pd.concat']",4
data/input/tseries/test.py:standard_scaler,standard_scaler,function,11,24,17,160,6.67,0,2,"['df', 'drop']","[None, None]","[None, None]",293,[],"['df.drop', 'np.mean', 'np.std', 'pd.concat']",4
data/input/tseries/test.py:fast_fracdiff,fast_fracdiff,function,17,38,34,231,6.08,1,0,"['x', 'cols', 'd']","[None, None, None]","[None, None, None]",318,[],"['len', 'int', 'np.ceil', 'np.arange', 'tuple', 'pl.ifft', 'pl.fft', 'np.real']",8
data/input/tseries/test.py:outlier_detect,outlier_detect,function,21,93,51,1214,13.05,0,4,"['data', 'col', 'threshold', 'method']","[None, None, None, None]","[None, None, '1', '""IQR""']",342,[],"['np.median', 'pd.Series', 'np.abs', 'print', 'pd.concat', 'tmp.any']",6
data/input/tseries/test.py:windsorization,windsorization,function,8,23,15,311,13.52,0,1,"['data', 'col', 'para', 'strategy']","[None, None, None, None]","[None, None, None, ""'both'""]",371,"['    """"""\n', '    top-coding & bottom coding (capping the maximum of a distribution at an arbitrarily set value,vice versa)\n', '    """"""\n']",['data.copy'],1
data/input/tseries/test.py:operations,operations,function,21,53,36,691,13.04,0,0,"['df', 'features']","[None, None]","[None, None]",396,[],"['df_new.min', 'pd.DataFrame', 'pd.concat']",3
data/input/tseries/test.py:initial_trend,initial_trend,function,5,14,12,83,5.93,1,0,"['series', 'slen']","[None, None]","[None, None]",427,[],"['range', 'float']",2
data/input/tseries/test.py:initial_seasonal_components,initial_seasonal_components,function,12,28,20,341,12.18,3,0,"['series', 'slen']","[None, None]","[None, None]",433,[],"['int', 'range', 'season_averages.append']",3
data/input/tseries/test.py:triple_exponential_smoothing,triple_exponential_smoothing,function,27,60,50,615,10.25,2,2,"['df', 'cols', 'slen', 'alpha', 'beta', 'gamma', 'n_preds']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",448,[],"['initial_seasonal_components', 'range', 'initial_trend', 'result.append', 'len']",5
data/input/tseries/test.py:naive_dec,naive_dec,function,7,19,18,228,12.0,1,0,"['df', 'columns', 'freq']","[None, None, None]","[None, None, '2']",484,[],[],0
data/input/tseries/test.py:bkb,bkb,function,6,11,11,94,8.55,1,0,"['df', 'cols']","[None, None]","[None, None]",507,[],['len'],1
data/input/tseries/test.py:butter_lowpass,butter_lowpass,function,15,34,30,276,8.12,1,0,"['cutoff', 'fs', 'order']","[None, None, None]","[None, '20', '5']",520,[],"['signal.butter', 'butter_lowpass_filter', 'butter_lowpass', 'signal.lfilter']",4
data/input/tseries/test.py:butter_lowpass_filter,butter_lowpass_filter,function,9,15,15,110,7.33,1,0,"['df', 'cols', 'cutoff', 'fs', 'order']","[None, None, None, None, None]","[None, None, None, '20', '5']",526,[],"['butter_lowpass', 'signal.lfilter']",2
data/input/tseries/test.py:instantaneous_phases,instantaneous_phases,function,6,10,10,98,9.8,1,0,"['df', 'cols']","[None, None]","[None, None]",542,[],['np.unwrap'],1
data/input/tseries/test.py:kalman_feat,kalman_feat,function,10,28,26,356,12.71,1,0,"['df', 'cols']","[None, None]","[None, None]",558,[],"['UnscentedKalmanFilter', 'np.sin', 'ukf.filter', 'ukf.smooth', 'smoothed_state_means.flatten', 'filtered_state_means.flatten']",6
data/input/tseries/test.py:perd_feat,perd_feat,function,7,13,13,128,9.85,1,0,"['df', 'cols']","[None, None]","[None, None]",579,[],['signal.periodogram'],1
data/input/tseries/test.py:fft_feat,fft_feat,function,10,18,15,238,13.22,1,0,"['df', 'cols']","[None, None]","[None, None]",593,[],"['pd.DataFrame', 'np.abs', 'np.angle']",3
data/input/tseries/test.py:harmonicradar_cw,harmonicradar_cw,function,15,25,24,178,7.12,1,0,"['df', 'cols', 'fs', 'fc']","[None, None, None, None]","[None, None, None, None]",611,[],"['np.sin', 'signal.welch']",2
data/input/tseries/test.py:saw,saw,function,7,9,9,61,6.78,1,0,"['df', 'cols']","[None, None]","[None, None]",633,[],['signal.sawtooth'],1
data/input/tseries/test.py:modify,modify,function,34,64,37,767,11.98,1,0,"['df', 'cols']","[None, None]","[None, None]",648,[],"['magnify', 'affine', 'crop', 'cross_sum', 'resample', 'trend', 'random_time_warp', 'random_crop', 'random_cross_sum', 'random_sidetrack', 'random_magnify', 'random_jitter', 'random_trend']",13
data/input/tseries/test.py:multiple_rolling,multiple_rolling,function,25,65,49,462,7.11,4,1,"['df', 'windows ', '2]', 'functions', '""std""]', 'columns']","[None, None, None, None, None, None]","[None, ' [1', None, '[""mean""', None, 'None']",677,[],['pd.concat'],1
data/input/tseries/test.py:multiple_lags,multiple_lags,function,9,30,25,192,6.4,2,1,"['df', 'start', 'end', 'columns']","[None, None, None, None]","[None, '1', '3', 'None']",702,[],"['range', 'df.assign']",2
data/input/tseries/test.py:prophet_feat,prophet_feat,function,21,42,37,555,13.21,1,0,"['df', 'cols', 'date', 'freq', 'train_size']","[None, None, None, None, None]","[None, None, None, None, '150']",729,[],"['prophet_dataframe', 'original_dataframe', 'pd.DataFrame', 'prophet_pred.set_index', 'Prophet', 'model.fit', 'len', 'model.make_future_dataframe', 'model.predict', 'list']",10
data/input/tseries/test.py:lowess,lowess,function,33,91,70,648,7.12,3,0,"['df', 'cols', 'y', 'f', 'iter']","[None, None, None, None, None]","[None, None, None, '2. / 3.', '3']",771,[],"['len', 'int', 'range', 'np.clip', 'np.zeros', 'np.ones', 'np.array', 'np.sum', 'linalg.solve', 'np.median']",10
data/input/tseries/test.py:autoregression,autoregression,function,21,43,34,459,10.67,2,2,"['df', 'drop', 'settings={""autoreg_lag""']","[None, None, '']","[None, 'None', '{""autoreg_lag"":4}']",806,[],"['df.drop', 'timer', 'np.zeros', 'range', 'AR', 'len', 'np.real', 'pd.concat']",8
data/input/tseries/test.py:muldiv,muldiv,function,11,21,18,222,10.57,2,1,"['df', 'feature_list']","[None, None]","[None, None]",841,[],[],0
data/input/tseries/test.py:decision_tree_disc,decision_tree_disc,function,12,19,18,245,12.89,1,0,"['df', 'cols', 'depth']","[None, None, None]","[None, None, '4']",865,[],"['DecisionTreeRegressor', 'tree_model.fit', 'tree_model.predict']",3
data/input/tseries/test.py:quantile_normalize,quantile_normalize,function,19,39,29,292,7.49,3,2,"['df', 'drop']","[None, None]","[None, None]",888,[],"['df.drop', 'dic.update', 'sorted', 'pd.DataFrame', 'sorted_df.mean', 'np.searchsorted', 'pd.concat']",7
data/input/tseries/test.py:haversine_distance,haversine_distance,function,14,30,26,230,7.67,0,0,"['row', 'lon', 'lat']","[None, None, None]","[None, '""Open""', '""Close""']",921,[],"['radians', 'sin', 'cos', 'atan2', 'sqrt']",5
data/input/tseries/test.py:tech,tech,function,2,7,7,96,13.71,0,0,['df'],[None],[None],945,[],['ta.add_all_ta_features'],1
data/input/tseries/test.py:genetic_feat,genetic_feat,function,10,37,36,524,14.16,0,0,"['df', 'num_gen', 'num_comp']","[None, None, None]","[None, '20', '10']",963,[],"['SymbolicTransformer', 'gp.fit_transform', 'pd.DataFrame', 'range', 'pd.concat']",5
data/input/tseries/test.py:pca_feature,pca_feature,function,21,69,50,803,11.64,0,5,"['df', 'memory_issues', 'mem_iss_component', 'variance_or_components', 'n_components', 'drop_cols', 'non_linear']","[None, None, None, None, None, None, None]","[None, 'False', 'False', '0.80', '5', 'None', 'True']",996,[],"['KernelPCA', 'ValueError', 'IncrementalPCA', 'PCA', 'pca.fit_transform', 'pd.concat', 'range', 'pd.DataFrame']",8
data/input/tseries/test.py:cross_lag,cross_lag,function,18,34,23,367,10.79,0,2,"['df', 'drop', 'lags', 'components']","[None, None, None, None]","[None, 'None', '1', '4']",1034,[],"['df.drop', 'df.shift', 'df_2.dropna', 'CCA', 'cca.fit', 'cca.transform', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",9
data/input/tseries/test.py:a_chi,a_chi,function,16,31,21,394,12.71,0,2,"['df', 'drop', 'lags', 'sample_steps']","[None, None, None, None]","[None, 'None', '1', '2']",1070,[],"['df.drop', 'df.shift', 'df_2.dropna', 'AdditiveChi2Sampler', 'chi2sampler.fit_transform', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",8
data/input/tseries/test.py:encoder_dataset,encoder_dataset,function,25,66,50,989,14.98,0,2,"['df', 'drop', 'dimesions']","[None, None, None]","[None, 'None', '20']",1108,[],"['minmax_scale', 'pd.DataFrame', 'encoded_train.add_prefix', 'pd.concat']",4
data/input/tseries/test.py:lle_feat,lle_feat,function,13,21,15,242,11.52,0,2,"['df', 'drop', 'components']","[None, None, None]","[None, 'None', '4']",1155,[],"['df.drop', 'LocallyLinearEmbedding', 'embedding.fit_transform', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",6
data/input/tseries/test.py:feature_agg,feature_agg,function,14,23,17,294,12.78,0,2,"['df', 'drop', 'components']","[None, None, None]","[None, 'None', '4']",1183,[],"['df.drop', 'min', 'cluster.FeatureAgglomeration', 'agglo.fit', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",7
data/input/tseries/test.py:neigh_feat,neigh_feat,function,15,28,20,304,10.86,0,2,"['df', 'drop', 'neighbors']","[None, None, None]","[None, None, '6']",1214,[],"['df.drop', 'min', 'NearestNeighbors', 'neigh.fit', 'neigh.kneighbors', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",8
data/input/tseries/test.py:set_property,set_property,function,7,25,22,177,7.08,0,1,"['key', 'value']","[None, None]","[None, None]",1251,"['    """"""\n', '    This method returns a decorator that sets the property key of the function to value\n', '    """"""\n']","['decorate_func', 'setattr']",2
data/input/tseries/test.py:abs_energy,abs_energy,function,5,10,10,76,7.6,0,1,['x'],[None],[None],1272,[],"['isinstance', 'np.asarray', 'np.dot']",3
data/input/tseries/test.py:cid_ce,cid_ce,function,10,24,19,166,6.92,0,3,"['x', 'normalize']","[None, None]","[None, None]",1290,[],"['isinstance', 'np.asarray', 'np.std', 'np.mean', 'np.diff', 'np.sqrt']",6
data/input/tseries/test.py:mean_abs_change,mean_abs_change,function,2,2,2,33,16.5,0,0,['x'],[None],[None],1316,[],['np.mean'],1
data/input/tseries/test.py:_roll,_roll,function,7,13,13,105,8.08,0,1,"['a', 'shift']","[None, None]","[None, None]",1331,[],"['isinstance', 'np.asarray', 'len', 'np.concatenate']",4
data/input/tseries/test.py:mean_second_derivative_central,mean_second_derivative_central,function,3,10,10,73,7.3,0,0,['x'],[None],[None],1337,[],"['np.array', '_roll', 'np.mean']",3
data/input/tseries/test.py:variance_larger_than_standard_deviation,variance_larger_than_standard_deviation,function,4,6,5,30,5.0,0,0,['x'],[None],[None],1352,[],"['np.var', 'np.sqrt']",2
data/input/tseries/test.py:var_index,var_index,function,19,52,43,377,7.25,1,0,"['time', 'param']","[None, None]","[None, 'var_index_param']",1368,[],"['param.items', 'np.power', 'np.mean', 'len', 'np.var', 'sum', 'final.append', 'keys.append', 'zip']",9
data/input/tseries/test.py:symmetry_looking,symmetry_looking,function,10,23,22,247,10.74,1,1,"['x', 'param=[{""r""']","[None, '']","[None, '[{""r"": 0.2}]']",1399,[],"['isinstance', 'np.asarray', 'np.abs', 'np.median', 'np.max', 'np.min']",6
data/input/tseries/test.py:has_duplicate_max,has_duplicate_max,function,5,11,11,88,8.0,0,1,['x'],[None],[None],1418,"['    """"""\n', '    Checks if the maximum value of x is observed more than once\n', '\n', '    :param x: the time series to calculate the feature of\n', '    :type x: numpy.ndarray\n', '    :return: the value of this feature\n', '    :return type: bool\n', '    """"""\n']","['isinstance', 'np.asarray', 'np.sum', 'np.max']",4
data/input/tseries/test.py:partial_autocorrelation,partial_autocorrelation,function,6,42,29,374,8.9,0,2,"['x', 'param=[{""lag""']","[None, '']","[None, '[{""lag"": 1}]']",1445,[],"['max', 'len', 'list']",3
data/input/tseries/test.py:augmented_dickey_fuller,augmented_dickey_fuller,function,26,64,40,410,6.41,2,4,"['x', 'param=[{""attr""']","[None, '']","[None, '[{""attr"": ""teststat""}]']",1476,[],['adfuller'],1
data/input/tseries/test.py:gskew,gskew,function,8,21,18,250,11.9,0,0,['x'],[None],[None],1503,[],"['np.median', 'np.percentile']",2
data/input/tseries/test.py:stetson_mean,stetson_mean,function,21,48,43,399,8.31,1,1,"['x', 'param']","[None, None]","[None, 'stestson_param']",1525,[],"['np.median', 'range', 'np.abs', 'np.sqrt', 'weight1.mean', 'np.mean']",6
data/input/tseries/test.py:length,length,function,1,2,2,12,6.0,0,0,['x'],[None],[None],1555,[],['len'],1
data/input/tseries/test.py:count_above_mean,count_above_mean,function,4,6,6,40,6.67,0,0,['x'],[None],[None],1568,[],"['np.mean', 'np.where']",2
data/input/tseries/test.py:get_length_sequences_where,get_length_sequences_where,function,2,24,20,126,5.25,0,2,['x'],[None],[None],1583,[],"['len', 'itertools.groupby']",2
data/input/tseries/test.py:longest_strike_below_mean,longest_strike_below_mean,function,5,16,14,129,8.06,0,1,['x'],[None],[None],1591,[],"['isinstance', 'np.asarray', 'np.max', 'np.mean']",4
data/input/tseries/test.py:wozniak,wozniak,function,15,74,47,497,6.72,3,3,"['magnitude', 'param']","[None, None]","[None, 'woz_param']",1608,[],"['len', 'np.std', 'np.mean', 'range', 'if', 'iters.append', 'enumerate']",7
data/input/tseries/test.py:last_location_of_maximum,last_location_of_maximum,function,4,12,11,71,5.92,0,0,['x'],[None],[None],1645,[],"['np.asarray', 'np.argmax', 'len']",3
data/input/tseries/test.py:fft_coefficient,fft_coefficient,function,17,80,53,602,7.53,1,1,"['x', 'param = [{""coeff""', '""attr""']","[None, '', ' ""real""}]']","[None, ' [{""coeff"": 10', None]",1662,[],"['min', 'set', 'complex_agg', 'np.abs', 'np.angle', 'len']",6
data/input/tseries/test.py:ar_coefficient,ar_coefficient,function,20,56,44,563,10.05,1,2,"['x', 'param=[{""coeff""', '""k""']","[None, '', ' 5}]']","[None, '[{""coeff"": 5', None]",1695,[],"['list', 'AR', 'calculated_AR.fit', 'res.items']",4
data/input/tseries/test.py:index_mass_quantile,index_mass_quantile,function,9,28,22,259,9.25,0,1,"['x', 'param=[{""q""']","[None, '']","[None, '[{""q"": 0.3}]']",1741,[],"['np.asarray', 'np.abs', 'sum', 'np.cumsum']",4
data/input/tseries/test.py:number_cwt_peaks,number_cwt_peaks,function,1,11,10,124,11.27,0,0,"['x', 'param']","[None, None]","[None, 'cwt_param']",1770,[],['len'],1
data/input/tseries/test.py:spkt_welch_density,spkt_welch_density,function,11,62,46,500,8.06,2,2,"['x', 'param=[{""coeff""']","[None, '']","[None, '[{""coeff"": 5}]']",1786,[],"['welch', 'nperseg=min', 'len', 'np.max', 'zip', 'list']",6
data/input/tseries/test.py:linear_trend_timewise,linear_trend_timewise,function,10,19,19,236,12.42,1,0,"['x', 'param= [{""attr""']","[None, '']","[None, ' [{""attr"": ""pvalue""}]']",1817,[],"['np.asarray', 'float', 'linregress', 'getattr']",4
data/input/tseries/test.py:c3,c3,function,8,25,20,155,6.2,0,2,"['x', 'lag']","[None, None]","[None, '3']",1839,[],"['isinstance', 'np.asarray', 'np.mean', '_roll']",4
data/input/tseries/test.py:binned_entropy,binned_entropy,function,11,24,21,170,7.08,0,1,"['x', 'max_bins']","[None, None]","[None, '10']",1860,[],"['isinstance', 'np.asarray', 'np.histogram', 'np.sum']",4
data/input/tseries/test.py:_embed_seq,_embed_seq,function,16,53,41,218,4.11,2,2,"['X', 'Tau', 'D']","[None, None, None]","[None, None, None]",1878,[],"['print', 'exit', 'np.zeros', 'range']",4
data/input/tseries/test.py:svd_entropy,svd_entropy,function,16,47,40,383,8.15,1,0,"['epochs', 'param']","[None, None]","[None, 'svd_param']",1895,[],"['svd_entropy_1d', '_embed_seq', 'sum', 'np.sum', 'np.log', 'final.append', 'enumerate']",7
data/input/tseries/test.py:_hjorth_mobility,_hjorth_mobility,function,7,12,10,114,9.5,0,0,['epochs'],[None],[None],1922,[],"['np.diff', 'np.std', 'np.divide']",3
data/input/tseries/test.py:hjorth_complexity,hjorth_complexity,function,8,16,13,179,11.19,0,0,['epochs'],[None],[None],1930,[],"['np.diff', 'np.std', 'np.divide', '_hjorth_mobility']",4
data/input/tseries/test.py:_estimate_friedrich_coefficients,_estimate_friedrich_coefficients,function,20,49,43,475,9.69,0,0,"['x', 'm', 'r']","[None, None, None]","[None, None, None]",1949,[],"['pd.DataFrame', 'np.diff', 'pd.qcut', 'df.groupby', 'result.dropna', 'np.polyfit']",6
data/input/tseries/test.py:max_langevin_fixed_point,max_langevin_fixed_point,function,6,14,12,176,12.57,0,0,"['x', 'r', 'm']","[None, None, None]","[None, '3', '30']",1968,[],"['_estimate_friedrich_coefficients', 'np.max']",2
data/input/tseries/test.py:willison_amplitude,willison_amplitude,function,1,7,7,72,10.29,0,0,"['X', 'param']","[None, None]","[None, 'will_param']",1993,[],[],0
data/input/tseries/test.py:percent_amplitude,percent_amplitude,function,13,30,28,334,11.13,1,0,"['x', 'param ']","[None, None]","[None, 'perc_param']",2008,[],"['np.max', 'np.min', 'np.median', 'final.append', 'abs', 'zip']",6
data/input/tseries/test.py:cad_prob,cad_prob,function,1,11,11,106,9.64,0,0,"['cads', 'param']","[None, None]","[None, 'cad_param']",2035,[],"['stats.percentileofscore', 'float']",2
data/input/tseries/test.py:zero_crossing_derivative,zero_crossing_derivative,function,5,20,20,171,8.55,0,0,"['epochs', 'param']","[None, None]","[None, 'zero_param']",2053,[],"['np.diff', 'np.apply_along_axis', 'np.sum']",3
data/input/tseries/test.py:detrended_fluctuation_analysis,detrended_fluctuation_analysis,function,32,123,92,751,6.11,2,4,['epochs'],[None],[None],2075,[],"['dfa_1d', 'np.array', 'np.mean', 'np.cumsum', 'np.floor', 'int', 'np.zeros', 'F', 'range', 'len', 'print', 'exit', 'list', 'np.vstack', 'np.ones', 'np.sqrt', 'np.log', 'np.apply_along_axis']",18
data/input/tseries/test.py:_embed_seq,_embed_seq,function,16,53,41,218,4.11,2,2,"['X', 'Tau', 'D']","[None, None, None]","[None, None, None]",2129,"['""""""#### **(28) Fractals**\n', '\n', 'In mathematics, more specifically in fractal geometry, a fractal dimension is a ratio providing a statistical index of complexity comparing how detail in a pattern (strictly speaking, a fractal pattern) changes with the scale at which it is measured.\n', '\n', '(i) Highuchi Fractal\n', '\n', 'Compute a Higuchi Fractal Dimension of a time series\n', '""""""\n']","['print', 'exit', 'np.zeros', 'range']",4
data/input/tseries/test.py:fisher_information,fisher_information,function,9,37,34,319,8.62,0,0,"['epochs', 'param']","[None, None]","[None, 'fisher_param']",2139,[],"['fisher_info_1d', '_embed_seq', 'sum', 'np.sum']",4
data/input/tseries/test.py:higuchi_fractal_dimension,higuchi_fractal_dimension,function,16,72,54,453,6.29,3,0,"['epochs', 'param']","[None, None]","[None, 'hig_para']",2166,[],"['hfd_1d', 'len', 'range', 'int', 'abs', 'np.floor', 'float', 'Lk.append', 'L.append', 'x.append', 'np.apply_along_axis']",11
data/input/tseries/test.py:petrosian_fractal_dimension,petrosian_fractal_dimension,function,20,50,42,282,5.64,1,2,['epochs'],[None],[None],2197,[],"['pfd_1d', 'np.diff', 'D.tolist', 'range', 'len', 'np.log10', 'np.apply_along_axis']",7
data/input/tseries/test.py:hurst_exponent,hurst_exponent,function,39,85,66,542,6.38,3,2,['epochs'],[None],[None],2232,[],"['hurst_1d', 'np.array', 'np.arange', 'np.cumsum', 'np.zeros', 'range', 'np.std', 'np.ptp', 'len', 'np.diff', 'max', 'np.log', 'np.column_stack', 'np.ones', 'np.apply_along_axis']",15
data/input/tseries/test.py:_embed_seq,_embed_seq,function,16,53,41,218,4.11,2,2,"['X', 'Tau', 'D']","[None, None, None]","[None, None, None]",2129,"['""""""#### **(28) Fractals**\n', '\n', 'In mathematics, more specifically in fractal geometry, a fractal dimension is a ratio providing a statistical index of complexity comparing how detail in a pattern (strictly speaking, a fractal pattern) changes with the scale at which it is measured.\n', '\n', '(i) Highuchi Fractal\n', '\n', 'Compute a Higuchi Fractal Dimension of a time series\n', '""""""\n']","['print', 'exit', 'np.zeros', 'range']",4
data/input/tseries/test.py:largest_lyauponov_exponent,largest_lyauponov_exponent,function,43,142,119,1210,8.52,0,0,"['epochs', 'param']","[None, None]","[None, 'lyaup_param']",2283,[],"['LLE_1d', '_embed_seq', 'len', 'np.tile', 'np.transpose', 'np.sqrt', 'np.tri', 'np.logical_and', 'np.sum', 'np.arange', 'np.vstack', 'np.ones', 'np.apply_along_axis']",13
data/input/tseries/test.py:whelch_method,whelch_method,function,15,31,29,272,8.77,1,0,"['data', 'param']","[None, None]","[None, 'whelch_param']",2346,[],"['signal.welch', 'pd.DataFrame', 'df.sort_values', 'final.append', 'zip']",5
data/input/tseries/test.py:find_freq,find_freq,function,18,39,35,441,11.31,1,0,"['serie', 'param']","[None, None]","[None, 'freq_param']",2366,[],"['np.array', 'range', 'len', 'pd.DataFrame', 'df.sort_values', 'final.append', 'zip']",7
data/input/tseries/test.py:flux_perc,flux_perc,function,15,28,24,400,14.29,0,0,['magnitude'],[None],[None],2393,[],"['np.sort', 'len', 'int']",3
data/input/tseries/test.py:range_cum_s,range_cum_s,function,10,18,18,138,7.67,0,0,['magnitude'],[None],[None],2417,[],"['np.std', 'len', 'np.mean', 'np.cumsum', 'np.max', 'np.min']",6
data/input/tseries/test.py:structure_func,structure_func,function,39,116,84,1092,9.41,2,3,"['time', 'param']","[None, None]","[None, 'struct_param']",2442,[],"['param.items', 'np.zeros', 'interp1d', 'np.linspace', 'np.max', 'f', 'np.arange', 'np.mean', 'np.power', 'np.abs', 'np.log10', 'len', 'np.polyfit', 'dict_final.items', 'zip']",15
data/input/tseries/test.py:kurtosis,kurtosis,function,5,8,8,72,9.0,0,1,['x'],[None],[None],2500,[],"['isinstance', 'pd.Series']",2
data/input/tseries/test.py:stetson_k,stetson_k,function,8,19,18,147,7.74,0,0,['x'],[None],[None],2512,"['    """"""A robust kurtosis statistic.""""""\n']","['len', 'stetson_mean', 'np.sqrt', 'np.mean']",4
data/input/tseries/test.py:model,model,function,12,21,13,437,20.81,0,0,['df_final'],[None],[None],2530,[],"['LGBMRegressor', 'df_final.head', 'model.fit', 'model.predict', 'mean_squared_error']",5
data/input/tseries/test.py:model,model,function,12,21,13,437,20.81,0,0,['df_final'],[None],[None],2530,[],"['LGBMRegressor', 'df_final.head', 'model.fit', 'model.predict', 'mean_squared_error']",5
data/input/tseries/test.py:feature_agg,feature_agg,function,14,23,17,294,12.78,0,2,"['df', 'drop', 'components']","[None, None, None]","[None, 'None', '4']",2752,[],"['df.drop', 'min', 'cluster.FeatureAgglomeration', 'agglo.fit', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",7
data/input/tseries/util_feat.py:ztest,ztest,function,4,5,5,27,5.4,0,0,[],[],[],65,[],['print'],1
data/input/tseries/util_feat.py:log,log,function,10,25,20,291,11.64,0,0,"['*s', '**kw']","[None, None]","[None, None]",73,[],"['print', 'log_time', 'log_error', 'traceback.print_exc']",4
data/input/tseries/util_feat.py:log_time,log_time,function,3,6,6,64,10.67,0,0,['*s'],[None],[None],76,[],['print'],1
data/input/tseries/util_feat.py:log_error,log_error,function,7,13,12,150,11.54,0,0,"['*s', 'exception']","[None, None]","[None, 'None']",80,[],"['print', 'traceback.print_exc']",2
data/input/tseries/util_feat.py:sdict,sdict,function,2,2,2,22,11.0,0,0,[],[],[],87,[],['defaultdict'],1
data/input/tseries/util_feat.py:kvect,kvect,function,2,2,2,29,14.5,0,0,[],[],[],90,[],['np.zeros'],1
data/input/tseries/util_feat.py:pd_col_flatten,pd_col_flatten,function,5,31,19,104,3.35,0,2,['cols'],[None],[None],102,[],[],0
data/input/tseries/util_feat.py:os_clean_memory,os_clean_memory,function,5,14,13,56,4.0,1,0,"['varlist', 'globx']","[None, None]","[None, None]",109,[],['gc.collect'],1
data/input/tseries/util_feat.py:save,save,function,3,8,8,85,10.62,0,0,"['dd', 'to_file']","[None, None]","[None, '""""']",117,[],"['pickle.dump', 'open']",2
data/input/tseries/util_feat.py:load,load,function,5,8,7,61,7.62,0,0,['to_file'],[None],"['""""']",123,[],['pickle.load'],1
data/input/tseries/util_feat.py:pd_cartesian,pd_cartesian,function,7,19,14,123,6.47,0,0,"['df1', 'df2']","[None, None]","[None, None]",129,[],"['list', 'pd.merge']",2
data/input/tseries/util_feat.py:os_run_cmd_list,os_run_cmd_list,function,11,39,33,201,5.15,1,1,"['ll', 'log_filename', 'sleep_sec']","[None, None, None]","[None, None, '10']",139,[],"['len', 'enumerate', 'log', 'os.system', 'time.sleep']",5
data/input/tseries/util_feat.py:pd_train_split_time,pd_train_split_time,function,20,70,48,465,6.64,1,3,"['df', 'test_period ', 'cols', 'coltime ', 'sort', 'minsize', 'verbose']","[None, None, None, None, None, None, None]","[None, ' 40', 'None', '""time_key""', 'True', '5', 'False']",160,[],"['list', 'df.sort_index', 'log', 'df.groupby', 'len']",5
data/input/tseries/util_feat.py:pd_histo,pd_histo,function,9,31,24,164,5.29,0,2,"['dfi', 'path_save', 'nbin', 'show']","[None, None, None, None]","[None, 'None', '20.0', 'False']",174,[],"['dfi.quantile', 'dfi.hist', 'plt.savefig', 'plt.show', 'plt.close']",5
data/input/tseries/util_feat.py:config_import,config_import,function,10,39,28,313,8.03,2,2,"['mod_name', 'globs', 'verbose']","[None, None, None]","['""mypath.config.genre_l2_model""', 'None', 'True']",183,[],"['__import__', 'hasattr', 'dir', 'name.startswith', 'print', 'globs.update', 'getattr']",7
data/input/tseries/util_feat.py:os_file_check,os_file_check,function,4,18,15,121,6.72,0,0,['fp'],[None],[None],200,[],"['log', 'os.stat', 'time.ctime']",3
data/input/tseries/util_feat.py:pd_add,pd_add,function,6,15,15,95,6.33,1,0,"['ddict', 'colsname', 'vector', 'tostr']","[None, None, None, None]","[None, None, None, '1']",208,[],"['enumerate', 'str']",2
data/input/tseries/util_feat.py:pd_filter,pd_filter,function,18,130,57,614,4.72,2,5,"['df', 'filter_dict', 'verbose']","[None, None, None]","[None, '""shop_id=11, l1_genre_id>600, l2_genre_id<80311,""', 'False']",214,"['    """"""\n', '     dfi = pd_filter2(dfa, ""shop_id=11, l1_genre_id>600, l2_genre_id<80311,"" ) \n', '     dfi2 = pd_filter(dfa, {""shop_id"" : 11} )\n', '     ### Dilter dataframe with basic expr\n', '    """"""\n']","['isinstance', 'filter_dict.items', 'filter_dict.split', 'x_convert', 'str', 'dict', 'float', 'x.strip', 'print', 'len', 'x.split']",11
data/input/tseries/util_feat.py:pd_read_file2,pd_read_file2,function,49,166,98,1023,6.16,3,9,"['path_glob', 'ignore_index', 'cols', 'verbose', 'nrows', 'concat_sort', 'n_pool', 'drop_duplicates', 'shop_id', '**kw']","[None, None, None, None, None, None, None, None, None, None]","['""*.pkl""', 'True', 'None', 'False', '-1', 'True', '1', 'None', 'None', None]",258,[],"['ThreadPool', 'glob.glob', 'pd.DataFrame', 'len', 'log_time', 'range', 'log', 'job_list.append', 'pool.apply_async', 'dfi.drop_duplicates', 'gc.collect', 'pd.concat']",12
data/input/tseries/util_feat.py:pd_read_file,pd_read_file,function,55,259,124,1668,6.44,4,13,"['path_glob', 'ignore_index', 'cols', 'verbose', 'nrows', 'concat_sort', 'n_pool', 'drop_duplicates', 'shop_id', '**kw']","[None, None, None, None, None, None, None, None, None, None]","['""*.pkl""', 'True', 'None', 'False', '-1', 'True', '1', 'None', 'None', None]",309,[],"['ThreadPool', 'glob.glob', 'pd.DataFrame', 'len', 'log_time', 'range', 'log', 'job_list.append', 'pool.apply_async', 'dfi.drop_duplicates', 'gc.collect', 'pd.concat', 'pd_read_file', 'enumerate', 'pd_reader_obj', 'print', 'dfi.head']",17
data/input/tseries/util_feat.py:pd_to_onehot,pd_to_onehot,function,18,93,60,514,5.53,1,3,"['df', 'colnames', 'map_dict', 'verbose']","[None, None, None, None]","[None, None, 'None', '0']",342,[],"['len', 'print', 'pd.CategoricalDtype', 'pd.get_dummies', 'pd.concat', 'preprocessing.LabelBinarizer', 'lb.fit_transform']",7
data/input/tseries/util_feat.py:to_timekey,to_timekey,function,4,6,5,82,13.67,0,0,['x_date'],[None],[None],392,[],"['x_date.timetuple', 'int']",2
data/input/tseries/util_feat.py:from_timekey,from_timekey,function,4,8,8,56,7.0,0,0,"['time_key', 'fmt']","[None, None]","[None, ""'%Y%m%d'""]",398,[],"['time.strftime', 'time.gmtime']",2
data/input/tseries/util_feat.py:is_holiday,is_holiday,function,9,24,21,159,6.62,0,0,['array'],[None],[None],407,"['    """"""\n', '      is_holiday([ pd.to_datetime(""2015/1/1"") ] * 10)  \n', '\n', '    """"""\n']","['holidays.CountryHoliday', 'np.array', 'x.astype']",3
data/input/tseries/util_feat.py:to_float,to_float,function,1,8,6,37,4.62,0,0,['v'],[None],[None],419,[],['float'],1
data/input/tseries/util_feat.py:todatetime2,todatetime2,function,2,9,7,54,6.0,0,0,['x'],[None],[None],428,[],['arrow.get'],1
data/input/tseries/util_feat.py:todatetime,todatetime,function,3,15,11,100,6.67,0,0,['x'],[None],[None],433,[],"['arrow.get', 'todatetime', 'pd.to_datetime', 'str']",4
data/input/tseries/util_feat.py:dd,dd,function,3,19,14,111,5.84,0,1,['x)'],['  return pd.to_datetime(str(x))import datetimedate_value):'],[None],436,[],['date_value.replace'],1
data/input/tseries/util_feat.py:weekmonth,weekmonth,function,3,19,14,111,5.84,0,1,['date_value'],[None],[None],440,[],['date_value.replace'],1
data/input/tseries/util_feat.py:weekyear2,weekyear2,function,1,6,6,53,8.83,0,0,['dt'],[None],[None],447,[],['datetime.datetime'],1
data/input/tseries/util_feat.py:todatetime,todatetime,function,3,15,11,100,6.67,0,0,['x'],[None],[None],451,[],"['arrow.get', 'todatetime', 'pd.to_datetime', 'str']",4
data/input/tseries/util_feat.py:weekyear2,weekyear2,function,1,6,6,53,8.83,0,0,['dt'],[None],[None],447,[],['datetime.datetime'],1
data/input/tseries/util_feat.py:weekday_excel,weekday_excel,function,5,16,14,86,5.38,0,1,['x'],[None],[None],461,[],"['arrow.get', 'str']",2
data/input/tseries/util_feat.py:weekyear_excel,weekyear_excel,function,12,47,40,285,6.06,0,1,['x'],[None],[None],468,[],"['arrow.get', 'str', 'dd.isocalendar', 'weekday_excel', 'int']",5
data/input/tseries/util_feat.py:datetime_generate,datetime_generate,function,8,18,17,174,9.67,0,0,"['start', 'ndays']","[None, None]","[""'2018-01-01'"", '100']",486,[],"['relativedelta', 'range']",2
data/input/tseries/util_feat.py:rmse,rmse,function,5,12,11,103,8.58,0,0,"['df', 'dfhat']","[None, None]","[None, None]",493,[],['np.sqrt'],1
data/input/tseries/util_feat.py:ffmat,ffmat,function,2,12,10,46,3.83,0,1,['x'],[None],[None],500,[],"['int', 'str']",2
data/input/tseries/util_feat.py:merge1,merge1,function,1,11,10,49,4.45,0,0,"['x', 'y']","[None, None]","[None, None]",505,[],"['int', 'ffmat']",2
data/input/tseries/util_feat.py:rmse,rmse,function,5,12,11,103,8.58,0,0,"['df', 'dfhat']","[None, None]","[None, None]",493,[],['np.sqrt'],1
data/input/tseries/util_feat.py:to_file,to_file,function,3,6,6,47,7.83,0,0,"['txt', 'filename', 'mode']","[None, None, None]","['""""', '""ztmp.txt""', ""'a'""]",527,[],"['open', 'fp.write']",2
data/input/tseries/util_feat.py:date_now,date_now,function,15,27,20,354,13.11,0,0,[],[],[],531,[],"['datetime.now', 'date_now_jp', 'datetime.timedelta', 'now_new.astimezone', 'now_pacific.strftime']",5
data/input/tseries/util_feat.py:date_now_jp,date_now_jp,function,14,17,15,224,13.18,0,0,"['fmt=""%Y-%m-%d %H', 'add_days']","['', None]","['""%Y-%m-%d %H:%M:%S %Z%z""', '0']",536,[],"['datetime.now', 'datetime.timedelta', 'now_new.astimezone', 'now_pacific.strftime']",4
data/input/tseries/util_feat.py:pd_col_to_onehot2,pd_col_to_onehot2,function,20,94,61,616,6.55,3,6,"['dfref', 'colname', 'colonehot', 'return_val']","[None, None, None, None]","[None, 'None', 'None', '""dataframe,column""']",549,"['    """"""\n', '    :param df:\n', '    :param colname:\n', '    :param colonehot: previous one hot columns\n', '    :param returncol:\n', '    :return:\n', '    """"""\n']","['copy.deepcopy', 'list', 'len', 'print', 'pd.concat', 'pd.get_dummies', 'coladded.append']",7
data/input/tseries/util_feat.py:pd_colcat_mergecol,pd_colcat_mergecol,function,17,43,33,255,5.93,2,1,"['df', 'col_list', 'x0', 'colid']","[None, None, None, None]","[None, None, None, '""easy_id""']",593,"['    """"""\n', '       Merge category onehot column\n', '    :param df:\n', '    :param l:\n', '    :param x0:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 't.rfind', 'int', 'print', 'dfz.set_index']",5
data/input/tseries/util_feat.py:pd_colcat_tonum,pd_colcat_tonum,function,22,65,46,561,8.63,1,4,"['df', 'colcat', 'drop_single_label', 'drop_fact_dict']","[None, None, None, None]","[None, '""all""', 'False', 'True']",619,"['    """"""\n', '    Encoding a data-set with mixed data (numerical and categorical) to a numerical-only data-set,\n', '    using the following logic:\n', '    * categorical with only a single value will be marked as zero (or dropped, if requested)\n', '    * categorical with two values will be replaced with the result of Pandas `factorize`\n', '    * categorical with more than two values will be replaced with the result of Pandas `get_dummies`\n', '    * numerical columns will not be modified\n', '    **Returns:** DataFrame or (DataFrame, dict). If `drop_fact_dict` is True, returns the encoded DataFrame.\n', '    else, returns a tuple of the encoded DataFrame and dictionary, where each key is a two-value column, and the\n', '    value is the original labels, as supplied by Pandas `factorize`. Will be empty if no two-value columns are\n', '    present in the data-set\n', '    Parameters\n', '    ----------\n', '    df : NumPy ndarray / Pandas DataFrame\n', '        The data-set to encode\n', '    colcat : sequence / string\n', ""        A sequence of the nominal (categorical) columns in the dataset. If string, must be 'all' to state that\n"", ""        all columns are nominal. If None, nothing happens. Default: 'all'\n"", '    drop_single_label : Boolean, default = False\n', '        If True, nominal columns with a only a single value will be dropped.\n', '    drop_fact_dict : Boolean, default = True\n', '        If True, the return value will be the encoded DataFrame alone. If False, it will be a tuple of\n', '        the DataFrame and the dictionary of the binary factorization (originating from pd.factorize)\n', '    """"""\n']","['convert', 'pd.DataFrame', 'dict', 'pd.unique', 'len', 'pd.factorize', 'pd.get_dummies', 'pd.concat']",8
data/input/tseries/util_feat.py:pd_colcat_mapping,pd_colcat_mapping,function,9,35,20,271,7.74,4,0,"['df', 'colname']","[None, None]","[None, None]",673,"['    """"""\n', '     for col in colcat :\n', '        df[col] = df[col].apply(lambda x : colcat_map[""cat_map""][col].get(x)  )\n', '    :param df:\n', '    :param colname:\n', '    :return:\n', '    """"""\n']",['enumerate'],1
data/input/tseries/util_feat.py:pd_colcat_toint,pd_colcat_toint,function,25,67,43,534,7.97,4,1,"['dfref', 'colname', 'colcat_map', 'suffix']","[None, None, None, None]","[None, None, 'None', 'None']",695,[],"['ddict.get', 'colname_new.append', 'enumerate']",3
data/input/tseries/util_feat.py:pd_col_tocluster,pd_col_tocluster,function,36,120,90,1319,10.99,1,5,"['df', 'colname', 'colexclude', 'colmodelmap', 'suffix', 'na_value', 'return_val', 'params = { ""KMeans_n_clusters"" ', '""KMeans_init""', '""KMeans_n_init""']","[None, None, None, None, None, None, None, '', "" 'k-means++'"", '10,""KMeans_max_iter"" : 300, ""KMeans_tol"": 0.0001, ""KMeans_precompute_distances"" : \'auto\',""KMeans_verbose"" : 0, ""KMeans_random_state"": None,""KMeans_copy_x"": True, ""KMeans_n_jobs"" : None, ""KMeans_algorithm"" : \'auto\'}']","[None, 'None', 'None', 'None', '""_bin""', '-1', '""dataframe,param""', ' { ""KMeans_n_clusters"" : 8', None, None]",719,"['    """"""\n', '    colbinmap = for each column, definition of bins\n', '    https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n', '       :param df:\n', '       :param method:\n', '       :return:\n', '    """"""\n']","['list', 'OrderedDict', 'dict2', 'bin_create_cluster', 'KMeans', 'kmeans_model.fit', 'print', 'np.where', 'colmodelmap.get', 'model.predict', 'df.groupby', 'colnew.append']",12
data/input/tseries/util_feat.py:pd_colnum_tocat,pd_colnum_tocat,function,42,147,98,1134,7.71,2,6,"['df', 'colname', 'colexclude', 'colbinmap', 'bins', 'suffix', 'method', 'na_value', 'return_val', 'params = { ""KMeans_n_clusters"" ', '""KMeans_init""', '""KMeans_n_init""']","[None, None, None, None, None, None, None, None, None, '', "" 'k-means++'"", '10,""KMeans_max_iter"" : 300, ""KMeans_tol"": 0.0001, ""KMeans_precompute_distances"" : \'auto\',""KMeans_verbose"" : 0, ""KMeans_random_state"": None,""KMeans_copy_x"": True, ""KMeans_n_jobs"" : None, ""KMeans_algorithm"" : \'auto\'}']","[None, 'None', 'None', 'None', '5', '""_bin""', '""uniform""', '-1', '""dataframe,param""', ' { ""KMeans_n_clusters"" : 8', None, None]",791,"['    """"""\n', '    colbinmap = for each column, definition of bins\n', '    https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n', '       :param df:\n', '       :param method:\n', '       :return:\n', '    """"""\n']","['list', 'OrderedDict', 'bin_create', 'dfc.min', 'dfc.max', 'range', 'bin_create_quantile', 'np.arange', 'dfc.quantile', 'print', 'colbinmap.get', 'len', 'pd.cut', 'df.groupby', 'colnew.append']",15
data/input/tseries/util_feat.py:pd_colnum_normalize,pd_colnum_normalize,function,12,34,25,294,8.65,2,0,"['df', 'colnum_log', 'colproba']","[None, None, None]","[None, None, None]",887,"['    """"""\n', '    :param df:\n', '    :param colnum_log:\n', '    :param colproba:\n', '    :return:\n', '    """"""\n']","['np.log', 'print']",2
data/input/tseries/util_feat.py:pd_col_remove,pd_col_remove,function,7,12,12,60,5.0,1,0,"['df', 'cols']","[None, None]","[None, None]",915,[],[],0
data/input/tseries/util_feat.py:pd_col_intersection,pd_col_intersection,function,3,9,8,113,12.56,0,0,"['df1', 'df2', 'colid']","[None, None, None]","[None, None, None]",924,"['    """"""\n', '    :param df1:\n', '    :param df2:\n', '    :param colid:\n', '    :return :\n', '    """"""\n']","['list', 'print', 'len']",3
data/input/tseries/util_feat.py:pd_col_merge_onehot,pd_col_merge_onehot,function,11,27,21,136,5.04,2,1,"['df', 'colname']","[None, None]","[None, None]",936,"['    """"""\n', '      Merge columns into single (hotn\n', '    :param df:\n', '    :param colname:\n', '    :return :\n', '    """"""\n']","['t[len', 'len', 'merge_array.append']",3
data/input/tseries/util_feat.py:pd_col_to_num,pd_col_to_num,function,9,27,24,184,6.81,1,0,"['df', 'colname', 'default']","[None, None, None]","[None, 'None', 'np.nan']",954,[],"['to_float', 'float', 'list']",3
data/input/tseries/util_feat.py:pd_col_filter,pd_col_filter,function,12,28,23,186,6.64,1,2,"['df', 'filter_val', 'iscol']","[None, None, None]","[None, 'None', '1']",970,"['    """"""\n', '   # Remove Columns where Index Value is not in the filter_value\n', ""   # filter1= X_client['client_id'].values\n"", '   :param df:\n', '   :param filter_val:\n', '   :param iscol:\n', '   :return:\n', '   """"""\n']","['col_delete.append', 'df.drop']",2
data/input/tseries/util_feat.py:pd_col_fillna,pd_col_fillna,function,22,75,55,684,9.12,1,6,"['dfref', 'colname', 'method', 'value', 'colgroupby', 'return_val', '']","[None, None, None, None, None, None, None]","[None, 'None', '""frequent""', 'None', 'None', '""dataframe,param""', None]",992,"['    """"""\n', '    Function to fill NaNs with a specific value in certain columns\n', '    Arguments:\n', '        df:            dataframe\n', '        colname:      list of columns to remove text\n', '        value:         value to replace NaNs with\n', '    Returns:\n', '        df:            new dataframe with filled values\n', '    """"""\n']","['list', 'df.groupby', 'print', 'pd_col_fillna_advanced']",4
data/input/tseries/util_feat.py:pd_col_fillna_advanced,pd_col_fillna_advanced,function,22,60,46,540,9.0,1,2,"['dfref', 'colname', 'method', 'colname_na', 'return_val']","[None, None, None, None, None]","[None, 'None', '""median""', 'None', '""dataframe,param""']",1038,"['    """"""\n', '    Function to fill NaNs with a specific value in certain columns\n', '    https://impyute.readthedocs.io/en/master/\n', '    Arguments:\n', '        df:            dataframe\n', '        colname:      list of columns to remove text\n', '        colname_na : target na coluns\n', '        value:         value to replace NaNs with\n', '    Returns:\n', '        df:            new dataframe with filled values\n', '     https://impyute.readthedocs.io/en/master/user_guide/overview.html\n', '    """"""\n']","['list', 'print', 'mice', 'pd.DataFrame', 'fast_knn']",5
data/input/tseries/util_feat.py:pd_col_fillna_datawig,pd_col_fillna_datawig,function,22,68,57,548,8.06,2,2,"['dfref', 'colname', 'method', 'colname_na', 'return_val']","[None, None, None, None, None]","[None, 'None', '""median""', 'None', '""dataframe,param""']",1080,"['    """"""\n', '    Function to fill NaNs with a specific value in certain columns\n', '    https://impyute.readthedocs.io/en/master/\n', '    Arguments:\n', '        df:            dataframe\n', '        colname:      list of columns to remove text\n', '        colname_na : target na coluns\n', '        value:         value to replace NaNs with\n', '    Returns:\n', '        df:            new dataframe with filled values\n', '     https://impyute.readthedocs.io/en/master/user_guide/overview.html\n', '    """"""\n']","['list', 'print', 'datawig.SimpleImputer', 'imputer.fit', 'imputer.predict']",5
data/input/tseries/util_feat.py:pd_row_drop_above_thresh,pd_row_drop_above_thresh,function,5,11,10,68,6.18,1,0,"['df', 'colnumlist', 'thresh']","[None, None, None]","[None, None, None]",1126,"['    """"""\n', '    Function to remove outliers above a certain threshold\n', '    Arguments:\n', '        df:     dataframe\n', '        col:    col from which to remove outliers\n', '        thresh: value above which to remove row\n', '        colnumlist:list\n', '    Returns:\n', '        df:     dataframe with outliers removed\n', '    """"""\n']",['df.drop'],1
data/input/tseries/util_feat.py:pd_pipeline_apply,pd_pipeline_apply,function,8,29,22,259,8.93,1,0,"['df', 'pipeline']","[None, None]","[None, None]",1146,"['    """"""\n', '      pipe_preprocess_colnum = [\n', '      (pd_col_to_num, {""val"": ""?"", })\n', '    , (pd_colnum_tocat, {""colname"": None, ""colbinmap"": colnum_binmap, \'bins\': 5,\n', '                         ""method"": ""uniform"", ""suffix"": ""_bin"",\n', '                         ""return_val"": ""dataframe""})\n', '    , (pd_col_to_onehot, {""colname"": None, ""colonehot"": colnum_onehot,\n', '                          ""return_val"": ""dataframe""})\n', '      ]\n', '    :param df:\n', '    :param pipeline:\n', '    :return:\n', '    """"""\n']","['copy.deepcopy', 'enumerate', 'print', 'str']",4
data/input/tseries/util_feat.py:pd_df_sampling,pd_df_sampling,function,11,32,26,272,8.5,0,1,"['df', 'coltarget', 'n1max', 'n2max', 'isconcat']","[None, None, None, None, None]","[None, '""y""', '10000', '-1', '1']",1170,"['    """"""\n', '        DownSampler\n', '    :param df:\n', '    :param coltarget: binary class\n', '    :param n1max:\n', '    :param n2max:\n', '    :param isconcat:\n', '    :return:\n', '    """"""\n']","['len', 'pd.concat', 'df2.sample', 'print']",4
data/input/tseries/util_feat.py:pd_df_stack,pd_df_stack,function,7,29,24,175,6.03,1,1,"['df_list', 'ignore_index']","[None, None]","[None, 'True']",1195,"['    """"""\n', '    Concat vertically dataframe\n', '    :param df_list:\n', '    :return:\n', '    """"""\n']","['enumerate', 'df0.append', 'print']",3
data/input/tseries/util_feat.py:pd_stat_correl_pair,pd_stat_correl_pair,function,12,33,30,316,9.58,1,1,"['df', 'coltarget', 'colname']","[None, None, None]","[None, 'None', 'None']",1214,"['    """"""\n', '      Genearte correletion between the column and target column\n', '      df represents the dataframe comprising the column and colname comprising the target column\n', '    :param df:\n', '    :param colname: list of columns\n', '    :param coltarget : target column\n', '    :return:\n', '    """"""\n']","['list', 'target_corr.append', 'pd.DataFrame', 'len']",4
data/input/tseries/util_feat.py:pd_stat_colcheck,pd_stat_colcheck,function,3,15,15,124,8.27,1,1,['df'],[None],[None],1238,"['    """"""\n', '    :param df:\n', '    :return :\n', '    """"""\n']","['len', 'np.dtype', 'print']",3
data/input/tseries/util_feat.py:pd_stat_jupyter_profile,pd_stat_jupyter_profile,function,10,13,12,202,15.54,0,0,"['df', 'savefile', 'title']","[None, None, None]","[None, '""report.html""', '""Pandas Profile""']",1249,"['    """""" Describe the tables\n', '        #Pandas-Profiling 2.0.0\n', '        df.profile_report()\n', '    """"""\n']","['print', 'df.profile_report', 'profile.to_file', 'profile.get_rejected_variables']",4
data/input/tseries/util_feat.py:pd_stat_distribution_colnum,pd_stat_distribution_colnum,function,18,74,66,580,7.84,1,2,['df'],[None],[None],1263,"['    """""" Describe the tables\n', '   """"""\n']","['getstat', 'list', 'len', 'pd.Series', 'pd.DataFrame', 'str', 'pd.concat']",7
data/input/tseries/util_feat.py:pd_stat_histogram,pd_stat_histogram,function,14,43,37,515,11.98,1,0,"['df', 'bins', 'coltarget']","[None, None, None]","[None, '50', '""diff""']",1316,"['    """"""\n', '    :param df:\n', '    :param bins:\n', '    :param coltarget:\n', '    :return:\n', '    """"""\n']","['np.histogram', 'pd.DataFrame', 'pd_stat_histogram_groupby', 'list', 'pd.concat']",5
data/input/tseries/util_feat.py:pd_stat_histogram_groupby,pd_stat_histogram_groupby,function,7,20,16,230,11.5,1,0,"['df', 'bins', 'coltarget', 'colgroupby']","[None, None, None, None]","[None, '50', '""diff""', '""y""']",1331,"['    """"""\n', '    :param df:\n', '    :param bins:\n', '    :param coltarget:\n', '    :param colgroupby:\n', '    :return:\n', '    """"""\n']","['pd_stat_histogram_groupby', 'list', 'pd.concat']",3
data/input/tseries/util_feat.py:pd_stat_na_perow,pd_stat_na_perow,function,12,39,33,218,5.59,2,1,"['df', 'n']","[None, None]","[None, '10 ** 6']",1350,"['    """"""\n', '    :param df:\n', '    :param n:\n', '    :return:\n', '    """"""\n']","['pd.isna', 'll.append', 'pd.DataFrame', 'len', 'np.array']",5
data/input/tseries/util_feat.py:pd_stat_distribution,pd_stat_distribution,function,18,74,66,580,7.84,1,2,['df'],[None],[None],1370,"['    """"""\n', '    :param df:\n', '    :return:\n', '    """"""\n']","['getstat', 'list', 'len', 'pd.Series', 'pd.DataFrame', 'str', 'pd.concat']",7
data/input/tseries/util_feat.py:convert,convert,function,13,80,37,732,9.15,0,5,"['data', 'to']","[None, None]","[None, None]",1444,"['    """"""\n', '    :param data:\n', '    :param to:\n', '    :return :\n', '    """"""\n']","['isinstance', 'np.array', 'data.as_matrix', 'data.tolist', 'pd.DataFrame', 'ValueError', 'TypeError']",7
data/input/tseries/util_feat.py:col_stat_getcategorydict_freq,col_stat_getcategorydict_freq,function,17,34,30,376,11.06,1,0,['catedict'],[None],[None],1483,"['    """""" Generate Frequency of category : Id, Freq, Freqin%, CumSum%, ZScore\n', '      given a dictionnary of category parsed previously\n', '  """"""\n']","['list', 'pd.DataFrame', 'df.sort_values', 'np.arange', 'len', 'catlist.append']",6
data/input/tseries/util_feat.py:col_extractname_colbin,col_extractname_colbin,function,11,28,26,165,5.89,1,1,['cols2'],[None],[None],1500,"['    """"""\n', '    1hot column name to generic column names\n', '    :param cols2:\n', '    :return:\n', '    """"""\n']","['ss.rfind', 'len', 'coln.append', 'np_drop_duplicates']",4
data/input/tseries/util_feat.py:col_getnumpy_indice,col_getnumpy_indice,function,7,22,17,115,5.23,1,1,"['colall', 'colcat']","[None, None]","[None, None]",1519,[],"['np_find_indice', 'enumerate']",2
data/input/tseries/util_feat.py:col_extractname,col_extractname,function,11,28,26,165,5.89,1,1,['cols2'],[None],[None],1530,"['    """"""\n', '    Column extraction\n', '    :param col_onehot\n', '    :return:\n', '    """"""\n']","['ss.rfind', 'len', 'coln.append', 'np_drop_duplicates']",4
data/input/tseries/util_feat.py:col_remove,col_remove,function,12,44,27,214,4.86,3,4,"['cols', 'colsremove', 'mode']","[None, None, None]","[None, None, '""exact""']",1553,"['    """"""\n', '    Parameters\n', '    ----------\n', '    cols : TYPE\n', '        DESCRIPTION.\n', '    colsremove : TYPE\n', '        DESCRIPTION.\n', '    mode : TYPE, optional\n', '        DESCRIPTION. The default is ""exact"", ""fuzzy""\n', '    Returns\n', '    -------\n', '    cols : TYPE\n', '        DESCRIPTION.  remove column name from list\n', '    """"""\n']","['cols.remove', 'cols3.append']",2
data/input/tseries/util_feat.py:pd_colnum_tocat_stat,pd_colnum_tocat_stat,function,53,151,105,1986,13.15,1,5,"['df', 'feature', 'target_col', 'bins', 'cuts']","[None, None, None, None, None]","[None, None, None, None, '0']",1605,"['    """"""\n', '    Bins continuous features into equal sample size buckets and returns the target mean in each bucket. Separates out\n', '    nulls into another bucket.\n', '    :param df: dataframe containg features and target column\n', '    :param feature: feature column name\n', '    :param target_col: target column\n', '    :param bins: Number bins required\n', '    :param cuts: if buckets of certain specific cuts are required. Used on test data to use cuts from train.\n', '    :return: If cuts are passed only df_grouped data is returned, else cuts and df_grouped data is returned\n', '    """"""\n']","['pd.isnull', 'df.reset_index', 'min', 'range', 'np.percentile', 'cuts.append', 'pd.cut', 'df.groupby', 'df_grouped.reset_index', 'list', 'df_grouped.rename', 'str', 'len', 'pd.concat']",14
data/input/tseries/util_feat.py:pd_stat_shift_trend_changes,pd_stat_shift_trend_changes,function,20,41,35,660,16.1,0,0,"['df', 'feature', 'target_col', 'threshold']","[None, None, None, None]","[None, None, None, '0.03']",1676,"['    """"""\n', '    Calculates number of times the trend of feature wrt target changed direction.\n', '    :param df: df_grouped dataset\n', '    :param feature: feature column name\n', '    :param target_col: target column\n', '    :param threshold: minimum % difference required to count as trend change\n', '    :return: number of trend chagnes for the feature\n', '    """"""\n']","['target_diffs.fillna', 'target_diffs.divide', 'target_diffs_norm.diff', 'target_diffs_lvl2.fillna', 'int']",5
data/input/tseries/util_feat.py:pd_stat_shift_trend_correlation,pd_stat_shift_trend_correlation,function,19,63,55,788,12.51,0,2,"['df', 'df_test', 'colname', 'target_col']","[None, None, None, None]","[None, None, None, None]",1701,"['    """"""\n', '    Calculates correlation between train and test trend of colname wrt target.\n', '    :param df: train df data\n', '    :param df_test: test df data\n', '    :param colname: colname column name\n', '    :param target_col: target column name\n', '    :return: trend correlation between train and test\n', '    """"""\n']","['df.merge', 'pd.isnull', 'len', 'np.corrcoef', 'print']",5
data/input/tseries/util_feat.py:pd_stat_shift_changes,pd_stat_shift_changes,function,32,98,79,1236,12.61,1,4,"['df', 'target_col', 'features_list', 'bins', 'df_test']","[None, None, None, None, None]","[None, None, '0', '10', '0']",1733,"['    """"""\n', '    Calculates trend changes and correlation between train/test for list of features\n', '    :param df: dfframe containing features and target columns\n', '    :param target_col: target column name\n', '    :param features_list: by default creates plots for all features. If list passed, creates plots of only those features.\n', '    :param bins: number of bins to be created from continuous colname\n', '    :param df_test: test df which has to be compared with input df for correlation\n', '    :return: dfframe with trend changes and trend correlation (if test df passed)\n', '    """"""\n']","['type', 'list', 'features_list.remove', 'ignored.append', 'pd_colnum_tocat_stat', 'pd_stat_shift_trend_correlation', 'pd_stat_shift_changes', 'stats_all.append', 'pd.DataFrame', 'len', 'print', 'str']",12
data/input/tseries/util_feat.py:lag_featrues,lag_featrues,function,92,432,183,6039,13.98,6,5,['df'],[None],[None],1778,[],"['range', 'pd.DataFrame', 'pd.concat', 'df.groupby', 'pd.merge', 'pd.get_dummies']",6
data/input/tseries/util_feat.py:to_namespace,to_namespace,class,5,9,8,99,11.0,0,0,[],[],[],93,[],[],0
data/input/tseries/util_feat.py:to_namespace:__init__,to_namespace:__init__,method,1,1,1,27,27.0,0,0,"['self', 'adict']","[None, None]","[None, None]",95,[],[],0
data/input/tseries/util_feat.py:to_namespace:get,to_namespace:get,method,2,2,2,28,14.0,0,0,"['self', 'key']","[None, None]","[None, None]",98,[],[],0
data/input/tseries_demand/Demand_forecasting.py:load_data,load_data,function,4,6,5,64,10.67,0,0,['datapath'],[None],[None],17,[],"['pd.read_csv', 'print']",2
data/input/tseries_demand/Demand_forecasting.py:pd_ts_date,pd_ts_date,function,22,118,73,882,7.47,1,9,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",29,"['    """""" DataFrame with date column""""""\n']","['isinstance', 'print', 'pars.get', 'pd.DataFrame', 'pd.to_datetime', 'pd.concat', 'list']",7
data/input/tseries_demand/Demand_forecasting.py:pd_ts_rolling,pd_ts_rolling,function,26,81,57,992,12.25,4,0,"['df', 'cols', 'pars']","[' pd.DataFrame', ' list', ' dict']","[None, 'None', 'None']",60,"['    """"""\n', '      Rolling statistics\n', '\n', '    """"""\n']","['pars.get', 'print', 'str', 'df.groupby', 'x.shift', 'col_new.append']",6
data/input/tseries_demand/Demand_forecasting.py:preprocessing_data,preprocessing_data,function,18,57,49,410,7.19,1,1,['df'],[None],[None],105,[],"['df.set_index', 'pd_ts_date', 'pd.concat', 'pd_ts_rolling']",4
data/input/tseries_demand/Demand_forecasting.py:train_model,train_model,function,9,53,51,606,11.43,0,0,"['train_x', 'train_y', 'test_x', 'test_y', 'col']","[None, None, None, None, None]","[None, None, None, None, None]",140,[],"['abs', 'lgb.Dataset', 'lgb.train']",3
data/input/tseries_m5/run_eval.py:features_to_category,features_to_category,function,12,21,18,251,11.95,2,0,"['df', 'nan_cols', 'cat_cols']","[None, None, None]","[None, None, None]",17,[],"['preprocessing.LabelEncoder', 'encoder.fit_transform']",2
data/input/tseries_m5/run_eval.py:update_meta_csv,update_meta_csv,function,17,52,40,663,12.75,1,2,"['featnames', 'filename', 'cat_cols']","[None, None, None]","[None, None, None]",30,[],"['pd.DataFrame', 'pd.read_csv', 'meta_csv.append', 'meta_csv.to_csv']",4
data/input/tseries_m5/run_eval.py:get_cat_num_features_from_meta_csv,get_cat_num_features_from_meta_csv,function,10,34,19,297,8.74,2,0,"['id_cols', 'dep_col']","[None, None]","[None, None]",48,[],['pd.read_csv'],1
data/input/tseries_m5/run_eval.py:get_file_feat_from_meta_csv,get_file_feat_from_meta_csv,function,12,23,19,369,16.04,2,0,"['selected_cols', 'id_cols']","[None, None]","[None, None]",56,[],"['pd.read_csv', 'print', 'file_feat_mapping.items']",3
data/input/tseries_m5/run_eval.py:features_generate_file,features_generate_file,function,8,24,24,393,16.38,0,1,"['dir_in', 'dir_out', 'my_fun_features', 'features_group_name', 'input_raw_path ', 'auxiliary_csv_path ', 'drop_cols ', 'index_cols ', 'merge_cols_mapping ', 'cat_cols ', 'id_cols', 'dep_col ', 'max_rows ', 'step_wise_saving ']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, ' None', ' None', ' None', ' None', ' None', ' None', 'None', ' None', ' 5', ' False']",66,[],"['pd.read_parquet', 'my_fun_features', 'dfnew.to_parquet', 'update_meta_csv']",4
data/input/tseries_m5/run_eval.py:feature_merge_df,feature_merge_df,function,10,35,26,213,6.09,2,1,"['df_list', 'cols_join']","[None, None]","[None, None]",79,[],"['print', 'dfall.join']",2
data/input/tseries_m5/run_eval.py:raw_merged_df,raw_merged_df,function,17,70,55,942,13.46,0,1,"['input_path', 'out_path', 'index_cols ', 'dep_col ', 'raw_merge_cols ', 'merge_cols_mapping ', 'nan_cols ', 'cat_cols ', 'max_rows']","[None, None, None, None, None, None, None, None, None]","['""data/""', ""'out/'"", ' None', ' None', ' None', ' None', ' None', ' None', '10']",89,[],"['pd.read_csv', 'pd.melt', 'df_calendar.drop', 'pd.merge', 'merged_df.merge', 'features_to_category', 'os.makedirs', 'merged_df.to_parquet']",8
data/input/tseries_m5/run_eval.py:features_get_cols,features_get_cols,function,11,49,31,480,9.8,2,3,"['mode ', 'id_cols ', 'dep_col ']","[None, None, None]","[' ""random""', ' None', ' None']",132,[],['get_cat_num_features_from_meta_csv'],1
data/input/tseries_m5/run_eval.py:get_file_names_to_load_from,get_file_names_to_load_from,function,5,8,7,106,13.25,0,0,"['file_name', 'path']","[None, None]","[None, None]",157,[],"['file_name.split', 'glob.glob']",2
data/input/tseries_m5/run_eval.py:load_data,load_data,function,21,57,44,712,12.49,3,1,"['path', 'selected_cols', 'id_cols']","[None, None, None]","[None, None, None]",163,[],"['print', 'get_file_feat_from_meta_csv', 'file_col_mapping.items', 'get_file_names_to_load_from', 'pd.read_parquet', 'pd.concat', 'feature_dfs.append', 'feature_merge_df', 'merged_df.sort_values', 'merged_df.drop']",10
data/input/tseries_m5/run_eval.py:X_transform,X_transform,function,4,10,9,65,6.5,0,0,"['df', 'selected_cols']","[None, None]","[None, None]",200,[],[],0
data/input/tseries_m5/run_eval.py:Y_transform,Y_transform,function,4,4,4,32,8.0,0,0,"['df', 'selected_col']","[None, None]","[None, None]",205,[],[],0
data/input/tseries_m5/run_eval.py:run_eval,run_eval,function,36,131,107,1446,11.04,1,0,"['input_path', 'max_rows ', 'n_experiments ', 'id_cols ', 'dep_col ']","[None, None, None, None, None]","[None, ' None', ' 3', ' None', ' None']",210,[],"['print', 'range', 'features_get_cols', 'load_data', 'X_transform', 'Y_transform', 'train_test_split', 'lgb.Dataset', 'lgb.train', 'clf.predict', 'np.sqrt', 'df_metrics.to_csv']",12
data/input/tseries_m5/run_eval.py:generate_feature_all,generate_feature_all,function,2,48,32,601,12.52,0,0,"['input_path ', 'out_path', 'input_raw_path ', 'auxiliary_csv_path ', 'drop_cols ', 'index_cols ', 'merge_cols_mapping ', 'cat_cols ', 'id_cols ', 'dep_col ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None]","[' ""data/output""', '"".""', ' "".""', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' 10']",272,[],"['features_generate_file', 'print']",2
data/input/tseries_m5/run_eval.py:main,main,function,5,107,64,1290,12.06,0,2,"['input_path ', 'out_path', 'do_generate_raw', 'do_generate_feature', 'max_rows ']","[None, None, None, None, None]","[' ""data/output""', '""""', 'True', 'True', ' 10']",281,[],"['raw_merged_df', 'generate_feature_all', 'run_eval']",3
data/input/tseries_m5/util_eval.py:create_timeseries1d,create_timeseries1d,function,14,26,24,314,12.08,1,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 'fun']","[None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 1000', "" 'D'"", ' 1', 'None']",7,[],"['pd.date_range', 'pd.DataFrame', 'range', 'data.append', 'pd.to_datetime', 'df.set_index', 'df.drop']",7
data/input/tseries_m5/util_eval.py:create_timeseries_2d,create_timeseries_2d,function,0,1,1,4,4.0,0,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 'fun']","[None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 660', "" 'D'"", ' 1', 'None']",21,[],[],0
data/input/tseries_m5/util_eval.py:create_timeseries_kd,create_timeseries_kd,function,19,37,31,369,9.97,2,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 'params']","[None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 660', "" 'D'"", ' 1', 'None']",26,[],"['pd.date_range', 'pd.DataFrame', 'len', 'range', 'data.append', 'pd.to_datetime', 'df.set_index', 'df.drop']",8
data/input/tseries_m5/util_eval.py:model_eval,model_eval,function,27,88,63,653,7.42,0,2,"['clf0', 'df', 'colX', 'coly', 'test_size', 'istrain', 'use_eval']","[None, None, None, None, None, None, None]","[None, None, None, '""y""', '0.5', '1', '0']",47,[],"['copy.deepcopy', 'X.reshape', 'len', 'print', 'train_test_split', 'gc.collect', 'clf.fit', 'clf.predict_proba', 'clf.predict', 'sk_showmetrics']",10
data/input/tseries_m5/util_eval.py:model_fit,model_fit,function,26,111,83,928,8.36,0,2,"['df2', 'cols_train', 'col_target ', 'save_suffix', 'modelname', 'dosave', 'coldate', 'test_size', 'coldate_limit ', 'dfeval', 'dirmodel ', '**kw']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, ' ""y""', '""area_gms_201909""', ' ""RandomForestClassifier""', '1', '""dateint""', '0.9', ' 201801', 'None', '""""', None]",84,[],"['print', 'sum', 'model_get', 'model_eval', 'feature_impt_rf', 'clf.predict', 'clf.predict_proba', 'metric_accuracy_check', 'os.makedirs', 'save_model', 'dfstat.to_csv', 'save_session', 'glob=globals']",13
data/input/tseries_m5/util_feat_m5.py:features_time_basic,features_time_basic,function,7,22,21,290,13.18,0,0,"['df', 'input_raw_path ', 'dir_out ', 'features_group_name ', 'auxiliary_csv_path ', 'drop_cols ', 'index_cols ', 'merge_cols_mapping ', 'cat_cols ', 'id_cols ', 'dep_col ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' 10']",30,[],['pd.to_datetime'],1
data/input/tseries_m5/util_feat_m5.py:features_lag,features_lag,function,92,431,183,6051,14.04,6,5,"['df', 'fname']","[None, None]","[None, None]",43,[],"['range', 'pd.DataFrame', 'pd.concat', 'df.groupby', 'pd.merge', 'pd.get_dummies', 'out_df.to_parquet']",7
data/input/tseries_m5/util_feat_m5.py:_get_tsfresh_melted_features_single_row,_get_tsfresh_melted_features_single_row,function,32,62,52,1024,16.52,3,1,"['single_row_df', 'index_cols']","[None, None]","[None, None]",210,[],"['re.match', 'range', 'len', 'single_row_df_T.rename', 'extract_features', 'feat_col_name.replace', 'X_feat.rename', 'np.repeat', 'pd.Series', 'X_feat_T.append', 'X_feat_T.set_index']",11
data/input/tseries_m5/util_feat_m5.py:_get_tsfresh_df_sales_melt,_get_tsfresh_df_sales_melt,function,22,69,52,854,12.38,3,2,"['df_sales', 'dir_out', 'features_group_name', 'drop_cols', 'df_calendar', 'index_cols', 'merge_cols_mapping', 'id_cols']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",243,[],"['df_calendar.drop', 'range', '_get_tsfresh_melted_features_single_row', 'pd.merge', 'str', 'merged_df_selected_cols.to_parquet', 'X_feat.append']",7
data/input/tseries_m5/util_feat_m5.py:features_tsfresh,features_tsfresh,function,35,95,74,1196,12.59,3,1,"['df', 'input_raw_path', 'dir_out', 'features_group_name', 'auxiliary_csv_path', 'drop_cols', 'index_cols', 'merge_cols_mapping', 'cat_cols ', 'id_cols ', 'dep_col ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, ' None', ' None', ' None', ' 5']",268,[],"['pd.read_csv', '_get_tsfresh_df_sales_melt', 'features_tsfresh_select', 'roll_time_series', 'X.fillna', 'extract_relevant_features', 'filtered_col_name.replace', 'X_filtered.rename', 'pd.concat']",9
data/input/tseries_m5/util_feat_m5.py:features_tsfresh_select,features_tsfresh_select,function,26,67,55,827,12.34,2,1,['df'],[None],[None],284,[],"['roll_time_series', 'X.fillna', 'extract_relevant_features', 'filtered_col_name.replace', 'X_filtered.rename', 'pd.concat']",6
data/input/tseries_m5/util_feat_m5.py:basic_time_features,basic_time_features,function,7,22,21,290,13.18,0,0,"['df', 'input_raw_path ', 'dir_out ', 'features_group_name ', 'auxiliary_csv_path ', 'drop_cols ', 'index_cols ', 'merge_cols_mapping ', 'cat_cols ', 'id_cols ', 'dep_col ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' 10']",310,[],['pd.to_datetime'],1
data/input/tseries_m5/util_feat_m5.py:features_mean,features_mean,function,0,1,1,4,4.0,0,0,"['df', 'input_raw_path ', 'max_rows ']","[None, None, None]","[None, ' None', ' 10']",322,[],[],0
data/input/tseries_m5/util_feat_m5.py:identity_features,identity_features,function,8,17,15,105,6.18,1,0,"['df', 'input_raw_path ', 'dir_out ', 'features_group_name ', 'auxiliary_csv_path ', 'drop_cols ', 'index_cols ', 'merge_cols_mapping ', 'cat_cols ', 'id_cols ', 'dep_col ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' 10']",326,[],['df.drop'],1
data/input/tseries_m5/util_feat_m5.py:features_rolling,features_rolling,function,15,49,33,769,15.69,4,0,"['df', 'input_raw_path ', 'dir_out ', 'features_group_name ', 'auxiliary_csv_path ', 'drop_cols ', 'index_cols ', 'merge_cols_mapping ', 'cat_cols ', 'id_cols ', 'dep_col ', 'max_rows ']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' None', ' 10']",332,[],"['print', 'df.groupby', 'x.shift', 'created_cols.append']",4
data/input/tseries_m5/util_feat_m5.py:features_lag,features_lag,function,92,431,183,6051,14.04,6,5,"['df', 'fname']","[None, None]","[None, None]",361,"[""''''\n"", 'import pandas as pd\n', 'import numpy as np\n', 'from sklearn import preprocessing\n', 'import datetime\n', 'from sklearn.preprocessing import MinMaxScaler\n', 'from keras.preprocessing.text import one_hot\n', 'from keras.preprocessing.sequence import pad_sequences\n', 'from keras.models import Sequential\n', 'from keras.layers import Dense\n', 'from keras.layers import Flatten\n', 'from keras.layers.embeddings import Embedding\n', 'def date_transform:\n', ""\tdf=pd.read_csv('calendar.csv')\n"", ""\tdf['date'] = pd.to_datetime(df['date'])\n"", ""\tdf['year'] = df['date'].dt.year\n"", ""\tdf['month'] = df['date'].dt.month\n"", ""\tdf['week'] = df['date'].dt.week\n"", ""\tdf['day'] = df['date'].dt.day\n"", ""\tdf['dayofweek'] = df['date'].dt.dayofweek\n"", ""\tdate_cols=df[['wday','month','year','week','day']]\n"", '\tscaler = MinMaxScaler(feature_range=(-0.5,0.5))\n', '\tscaler.fit(date_cols)\n', '\ttransformed=scaler.transform(date_cols)\n', '\tnew_df=pd.DataFrame(transformed,columns=date_cols.columns)\n', ""\tembeded=df[['event_name_1','event_type_1','event_name_2','event_type_2']]\n"", ""\tunique=embeded['event_name_1'].unique()\n"", ""\tunique_vals=np.append(unique,embeded['event_name_2'].unique())\n"", '\tevent=pd.DataFrame(unique_vals,)\n', ""\tarray_nm=event['event'].unique()\n"", '\tlist_array=str(list(array_nm))\n', '\tvocab_size = 100\n', '\tencoded_docs = [one_hot(d, vocab_size) for d in list_array]\n', '\tmax_length = 4\n', ""\tpadded_docs1 = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"", ""\tunique=embeded['event_type_1'].unique()\n"", ""\tunique_vals=np.append(unique,embeded['event_type_2'].unique())\n"", '\tevent=pd.DataFrame(unique_vals,)\n', ""\tarray_nm=event['event'].unique()\n"", '\tlist_array=str(list(array_nm))\n', '\tvocab_size = 100\n', '\tencoded_docs = [one_hot(d, vocab_size) for d in list_array]\n', '\tmax_length = 4\n', ""\tpadded_docs2 = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"", ""\tvalues=df[df['snap_CA','snap_TX']]\n"", '\treturn new_df,padded_docs1,padded_docs2,values\n', 'def sales:\n', ""\tsales=pd.read_csv('sell_prices.csv')\n"", ""\tsales['dept_id']=str(sales['item_id'])\n"", '\tdef remove(x):\n', ""    \tvalue=x.split('_')\n"", '   \t    dept=value[0]+""_""+value[1]\n', '    \treturn dept\n', ""    sales['dept']=sales['item_id'].apply(remove)\n"", ""    price_sum=pd.DataFrame(sales.groupby('wm_yr_wk')['sell_price'].sum())\n"", '    scaler = MinMaxScaler()\n', '    scaler.fit(price_sum)\n', ""    price_sum['normalized']=scaler.transform(price_sum)\n"", ""    x=pd.DataFrame(sales.groupby('dept')['sell_price'].unique())\n"", '\tx.reset_index(inplace=True)\n', '\tdef recive(v):\n', '    \tsum=0\n', '    \tfor i in v:\n', '        sum+=i\n', '    \treturn round(sum,2)\n', '\n', ""\tx['summed']=x['sell_price'].apply(recive)\n"", '\tscaler = MinMaxScaler()\n', ""\tx.set_index(keys='dept',inplace=True)\n"", ""\tx.drop('sell_price',inplace=True,axis=1)\n"", '\tscaler.fit(x)\n', ""\tx['transformed']=scaler.transform(x)\n"", '\treturn price_sum,x\n', 'def sales_validation:\n', ""\tsales_eval=pd.read_csv('sales_train_validation.csv')\n"", ""\tTARGET='sales'\n"", ""\tindex_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n"", ""\tsales_eval = pd.melt(sales_eval,id_vars = index_columns,var_name = 'd',value_name = TARGET)\n"", ""\ttemp_df = sales_eval[['id','d',TARGET]]\n"", '\t#lag=1\n', '\ti=1\n', ""\tprint('Shifting:', i)\n"", ""\ttemp_df['lag_'+str(i)] = temp_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(i))\n"", '\t#Moving avg=(7,28)\n', ""\ttemp_df1 = sales_eval[['id','d','sales']]\n"", '\tfor i in [7,28]:\n', ""    \tprint('Rolling period:', i)\n"", ""    \ttemp_df['rolling_mean_'+str(i)] = temp_df1.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).mean())\n"", ""    \ttemp_df['rolling_std_'+str(i)]  = temp_df1.groupby(['id'])[TARGET].transform(lambda x: x.shift(1).rolling(i).std())\n"", '    \treturn temp_df\n', ""'''\n""]","['range', 'pd.DataFrame', 'pd.concat', 'df.groupby', 'pd.merge', 'pd.get_dummies', 'out_df.to_parquet']",7
data/input/tseries_m5_gluon/timeseries_m5.py:save_to_file,save_to_file,function,11,21,21,221,10.52,1,0,"['path', 'data']","[None, None]","['""""', 'None']",153,[],"['print', 'os.makedirs', 'open', 'fp.write']",4
data/input/tseries_m5_gluon/timeseries_m5.py:plot_prob_forecasts,plot_prob_forecasts,function,16,39,39,485,12.44,0,1,"['ts_entry', 'forecast_entry', 'path', 'sample_id', 'inline']","[None, None, None, None, None]","[None, None, None, None, 'True']",481,[],"['plt.subplots', 'forecast_entry.plot', 'ax.axvline', 'plt.legend', 'plt.savefig', 'plt.show', 'plt.clf', 'plt.close']",8
data/input/tseries_m5_gluon/util_gluonts.py:log,log,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],34,[],['print'],1
data/input/tseries_m5_gluon/util_gluonts.py:tag_create,tag_create,function,3,14,12,63,4.5,0,0,"['pars', 'sep']","[None, None]","[None, '"";""']",37,[],"['str', 'pars.items']",2
data/input/tseries_m5_gluon/util_gluonts.py:cols_remove,cols_remove,function,4,13,10,37,2.85,1,1,"['col1', 'col_remove']","[None, None]","[None, None]",42,[],[],0
data/input/tseries_m5_gluon/util_gluonts.py:create_timeseries1d,create_timeseries1d,function,14,25,23,314,12.56,1,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 'fun']","[None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 1000', "" 'D'"", ' 1', 'None']",49,[],"['pd.date_range', 'pd.DataFrame', 'range', 'data.append', 'pd.to_datetime', 'df.set_index', 'df.drop']",7
data/input/tseries_m5_gluon/util_gluonts.py:create_timeseries_2d,create_timeseries_2d,function,0,1,1,4,4.0,0,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 'fun']","[None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 660', "" 'D'"", ' 1', 'None']",63,[],[],0
data/input/tseries_m5_gluon/util_gluonts.py:create_timeseries_kd,create_timeseries_kd,function,19,35,30,369,10.54,2,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 'params']","[None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 660', "" 'D'"", ' 1', 'None']",68,[],"['pd.date_range', 'pd.DataFrame', 'len', 'range', 'data.append', 'pd.to_datetime', 'df.set_index', 'df.drop']",8
data/input/tseries_m5_gluon/util_gluonts.py:load_datasset_m5,load_datasset_m5,function,30,87,79,13826,158.92,0,0,['data_folder '],[None],"[' ""kaggle_data/m5_dataset""']",86,[],"['pd.read_csv', 'len', 'cols_remove', 'print']",4
data/input/tseries_m5_gluon/util_gluonts.py:forecast_metrics,forecast_metrics,function,12,26,23,286,11.0,0,2,"['tss', 'forecasts', 'quantiles', '0.5', '0.9]', 'show', 'dir_save']","[None, None, None, None, None, None, None]","[None, None, '[0.1', None, None, 'True', 'None']",132,[],"['Evaluator', 'evaluator', 'iter', 'num_series=len', 'log', 'json.dump']",6
data/input/tseries_m5_gluon/util_gluonts.py:plot_prob_forecasts,plot_prob_forecasts,function,16,53,53,656,12.38,0,0,"['tss', 'forecasts', 'plot_length ', 'prediction_intervals ', '']","[None, None, None, None, None]","[None, None, ' 90', '  (0.10', None]",145,[],"['log', 'plot_prob_forecasts', 'plt.subplots', 'forecast_entry.plot', 'plt.grid', 'plt.legend', 'plt.show']",7
data/input/tseries_m5_gluon/util_gluonts.py:plot_util,plot_util,function,30,73,68,874,11.97,1,2,"['tss', 'forecasts', 'prediction_length']","[None, None, None]","[None, None, '14']",166,[],"['os.makedirs', 'plot_prob_forecasts', 'plt.subplots', 'forecast_entry.plot', 'ax.axvline', 'plt.legend', 'plt.show', 'plt.clf', 'plt.savefig', 'plt.close', 'print', 'tqdm']",12
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_get_cardinality,gluonts_get_cardinality,function,6,24,22,180,7.5,1,1,"['dataset_path', 'TD']","[None, None]","['""""', 'None']",199,"['  """"""\n', '    Cardinality of gluonts dataset\n', '  :param dataset_path:\n', '  :param TD:\n', '  :return:\n', '  """"""\n']",['json.load'],1
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_dataset_check,gluonts_dataset_check,function,10,38,30,210,5.53,0,0,"['TD', 'nline']","[None, None]","[None, '3']",214,[],"['glob.glob', 'open', 'range', 'fp.readline', 'w.replace', 'json.loads']",6
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_create_dynamic,gluonts_create_dynamic,function,21,39,34,417,10.69,1,2,"['df', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, 'True', '28', '10', '1', '1']",231,"['    """"""\n', '        N_cat x N-timseries\n', '        \n', '        size of Dynamic == Length of time series.\n', '        \n', '        File ""C:\\D\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\core\\shape_base.py"", line 283, in vstack\n', '            return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)    \n', '        ValueError: all the input array dimensions except for the concatenation axis must match exactly    \n', '        https://github.com/awslabs/gluon-ts/issues/468        \n', '        \n', '        I think the issue is that when you run on the test in the way above, \n', '        the predict method would expect that your target has some length n and the dynamic features n + prediction length.\n', '        \n', '    """"""\n']",[],0
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_create_static,gluonts_create_static,function,17,24,23,473,19.71,1,0,"['df_static', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, '1', '28', '10', '1', '1']",292,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']","['static_cat_list.append', 'np.unique', 'static_cat_cardinalities.append', 'np.concatenate', 'static_cat.reshape', 'len']",6
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_static_cardinalities,gluonts_static_cardinalities,function,10,17,17,290,17.06,1,0,"['df_static', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, '1', '28', '10', '1', '1']",321,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']","['np.unique', 'static_cat_cardinalities.append', 'list']",3
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_create_timeseries,gluonts_create_timeseries,function,9,25,18,332,13.28,1,1,"['df_timeseries', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, '1', '28', '10', '1', '1']",346,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']","['np.ones', 'train_target_values.copy']",2
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_create_startdate,gluonts_create_startdate,function,4,8,7,71,8.88,1,0,"['date', 'freq', 'n_timeseries']","[None, None, None]","['""2011-01-29""', '""1D""', '1']",366,[],['range'],1
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_create_dataset,gluonts_create_dataset,function,34,117,64,820,7.01,1,6,"['timeseries_list', 'start_dates_list', 'feat_dynamic_list', 'feat_static_list', 'feat_static_real_list', 'freq']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', '""D""']",372,[],"['len', 'zip', 'target.tolist', 'fdr.tolist', 'train_ds.append']",5
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_save_to_file,gluonts_save_to_file,function,12,23,23,257,11.17,1,0,"['path', 'data']","['Path', ' List[Dict]']","[None, None]",403,[],"['print', 'os.makedirs', 'open', 'fp.write']",4
data/input/tseries_m5_gluon/util_gluonts.py:pandas_to_gluonts_multiseries,pandas_to_gluonts_multiseries,function,37,98,74,1812,18.49,0,4,"['df_timeseries', 'df_dynamic', 'df_static', ""pars={'submission'"", ""'single_pred_length'"", ""'submission_pred_length'"", ""'n_timeseries'"", ""'start_date'"", ""'freq'"", 'path_save', 'return_df']","[None, None, None, '', '28', '10', '1', '""2011-01-29""', '""D""}', None, None]","[None, None, None, ""{'submission':True"", None, None, None, None, None, 'None', 'False']",415,[],"['np.array', 'len', 'gluonts_create_dynamic', 'gluonts_create_static', 'gluonts_create_timeseries', 'gluonts_create_startdate', 'gluonts_create_dataset', 'gluonts_save_to_file', 'open', 'f.write', 'json.dumps']",11
data/input/tseries_m5_gluon/util_gluonts.py:pandas_to_gluonts,pandas_to_gluonts,function,38,198,86,3576,18.06,0,6,"['df_timeseries', 'df_dynamic', 'df_static', ""pars={'submission'"", ""'single_pred_length'"", ""'submission_pred_length'"", ""'n_timeseries'"", ""'start_date'"", ""'freq'"", 'path_save', 'return_df']","[None, None, None, '', '28', '10', '1', '""2011-01-29""', '""D""}', None, None]","[None, None, None, ""{'submission':True"", None, None, None, None, None, 'None', 'False']",472,[],"['np.array', 'len', 'gluonts_create_dynamic', 'gluonts_create_static', 'gluonts_create_timeseries', 'gluonts_create_startdate', 'gluonts_create_dataset', 'gluonts_save_to_file', 'open', 'f.write', 'json.dumps', 'pandas_to_gluonts']",12
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_to_pandas,gluonts_to_pandas,function,71,238,139,2740,11.51,5,15,"['dataset_path', 'data_type']","[None, None]","[None, None]",521,[],"['load_datasets', 'deepcopy', 'json.load', 'k.lstrip', 'd.items', 'print', 'np.transpose', 'all_static_Real.append', 'all_static.append', 'all_targets.append', 'len', 'df_static_real.astype', 'df_dynamic.rename', 'df_dynamic.astype', 'df_timeseries.rename', 'df_timeseries.astype', 'df_static.rename', 'df_static.astype']",18
data/input/tseries_m5_gluon/util_gluonts.py:pd_difference,pd_difference,function,23,77,62,799,10.38,0,5,"['df1', 'df2', 'is_real']","[None, None, None]","[None, None, 'False']",636,"['  """"""Identify differences between two pandas DataFrames\n', '    \n', '  """"""\n']","['print', 'any', 'df2.astype', 'df1.equals', 'df2.isnull', 'diff_mask.stack', 'np.where', 'pd.DataFrame']",8
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_json_check,gluonts_json_check,function,8,23,18,390,16.96,0,0,[],[],[],673,[],"['Path', 'gluonts_to_pandas', 'print']",3
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_model_eval,gluonts_model_eval,function,48,146,115,1783,12.21,0,4,"['estimator', 'TD', 'cardinalities', 'istrain', 'ismetric', 'isplot', 'pars']","[None, None, None, None, None, None, None]","['None', 'None', 'None', 'True', 'True', 'True', 'None']",692,[],"['DeepAREstimator', 'p.get', 'Trainer', 'estimator.train', 'make_evaluation_predictions', 'list', 'plot_prob_forecasts', 'forecast_metrics', 'gluonts_model_eval_all', 'load_datasets', 'estimator', 'log', 'Evaluator', 'num_series=len', 'type']",15
data/input/tseries_m5_gluon/util_gluonts.py:gluonts_model_eval_all,gluonts_model_eval_all,function,18,44,40,664,15.09,0,0,"['dataset_name', 'estimator']","[None, None]","[None, None]",734,[],"['load_datasets', 'estimator', 'log', 'estimator.train', 'make_evaluation_predictions', 'Evaluator', 'num_series=len', 'type']",8
data/input/tseries_m5_gluon/util_gluonts.py:test_gluonts_to_pandas,test_gluonts_to_pandas,function,17,35,32,753,21.51,0,0,[],[],[],761,[],"['load_datasset_m5', 'gluonts_static_cardinalities', 'pandas_to_gluonts', 'dataset_path=Path', 'gluonts_to_pandas', 'print']",6
data/input/tseries_m5_gluon/util_gluonts.py:test_pandas_to_gluonts,test_pandas_to_gluonts,function,21,48,42,594,12.38,0,0,['root'],[None],[None],798,[],"['load_datasset_m5', 'pandas_to_gluonts', 'gluonts_static_cardinalities', 'load_datasets', 'fire.Fire']",5
data/input/tseries_m5_gluon/util_gluon_colab.py:create_timeseries1d,create_timeseries1d,function,14,26,24,314,12.08,1,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 'fun']","[None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 1000', "" 'D'"", ' 1', 'None']",64,[],"['pd.date_range', 'pd.DataFrame', 'range', 'data.append', 'pd.to_datetime', 'df.set_index', 'df.drop']",7
data/input/tseries_m5_gluon/util_gluon_colab.py:create_timeseries_2d,create_timeseries_2d,function,0,1,1,4,4.0,0,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 'fun']","[None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 660', "" 'D'"", ' 1', 'None']",78,[],[],0
data/input/tseries_m5_gluon/util_gluon_colab.py:create_timeseries_kd,create_timeseries_kd,function,19,37,31,369,9.97,2,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 'params']","[None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 660', "" 'D'"", ' 1', 'None']",83,[],"['pd.date_range', 'pd.DataFrame', 'len', 'range', 'data.append', 'pd.to_datetime', 'df.set_index', 'df.drop']",8
data/input/tseries_m5_gluon/util_gluon_colab.py:gluonts_create_dynamic,gluonts_create_dynamic,function,17,38,30,452,11.89,1,2,"['df_dynamic', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, 'True', '28', '10', '1', '1']",110,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']",['str'],1
data/input/tseries_m5_gluon/util_gluon_colab.py:gluonts_create_static,gluonts_create_static,function,17,24,23,473,19.71,1,0,"['df_static', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, '1', '28', '10', '1', '1']",132,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']","['static_cat_list.append', 'np.unique', 'static_cat_cardinalities.append', 'np.concatenate', 'static_cat.reshape', 'len']",6
data/input/tseries_m5_gluon/util_gluon_colab.py:gluonts_static_cardinalities,gluonts_static_cardinalities,function,11,17,17,284,16.71,1,0,"['df_static', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, '1', '28', '10', '1', '1']",160,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']","['np.unique', 'static_cat_cardinalities.append']",2
data/input/tseries_m5_gluon/util_gluon_colab.py:gluonts_create_timeseries,gluonts_create_timeseries,function,9,25,18,332,13.28,1,1,"['df_timeseries', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, '1', '28', '10', '1', '1']",184,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']","['np.ones', 'train_target_values.copy']",2
data/input/tseries_m5_gluon/util_gluon_colab.py:create_startdate,create_startdate,function,4,8,7,71,8.88,1,0,"['date', 'freq', 'n_timeseries']","[None, None, None]","['""2011-01-29""', '""1D""', '1']",203,[],['range'],1
data/input/tseries_m5_gluon/util_gluon_colab.py:gluonts_create_dataset,gluonts_create_dataset,function,9,60,46,471,7.85,0,0,"['train_timeseries_list', 'start_dates_list', 'train_dynamic_list', 'train_static_list', 'freq']","[None, None, None, None, None]","[None, None, None, None, '""D""']",208,[],"['target.tolist', 'fdr.tolist', 'fsc.tolist', 'zip']",4
data/input/tseries_m5_gluon/util_gluon_colab.py:gluonts_save_to_file,gluonts_save_to_file,function,12,23,23,257,11.17,1,0,"['path', 'data']","['Path', ' List[Dict]']","[None, None]",227,[],"['print', 'os.makedirs', 'open', 'fp.write']",4
data/input/tseries_m5_gluon/util_gluon_colab.py:pandas_to_gluonts_multiseries,pandas_to_gluonts_multiseries,function,37,105,76,1804,17.18,0,4,"['df_timeseries', 'df_dynamic', 'df_static', ""pars={'submission'"", ""'single_pred_length'"", ""'submission_pred_length'"", ""'n_timeseries'"", ""'start_date'"", ""'freq'"", 'path_save', 'return_df']","[None, None, None, '', '28', '10', '1', '""2011-01-29""', '""D""}', None, None]","[None, None, None, ""{'submission':True"", None, None, None, None, None, 'None', 'False']",239,[],"['np.array', 'len', 'gluonts_create_dynamic', 'gluonts_create_static', 'gluonts_create_timeseries', 'create_startdate', 'gluonts_create_dataset', 'gluonts_save_to_file', 'open', 'f.write', 'json.dumps']",11
data/input/tseries_m5_gluon/util_gluon_colab.py:cols_remove,cols_remove,function,4,13,10,37,2.85,1,1,"['col1', 'col_remove']","[None, None]","[None, None]",295,[],[],0
data/input/tseries_m5_gluon/util_gluon_colab.py:timeseries_to_supervised,timeseries_to_supervised,function,7,17,15,146,8.59,0,0,"['data', 'lag']","[None, None]","[None, '1']",307,[],"['pd.DataFrame', 'range', 'columns.append', 'pd.concat', 'df.fillna']",5
data/input/tseries_m5_gluon/util_gluon_colab.py:difference,difference,function,10,14,14,125,8.93,1,0,"['dataset', 'interval']","[None, None]","[None, '1']",316,[],"['list', 'range', 'len', 'diff.append', 'pd.Series']",5
data/input/tseries_m5_gluon/util_gluon_colab.py:inverse_difference,inverse_difference,function,3,3,3,29,9.67,0,0,"['history', 'yhat', 'interval']","[None, None, None]","[None, None, '1']",324,[],[],0
data/input/tseries_m5_gluon/util_gluon_colab.py:scale,scale,function,13,19,17,274,14.42,0,0,"['train', 'test']","[None, None]","[None, None]",328,[],"['MinMaxScaler', 'scaler.fit', 'train.reshape', 'scaler.transform', 'test.reshape']",5
data/input/tseries_m5_gluon/util_gluon_colab.py:invert_scale,invert_scale,function,10,17,16,146,8.59,1,0,"['scaler', 'X', 'value']","[None, None, None]","[None, None, None]",341,[],"['np.array', 'array.reshape', 'len', 'scaler.inverse_transform']",4
data/input/tseries_m5_gluon/util_gluon_colab.py:fit_lstm,fit_lstm,function,18,42,40,566,13.48,1,0,"['train', 'batch_size', 'nb_epoch', 'lstm_neurons', 'timesteps', 'dense_neurons', 'mdn_output', 'mdn_Nmixes']","[None, None, None, None, None, None, None, None]","[None, '1', '5', '1', '1', '1', '1', '1']",349,[],"['X.reshape', 'Sequential', 'model.add', 'Adam', 'model.compile', 'print', 'range', 'model.fit', 'model.reset_states']",9
data/input/tseries_m5_gluon/util_gluon_colab.py:forecast_lstm,forecast_lstm,function,6,9,9,91,10.11,0,0,"['model', 'batch_size', 'timesteps', 'X']","[None, None, None, None]","[None, None, None, None]",368,[],"['X.reshape', 'len', 'model.predict']",3
data/input/tseries_m5_gluon/util_gluon_colab.py:test_supervised,test_supervised,function,0,1,1,4,4.0,0,0,[],[],[],379,[],[],0
data/input/tseries_online/clean_split.py:split_train_test,split_train_test,function,52,115,89,1488,12.94,2,3,['df'],[None],[None],17,"['    """"""\n', '    fill null values with median of the column\n', '    Dates are inserted multiple times: number of days, year, month, day\n', '    and binary vectors for year, month and day.\n', '    finally redundant columns are removed\n', '    """"""\n']","['np.zeros', 'col.startswith', 'columns.append', 'np.unique', 'len', 'X_cat.append', 'X_nums.append', 'tmp.fillna', 'X_date.append', 'enumerate', 'X_id.append', 'int', 'np.hstack', 'np.vstack']",14
data/input/tseries_online/clean_split.py:save_data,save_data,function,7,15,13,388,25.87,0,0,"['data_info', 'base_path']","[None, None]","[None, None]",110,[],"['open', 'json.dump']",2
data/input/tseries_online/clean_split.py:main,main,function,27,72,60,781,10.85,1,0,[],[],[],120,[],"['pd.read_csv', 'split_train_test', 'range', 'pd.DataFrame', 'np.vstack', 'zip', 'X_train.rename', 'X_test.rename', 'save_data']",9
data/input/zsandbox/test_encoder.py:os_get_function_name,os_get_function_name,function,4,4,4,47,11.75,0,0,[],[],[],37,[],['sys._getframe'],1
data/input/zsandbox/test_encoder.py:global_pars_update,global_pars_update,function,20,48,33,960,20.0,0,0,"['model_dict', 'data_name', 'config_name']","[None, None, None]","[None, None, None]",41,[],[],0
data/input/zsandbox/test_encoder.py:titanic1,titanic1,function,9,124,71,1165,9.4,0,0,['path_model_out'],[None],"['""""']",91,"['    """"""\n', '       Contains all needed informations for Light GBM Classifier model,\n', '       used for titanic classification task\n', '    """"""\n']","['os_get_function_name', 'post_process_fun', 'int', 'pre_process_fun']",4
source/bin/auto_feature_AFEM/AFE.py:timer,timer,function,8,17,15,188,11.06,0,0,['func'],[None],[None],25,[],"['wrapper', 'time.time', 'func', 'print']",4
source/bin/auto_feature_AFEM/AFE.py:ValueType,ValueType,class,9,18,18,143,7.94,0,0,[],[],[],14,[],[],0
source/bin/auto_feature_AFEM/AFE.py:BasePath,BasePath,class,26,66,48,702,10.64,2,2,[],[],[],35,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Path,Path,class,22,43,31,765,17.79,0,0,[],[],[],68,[],[],0
source/bin/auto_feature_AFEM/AFE.py:EntitySet,EntitySet,class,63,223,135,3222,14.45,8,10,[],[],[],103,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Entity,Entity,class,41,79,60,1264,16.0,1,4,[],[],[],232,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Function,Function,class,4,7,7,67,9.57,0,0,[],[],[],284,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Generator,Generator,class,224,858,428,11649,13.58,18,38,[],[],[],291,[],[],0
source/bin/auto_feature_AFEM/AFE.py:BasePath:__init__,BasePath:__init__,method,11,24,23,224,9.33,1,1,"['self', 'pathstype', 'name']","[None, None, None]","[None, None, 'None']",37,[],"['super', 'len', 'self._inversepathstype']",3
source/bin/auto_feature_AFEM/AFE.py:BasePath:getpathname,BasePath:getpathname,method,2,2,2,15,7.5,0,0,['self'],[None],[None],47,[],[],0
source/bin/auto_feature_AFEM/AFE.py:BasePath:getpathentities,BasePath:getpathentities,method,2,2,2,23,11.5,0,0,['self'],[None],[None],49,[],[],0
source/bin/auto_feature_AFEM/AFE.py:BasePath:getpathstype,BasePath:getpathstype,method,2,2,2,20,10.0,0,0,['self'],[None],[None],51,[],[],0
source/bin/auto_feature_AFEM/AFE.py:BasePath:getinversepathstype,BasePath:getinversepathstype,method,2,2,2,27,13.5,0,0,['self'],[None],[None],53,[],[],0
source/bin/auto_feature_AFEM/AFE.py:BasePath:_inversepathstype,BasePath:_inversepathstype,method,8,18,16,167,9.28,1,1,['self'],[None],[None],55,[],['newpath.append'],1
source/bin/auto_feature_AFEM/AFE.py:BasePath:getlastentityid,BasePath:getlastentityid,method,2,2,2,26,13.0,0,0,['self'],[None],[None],65,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Path:__init__,Path:__init__,method,13,14,14,221,15.79,0,0,"['self', 'pathstype', 'df', 'firstindex', 'start_time_index', 'lastindex', 'last_time_index', 'name', 'start_part_id']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, 'None', 'None']",70,[],['super'],1
source/bin/auto_feature_AFEM/AFE.py:Path:getfirstkey,Path:getfirstkey,method,2,2,2,21,10.5,0,0,['self'],[None],[None],79,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Path:getlastkey,Path:getlastkey,method,2,2,2,20,10.0,0,0,['self'],[None],[None],82,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Path:getstarttimeindex,Path:getstarttimeindex,method,2,2,2,27,13.5,0,0,['self'],[None],[None],85,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Path:getlasttimeindex,Path:getlasttimeindex,method,2,2,2,26,13.0,0,0,['self'],[None],[None],88,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Path:getpathdetail,Path:getpathdetail,method,2,5,5,156,31.2,0,0,['self'],[None],[None],91,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Path:getstartpartname,Path:getstartpartname,method,2,2,2,24,12.0,0,0,['self'],[None],[None],96,[],[],0
source/bin/auto_feature_AFEM/AFE.py:EntitySet:__init__,EntitySet:__init__,method,6,8,8,100,12.5,0,0,"['self', 'name']","[None, None]","[None, None]",105,[],"['super', 'nx.MultiDiGraph']",2
source/bin/auto_feature_AFEM/AFE.py:EntitySet:draw,EntitySet:draw,method,1,1,1,34,34.0,0,0,['self'],[None],[None],110,[],['nx.draw_networkx'],1
source/bin/auto_feature_AFEM/AFE.py:EntitySet:entity_from_dataframe,EntitySet:entity_from_dataframe,method,3,5,5,122,24.4,0,0,"['self', 'entity_id', 'dataframe', 'index', 'time_index', 'variable_types']","[None, None, None, None, None, None]","[None, None, None, None, 'None', 'None']",124,[],['Entity'],1
source/bin/auto_feature_AFEM/AFE.py:EntitySet:addrelationship,EntitySet:addrelationship,method,1,3,3,93,31.0,0,0,"['self', 'entityA', 'entityB', 'keyA', 'keyB']","[None, None, None, None, None]","[None, None, None, None, None]",129,[],[],0
source/bin/auto_feature_AFEM/AFE.py:EntitySet:search_path,EntitySet:search_path,method,7,18,17,217,12.06,0,1,"['self', 'targetnode', 'maxdepth', 'max_famous_son']","[None, None, None, None]","[None, None, None, None]",132,[],"['nx.shortest_path', 'self._search_path', 'enumerate']",3
source/bin/auto_feature_AFEM/AFE.py:EntitySet:_search_path,EntitySet:_search_path,method,38,134,81,1727,12.89,6,9,"['self', 'shortpaths', 'targetnode', 'maxdepth', 'max_famous_son']","[None, None, None, None, None]","[None, None, None, None, None]",143,[],"['pathstypetransform', 'range', 'tmppath.append', 'len', 'pathstype.append', 'paths.append', 'self._search_path']",7
source/bin/auto_feature_AFEM/AFE.py:EntitySet:_pathstype,EntitySet:_pathstype,method,9,21,17,418,19.9,2,0,"['self', 'paths']","[None, None]","[None, None]",209,[],"['range', 'subpathstype.append', 'pathstype.append']",3
source/bin/auto_feature_AFEM/AFE.py:EntitySet:collectiontransform,EntitySet:collectiontransform,method,0,1,1,4,4.0,0,0,"['self', 'path', 'target']","[None, None, None]","[None, None, None]",224,[],[],0
source/bin/auto_feature_AFEM/AFE.py:EntitySet:getentity,EntitySet:getentity,method,2,11,11,90,8.18,0,0,"['self', 'entityid']","[None, None]","[None, None]",226,[],"['Exception', 'str']",2
source/bin/auto_feature_AFEM/AFE.py:Entity:__init__,Entity:__init__,method,20,38,29,612,16.11,0,3,"['self', 'entity_id', 'dataframe', 'index', 'time_index', 'variable_types']","[None, None, None, None, None, None]","[None, None, None, None, 'None', 'None']",234,[],"['super', 'dataframe.add_prefix', 'pd.to_datetime']",3
source/bin/auto_feature_AFEM/AFE.py:Entity:getcolumns,Entity:getcolumns,method,4,8,7,99,12.38,0,1,"['self', 'columns']","[None, None]","[None, None]",255,[],['isinstance'],1
source/bin/auto_feature_AFEM/AFE.py:Entity:getfeattype,Entity:getfeattype,method,4,5,4,79,15.8,0,0,"['self', 'featname']","[None, None]","[None, None]",263,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Entity:getfeatname,Entity:getfeatname,method,4,7,7,55,7.86,1,0,['self'],[None],[None],270,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Entity:merge,Entity:merge,method,8,8,8,210,26.25,0,0,"['self', 'features', 'path', 'how']","[None, None, None, None]","[None, None, None, ""'right'""]",273,[],"['path.getpathdetail', 'merge']",2
source/bin/auto_feature_AFEM/AFE.py:Function:__init__,Function:__init__,method,3,4,4,44,11.0,0,0,"['self', 'arg']","[None, None]","[None, None]",286,[],['super'],1
source/bin/auto_feature_AFEM/AFE.py:Generator:__init__,Generator:__init__,method,5,7,7,79,11.29,0,0,"['self', 'es']","[None, None]","[None, None]",292,[],['super'],1
source/bin/auto_feature_AFEM/AFE.py:Generator:reload_data,Generator:reload_data,method,2,2,2,10,5.0,0,0,"['self', 'es']","[None, None]","[None, None]",298,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Generator:layer,Generator:layer,method,30,68,54,1150,16.91,2,3,"['self', 'path', 'start_part', 'start_part_id']","[None, None, None, None]","[None, None, 'None', 'None']",302,[],"['hash', 'self._layer', 'layers', 'len', 'self.layer', 'range', 'dataframe.merge', 'Path']",8
source/bin/auto_feature_AFEM/AFE.py:Generator:layers,Generator:layers,method,18,44,37,768,17.45,2,2,"['self', 'paths', 'start_part', 'start_part_id']","[None, None, None, None]","[None, None, 'None', 'None']",320,[],"['len', 'self.layer', 'range', 'dataframe.merge', 'Path']",5
source/bin/auto_feature_AFEM/AFE.py:Generator:defaultfunc,Generator:defaultfunc,method,14,39,28,563,14.44,1,1,"['self', 'path']","[None, None]","[None, None]",350,[],"['len', 'targetentity.getfeatname', 'targetentity.getfeattype']",3
source/bin/auto_feature_AFEM/AFE.py:Generator:_layer,Generator:_layer,method,50,164,99,2206,13.45,2,8,"['self', 'path', 'start_part', 'start_part_id']","[None, None, None, None]","[None, None, 'None', 'None']",375,[],"['path.getinversepathstype', 'pd.DataFrame', 'range', 'set', 'leftcolumns.add', 'len', 'leftremain.append', 'list', 'rightcolumns.add', 'rightremain.append', 'start_part[list', 'dataframe[list', 'dataframe.drop_duplicates', 'Path']",14
source/bin/auto_feature_AFEM/AFE.py:Generator:pathfilter,Generator:pathfilter,method,11,18,16,340,18.89,0,1,"['self', 'path', 'function', 'start_part', 'start_part_id']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",446,[],"['hash', 'path.getstartpartname']",2
source/bin/auto_feature_AFEM/AFE.py:Generator:aggregate,Generator:aggregate,method,53,259,134,2922,11.28,4,12,"['self', 'path', 'function', 'iftimeuse ', 'winsize', 'lagsize']","[None, None, None, None, None, None]","[None, None, None, ' True', ""'all'"", ""'last'""]",458,[],"['step', 'float', 'Exception', 'int', 'hash', 'path.getstartpartname', 'isinstance', 'feature_cols.extend', 'feature_cols.append', 'path.getstarttimeindex', 'path.getlasttimeindex', 'lag<step', 'lag>step', 'len', 'tmpmerged.groupby', 'set', 'callable', 'newfeats.append', 'newfeats_name.append', 'NotImplementedError', 'pd.concat']",21
source/bin/auto_feature_AFEM/AFE.py:Generator:add_compute_series,Generator:add_compute_series,method,7,10,10,179,17.9,1,0,"['self', 'compute_series', 'start_part']","[None, None, None]","[None, None, 'None']",576,[],"['layer', 'aggregate', 'newfeats.merge']",3
source/bin/auto_feature_AFEM/AFE.py:Generator:pathcompute,Generator:pathcompute,method,46,112,81,1554,13.88,3,7,"['self', 'cs', 'ngroups', 'njobs']","[None, None, None, None]","[None, None, ""'auto'"", '1']",584,[],"['isinstance', 'set', 'path.getpathstype', 'selected.add', 'list', 'es.getentity', 'selected.append', 'len', 'min', 'range', 'int', 'time.time', 'start.groupby', 'dfgroups.append', 'xrange', 'self.collect_agg', 'Pool', 'p.map', 'pd.concat', 'merge']",20
source/bin/auto_feature_AFEM/AFE.py:Generator:collect_agg,Generator:collect_agg,method,15,23,21,363,15.78,1,1,"['self', 'inputs']","[None, None]","[None, None]",649,[],"['tqdm', 'self.layer', 'newpath.getpathdetail', 'newfeats.append', 'len', 'pd.concat']",6
source/bin/auto_feature_AFEM/AFE.py:Generator:layer_sequencal_agg,Generator:layer_sequencal_agg,method,28,65,50,881,13.55,2,2,"['self', 'path', 'es', 'ngroups ', 'njobs']","[None, None, None, None, None]","[None, None, None, ' None', '1']",672,[],"['aggregation', 'inverse', 'range', 'int', 'time.time', 'dfgroups.append', 'xrange', 'collect_agg', 'Pool', 'p.map']",10
source/bin/auto_feature_AFEM/AFE.py:Generator:transform,Generator:transform,method,0,1,1,4,4.0,0,0,"['self', 'path', 'featurenames', 'function']","[None, None, None, None]","[None, None, None, None]",710,[],[],0
source/bin/auto_feature_AFEM/AFE.py:Generator:singlepathcompunation,Generator:singlepathcompunation,method,11,20,20,286,14.3,1,1,"['self', 'pathstype', 'targetfeatures', 'functionset']","[None, None, None, None]","[None, None, None, None]",714,[],"['checkorder', 'Exception', 'self.mergelayer']",3
source/bin/auto_feature_AFEM/AFE.py:Generator:pathcompunation,Generator:pathcompunation,method,20,38,32,416,10.95,1,2,"['self', 'pathsfunc']","[None, None]","[None, None]",723,[],"['self.mergelayer', 'time_index.append', 'func']",3
source/bin/deltapy/extract.py:set_property,set_property,function,7,25,22,177,7.08,0,1,"['key', 'value']","[None, None]","[None, None]",19,"['    """"""\n', '    This method returns a decorator that sets the property key of the function to value\n', '    """"""\n']","['decorate_func', 'setattr']",2
source/bin/deltapy/extract.py:abs_energy,abs_energy,function,5,10,10,76,7.6,0,1,['x'],[None],[None],35,[],"['isinstance', 'np.asarray', 'np.dot']",3
source/bin/deltapy/extract.py:cid_ce,cid_ce,function,10,24,19,166,6.92,0,3,"['x', 'normalize']","[None, None]","[None, None]",47,[],"['isinstance', 'np.asarray', 'np.std', 'np.mean', 'np.diff', 'np.sqrt']",6
source/bin/deltapy/extract.py:mean_abs_change,mean_abs_change,function,2,2,2,33,16.5,0,0,['x'],[None],[None],67,[],['np.mean'],1
source/bin/deltapy/extract.py:_roll,_roll,function,7,13,13,105,8.08,0,1,"['a', 'shift']","[None, None]","[None, None]",76,[],"['isinstance', 'np.asarray', 'len', 'np.concatenate']",4
source/bin/deltapy/extract.py:mean_second_derivative_central,mean_second_derivative_central,function,3,10,10,73,7.3,0,0,['x'],[None],[None],82,[],"['np.array', '_roll', 'np.mean']",3
source/bin/deltapy/extract.py:variance_larger_than_standard_deviation,variance_larger_than_standard_deviation,function,4,6,5,30,5.0,0,0,['x'],[None],[None],93,[],"['np.var', 'np.sqrt']",2
source/bin/deltapy/extract.py:var_index,var_index,function,19,52,43,377,7.25,1,0,"['time', 'param']","[None, None]","[None, 'var_index_param']",106,[],"['param.items', 'np.power', 'np.mean', 'len', 'np.var', 'sum', 'final.append', 'keys.append', 'zip']",9
source/bin/deltapy/extract.py:symmetry_looking,symmetry_looking,function,10,23,22,247,10.74,1,1,"['x', 'param=[{""r""']","[None, '']","[None, '[{""r"": 0.2}]']",132,[],"['isinstance', 'np.asarray', 'np.abs', 'np.median', 'np.max', 'np.min']",6
source/bin/deltapy/extract.py:has_duplicate_max,has_duplicate_max,function,5,11,11,88,8.0,0,1,['x'],[None],[None],147,"['    """"""\n', '    Checks if the maximum value of x is observed more than once\n', '\n', '    :param x: the time series to calculate the feature of\n', '    :type x: numpy.ndarray\n', '    :return: the value of this feature\n', '    :return type: bool\n', '    """"""\n']","['isinstance', 'np.asarray', 'np.sum', 'np.max']",4
source/bin/deltapy/extract.py:partial_autocorrelation,partial_autocorrelation,function,6,42,29,374,8.9,0,2,"['x', 'param=[{""lag""']","[None, '']","[None, '[{""lag"": 1}]']",166,[],"['max', 'len', 'list']",3
source/bin/deltapy/extract.py:augmented_dickey_fuller,augmented_dickey_fuller,function,26,64,40,410,6.41,2,4,"['x', 'param=[{""attr""']","[None, '']","[None, '[{""attr"": ""teststat""}]']",190,[],['adfuller'],1
source/bin/deltapy/extract.py:gskew,gskew,function,8,21,18,250,11.9,0,0,['x'],[None],[None],214,[],"['np.median', 'np.percentile']",2
source/bin/deltapy/extract.py:stetson_mean,stetson_mean,function,21,48,43,399,8.31,1,1,"['x', 'param']","[None, None]","[None, 'stestson_param']",234,[],"['np.median', 'range', 'np.abs', 'np.sqrt', 'weight1.mean', 'np.mean']",6
source/bin/deltapy/extract.py:length,length,function,1,2,2,12,6.0,0,0,['x'],[None],[None],261,[],['len'],1
source/bin/deltapy/extract.py:count_above_mean,count_above_mean,function,4,6,6,40,6.67,0,0,['x'],[None],[None],270,[],"['np.mean', 'np.where']",2
source/bin/deltapy/extract.py:get_length_sequences_where,get_length_sequences_where,function,2,24,20,126,5.25,0,2,['x'],[None],[None],282,[],"['len', 'itertools.groupby']",2
source/bin/deltapy/extract.py:longest_strike_below_mean,longest_strike_below_mean,function,5,16,14,129,8.06,0,1,['x'],[None],[None],290,[],"['isinstance', 'np.asarray', 'np.max', 'np.mean']",4
source/bin/deltapy/extract.py:wozniak,wozniak,function,15,74,47,497,6.72,3,3,"['magnitude', 'param']","[None, None]","[None, 'woz_param']",304,[],"['len', 'np.std', 'np.mean', 'range', 'if', 'iters.append', 'enumerate']",7
source/bin/deltapy/extract.py:last_location_of_maximum,last_location_of_maximum,function,4,12,11,71,5.92,0,0,['x'],[None],[None],336,[],"['np.asarray', 'np.argmax', 'len']",3
source/bin/deltapy/extract.py:fft_coefficient,fft_coefficient,function,17,80,53,602,7.53,1,1,"['x', 'param = [{""coeff""', '""attr""']","[None, '', ' ""real""}]']","[None, ' [{""coeff"": 10', None]",346,[],"['min', 'set', 'complex_agg', 'np.abs', 'np.angle', 'len']",6
source/bin/deltapy/extract.py:ar_coefficient,ar_coefficient,function,20,56,44,563,10.05,1,2,"['x', 'param=[{""coeff""', '""k""']","[None, '', ' 5}]']","[None, '[{""coeff"": 5', None]",377,[],"['list', 'AR', 'calculated_AR.fit', 'res.items']",4
source/bin/deltapy/extract.py:index_mass_quantile,index_mass_quantile,function,9,28,22,259,9.25,0,1,"['x', 'param=[{""q""']","[None, '']","[None, '[{""q"": 0.3}]']",416,[],"['np.asarray', 'np.abs', 'sum', 'np.cumsum']",4
source/bin/deltapy/extract.py:number_cwt_peaks,number_cwt_peaks,function,1,11,10,124,11.27,0,0,"['x', 'param']","[None, None]","[None, 'cwt_param']",441,[],['len'],1
source/bin/deltapy/extract.py:spkt_welch_density,spkt_welch_density,function,11,62,46,500,8.06,2,2,"['x', 'param=[{""coeff""']","[None, '']","[None, '[{""coeff"": 5}]']",451,[],"['welch', 'nperseg=min', 'len', 'np.max', 'zip', 'list']",6
source/bin/deltapy/extract.py:linear_trend_timewise,linear_trend_timewise,function,10,19,19,236,12.42,1,0,"['x', 'param= [{""attr""']","[None, '']","[None, ' [{""attr"": ""pvalue""}]']",475,[],"['np.asarray', 'float', 'linregress', 'getattr']",4
source/bin/deltapy/extract.py:c3,c3,function,8,25,20,155,6.2,0,2,"['x', 'lag']","[None, None]","[None, '3']",495,[],"['isinstance', 'np.asarray', 'np.mean', '_roll']",4
source/bin/deltapy/extract.py:binned_entropy,binned_entropy,function,11,24,21,170,7.08,0,1,"['x', 'max_bins']","[None, None]","[None, '10']",510,[],"['isinstance', 'np.asarray', 'np.histogram', 'np.sum']",4
source/bin/deltapy/extract.py:_embed_seq,_embed_seq,function,16,53,41,218,4.11,2,2,"['X', 'Tau', 'D']","[None, None, None]","[None, None, None]",527,[],"['print', 'exit', 'np.zeros', 'range']",4
source/bin/deltapy/extract.py:svd_entropy,svd_entropy,function,16,47,40,383,8.15,1,0,"['epochs', 'param']","[None, None]","[None, 'svd_param']",544,[],"['svd_entropy_1d', '_embed_seq', 'sum', 'np.sum', 'np.log', 'final.append', 'enumerate']",7
source/bin/deltapy/extract.py:_hjorth_mobility,_hjorth_mobility,function,7,12,10,114,9.5,0,0,['epochs'],[None],[None],568,[],"['np.diff', 'np.std', 'np.divide']",3
source/bin/deltapy/extract.py:hjorth_complexity,hjorth_complexity,function,8,16,13,179,11.19,0,0,['epochs'],[None],[None],576,[],"['np.diff', 'np.std', 'np.divide', '_hjorth_mobility']",4
source/bin/deltapy/extract.py:_estimate_friedrich_coefficients,_estimate_friedrich_coefficients,function,20,49,43,475,9.69,0,0,"['x', 'm', 'r']","[None, None, None]","[None, None, None]",589,[],"['pd.DataFrame', 'np.diff', 'pd.qcut', 'df.groupby', 'result.dropna', 'np.polyfit']",6
source/bin/deltapy/extract.py:max_langevin_fixed_point,max_langevin_fixed_point,function,6,14,12,176,12.57,0,0,"['x', 'r', 'm']","[None, None, None]","[None, '3', '30']",611,[],"['_estimate_friedrich_coefficients', 'np.max']",2
source/bin/deltapy/extract.py:willison_amplitude,willison_amplitude,function,1,7,7,72,10.29,0,0,"['X', 'param']","[None, None]","[None, 'will_param']",629,[],[],0
source/bin/deltapy/extract.py:percent_amplitude,percent_amplitude,function,13,30,28,334,11.13,1,0,"['x', 'param ']","[None, None]","[None, 'perc_param']",641,[],"['np.max', 'np.min', 'np.median', 'final.append', 'abs', 'zip']",6
source/bin/deltapy/extract.py:cad_prob,cad_prob,function,1,11,11,106,9.64,0,0,"['cads', 'param']","[None, None]","[None, 'cad_param']",665,[],"['stats.percentileofscore', 'float']",2
source/bin/deltapy/extract.py:zero_crossing_derivative,zero_crossing_derivative,function,5,20,20,171,8.55,0,0,"['epochs', 'param']","[None, None]","[None, 'zero_param']",677,[],"['np.diff', 'np.apply_along_axis', 'np.sum']",3
source/bin/deltapy/extract.py:detrended_fluctuation_analysis,detrended_fluctuation_analysis,function,32,123,92,751,6.11,2,4,['epochs'],[None],[None],689,[],"['dfa_1d', 'np.array', 'np.mean', 'np.cumsum', 'np.floor', 'int', 'np.zeros', 'F', 'range', 'len', 'print', 'exit', 'list', 'np.vstack', 'np.ones', 'np.sqrt', 'np.log', 'np.apply_along_axis']",18
source/bin/deltapy/extract.py:_embed_seq,_embed_seq,function,16,53,41,218,4.11,2,2,"['X', 'Tau', 'D']","[None, None, None]","[None, None, None]",736,"['        """"""Compute Petrosian Fractal Dimension of a time series from either two\n', '        cases below:\n', '            1. X, the time series of type list (default)\n', '            2. D, the first order differential sequence of X (if D is provided,\n', '               recommended to speed up)\n', ""        In case 1, D is computed using Numpy's difference function.\n"", '        To speed up, it is recommended to compute D before calling this function\n', '        because D may also be used by other functions whereas computing it here\n', '        again will slow down.\n', '        """"""\n']","['print', 'exit', 'np.zeros', 'range']",4
source/bin/deltapy/extract.py:fisher_information,fisher_information,function,9,37,34,319,8.62,0,0,"['epochs', 'param']","[None, None]","[None, 'fisher_param']",746,[],"['fisher_info_1d', '_embed_seq', 'sum', 'np.sum']",4
source/bin/deltapy/extract.py:higuchi_fractal_dimension,higuchi_fractal_dimension,function,16,72,54,453,6.29,3,0,"['epochs', 'param']","[None, None]","[None, 'hig_param']",766,[],"['hfd_1d', 'len', 'range', 'int', 'abs', 'np.floor', 'float', 'Lk.append', 'L.append', 'x.append', 'np.apply_along_axis']",11
source/bin/deltapy/extract.py:petrosian_fractal_dimension,petrosian_fractal_dimension,function,20,50,42,282,5.64,1,2,['epochs'],[None],[None],794,[],"['pfd_1d', 'np.diff', 'D.tolist', 'range', 'len', 'np.log10', 'np.apply_along_axis']",7
source/bin/deltapy/extract.py:hurst_exponent,hurst_exponent,function,39,85,66,542,6.38,3,2,['epochs'],[None],[None],824,[],"['hurst_1d', 'np.array', 'np.arange', 'np.cumsum', 'np.zeros', 'range', 'np.std', 'np.ptp', 'len', 'np.diff', 'max', 'np.log', 'np.column_stack', 'np.ones', 'np.apply_along_axis']",15
source/bin/deltapy/extract.py:_embed_seq,_embed_seq,function,16,53,41,218,4.11,2,2,"['X', 'Tau', 'D']","[None, None, None]","[None, None, None]",736,"['        """"""Compute Petrosian Fractal Dimension of a time series from either two\n', '        cases below:\n', '            1. X, the time series of type list (default)\n', '            2. D, the first order differential sequence of X (if D is provided,\n', '               recommended to speed up)\n', ""        In case 1, D is computed using Numpy's difference function.\n"", '        To speed up, it is recommended to compute D before calling this function\n', '        because D may also be used by other functions whereas computing it here\n', '        again will slow down.\n', '        """"""\n']","['print', 'exit', 'np.zeros', 'range']",4
source/bin/deltapy/extract.py:largest_lyauponov_exponent,largest_lyauponov_exponent,function,43,142,119,1210,8.52,0,0,"['epochs', 'param']","[None, None]","[None, 'lyaup_param']",871,[],"['LLE_1d', '_embed_seq', 'len', 'np.tile', 'np.transpose', 'np.sqrt', 'np.tri', 'np.logical_and', 'np.sum', 'np.arange', 'np.vstack', 'np.ones', 'np.apply_along_axis']",13
source/bin/deltapy/extract.py:whelch_method,whelch_method,function,15,31,29,272,8.77,1,0,"['data', 'param']","[None, None]","[None, 'whelch_param']",926,[],"['signal.welch', 'pd.DataFrame', 'df.sort_values', 'final.append', 'zip']",5
source/bin/deltapy/extract.py:find_freq,find_freq,function,18,39,35,441,11.31,1,0,"['serie', 'param']","[None, None]","[None, 'freq_param']",948,[],"['np.array', 'range', 'len', 'pd.DataFrame', 'df.sort_values', 'final.append', 'zip']",7
source/bin/deltapy/extract.py:flux_perc,flux_perc,function,15,28,24,400,14.29,0,0,['magnitude'],[None],[None],969,[],"['np.sort', 'len', 'int']",3
source/bin/deltapy/extract.py:range_cum_s,range_cum_s,function,10,18,18,138,7.67,0,0,['magnitude'],[None],[None],990,[],"['np.std', 'len', 'np.mean', 'np.cumsum', 'np.max', 'np.min']",6
source/bin/deltapy/extract.py:structure_func,structure_func,function,39,116,84,1092,9.41,2,3,"['time', 'param']","[None, None]","[None, 'struct_param']",1007,[],"['param.items', 'np.zeros', 'interp1d', 'np.linspace', 'np.max', 'f', 'np.arange', 'np.mean', 'np.power', 'np.abs', 'np.log10', 'len', 'np.polyfit', 'dict_final.items', 'zip']",15
source/bin/deltapy/extract.py:kurtosis,kurtosis,function,5,8,8,72,9.0,0,1,['x'],[None],[None],1061,[],"['isinstance', 'pd.Series']",2
source/bin/deltapy/extract.py:stetson_k,stetson_k,function,8,19,18,147,7.74,0,0,['x'],[None],[None],1074,"['    """"""A robust kurtosis statistic.""""""\n']","['len', 'stetson_mean', 'np.sqrt', 'np.mean']",4
source/bin/deltapy/interact.py:lowess,lowess,function,33,91,70,648,7.12,3,0,"['df', 'cols', 'y', 'f', 'iter']","[None, None, None, None, None]","[None, None, None, '2. / 3.', '3']",12,[],"['len', 'int', 'range', 'np.clip', 'np.zeros', 'np.ones', 'np.array', 'np.sum', 'linalg.solve', 'np.median']",10
source/bin/deltapy/interact.py:autoregression,autoregression,function,21,43,34,459,10.67,2,2,"['df', 'drop', 'settings={""autoreg_lag""']","[None, None, '']","[None, 'None', '{""autoreg_lag"":4}']",44,"['    """"""\n', '    This function calculates the autoregression for each channel.\n', '    :param x: the input signal. Its size is (number of channels, samples).\n', '    :param settings: a dictionary with one attribute, ""autoreg_lag"", that is the max lag for autoregression.\n', '    :return: the ""final_value"" is a matrix (number of channels, autoreg_lag) indicating the parameters of\n', '      autoregression for each channel.\n', '    """"""\n']","['df.drop', 'timer', 'np.zeros', 'range', 'AR', 'len', 'np.real', 'pd.concat']",8
source/bin/deltapy/interact.py:muldiv,muldiv,function,11,21,18,222,10.57,2,1,"['df', 'feature_list']","[None, None]","[None, None]",80,[],[],0
source/bin/deltapy/interact.py:decision_tree_disc,decision_tree_disc,function,12,19,18,245,12.89,1,0,"['df', 'cols', 'depth']","[None, None, None]","[None, None, '4']",95,[],"['DecisionTreeRegressor', 'tree_model.fit', 'tree_model.predict']",3
source/bin/deltapy/interact.py:quantile_normalize,quantile_normalize,function,19,39,29,292,7.49,3,2,"['df', 'drop']","[None, None]","[None, None]",108,[],"['df.drop', 'dic.update', 'sorted', 'pd.DataFrame', 'sorted_df.mean', 'np.searchsorted', 'pd.concat']",7
source/bin/deltapy/interact.py:haversine_distance,haversine_distance,function,14,30,26,230,7.67,0,0,"['row', 'lon', 'lat']","[None, None, None]","[None, '""Open""', '""Close""']",134,[],"['radians', 'sin', 'cos', 'atan2', 'sqrt']",5
source/bin/deltapy/interact.py:tech,tech,function,2,7,7,96,13.71,0,0,['df'],[None],[None],152,[],['ta.add_all_ta_features'],1
source/bin/deltapy/interact.py:genetic_feat,genetic_feat,function,10,37,36,524,14.16,0,0,"['df', 'num_gen', 'num_comp']","[None, None, None]","[None, '20', '10']",161,[],"['SymbolicTransformer', 'gp.fit_transform', 'pd.DataFrame', 'range', 'pd.concat']",5
source/bin/deltapy/mapper.py:pca_feature,pca_feature,function,21,69,50,803,11.64,0,5,"['df', 'memory_issues', 'mem_iss_component', 'variance_or_components', 'n_components', 'drop_cols', 'non_linear']","[None, None, None, None, None, None, None]","[None, 'False', 'False', '0.80', '5', 'None', 'True']",13,[],"['KernelPCA', 'ValueError', 'IncrementalPCA', 'PCA', 'pca.fit_transform', 'pd.concat', 'range', 'pd.DataFrame']",8
source/bin/deltapy/mapper.py:cross_lag,cross_lag,function,18,34,23,367,10.79,0,2,"['df', 'drop', 'lags', 'components']","[None, None, None, None]","[None, 'None', '1', '4']",43,[],"['df.drop', 'df.shift', 'df_2.dropna', 'CCA', 'cca.fit', 'cca.transform', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",9
source/bin/deltapy/mapper.py:a_chi,a_chi,function,16,31,21,394,12.71,0,2,"['df', 'drop', 'lags', 'sample_steps']","[None, None, None, None]","[None, 'None', '1', '2']",71,[],"['df.drop', 'df.shift', 'df_2.dropna', 'AdditiveChi2Sampler', 'chi2sampler.fit_transform', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",8
source/bin/deltapy/mapper.py:encoder_dataset,encoder_dataset,function,25,66,50,989,14.98,0,2,"['df', 'drop', 'dimesions']","[None, None, None]","[None, 'None', '20']",99,[],"['minmax_scale', 'pd.DataFrame', 'encoded_train.add_prefix', 'pd.concat']",4
source/bin/deltapy/mapper.py:lle_feat,lle_feat,function,13,21,15,242,11.52,0,2,"['df', 'drop', 'components']","[None, None, None]","[None, 'None', '4']",136,[],"['df.drop', 'LocallyLinearEmbedding', 'embedding.fit_transform', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",6
source/bin/deltapy/mapper.py:feature_agg,feature_agg,function,14,23,17,294,12.78,0,2,"['df', 'drop', 'components']","[None, None, None]","[None, 'None', '4']",154,[],"['df.drop', 'min', 'cluster.FeatureAgglomeration', 'agglo.fit', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",7
source/bin/deltapy/mapper.py:neigh_feat,neigh_feat,function,15,28,20,304,10.86,0,2,"['df', 'drop', 'neighbors']","[None, None, None]","[None, None, '6']",177,[],"['df.drop', 'min', 'NearestNeighbors', 'neigh.fit', 'neigh.kneighbors', 'pd.DataFrame', 'df.add_prefix', 'pd.concat']",8
source/bin/deltapy/transform.py:infer_seasonality,infer_seasonality,function,1,3,3,89,29.67,0,0,"['train', 'index=0)', 'normallytrain', 'min_period', 'max_period=None)try']","[None, '', None, None, '']","[None, '0): ##skip the first one', None, '4', 'None)try:']",11,[],['int'],1
source/bin/deltapy/transform.py:robust_scaler,robust_scaler,function,14,27,21,224,8.3,0,2,"['df', 'drop', 'quantile_range', '75']","[None, None, None, None]","[None, 'None', '(25', None]",22,[],"['df.drop', 'np.median', 'np.percentile', 'pd.concat']",4
source/bin/deltapy/transform.py:standard_scaler,standard_scaler,function,11,24,17,160,6.67,0,2,"['df', 'drop']","[None, None]","[None, None]",39,[],"['df.drop', 'np.mean', 'np.std', 'pd.concat']",4
source/bin/deltapy/transform.py:fast_fracdiff,fast_fracdiff,function,17,38,34,231,6.08,1,0,"['x', 'cols', 'd']","[None, None, None]","[None, None, None]",57,[],"['len', 'int', 'np.ceil', 'np.arange', 'tuple', 'pl.ifft', 'pl.fft', 'np.real']",8
source/bin/deltapy/transform.py:outlier_detect,outlier_detect,function,21,93,51,1214,13.05,0,4,"['data', 'col', 'threshold', 'method']","[None, None, None, None]","[None, None, '1', '""IQR""']",75,[],"['np.median', 'pd.Series', 'np.abs', 'print', 'pd.concat', 'tmp.any']",6
source/bin/deltapy/transform.py:windsorization,windsorization,function,8,23,15,311,13.52,0,1,"['data', 'col', 'para', 'strategy']","[None, None, None, None]","[None, None, None, ""'both'""]",104,"['    """"""\n', '    top-coding & bottom coding (capping the maximum of a distribution at an arbitrarily set value,vice versa)\n', '    """"""\n']",['data.copy'],1
source/bin/deltapy/transform.py:operations,operations,function,21,53,36,691,13.04,0,0,"['df', 'features']","[None, None]","[None, None]",125,[],"['df_new.min', 'pd.DataFrame', 'pd.concat']",3
source/bin/deltapy/transform.py:initial_trend,initial_trend,function,5,14,12,83,5.93,1,0,"['series', 'slen']","[None, None]","[None, None]",150,[],"['range', 'float']",2
source/bin/deltapy/transform.py:initial_seasonal_components,initial_seasonal_components,function,12,28,20,341,12.18,3,0,"['series', 'slen']","[None, None]","[None, None]",156,[],"['int', 'range', 'season_averages.append']",3
source/bin/deltapy/transform.py:triple_exponential_smoothing,triple_exponential_smoothing,function,27,60,50,615,10.25,2,2,"['df', 'cols', 'slen', 'alpha', 'beta', 'gamma', 'n_preds']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",171,[],"['initial_seasonal_components', 'range', 'initial_trend', 'result.append', 'len']",5
source/bin/deltapy/transform.py:naive_dec,naive_dec,function,7,19,19,228,12.0,1,0,"['df', 'columns', 'freq']","[None, None, None]","[None, None, '2']",200,[],[],0
source/bin/deltapy/transform.py:bkb,bkb,function,6,11,11,94,8.55,1,0,"['df', 'cols']","[None, None]","[None, None]",213,[],['len'],1
source/bin/deltapy/transform.py:butter_lowpass,butter_lowpass,function,15,34,30,276,8.12,1,0,"['cutoff', 'fs', 'order']","[None, None, None]","[None, '20', '5']",223,[],"['signal.butter', 'butter_lowpass_filter', 'butter_lowpass', 'signal.lfilter']",4
source/bin/deltapy/transform.py:butter_lowpass_filter,butter_lowpass_filter,function,9,15,15,110,7.33,1,0,"['df', 'cols', 'cutoff', 'fs', 'order']","[None, None, None, None, None]","[None, None, None, '20', '5']",229,[],"['butter_lowpass', 'signal.lfilter']",2
source/bin/deltapy/transform.py:instantaneous_phases,instantaneous_phases,function,6,10,10,98,9.8,1,0,"['df', 'cols']","[None, None]","[None, None]",241,[],['np.unwrap'],1
source/bin/deltapy/transform.py:kalman_feat,kalman_feat,function,10,28,26,356,12.71,1,0,"['df', 'cols']","[None, None]","[None, None]",251,[],"['UnscentedKalmanFilter', 'np.sin', 'ukf.filter', 'ukf.smooth', 'smoothed_state_means.flatten', 'filtered_state_means.flatten']",6
source/bin/deltapy/transform.py:perd_feat,perd_feat,function,7,13,13,128,9.85,1,0,"['df', 'cols']","[None, None]","[None, None]",265,[],['signal.periodogram'],1
source/bin/deltapy/transform.py:fft_feat,fft_feat,function,10,18,15,238,13.22,1,0,"['df', 'cols']","[None, None]","[None, None]",277,[],"['pd.DataFrame', 'np.abs', 'np.angle']",3
source/bin/deltapy/transform.py:harmonicradar_cw,harmonicradar_cw,function,15,25,24,178,7.12,1,0,"['df', 'cols', 'fs', 'fc']","[None, None, None, None]","[None, None, None, None]",291,[],"['np.sin', 'signal.welch']",2
source/bin/deltapy/transform.py:saw,saw,function,7,9,9,61,6.78,1,0,"['df', 'cols']","[None, None]","[None, None]",311,[],['signal.sawtooth'],1
source/bin/deltapy/transform.py:modify,modify,function,34,64,37,767,11.98,1,0,"['df', 'cols']","[None, None]","[None, None]",321,[],"['magnify', 'affine', 'crop', 'cross_sum', 'resample', 'trend', 'random_time_warp', 'random_crop', 'random_cross_sum', 'random_sidetrack', 'random_magnify', 'random_jitter', 'random_trend']",13
source/bin/deltapy/transform.py:multiple_rolling,multiple_rolling,function,25,65,49,462,7.11,4,1,"['df', 'windows ', '2]', 'functions', '""std""]', 'columns']","[None, None, None, None, None, None]","[None, ' [1', None, '[""mean""', None, 'None']",345,[],['pd.concat'],1
source/bin/deltapy/transform.py:multiple_lags,multiple_lags,function,9,30,25,192,6.4,2,1,"['df', 'start', 'end', 'columns']","[None, None, None, None]","[None, '1', '3', 'None']",366,[],"['range', 'df.assign']",2
source/bin/deltapy/transform.py:prophet_feat,prophet_feat,function,21,42,37,555,13.21,1,0,"['df', 'cols', 'date', 'freq', 'train_size']","[None, None, None, None, None]","[None, None, None, None, '150']",384,[],"['prophet_dataframe', 'original_dataframe', 'pd.DataFrame', 'prophet_pred.set_index', 'Prophet', 'model.fit', 'len', 'model.make_future_dataframe', 'model.predict', 'list']",10
source/bin/feature-engineering-by-reinforcement-learning/cafem.py:generate_trajectories,generate_trajectories,function,41,91,81,1064,11.69,3,2,['task'],[None],[None],71,[],"['load', 'Env', 'Model', 'tf.ConfigProto', 'tf.Session', 'localsess.run', 'range', 'env.reset', 'np.copy', 'ma.masked_array', 'env.step', 'tmp_buffer.append']",12
source/bin/feature-engineering-by-reinforcement-learning/cafem.py:main,main,function,97,230,178,2987,12.99,8,1,[],[],[],122,[],"['Model', 'Tasks', 'tf.Session', 'sess.run', 'saver.save', 'pd.read_csv', 'tf.trainable_variables', 'updateTargetGraph', 'tqdm', 'tasks.sample', 'Pool', 'print', 'np.array', 'pool.close', 'pool.join', 'enumerate', 'generate_trajectories', 'task_buff.sample', 'np.split', 'np.reshape', 'np.clip', 'inputsa.append', 'labela.append', 'inputsb.append', 'labelb.append', 'actiona.append', 'actionb.append', 'updateTarget', 'open', 'f.write', 'f.close', 'data.iterrows', 'data.to_csv']",33
source/bin/feature-engineering-by-reinforcement-learning/cafem.py:Tasks,Tasks,class,27,51,43,600,11.76,3,2,[],[],[],44,[],[],0
source/bin/feature-engineering-by-reinforcement-learning/cafem.py:Tasks:__init__,Tasks:__init__,method,22,42,36,489,11.64,3,2,"['self', 'datasetids', 'buffer_size']","[None, None, None]","[None, None, '100']",45,[],"['pd.read_csv', 'open', 'tqdm', 'load', 'enumerate', 'f.write', 'f.close']",7
source/bin/feature-engineering-by-reinforcement-learning/cafem.py:Tasks:sample,Tasks:sample,method,3,4,3,45,11.25,0,0,"['self', 'n']","[None, None]","[None, None]",64,[],['random.sample'],1
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:one_mse_func,one_mse_func,function,9,15,13,230,15.33,0,0,[],[],[],29,[],"['one_relative_abs', 'mean_absolute_error', 'np.mean', 'np.abs', 'make_scorer']",5
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:load,load,function,24,38,33,411,10.82,1,3,['f_path'],[None],[None],115,[],"['LabelEncoder', 'loadarff', 'np.array', 'meta.names', 'meta.types', 'enumerate', 'le.fit_transform', 'dataset.astype']",8
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:updateTargetGraph,updateTargetGraph,function,7,25,20,215,8.6,1,0,"['tfVars', 'tau']","[None, None]","[None, None]",805,[],"['len', 'enumerate', 'op_holder.append']",3
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:updateTarget,updateTarget,function,10,32,26,279,8.72,2,0,"['tfVars', 'tau']","[None, None]","[None, None]",813,[],"['len', 'enumerate', 'op_holder.append', 'updateTarget', 'sess.run']",5
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:performance,performance,function,20,43,36,459,10.67,3,0,"['did', 'maxfeat', 'step']","[None, None, None]","[None, None, None]",983,[],"['load', 'Env', 'range', 'pd.read_csv', 'actions.append', 'trfs.append', 'env.batch_perform']",7
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Evaluater,Evaluater,class,63,147,105,1785,12.14,1,6,[],[],[],43,[],[],0
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Env,Env,class,191,931,396,10896,11.7,19,47,[],[],[],138,[],[],0
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Buffer,Buffer,class,12,32,25,438,13.69,0,2,[],[],[],585,[],[],0
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Model,Model,class,136,469,287,6412,13.67,7,3,[],[],[],605,[],[],0
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Evaluater:__init__,Evaluater:__init__,method,19,52,33,727,13.98,0,4,"['self', 'cv', 'stratified', 'n_jobs', 'tasktype', 'evaluatertype', 'n_estimators', '100000))']","[None, None, None, None, None, None, None, '']","[None, '5', 'True', '1', '""C""', '""rf""', '20', None]",45,[],"['StratifiedKFold', 'KFold', 'RandomForestClassifier', 'RandomForestRegressor', 'LogisticRegression', 'Lasso']",6
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Evaluater:CV,Evaluater:CV,method,29,46,42,524,11.39,1,1,"['self', 'X', 'y']","[None, None, None]","[None, None, None]",73,[],"['np.nan_to_num', 'np.clip', 'one_mse_func', 'cross_val_score', 'abs', 'CV2', 'res.append', 'np.array']",8
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Evaluater:CV2,Evaluater:CV2,method,19,26,25,306,11.77,1,0,"['self', 'X', 'y']","[None, None, None]","[None, None, None]",85,[],"['res.append', 'np.array']",2
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Evaluater:metrics,Evaluater:metrics,method,15,32,26,346,10.81,0,1,"['self', 'y_true', 'y_pred']","[None, None, None]","[None, None, None]",102,[],"['f1_score', 'roc_auc_score', 'log_loss', 'mean_absolute_error', 'np.mean', 'mean_squared_error']",6
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Env:__init__,Env:__init__,method,50,86,75,1211,14.08,3,4,"['self', 'dataset', 'feature', 'globalreward', 'maxdepth', 'evalcount', 'binsize', 'opt_type', 'tasktype', 'evaluatertype', '\\100000)', 'historysize', 'pretransform', 'n_jobs=1)']","[None, None, None, None, None, None, None, None, None, None, None, None, None, '']","[None, None, None, 'True', '5', '10', '100', ""'o1'"", '""C""', ""'rf'"", None, '5', 'None', '1):']",139,[],"['np.array', 'len', 'range', 'print', 'self.fe', 'value.append', 'Evaluater', 'np.copy', 'self._init']",9
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Env:_init,Env:_init,method,24,40,37,735,18.38,0,0,['self'],[None],[None],192,[],"['np.copy', 'self._QSA', 'np.concatenate', 'np.array']",4
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Env:node2root,Env:node2root,method,8,17,15,188,11.06,2,0,"['self', 'adict', 'node']","[None, None, None]","[None, None, None]",222,[],"['apath.append', 'range']",2
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Env:step,Env:step,method,100,399,218,4709,11.8,7,23,"['self', 'action']","[None, None]","[None, None]",231,[],"['set', 'getattr', 'feature.min', 'np.log', 'np.sqrt', 'MinMaxScaler', 'mmn.fit_transform', 'np.var', 'stats.zscore', 'np.nan_to_num', 'np.clip', 'math.sqrt', 'np.insert', 'self.node2root', 'np.concatenate', 'len', 'np.copy', 'self._QSA', 'range', 'abs', 'np.array', 'allperf.argmax', 'allperf.max']",23
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Env:_QSA,Env:_QSA,method,27,146,60,1570,10.75,1,7,['self'],[None],[None],432,[],"['np.median', 'feat_0.min', 'abs', 'np.arange', 'np.bincount', 'len', 'feat_1.min', 'np.concatenate', 'range', 'feat_0.max', 'feat_1.max', 'QSA.append']",12
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Env:reset,Env:reset,method,5,5,5,75,15.0,0,0,['self'],[None],[None],486,[],['self._init'],1
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Env:fe,Env:fe,method,39,221,104,2054,9.29,6,13,"['self', 'operators', 'feat_id']","[None, None, None]","[None, None, None]",492,[],"['type', 'set', 'getattr', 'feature.min', 'np.log', 'np.sqrt', 'MinMaxScaler', 'mmn.fit_transform', 'np.var', 'stats.zscore', 'len', 'np.nan_to_num', 'np.clip', 'math.sqrt', 'np.insert', 'np.delete', 'range']",17
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Buffer:__init__,Buffer:__init__,method,3,4,4,43,10.75,0,0,"['self', 'buffer_size ']","[None, None]","[None, ' 50000']",586,[],[],0
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Buffer:add,Buffer:add,method,3,8,8,129,16.12,0,1,"['self', 'experience']","[None, None]","[None, None]",590,[],['len'],1
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Buffer:sample,Buffer:sample,method,5,11,10,180,16.36,0,1,"['self', 'size']","[None, None]","[None, None]",595,[],"['len', 'np.copy']",2
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Model:__init__,Model:__init__,method,35,61,44,1002,16.43,0,1,"['self', 'opt_size', 'input_size', 'name', 'meta', 'update_lr', 'meta_lr', 'num_updates', 'maml', 'qsasize']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, 'False', '1e-3', '0.001', '1', 'True', '200']",606,[],"['tf.placeholder', 'self.construct_fc_weights', 'self.network', 'self.construct_model', 'tf.global_variables_initializer']",5
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Model:mse,Model:mse,method,2,3,3,45,15.0,0,0,"['self', 'y_pred', 'y_true']","[None, None, None]","[None, None, None]",635,[],['tf.reduce_sum'],1
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Model:construct_fc_weights,Model:construct_fc_weights,method,14,83,55,1161,13.99,1,1,['self'],[None],[None],638,[],"['tf.Variable', 'range', 'len', 'str', 'tf.truncated_normal']",5
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Model:forward,Model:forward,method,14,114,47,1237,10.85,3,1,"['self', 'inp', 'weights', 'reuse']","[None, None, None, None]","[None, None, None, 'False']",668,[],"['normalize', 'range', 'len', 'str', 'scope=str', 'tf.matmul']",6
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Model:L2loss,Model:L2loss,method,5,11,9,97,8.82,1,0,"['self', 'weights', 'reg']","[None, None, None]","[None, None, None]",702,[],[],0
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Model:network,Model:network,method,12,19,19,376,19.79,0,0,['self'],[None],[None],708,[],"['self.forward', 'tf.one_hot', 'tf.reduce_sum', 'self.loss_func']",4
source/bin/feature-engineering-by-reinforcement-learning/MLFE.py:Model:construct_model,Model:construct_model,method,67,155,129,2194,14.15,2,0,['self'],[None],[None],719,[],"['tf.variable_scope', 'training_scope.reuse_variables', 'forward_Q', 'tf.one_hot', 'tf.reduce_sum', 'task_metalearn', 'self.loss_func', 'tf.gradients', 'list', 'dict', 'zip', 'weights.keys', 'task_outputbs.append', 'task_lossesb.append', 'range', 'fast_weights.keys', 'tf.map_fn', 'tf.to_float', 'optimizer.compute_gradients', 'optimizer.apply_gradients']",20
source/bin/feature-engineering-by-reinforcement-learning/scripted_runner.py:run_cafem,run_cafem,function,3,4,3,134,33.5,0,0,[],[],[],2,[],['subprocess.Popen'],1
source/bin/feature-engineering-by-reinforcement-learning/scripted_runner.py:run_single_afem,run_single_afem,function,3,4,3,209,52.25,0,0,[],[],[],6,[],['subprocess.Popen'],1
source/bin/feature-engineering-by-reinforcement-learning/single_afem.py:simulate,simulate,function,35,60,54,956,15.93,1,4,['inp'],[None],[None],8,[],"['Model', 'tf.ConfigProto', 'tf.Session', 'saver.restore', 'env.reset', 'range', 'np.copy', 'localsess.run', 'ma.masked_array', 'env.step', 'tmp_buffer.append', 'localsess.close']",12
source/bin/feature-engineering-by-reinforcement-learning/single_afem.py:main,main,function,162,536,363,6258,11.68,8,15,[],[],[],56,[],"['os.mkdir', 'pd.read_csv', 'load', 'Model', 'tf.ConfigProto', 'tf.Session', 'tqdm', 'Buffer', 'len', 'print', 'saver.restore', 'saver.save', 'sess.run', 'Env', 'open', 'f.write', 'f.close', 'tf.trainable_variables', 'updateTargetGraph', 'Pool', 'pool.map', 'range', 'pool.close', 'pool.join', 'buff.add', 'best_seq.append', 'best_pfm.append', 'simulate', 'buff.sample', 'np.split', 'np.array', 'np.reshape', 'enumerate', 'np.clip', 'mean_loss.append', 'updateTarget', 'np.around', 'ma.masked_array', 'pretransform.append', 'max', 'np.copy', 'str', 'env.step', 'pretransform_test.append', 'predictions.append', 'test_pfm.append', 'data.iterrows', 'data.to_csv']",48
source/bin/feature-engineering-by-reinforcement-learning/utils.py:normalize,normalize,function,2,2,2,21,10.5,0,0,"['inp', 'activation', 'reuse', 'scope', 'norm']","[None, None, None, None, None]","[None, None, None, None, None]",34,[],['activation'],1
source/bin/feature-engineering-by-reinforcement-learning/utils.py:load_pretransform,load_pretransform,function,12,19,16,445,23.42,0,0,['fdir'],[None],[None],48,[],"['pd.read_csv', 'transform.fillna', 'np.argmax', 'print', 'int']",5
source/bin/feature-engineering-by-reinforcement-learning/utils.py:get_result,get_result,function,17,36,31,742,20.61,1,1,"['mark', 'did', 'plot']","[None, None, None]","[None, None, 'True']",64,[],"['pd.read_csv', 'score_te.drop_duplicates', 'res.append', 'plt.plot', 'plt.show', 'print', 'pd.DataFrame']",7
source/bin/feature-engineering-by-reinforcement-learning/utils.py:plot,plot,function,42,133,106,1586,11.92,3,1,"['fpath1', 'fpath2', 'size', 'name']","[None, None, None, None]","[None, None, '30', ""''""]",91,[],"['pd.read_csv', 'plt.figure', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'plt.xticks', 'plt.plot', 'len', 'range', 'plt.legend', 'plt.show', 'plot2', 'np.arange', 'plt.subplots', 'ax.bar', 'plt.yticks', 'zip', 'plt.text', 'ax.set_ylabel', 'ax.set_title', 'ax.legend']",21
source/bin/feature-engineering-by-reinforcement-learning/utils.py:plot2,plot2,function,29,96,77,959,9.99,2,0,[],[],[],111,[],"['np.arange', 'plt.subplots', 'ax.bar', 'plt.yticks', 'zip', 'plt.text', 'ax.set_ylabel', 'ax.set_title', 'plt.xticks', 'ax.legend', 'plt.show']",11
source/bin/hunga_bunga/classification.py:run_all_classifiers,run_all_classifiers,function,3,44,30,562,12.77,0,0,"['x', 'y', 'small ', 'normalize_x ', 'n_jobs']","[None, None, None, None, None]","[None, None, ' True', ' True', 'cpu_count(']",177,[],"['main_loop', 'StandardScaler']",2
source/bin/hunga_bunga/classification.py:HungaBungaClassifier,HungaBungaClassifier,class,27,64,59,787,12.3,0,0,[],[],[],182,[],[],0
source/bin/hunga_bunga/classification.py:HungaBungaClassifier:__init__,HungaBungaClassifier:__init__,method,21,24,23,294,12.25,0,0,"['self', 'brain', 'test_size ', 'n_splits ', 'random_state', 'upsample', 'scoring', 'verbose', 'normalize_x ', 'n_jobs ']","[None, None, None, None, None, None, None, None, None, None]","[None, 'False', ' 0.2', ' 5', 'None', 'True', 'None', 'False', ' True', 'cpu_count(']",183,[],['super'],1
source/bin/hunga_bunga/classification.py:HungaBungaClassifier:fit,HungaBungaClassifier:fit,method,3,14,14,256,18.29,0,0,"['self', 'x', 'y']","[None, None, None]","[None, None, None]",197,[],['run_all_classifiers'],1
source/bin/hunga_bunga/classification.py:HungaBungaClassifier:predict,HungaBungaClassifier:predict,method,2,2,2,27,13.5,0,0,"['self', 'x']","[None, None]","[None, None]",201,[],[],0
source/bin/hunga_bunga/core.py:upsample_indices_clf,upsample_indices_clf,function,15,36,32,373,10.36,1,1,"['inds', 'y']","[None, None]","[None, None]",62,[],"['len', 'dict', 'max', 'countByClass.items', 'int', 'extras.append', 'np.concatenate']",7
source/bin/hunga_bunga/core.py:cv_clf,cv_clf,function,5,19,18,198,10.42,1,1,"['x', 'y', 'test_size ', 'n_splits ', 'random_state', 'doesUpsample ']","[None, None, None, None, None, None]","[None, None, ' 0.2', ' 5', 'None', ' True']",75,[],['sss'],1
source/bin/hunga_bunga/core.py:cv_reg,cv_reg,function,6,9,8,64,7.11,0,0,"['x', 'test_size ', 'n_splits ', 'random_state=None)', 'test_size', 'random_state', 'params', 'x', 'y)']","[None, None, None, '', None, None, None, None, '']","[None, ' 0.2', ' 5', 'None): return ss(n_splits', None, 'random_state).split(x)klass', None, None, None]",81,[],"['time', 'klass', 'clf.fit']",3
source/bin/hunga_bunga/core.py:timeit,timeit,function,6,9,8,64,7.11,0,0,"['klass', 'params', 'x', 'y']","[None, None, None, None]","[None, None, None, None]",84,[],"['time', 'klass', 'clf.fit']",3
source/bin/hunga_bunga/core.py:main_loop,main_loop,function,33,179,120,1746,9.75,2,15,"['models_n_params', 'x', 'y', 'isClassification', 'test_size ', 'n_splits ', 'random_state', 'upsample', 'scoring', 'verbose', 'n_jobs ']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, ' 0.2', ' 5', 'None', 'True', 'None', 'True', 'cpu_count(']",91,[],"['cv_', 'cv_clf', 'cv_reg', 'print', 'enumerate', 'len', 'type', 'GridSearchCVProgressBar', 'cv=cv_', 'RandomizedSearchCVProgressBar', 'clf_search.fit', 'timeit', 'pprint', 'res.append', 'traceback.print_exc', 'np.argmax']",16
source/bin/hunga_bunga/core.py:GridSearchCVProgressBar,GridSearchCVProgressBar,class,16,35,30,580,16.57,0,0,[],[],[],28,[],[],0
source/bin/hunga_bunga/core.py:RandomizedSearchCVProgressBar,RandomizedSearchCVProgressBar,class,16,35,30,592,16.91,0,0,[],[],[],45,[],[],0
source/bin/hunga_bunga/core.py:GridSearchCVProgressBar:_get_param_iterator,GridSearchCVProgressBar:_get_param_iterator,method,15,33,29,550,16.67,0,0,['self'],[None],[None],29,[],"['super', 'list', 'len', 'getattr', 'ParallelProgressBar', '__call__', 'tqdm', 'iterable.set_description']",8
source/bin/hunga_bunga/core.py:RandomizedSearchCVProgressBar:_get_param_iterator,RandomizedSearchCVProgressBar:_get_param_iterator,method,15,33,29,562,17.03,0,0,['self'],[None],[None],29,[],"['super', 'list', 'len', 'getattr', 'ParallelProgressBar', '__call__', 'tqdm', 'iterable.set_description']",8
source/bin/hunga_bunga/regression.py:gen_reg_data,gen_reg_data,function,6,15,14,149,9.93,0,0,"['x_mu', 'x_sigma', 'num_samples', 'num_features', 'y_formula', 'y_sigma']","[None, None, None, None, None, None]","['10.', '1.', '100', '3', 'sum', '1.']",287,[],['np.apply_along_axis'],1
source/bin/hunga_bunga/regression.py:run_all_regressors,run_all_regressors,function,3,44,30,563,12.8,0,0,"['x', 'y', 'small ', 'normalize_x ', 'n_jobs']","[None, None, None, None, None]","[None, None, ' True', ' True', 'cpu_count(']",292,[],"['main_loop', 'StandardScaler']",2
source/bin/hunga_bunga/regression.py:HungaBungaRegressor,HungaBungaRegressor,class,26,63,58,785,12.46,0,0,[],[],[],297,[],[],0
source/bin/hunga_bunga/regression.py:HungaBungaRegressor:__init__,HungaBungaRegressor:__init__,method,20,23,22,293,12.74,0,0,"['self', 'brain', 'test_size ', 'n_splits ', 'random_state', 'upsample', 'scoring', 'verbose', 'normalize_x ', 'n_jobs ']","[None, None, None, None, None, None, None, None, None, None]","[None, 'False', ' 0.2', ' 5', 'None', 'True', 'None', 'False', ' True', 'cpu_count(']",298,[],['super'],1
source/bin/hunga_bunga/regression.py:HungaBungaRegressor:fit,HungaBungaRegressor:fit,method,3,14,14,255,18.21,0,0,"['self', 'x', 'y']","[None, None, None]","[None, None, None]",312,[],['run_all_regressors'],1
source/bin/hunga_bunga/regression.py:HungaBungaRegressor:predict,HungaBungaRegressor:predict,method,2,2,2,27,13.5,0,0,"['self', 'x']","[None, None]","[None, None]",316,[],[],0
source/bin/hunga_bunga/__init__.py:HungaBungaZeroKnowledge,HungaBungaZeroKnowledge,class,30,84,66,1169,13.92,0,0,[],[],[],10,[],[],0
source/bin/hunga_bunga/__init__.py:HungaBungaZeroKnowledge:__init__,HungaBungaZeroKnowledge:__init__,method,22,26,25,325,12.5,0,0,"['self', 'brain', 'test_size ', 'n_splits ', 'random_state', 'upsample', 'scoring', 'verbose', 'normalize_x ', 'n_jobs ']","[None, None, None, None, None, None, None, None, None, None]","[None, 'False', ' 0.2', ' 5', 'None', 'True', 'None', 'True', ' True', 'cpu_count(']",11,[],['super'],1
source/bin/hunga_bunga/__init__.py:HungaBungaZeroKnowledge:fit,HungaBungaZeroKnowledge:fit,method,6,32,20,608,19.0,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, None]",26,[],"['HungaBungaClassifier', 'HungaBungaRegressor']",2
source/bin/hunga_bunga/__init__.py:HungaBungaZeroKnowledge:predict,HungaBungaZeroKnowledge:predict,method,2,2,2,27,13.5,0,0,"['self', 'x']","[None, None]","[None, None]",37,[],[],0
source/models/akeras/Autokeras.py:get_config_file,get_config_file,function,2,6,6,91,15.17,0,0,[],[],[],25,[],[],0
source/models/akeras/Autokeras.py:get_params,get_params,function,15,34,30,416,12.24,0,1,"['param_pars', '**kw']","[None, None]","['None', None]",52,[],"['path_norm', 'json.load', 'path_norm_dict', 'Exception']",4
source/models/akeras/Autokeras.py:get_dataset_imbd,get_dataset_imbd,function,24,76,56,680,8.95,1,0,['data_pars'],[None],[None],73,[],"['imdb.load_data', 'y_train.reshape', 'y_test.reshape', 'imdb.get_word_index', 'word_to_id.items', 'list', 'np.array']",7
source/models/akeras/Autokeras.py:get_dataset_titanic,get_dataset_titanic,function,15,25,18,341,13.64,0,0,['data_pars'],[None],[None],97,[],"['path_norm', 'pd.read_csv', 'x_train.pop', 'x_test.pop']",4
source/models/akeras/Autokeras.py:get_dataset,get_dataset,function,36,141,87,1472,10.44,1,1,['data_pars'],[None],[None],112,[],"['imdb.load_data', 'y_train.reshape', 'y_test.reshape', 'imdb.get_word_index', 'word_to_id.items', 'list', 'np.array', 'get_dataset_titanic', 'path_norm', 'pd.read_csv', 'x_train.pop', 'x_test.pop', 'get_dataset', 'get_dataset_imbd', 'mnist.load_data', 'Exception']",16
source/models/akeras/Autokeras.py:fit,fit,function,10,16,16,257,16.06,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kwargs']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",129,[],"['get_dataset', 'print', 'os.makedirs', 'model.fit']",4
source/models/akeras/Autokeras.py:predict,predict,function,7,9,7,92,10.22,0,0,"['model', 'session', 'data_pars', 'compute_pars', 'out_pars']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",144,[],"['get_dataset', 'model.predict']",2
source/models/akeras/Autokeras.py:evaluate,evaluate,function,6,8,7,76,9.5,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",151,[],"['get_dataset', 'model.evaluate']",2
source/models/akeras/Autokeras.py:save,save,function,1,1,1,72,72.0,0,0,"['model', 'session', 'save_pars', 'config_mode']","[None, None, None, None]","[None, 'None', 'None', '""test""']",156,[],['model.save'],1
source/models/akeras/Autokeras.py:load,load,function,2,3,3,111,37.0,0,0,"['load_pars', 'config_mode']","[None, None]","[None, '""test""']",160,[],['load_model'],1
source/models/akeras/Autokeras.py:test_single,test_single,function,21,75,55,1199,15.99,0,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""json""', '""test""']",166,[],"['log', 'get_params', 'Model', 'fit', 'predict', 'print', 'evaluate', 'fitted_model.export_model', 'save', 'load']",10
source/models/akeras/Autokeras.py:test,test,function,25,93,70,1347,14.48,1,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""json""', '""test""']",203,[],"['log', 'get_params', 'Model', 'fit', 'predict', 'print', 'evaluate', 'fitted_model.export_model', 'save', 'load', 'test', 'test_single']",12
source/models/akeras/Autokeras.py:Model,Model,class,9,29,22,465,16.03,0,2,[],[],[],31,[],[],0
source/models/akeras/Autokeras.py:Model:__init__,Model:__init__,method,8,23,16,383,16.65,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars', 'out_pars']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",32,[],"['ak.TextClassifier', 'ak.ImageClassifier', 'ak.StructuredDataClassifier']",3
source/models/akeras/charcnn.py:fit,fit,function,18,35,30,467,13.34,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",120,"['    """"""\n', '    """"""\n']","['get_dataset', 'EarlyStopping']",2
source/models/akeras/charcnn.py:evaluate,evaluate,function,21,41,33,440,10.73,0,2,"['model', 'session', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",147,"['    """"""\n', '       Return metrics ofw the model when fitted.\n', '    """"""\n']","['get_dataset', 'compute_pars.get', 'ypred.argmax', 'np.argmax', 'accuracy_score']",5
source/models/akeras/charcnn.py:predict,predict,function,10,16,14,194,12.12,0,1,"['model', 'session', 'data_pars', 'out_pars', 'compute_pars', '**kw']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",169,[],"['get_dataset', 'compute_pars.get']",2
source/models/akeras/charcnn.py:reset_model,reset_model,function,0,1,1,4,4.0,0,0,[],[],[],185,[],[],0
source/models/akeras/charcnn.py:save,save,function,5,8,8,96,12.0,0,0,"['model', 'save_pars', 'session']","[None, None, None]","['None', 'None', 'None']",189,[],"['print', 'save_keras']",2
source/models/akeras/charcnn.py:load,load,function,12,15,14,134,8.93,0,0,['load_pars'],[None],['None'],195,[],"['load_keras', 'Model']",2
source/models/akeras/charcnn.py:str_to_indexes,str_to_indexes,function,13,25,21,163,6.52,1,1,['s'],[None],[None],206,"['    """"""\n', '    Convert a string to character indexes based on character dictionary.\n', '\n', '    Args:\n', '        s (str): String to be converted to indexes\n', '\n', '    Returns:\n', '        str2idx (np.ndarray): Indexes of characters in s\n', '\n', '    """"""\n']","['s.lower', 'min', 'np.zeros', 'range']",4
source/models/akeras/charcnn.py:tokenize,tokenize,function,19,34,33,383,11.26,1,0,"['data', 'num_of_classes']","[None, None]","[None, '4']",226,[],"['print', 'data.to_numpy', 'len', 'np.eye', 'batch_indices.append', 'int', 'classes.append', 'np.asarray']",8
source/models/akeras/charcnn.py:get_dataset,get_dataset,function,8,9,9,109,12.11,0,0,"['data_pars', '**kw']","[None, None]","['None', None]",244,"['    """"""\n', '      JSON data_pars to get dataset\n', '      ""data_pars"":    { ""data_path"": ""dataset/GOOG-year.csv"", ""data_type"": ""pandas"",\n', '      ""size"": [0, 0, 6], ""output_size"": [0, 6] },\n', '    """"""\n']","['DataLoader', 'loader.compute', 'loader.get_data']",3
source/models/akeras/charcnn.py:get_params,get_params,function,29,137,114,1356,9.9,0,2,"['param_pars', '**kw']","[None, None]","['{}', None]",256,[],"['JsonComment', 'path_norm', 'json.load', 'log', 'Exception']",5
source/models/akeras/charcnn.py:test,test,function,30,94,68,1318,14.02,0,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""json""', '""test""']",323,[],"['path_norm', 'log', 'get_params', 'get_dataset', 'print', 'Model', 'fit', 'predict', 'evaluate', 'save', 'load']",11
source/models/akeras/charcnn.py:Model,Model,class,5,33,31,544,16.48,0,1,[],[],[],99,[],[],0
source/models/akeras/charcnn.py:Model:__init__,Model:__init__,method,4,27,25,475,17.59,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",100,[],['CharCNNKim'],1
source/models/akeras/charcnn_zhang.py:fit,fit,function,18,35,30,467,13.34,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","[None, '{}', '{}', '{}', None]",61,"['    """"""\n', '    """"""\n']","['get_dataset', 'EarlyStopping']",2
source/models/akeras/charcnn_zhang.py:evaluate,evaluate,function,2,4,3,20,5.0,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","[None, '{}', '{}', '{}', None]",85,"['    """"""\n', '       Return metrics ofw the model when fitted.\n', '    """"""\n']",[],0
source/models/akeras/charcnn_zhang.py:predict,predict,function,10,16,14,194,12.12,0,1,"['model', 'sess', 'data_pars', 'out_pars', 'compute_pars', '**kw']","[None, None, None, None, None, None]","[None, 'None', '{}', '{}', '{}', None]",94,[],"['get_dataset', 'compute_pars.get']",2
source/models/akeras/charcnn_zhang.py:reset_model,reset_model,function,0,1,1,4,4.0,0,0,[],[],[],110,[],[],0
source/models/akeras/charcnn_zhang.py:save,save,function,5,8,8,86,10.75,0,0,"['model', 'session', 'save_pars']","[None, None, None]","['None', 'None', '{}']",114,[],"['print', 'save_keras']",2
source/models/akeras/charcnn_zhang.py:load,load,function,9,12,11,111,9.25,0,0,['load_pars'],[None],['{}'],120,[],"['print', 'load_keras']",2
source/models/akeras/charcnn_zhang.py:get_dataset,get_dataset,function,8,9,9,109,12.11,0,0,"['data_pars', '**kw']","[None, None]","['None', None]",129,"['    """"""\n', '      JSON data_pars to get dataset\n', '      ""data_pars"":    { ""data_path"": ""dataset/GOOG-year.csv"", ""data_type"": ""pandas"",\n', '      ""size"": [0, 0, 6], ""output_size"": [0, 6] },\n', '    """"""\n']","['DataLoader', 'loader.compute', 'loader.get_data']",3
source/models/akeras/charcnn_zhang.py:get_params,get_params,function,29,138,108,1366,9.9,0,2,"['param_pars', '**kw']","[None, None]","['{}', None]",141,[],"['JsonComment', 'path_norm', 'json.load', 'log', 'Exception']",5
source/models/akeras/charcnn_zhang.py:test,test,function,28,77,57,1093,14.19,0,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""json""', '""test""']",215,[],"['path_norm', 'log', 'get_params', 'get_dataset', 'Model', 'fit', 'predict', 'evaluate', 'print', 'save', 'load']",11
source/models/akeras/charcnn_zhang.py:Model,Model,class,4,35,33,576,16.46,0,1,[],[],[],41,[],[],0
source/models/akeras/charcnn_zhang.py:Model:__init__,Model:__init__,method,3,29,27,507,17.48,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",42,[],['CharCNNZhang'],1
source/models/akeras/deepctr.py:_preprocess_criteo,_preprocess_criteo,function,33,112,62,1047,9.35,2,1,"['df', '**kw']","[None, None]","[None, None]",166,[],"['kw.get', 'str', 'range', 'MinMaxScaler', 'mms.fit_transform', 'LabelEncoder', 'lbe.fit_transform', 'enumerate', 'train_test_split']",9
source/models/akeras/deepctr.py:_preprocess_movielens,_preprocess_movielens,function,53,244,135,2546,10.43,5,4,"['df', '**kw']","[None, None]","[None, None]",202,[],"['kw.get', 'LabelEncoder', 'lbe.fit_transform', 'train_test_split', 'split', 'x.split', 'len', 'list', 'np.array', 'max', 'pad_sequences', 'vocabulary_size=len', 'SparseFeat', 'VarLenSparseFeat', 'get_feature_names']",15
source/models/akeras/deepctr.py:get_dataset,get_dataset,function,34,102,57,1218,11.94,0,4,"['data_pars', '**kw']","[None, None]","['None', None]",293,[],"['data_pars.get', 'get_xy_fd_dien', 'get_xy_fd_din', 'get_xy_fd_dsin', 'get_test_data', 'pd.read_pickle', 'pd.read_csv', '_preprocess_criteo', '_preprocess_movielens', 'train_test_split']",10
source/models/akeras/deepctr.py:fit,fit,function,8,12,12,213,17.75,0,0,"['model', 'session', 'compute_pars', 'data_pars', 'out_pars', '**kwargs']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",337,"['    """"""\n', '          Classe Model --> model,   model.model contains thte sub-model\n', '    """"""\n']",[],0
source/models/akeras/deepctr.py:predict,predict,function,8,10,9,144,14.4,0,0,"['model', 'session', 'compute_pars', 'data_pars', 'out_pars', '**kwargs']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",354,[],[],0
source/models/akeras/deepctr.py:metrics,metrics,function,3,6,5,71,11.83,0,0,"['ypred', 'ytrue', 'session', 'compute_pars', 'data_pars', 'out_pars', '**kwargs']","[None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', 'None', None]",361,[],['mean_squared_error'],1
source/models/akeras/deepctr.py:reset_model,reset_model,function,0,1,1,4,4.0,0,0,[],[],[],366,[],[],0
source/models/akeras/deepctr.py:path_setup,path_setup,function,8,15,14,198,13.2,0,0,"['out_folder', 'sublevel', 'data_path']","[None, None, None]","['""""', '0', '""dataset/""']",374,[],"['os_package_root_path', 'os.getcwd', 'os.makedirs', 'log']",4
source/models/akeras/deepctr.py:_config_process,_config_process,function,9,13,12,174,13.38,0,0,['config'],[None],[None],383,[],[],0
source/models/akeras/deepctr.py:config_load,config_load,function,11,24,18,300,12.5,0,0,"['data_path', 'file_default', 'config_mode']","[None, None, None]","[None, None, None]",391,[],"['Path', 'json.load', '_config_process']",3
source/models/akeras/deepctr.py:get_params,get_params,function,20,303,87,4037,13.32,0,2,"['choice', 'data_path', 'config_mode', '**kwargs']","[None, None, None, None]","['""""', '""dataset/""', '""test""', None]",402,[],"['config_load', 'log', 'path_setup', 'path_norm']",4
source/models/akeras/deepctr.py:test,test,function,28,80,56,1275,15.94,0,0,"['data_path', 'pars_choice', '**kwargs']","[None, None, None]","['""dataset/""', '0', None]",504,[],"['log', 'get_params', 'print', 'get_dataset', 'module_load_full', 'fit', 'predict', 'metrics', 'save_keras', 'load_keras']",10
source/models/akeras/deepctr.py:Model,Model,class,17,72,51,906,12.58,0,3,[],[],[],135,[],[],0
source/models/akeras/deepctr.py:Model:__init__,Model:__init__,method,16,66,45,829,12.56,0,3,"['self', 'model_pars', 'data_pars', 'compute_pars', '**kwargs']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",136,[],"['model_pars.get', 'list', 'ValueError', 'getattr', 'modeli']",5
source/models/akeras/namentity_crm_bilstm.py:fit,fit,function,23,49,47,674,13.76,0,1,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",97,"['    """"""\n', '    """"""\n']","['get_dataset', 'EarlyStopping', 'os.makedirs', 'ModelCheckpoint']",4
source/models/akeras/namentity_crm_bilstm.py:evaluate,evaluate,function,4,4,4,43,10.75,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",133,"['    """"""\n', '       Return metrics ofw the model when fitted.\n', '    """"""\n']",[],0
source/models/akeras/namentity_crm_bilstm.py:predict,predict,function,13,18,17,197,10.94,0,1,"['model', 'sess', 'data_pars', 'out_pars', 'compute_pars', '**kw']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",142,[],"['get_dataset', 'compute_pars.get']",2
source/models/akeras/namentity_crm_bilstm.py:reset_model,reset_model,function,0,1,1,4,4.0,0,0,[],[],[],158,[],[],0
source/models/akeras/namentity_crm_bilstm.py:save,save,function,5,8,8,86,10.75,0,0,"['model', 'session', 'save_pars']","[None, None, None]","['None', 'None', 'None']",162,[],"['print', 'save_keras']",2
source/models/akeras/namentity_crm_bilstm.py:load,load,function,9,12,11,111,9.25,0,0,['load_pars'],[None],[None],169,[],"['print', 'load_keras']",2
source/models/akeras/namentity_crm_bilstm.py:get_dataset,get_dataset,function,5,5,5,69,13.8,0,0,['data_pars'],[None],[None],181,[],"['DataLoader', 'loader.compute', 'loader.get_data']",3
source/models/akeras/namentity_crm_bilstm.py:get_params,get_params,function,28,89,78,935,10.51,0,2,"['param_pars', '**kw']","[None, None]","['{}', None]",187,[],"['JsonComment', 'path_norm', 'json.load', 'log', 'Exception']",5
source/models/akeras/namentity_crm_bilstm.py:test,test,function,26,82,59,1179,14.38,0,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""json""', '""test""']",235,[],"['log', 'get_params', 'get_dataset', 'module_load_full', 'fit', 'predict', 'fit_metrics', 'print']",8
source/models/akeras/namentity_crm_bilstm.py:Model,Model,class,29,64,58,780,12.19,0,1,[],[],[],59,[],[],0
source/models/akeras/namentity_crm_bilstm.py:Model:__init__,Model:__init__,method,28,58,52,703,12.12,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars', '**kwargs']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",60,[],"['get_dataset', 'internal_states.get', 'Input', 'Embedding', 'Bidirectional', 'LSTM', 'TimeDistributed', 'CRF', 'crf', 'KModel', 'model.compile', 'model.summary']",12
source/models/akeras/preprocess.py:os_package_root_path,os_package_root_path,function,10,19,16,165,8.68,1,0,"['filepath', 'sublevel', 'path_add']","[None, None, None]","[None, '0', '""""']",48,"['    """"""\n', '       get the module package root folder\n', '    """"""\n']","['Path', 'range']",2
source/models/akeras/preprocess.py:log,log,function,3,11,10,65,5.91,0,0,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '1']",61,[],['print'],1
source/models/akeras/preprocess.py:_preprocess_criteo,_preprocess_criteo,function,32,117,65,1052,8.99,4,1,"['df', '**kw']","[None, None]","[None, None]",70,[],"['kw.get', 'str', 'range', 'MinMaxScaler', 'mms.fit_transform', 'LabelEncoder', 'lbe.fit_transform', 'enumerate', 'train_test_split']",9
source/models/akeras/preprocess.py:_preprocess_movielens,_preprocess_movielens,function,50,239,132,2494,10.44,5,4,"['df', '**kw']","[None, None]","[None, None]",106,[],"['kw.get', 'LabelEncoder', 'lbe.fit_transform', 'train_test_split', 'split', 'x.split', 'len', 'list', 'np.array', 'max', 'pad_sequences', 'vocabulary_size=len', 'SparseFeat', 'VarLenSparseFeat', 'get_feature_names']",15
source/models/akeras/preprocess.py:_preprocess_none,_preprocess_none,function,12,17,15,183,10.76,0,0,"['df', '**kw']","[None, None]","[None, None]",198,[],['train_test_split'],1
source/models/akeras/preprocess.py:get_dataset,get_dataset,function,9,16,14,188,11.75,0,1,['**kw'],[None],[None],208,[],"['kw.get', 'pd.read_pickle', 'pd.read_csv']",3
source/models/akeras/preprocess.py:test,test,function,18,44,27,518,11.77,0,0,"['data_path', 'pars_choice']","[None, None]","['""dataset/""', '0']",226,[],"['log', 'get_params', 'print', 'get_dataset', '_preprocess_criteo', '_preprocess_movielens', '_preprocess_none']",7
source/models/akeras/textcnn.py:fit,fit,function,14,24,23,281,11.71,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",61,"['  """"""\n', '  """"""\n']","['get_dataset', 'EarlyStopping']",2
source/models/akeras/textcnn.py:evaluate,evaluate,function,2,4,3,20,5.0,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",83,"['    """"""\n', '       Return metrics ofw the model when fitted.\n', '    """"""\n']",[],0
source/models/akeras/textcnn.py:predict,predict,function,12,18,16,155,8.61,0,1,"['model', 'sess', 'data_pars', 'out_pars', 'compute_pars', '**kw']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",93,[],"['get_dataset', 'kw.get']",2
source/models/akeras/textcnn.py:reset_model,reset_model,function,0,1,1,4,4.0,0,0,[],[],[],110,[],[],0
source/models/akeras/textcnn.py:save,save,function,5,8,8,86,10.75,0,0,"['model', 'session', 'save_pars']","[None, None, None]","['None', 'None', '{}']",117,[],"['print', 'save_keras']",2
source/models/akeras/textcnn.py:load,load,function,8,10,10,95,9.5,0,0,['load_pars'],[None],['{}'],124,[],"['print', 'load_keras']",2
source/models/akeras/textcnn.py:get_dataset,get_dataset,function,18,26,21,319,12.27,0,0,"['data_pars', '**kw']","[None, None]","['None', None]",134,"['  """"""\n', '    JSON data_pars to get dataset\n', '    ""data_pars"":    { ""data_path"": ""dataset/GOOG-year.csv"", ""data_type"": ""pandas"",\n', '    ""size"": [0, 0, 6], ""output_size"": [0, 6] },\n', '  """"""\n']","['print', 'DataLoader', 'loader.compute', 'loader.get_data', 'sequence.pad_sequences']",5
source/models/akeras/textcnn.py:get_params,get_params,function,25,91,78,937,10.3,0,2,"['param_pars', '**kw']","[None, None]","['{}', None]",156,[],"['JsonComment', 'path_norm', 'json.load', 'log', 'Exception']",5
source/models/akeras/textcnn.py:test,test,function,10,34,25,449,13.21,0,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""json""', '""test""']",199,[],"['log', 'get_params', 'print', 'get_dataset']",4
source/models/akeras/textcnn.py:Model,Model,class,9,27,25,373,13.81,0,0,[],[],[],44,[],[],0
source/models/akeras/textcnn.py:Model:__init__,Model:__init__,method,8,21,19,304,14.48,0,0,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",45,[],['TextCNN'],1
source/models/akeras/util.py:os_package_root_path,os_package_root_path,function,10,19,16,147,7.74,1,0,"['filepath', 'sublevel', 'path_add']","[None, None, None]","[None, '0', '""""']",22,"['    """"""\n', '       get the module package root folder\n', '    """"""\n']","['Path', 'range']",2
source/models/akeras/util.py:log,log,function,3,11,10,65,5.91,0,0,"['*s', 'n', 'm']","[None, None, None]","[None, '0', '1']",35,[],['print'],1
source/models/akeras/util.py:_config_process,_config_process,function,9,23,21,316,13.74,0,0,"['data_path', 'config_mode']","[None, None]","[None, '""test""']",42,[],"['Path', 'open', 'json.load']",3
source/models/akeras/util.py:get_dataset,get_dataset,function,17,39,33,469,12.03,0,2,['**kw'],[None],[None],55,[],"['kw.get', 'pd.read_pickle', 'pd.read_csv', 'ListDataset', 'range', 'next', 'to_pandas', 'train_series.plot', 'plt.savefig']",9
source/models/akeras/util.py:fit,fit,function,0,1,1,4,4.0,0,0,"['model', 'data_pars', 'model_pars', 'compute_pars', 'out_pars', 'session', '**kwargs']","[None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', 'None', None]",80,"['        """"""\n', '          Classe Model --> model,   model.model contains thte sub-model\n', '\n', '        """"""\n']",[],0
source/models/akeras/util.py:predict,predict,function,0,1,1,4,4.0,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",91,[],[],0
source/models/akeras/util.py:metrics,metrics,function,0,1,1,4,4.0,0,0,"['ypred', 'data_pars', 'compute_pars', 'out_pars', '**kwargs']","[None, None, None, None, None]","[None, None, 'None', 'None', None]",97,[],[],0
source/models/akeras/util.py:save,save,function,3,12,11,112,9.33,0,1,"['model', 'path']","[None, None]","[None, None]",115,[],"['print', 'save_model']",2
source/models/akeras/util.py:load,load,function,8,20,17,173,8.65,0,1,['path'],[None],[None],122,[],"['print', 'Model_empty', 'load_model']",3
source/models/akeras/util.py:Model_empty,Model_empty,class,2,6,6,68,11.33,0,0,[],[],[],109,[],[],0
source/models/akeras/util.py:Model_empty:__init__,Model_empty:__init__,method,1,2,2,15,7.5,0,0,"['self', 'model_pars', 'compute_pars']","[None, None, None]","[None, 'None', 'None']",110,[],[],0
source/models/atorch/matchZoo.py:get_task,get_task,function,19,70,46,808,11.54,1,3,"['model_pars', 'task']","[None, None]","[None, None]",85,[],"['TASKS.keys', 'list', 'Exception', '_metrics.keys']",4
source/models/atorch/matchZoo.py:get_glove_embedding_matrix,get_glove_embedding_matrix,function,9,13,10,282,21.69,0,0,"['term_index', 'dimension']","[None, None]","[None, None]",124,[],"['glove_embedding.build_matrix', 'np.sqrt']",2
source/models/atorch/matchZoo.py:get_data_loader,get_data_loader,function,18,88,69,1197,13.6,0,3,"['model_name', 'preprocessor', 'preprocess_pars', 'raw_data']","[None, None, None, None]","[None, None, None, None]",133,[],"['preprocessor.transform', 'preprocessor.fit_transform', 'pp.get', 'get_glove_embedding_matrix']",4
source/models/atorch/matchZoo.py:get_config_file,get_config_file,function,2,6,6,89,14.83,0,0,[],[],[],207,[],[],0
source/models/atorch/matchZoo.py:get_raw_dataset,get_raw_dataset,function,11,31,28,396,12.77,0,1,"['data_info', '**args']","[None, None]","[None, None]",222,[],"['args.get', 'data_info.get', 'Exception']",3
source/models/atorch/matchZoo.py:get_dataset,get_dataset,function,18,52,46,572,11.0,0,1,"['_model', 'preprocessor', '_preprocessor_pars', 'data_pars']","[None, None, None, None]","[None, None, None, None]",308,[],"['DataLoader', 'loader.compute', 'loader.get_data', 'get_data_loader', 'Exception']",5
source/models/atorch/matchZoo.py:fit,fit,function,24,78,54,855,10.96,1,1,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kwargs']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",336,[],"['compute_pars.get', 'model0.named_parameters', 'any', 'model0.parameters', 'list', 'json_norm', 'trainer.run']",7
source/models/atorch/matchZoo.py:predict,predict,function,14,34,32,602,17.71,0,1,"['model', 'session', 'data_pars', 'compute_pars', 'out_pars']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",390,"['    """"""\n', '        https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/trainers/trainer.py#L341\n', '\n', '       Trainer:\n', '        def predict(\n', '        self,\n', '        dataloader: DataLoader\n', '    ) -> np.array:\n', '        \n', '        Generate output predictions for the input samples.\n', '        dataloader: input DataLoader\n', '        :return: predictions\n', '    \n', '        with torch.no_grad():\n', '            self._model.eval()\n', '            predictions = []\n', '            for batch in dataloader:\n', '                inputs = batch[0]\n', '                outputs = self._model(inputs).detach().cpu()\n', '                predictions.append(outputs)\n', '            self._model.train()\n', '            return torch.cat(predictions, dim=0).numpy()\n', '\n', '    """"""\n']","['get_raw_dataset', 'get_data_loader', 'session.predict']",3
source/models/atorch/matchZoo.py:evaluate,evaluate,function,0,1,1,4,4.0,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",440,[],[],0
source/models/atorch/matchZoo.py:save,save,function,4,10,10,103,10.3,0,0,"['model', 'session', 'save_pars']","[None, None, None]","[None, 'None', 'None']",444,"['    """"""\n', '      trainer == session\n', '          save_dir: Directory to save trainer.\n', '`       save_all: Bool. If True, save `Trainer` instance; If False,\n', '        only save model. Defaults to False.\n', '\n', '     https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/trainers/trainer.py#L369   \n', '\n', '    """"""\n']","['session.save', 'session.save_model']",2
source/models/atorch/matchZoo.py:load,load,function,0,1,1,4,4.0,0,0,['load_pars'],[None],[None],460,"['    """"""\n', '     need trainer instance\n', '     https://github.com/NTMC-Community/MatchZoo-py/blob/master/matchzoo/trainers/trainer.py#L415\n', ' \n', '    """"""\n']",[],0
source/models/atorch/matchZoo.py:get_params,get_params,function,15,34,30,413,12.15,0,1,"['param_pars', '**kw']","[None, None]","['None', None]",471,[],"['path_norm', 'json.load', 'path_norm_dict', 'Exception']",4
source/models/atorch/matchZoo.py:test_train,test_train,function,19,79,55,1187,15.03,0,0,"['data_path', 'pars_choice', 'model_name']","[None, None, None]","[None, None, None]",493,[],"['log', 'get_params', 'Model', 'fit', 'save', 'load', 'predict', 'print']",8
source/models/atorch/matchZoo.py:Model,Model,class,37,81,73,1335,16.48,1,3,[],[],[],241,[],[],0
source/models/atorch/matchZoo.py:Model:__init__,Model:__init__,method,36,75,67,1253,16.71,1,3,"['self', 'model_pars', 'data_pars', 'compute_pars', 'out_pars']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",242,[],"['deepcopy', 'mpars.items', 'get_task', 'model_pars.get', 'get_glove_embedding_matrix', 'get_dataset']",6
source/models/atorch/textcnn.py:_train,_train,function,30,48,43,508,10.58,1,0,"['m', 'device', 'train_itr', 'optimizer', 'epoch', 'max_epoch']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",47,[],"['m.train', 'torch.transpose', 'text.to', 'target.to', 'optimizer.zero_grad', 'm', 'F.cross_entropy', 'loss.backward', 'optimizer.step', 'loss.item', 'torch.max', 'len']",12
source/models/atorch/textcnn.py:_valid,_valid,function,26,46,39,446,9.7,1,0,"['m', 'device', 'test_itr']","[None, None, None]","[None, None, None]",72,[],"['m.eval', 'torch.transpose', 'text.to', 'target.to', 'm', 'F.cross_entropy', 'loss.item', 'torch.max', 'len']",9
source/models/atorch/textcnn.py:_get_device,_get_device,function,3,8,8,75,9.38,0,0,[],[],[],95,[],['torch.device'],1
source/models/atorch/textcnn.py:get_config_file,get_config_file,function,2,7,7,90,12.86,0,0,[],[],[],102,[],['os_package_root_path'],1
source/models/atorch/textcnn.py:get_data_file,get_data_file,function,2,5,5,80,16.0,0,0,[],[],[],108,[],['os_package_root_path'],1
source/models/atorch/textcnn.py:analyze_datainfo_paths,analyze_datainfo_paths,function,18,60,46,703,11.72,0,2,['data_info'],[None],[None],113,[],"['path_norm', 'data_info.get', 'Exception', 'dataset.find', 'dataset.split', 'os.makedirs']",6
source/models/atorch/textcnn.py:split_train_valid,split_train_valid,function,19,29,27,432,14.9,0,0,"['data_info', '**args']","[None, None]","[None, None]",150,[],"['args.get', 'analyze_datainfo_paths', 'pd.read_csv', 'RandomState', 'df.sample', 'print', 'train_data.to_csv', 'test_data.to_csv']",8
source/models/atorch/textcnn.py:clean_str,clean_str,function,4,72,30,484,6.72,0,0,['string'],[None],[None],169,"['    """"""\n', '    Tokenization/string cleaning for all datasets except for SST.\n', '    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n', '    """"""\n']","['re.sub', 'string.strip']",2
source/models/atorch/textcnn.py:create_tabular_dataset,create_tabular_dataset,function,35,105,93,1304,12.42,1,0,"['data_info', '**args']","[None, None]","[None, None]",190,[],"['args.get', 'analyze_datainfo_paths', 'spacy.load', 'log', 'os.system', 'importlib.import_module', 'tokenizer', 'spacy_en.tokenizer', 'Field', 'print', 'TabularDataset', 'TEXT.build_vocab', 'LABEL.build_vocab']",13
source/models/atorch/textcnn.py:create_data_iterator,create_data_iterator,function,5,16,12,233,14.56,0,0,"['batch_size', 'tabular_train', 'tabular_valid', 'd']","[None, None, None, None]","[None, None, None, None]",256,[],['Iterator'],1
source/models/atorch/textcnn.py:evaluate,evaluate,function,12,15,15,198,13.2,0,0,"['model', 'session', 'data_pars', 'compute_pars', 'out_pars', '**kwargs']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",337,[],"['_get_device', 'data_pars.copy', 'data_pars.update', 'get_dataset', '_valid']",5
source/models/atorch/textcnn.py:fit,fit,function,37,93,75,952,10.24,1,1,"['model', 'sess', 'data_pars', 'compute_pars', 'out_pars', '**kwargs']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",349,[],"['_get_device', 'optim.Adam', 'get_dataset', 'model0.rebuild_embed', 'range', '_train', 'print', '_valid', 'log', 'os.makedirs', 'torch.save', 'train_loss.append', 'train_acc.append', 'test_loss.append', 'test_acc.append']",15
source/models/atorch/textcnn.py:get_dataset,get_dataset,function,22,55,48,567,10.31,0,1,"['data_pars', 'out_pars', '**kwargs']","[None, None, None]","['None', 'None', None]",412,[],"['_get_device', 'DataLoader', 'loader.compute', 'loader.get_data', 'create_data_iterator', 'Exception']",6
source/models/atorch/textcnn.py:predict,predict,function,23,36,32,361,10.03,0,1,"['model', 'session', 'data_pars', 'compute_pars', 'out_pars', 'return_ytrue']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', '1']",439,[],"['data_pars.copy', 'data_pars.update', 'print', 'get_dataset', 'next', 'torch.transpose', 'model0.rebuild_embed', 'model0', 'torch.max']",9
source/models/atorch/textcnn.py:save,save,function,4,8,8,67,8.38,0,0,"['model', 'session', 'save_pars']","[None, None, None]","[None, 'None', 'None']",465,[],['save_tch'],1
source/models/atorch/textcnn.py:load,load,function,5,7,7,62,8.86,0,0,['load_pars'],[None],[' None'],471,[],['load_tch'],1
source/models/atorch/textcnn.py:get_params,get_params,function,29,103,88,1153,11.19,0,2,"['param_pars', '**kw']","[None, None]","['None', None]",478,[],"['JsonComment', 'path_norm', 'json.load', 'log']",4
source/models/atorch/textcnn.py:test,test,function,31,117,75,1564,13.37,0,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""json""', '""test""']",533,[],"['path_norm', 'log', 'get_params', 'get_dataset', 'print', 'Model', 'fit', 'save', 'load', 'predict', 'evaluate']",11
source/models/atorch/textcnn.py:TextCNN,TextCNN,class,29,76,62,910,11.97,1,0,[],[],[],268,[],[],0
source/models/atorch/textcnn.py:Model,Model,class,10,26,24,252,9.69,0,1,[],[],[],321,[],[],0
source/models/atorch/textcnn.py:TextCNN:__init__,TextCNN:__init__,method,14,33,30,424,12.85,1,0,"['self', 'model_pars', 'data_pars', 'compute_pars', '**kwargs']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",270,[],"['print', 'super', 'model_pars.get', 'nn.ModuleList', 'nn.Dropout', 'nn.Linear', 'len']",7
source/models/atorch/textcnn.py:TextCNN:rebuild_embed,TextCNN:rebuild_embed,method,3,3,3,101,33.67,0,0,"['self', 'vocab_built']","[None, None]","[None, None]",295,[],['nn.Embedding'],1
source/models/atorch/textcnn.py:TextCNN:forward,TextCNN:forward,method,12,28,22,250,8.93,0,0,"['self', 'x']","[None, None]","[None, None]",302,[],"['self.embed', 'emb_x.unsqueeze', 'x.size', 'torch.cat', 'fc_x.squeeze', 'self.dropout', 'self.fc']",7
source/models/atorch/textcnn.py:Model:__init__,Model:__init__,method,9,20,18,184,9.2,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",322,[],['TextCNN'],1
source/models/atorch/torchhub.py:_train,_train,function,29,49,42,473,9.65,1,1,"['m', 'device', 'train_itr', 'criterion', 'optimizer', 'epoch', 'max_epoch', 'imax']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, '1']",42,[],"['m.train', 'enumerate', 'image.to', 'target.to', 'optimizer.zero_grad', 'm', 'criterion', 'loss.backward', 'optimizer.step', 'loss.item', 'torch.max', 'len']",12
source/models/atorch/torchhub.py:_valid,_valid,function,26,46,39,411,8.93,1,1,"['m', 'device', 'test_itr', 'criterion', 'imax']","[None, None, None, None, None]","[None, None, None, None, '1']",68,[],"['m.eval', 'enumerate', 'image.to', 'target.to', 'm', 'criterion', 'loss.item', 'torch.max', 'len']",9
source/models/atorch/torchhub.py:_get_device,_get_device,function,3,8,7,76,9.5,0,0,[],[],[],91,[],['torch.device'],1
source/models/atorch/torchhub.py:get_config_file,get_config_file,function,2,2,2,49,24.5,0,0,[],[],[],96,[],['path_norm'],1
source/models/atorch/torchhub.py:get_params,get_params,function,15,34,30,416,12.24,0,1,"['param_pars', '**kw']","[None, None]","['None', None]",144,[],"['path_norm', 'json.load', 'path_norm_dict', 'Exception']",4
source/models/atorch/torchhub.py:get_dataset,get_dataset,function,10,38,33,374,9.84,0,1,"['data_pars', '**kw']","[None, None]","['None', None]",183,[],"['DataLoader', 'loader.compute', 'loader.get_data', 'Exception']",4
source/models/atorch/torchhub.py:fit,fit,function,40,104,83,1140,10.96,1,1,"['model', 'data_pars', 'compute_pars', 'out_pars', '**kwargs']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",208,[],"['nn.CrossEntropyLoss', '_get_device', 'model0.to', 'optim.Adam', 'get_dataset', 'compute_pars.get', 'len', 'os.makedirs', 'range', '_train', 'print', '_valid', 'torch.save', 'train_loss.append', 'train_acc.append', 'test_loss.append', 'test_acc.append']",17
source/models/atorch/torchhub.py:predict,predict,function,55,90,78,1036,11.51,1,3,"['model', 'session', 'data_pars', 'compute_pars', 'out_pars', 'imax ', 'return_ytrue']","[None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', ' 1', '1']",253,[],"['compute_pars.get', 'model0.buildNoiseData', 'torch.no_grad', 'model0.test', 'plt.imshow', 'os.makedirs', 'plt.savefig', 'os.system', '_get_device', 'get_dataset', 'enumerate', 'image.to', 'model', 'torch.max', 'y_pred.append', 'y_true.append', 'np.vstack']",17
source/models/atorch/torchhub.py:evaluate,evaluate,function,0,1,1,4,4.0,0,0,"['model', 'data_pars', 'compute_pars', 'out_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",299,[],[],0
source/models/atorch/torchhub.py:save,save,function,14,36,34,406,11.28,0,0,"['model', 'session', 'save_pars']","[None, None, None]","[None, 'None', 'None']",303,[],"['copy.deepcopy', 'path_norm', 'os.makedirs', 'save_tch', 'pickle.dump', 'open', 'log', 'os.listdir']",8
source/models/atorch/torchhub.py:load,load,function,18,35,32,376,10.74,0,0,['load_pars'],[None],[None],325,[],"['copy.deepcopy', 'path_norm', 'pickle.load', 'open', 'Model', 'load_tch']",6
source/models/atorch/torchhub.py:test,test,function,24,150,65,2196,14.64,0,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""json""', '""test""']",347,[],"['log', 'get_params', 'get_dataset', 'Model', 'fit', 'predict', 'evaluate', 'print', 'save', 'load', 'test2']",11
source/models/atorch/torchhub.py:test2,test2,function,17,67,51,983,14.67,0,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""json""', '""test""']",389,[],"['log', 'get_params', 'Model', 'predict', 'save', 'load', 'print']",7
source/models/atorch/torchhub.py:Model,Model,class,25,70,53,789,11.27,0,3,[],[],[],105,[],[],0
source/models/atorch/torchhub.py:Model:__init__,Model:__init__,method,24,64,47,707,11.05,0,3,"['self', 'model_pars', 'data_pars', 'compute_pars', 'out_pars']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",106,[],"['copy.deepcopy', 'm.get', 'bool', 'compute_pars.get', '_get_device', 'hub.load', 'nn.Linear']",7
source/models/atorch/torch_ctr.py:log,log,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],63,[],['print'],1
source/models/atorch/torch_ctr.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",71,[],['Model'],1
source/models/atorch/torch_ctr.py:customModel,customModel,function,2,2,2,11,5.5,0,0,[],[],[],78,[],[],0
source/models/atorch/torch_ctr.py:fit,fit,function,22,40,35,479,11.97,1,1,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",105,"['    """"""\n', '    """"""\n']","['get_dataset', 'torch.tensor', 'log', 'compute_pars.get', 'pd.DataFrame']",5
source/models/atorch/torch_ctr.py:predict,predict,function,23,57,48,666,11.68,0,3,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', 'None', '{}', None]",134,[],"['compute_pars2.get', 'get_dataset', 'list', 'len', 'torch.tensor', 'post_process_fun', 'print']",7
source/models/atorch/torch_ctr.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],177,[],[],0
source/models/atorch/torch_ctr.py:save,save,function,8,27,23,302,11.19,0,0,"['path', 'info']","[None, None]","['None', 'None']",182,"['    """"""\n', '       Custom saving\n', '    """"""\n']","['os.makedirs', 'pickle.dump', 'open']",3
source/models/atorch/torch_ctr.py:load_model,load_model,function,16,26,24,258,9.92,0,0,['path'],[None],"['""""']",199,[],"['pickle.load', 'Model']",2
source/models/atorch/torch_ctr.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",212,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/atorch/torch_ctr.py:preprocess,preprocess,function,21,67,37,705,10.52,0,2,['prepro_pars'],[None],[None],223,[],"['make_classification', 'train_test_split', 'pd.read_csv']",3
source/models/atorch/torch_ctr.py:get_dataset,get_dataset,function,7,49,33,415,8.47,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",253,"['    """"""\n', '      ""ram""  :\n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/atorch/torch_ctr.py:get_params,get_params,function,13,30,27,302,10.07,0,1,"['param_pars', '**kw']","[None, None]","['{}', None]",279,[],"['json.load', 'Exception']",2
source/models/atorch/torch_ctr.py:test,test,function,28,227,96,2740,12.07,0,0,['config'],[None],"[""''""]",299,"['    """"""\n', '       python torch_tabular.py test\n', '\n', '    """"""\n']","['train_test_split', 'Model', 'print', 'fit', 'predict', 'save', 'load_model', 'test2']",8
source/models/atorch/torch_ctr.py:test2,test2,function,27,113,94,1359,12.03,0,0,['config'],[None],"[""''""]",359,"['    """"""\n', '       python torch_tabular.py test\n', '\n', '    """"""\n']","['train_test_split', 'Model', 'print', 'fit', 'predict', 'save', 'load_model']",7
source/models/atorch/torch_ctr.py:Model,Model,class,17,32,29,327,10.22,0,2,[],[],[],82,[],[],0
source/models/atorch/torch_ctr.py:Model:__init__,Model:__init__,method,16,27,24,259,9.59,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",83,[],"['TabularModel', 'log']",2
source/models/atorch/transformer_sentence.py:fit,fit,function,39,63,56,1184,18.79,0,0,"['model', 'data_pars', 'model_pars', 'compute_pars', 'out_pars', '*args', '**kw']","[None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None, None]",130,"['    """"""\n', '    """"""\n']","['log', 'get_dataset', 'SentencesDataset', 'DataLoader', 'train_reader.get_num_labels', 'getattr', 'train_loss.float', 'EmbeddingSimilarityEvaluator']",8
source/models/atorch/transformer_sentence.py:evaluate,evaluate,function,3,5,4,35,7.0,0,0,"['model', 'session', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",171,"['    """"""\n', '       Return metrics of the model when fitted.\n', '    """"""\n']",[],0
source/models/atorch/transformer_sentence.py:predict,predict,function,12,22,21,237,10.77,1,1,"['model', 'session', 'data_pars', 'out_pars', 'compute_pars', '**kw']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",179,[],"['get_dataset', 'reader.get_examples', 'sum']",3
source/models/atorch/transformer_sentence.py:reset_model,reset_model,function,0,1,1,4,4.0,0,0,[],[],[],197,[],[],0
source/models/atorch/transformer_sentence.py:save,save,function,2,3,3,46,15.33,0,0,"['model', 'session', 'save_pars']","[None, None, None]","[None, 'None', 'None']",201,[],['torch.save'],1
source/models/atorch/transformer_sentence.py:load,load,function,5,6,5,83,13.83,0,0,['load_pars'],[None],['None'],205,[],"['Model', 'torch.load']",2
source/models/atorch/transformer_sentence.py:get_dataset,get_dataset,function,8,9,9,109,12.11,0,0,"['data_pars', '**kw']","[None, None]","['None', None]",213,"['    """"""\n', '    JSON data_pars to get dataset\n', '    ""data_pars"":    { ""data_path"": ""dataset/GOOG-year.csv"", ""data_type"": ""pandas"",\n', '    ""size"": [0, 0, 6], ""output_size"": [0, 6] },\n', '    """"""\n']","['DataLoader', 'loader.compute', 'loader.get_data']",3
source/models/atorch/transformer_sentence.py:fit2,fit2,function,32,50,46,835,16.7,0,0,"['model', 'data_pars', 'model_pars', 'compute_pars', 'out_pars', '*args', '**kw']","[None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None, None]",227,"['    """"""\n', '    """"""\n']","['log', 'get_dataset2', 'getattr', 'train_loss.float', 'EmbeddingSimilarityEvaluator']",5
source/models/atorch/transformer_sentence.py:predict2,predict2,function,14,21,20,229,10.9,1,0,"['model', 'session', 'data_pars', 'out_pars', 'compute_pars', '**kw']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', None]",260,[],"['get_dataset2', 'reader.get_examples', 'sum']",3
source/models/atorch/transformer_sentence.py:get_dataset2,get_dataset2,function,35,166,89,1883,11.34,0,7,"['data_pars', 'model', '**kw']","[None, None, None]","['None', 'None', None]",279,"['    """"""\n', '    JSON data_pars to get dataset\n', '    ""data_pars"":    { ""data_path"": ""dataset/GOOG-year.csv"", ""data_type"": ""pandas"",\n', '    ""size"": [0, 0, 6], ""output_size"": [0, 6] },\n', '    """"""\n']","['data_pars.get', 'get_reader', 'Reader', 'get_filename', 'log', 'data_pars.copy', 'train_pars.update', 'SentencesDataset', 'DataLoader', 'val_pars.update', 'train_reader.get_num_labels']",11
source/models/atorch/transformer_sentence.py:get_params,get_params,function,11,26,20,321,12.35,0,2,"['param_pars', '**kw']","[None, None]","[None, None]",351,[],"['path_norm', 'json.load', 'path_norm_dict']",3
source/models/atorch/transformer_sentence.py:test,test,function,29,96,69,1330,13.85,0,0,"['data_path', 'pars_choice', 'config_mode']","[None, None, None]","['""dataset/""', '""test01""', '""test""']",403,[],"['path_norm', 'log', 'get_params', 'get_dataset', 'Model', 'fit', 'predict', 'evaluate', 'print', 'save', 'load']",11
source/models/atorch/transformer_sentence.py:Model,Model,class,18,51,48,809,15.86,0,1,[],[],[],107,[],[],0
source/models/atorch/transformer_sentence.py:Model:__init__,Model:__init__,method,17,45,42,732,16.27,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars', '**kwargs']","[None, None, None, None, None]","[None, 'None', 'None', 'None', None]",108,[],"['getattr', 'self.EmbeddingModel', 'models.Pooling', 'model_pars.get', 'SentenceTransformer']",5
source/models/atorch/util_transformer.py:convert_example_to_feature,convert_example_to_feature,function,36,152,89,1846,12.14,0,9,"['example_row', 'pad_token', 'sequence_a_segment_id', 'sequence_b_segment_id', 'cls_token_segment_id', 'pad_token_segment_id', 'mask_padding_with_zero', 'sep_token_extra']","[None, None, None, None, None, None, None, None]","[None, '0', '0', '1', '1', '0', 'True', 'False']",124,[],"['tokenizer.tokenize', '_truncate_seq_pair', 'len', 'tokenizer.convert_tokens_to_ids', 'float', 'KeyError', 'InputFeatures']",7
source/models/atorch/util_transformer.py:convert_examples_to_features,convert_examples_to_features,function,10,37,34,389,10.51,1,0,"['examples', 'label_list', 'max_seq_length', 'tokenizer', 'output_mode', 'cls_token_at_end', 'sep_token_extra', 'pad_on_left', 'cls_token', 'sep_token', 'pad_token', 'sequence_a_segment_id', 'sequence_b_segment_id', 'cls_token_segment_id', 'pad_token_segment_id', 'mask_padding_with_zero', ') - 2)']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '']","[None, None, None, None, None, 'False', 'False', 'False', ""'[CLS]'"", ""'[SEP]'"", '0', '0', '1', '1', '0', 'True', None]",213,"['    """""" Loads a data file into a list of `InputBatch`s\n', '        `cls_token_at_end` define the location of the CLS token:\n', '            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n', '            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n', '        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n', '    """"""\n']","['enumerate', 'Pool', 'list', 'total=len']",4
source/models/atorch/util_transformer.py:_truncate_seq_pair,_truncate_seq_pair,function,5,16,13,152,9.5,1,2,"['tokens_a', 'tokens_b', 'max_length']","[None, None, None]","[None, None, None]",238,"['    """"""Truncates a sequence pair in place to the maximum length.""""""\n']","['len', 'tokens_a.pop', 'tokens_b.pop']",3
source/models/atorch/util_transformer.py:InputExample,InputExample,class,9,14,14,123,8.79,0,0,[],[],[],37,[],[],0
source/models/atorch/util_transformer.py:InputFeatures,InputFeatures,class,9,14,14,164,11.71,0,0,[],[],[],58,[],[],0
source/models/atorch/util_transformer.py:DataProcessor,DataProcessor,class,14,48,36,468,9.75,1,1,[],[],[],68,[],[],0
source/models/atorch/util_transformer.py:BinaryProcessor,BinaryProcessor,class,13,48,38,519,10.81,1,0,[],[],[],96,[],[],0
source/models/atorch/util_transformer.py:TransformerDataReader,TransformerDataReader,class,15,29,26,490,16.9,0,0,[],[],[],264,[],[],0
source/models/atorch/util_transformer.py:InputExample:__init__,InputExample:__init__,method,8,8,8,69,8.62,0,0,"['self', 'guid', 'text_a', 'text_b', 'label']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",40,"['        """"""Constructs a InputExample.\n', '\n', '        Args:\n', '            guid: Unique id for the example.\n', '            text_a: string. The untokenized text of the first sequence. For single\n', '            sequence tasks, only this sequence must be specified.\n', '            text_b: (Optional) string. The untokenized text of the second sequence.\n', '            Only must be specified for sequence pair tasks.\n', '            label: (Optional) string. The label of the example. This should be\n', '            specified for train and dev examples, but not for test examples.\n', '        """"""\n']",[],0
source/models/atorch/util_transformer.py:InputFeatures:__init__,InputFeatures:__init__,method,8,8,8,103,12.88,0,0,"['self', 'input_ids', 'input_mask', 'segment_ids', 'label_id']","[None, None, None, None, None]","[None, None, None, None, None]",61,[],[],0
source/models/atorch/util_transformer.py:DataProcessor:get_train_examples,DataProcessor:get_train_examples,method,1,2,2,26,13.0,0,0,"['self', 'data_dir']","[None, None]","[None, None]",71,"['        """"""Gets a collection of `InputExample`s for the train set.""""""\n']",['NotImplementedError'],1
source/models/atorch/util_transformer.py:DataProcessor:get_dev_examples,DataProcessor:get_dev_examples,method,1,2,2,26,13.0,0,0,"['self', 'data_dir']","[None, None]","[None, None]",75,"['        """"""Gets a collection of `InputExample`s for the dev set.""""""\n']",['NotImplementedError'],1
source/models/atorch/util_transformer.py:DataProcessor:get_labels,DataProcessor:get_labels,method,1,2,2,26,13.0,0,0,['self'],[None],[None],79,"['        """"""Gets the list of labels for this data set.""""""\n']",['NotImplementedError'],1
source/models/atorch/util_transformer.py:DataProcessor:_read_tsv,DataProcessor:_read_tsv,method,9,29,25,234,8.07,1,1,"['cls', 'input_file', 'quotechar']","[None, None, None]","[None, None, 'None']",84,"['        """"""Reads a tab separated value file.""""""\n']","['open', 'csv.reader', 'list', 'lines.append']",4
source/models/atorch/util_transformer.py:BinaryProcessor:get_train_examples,BinaryProcessor:get_train_examples,method,2,5,5,88,17.6,0,0,"['self', 'data_dir']","[None, None]","[None, None]",71,"['        """"""Gets a collection of `InputExample`s for the train set.""""""\n']","['self._create_examples', 'self._read_tsv']",2
source/models/atorch/util_transformer.py:BinaryProcessor:get_dev_examples,BinaryProcessor:get_dev_examples,method,2,5,5,84,16.8,0,0,"['self', 'data_dir']","[None, None]","[None, None]",75,"['        """"""Gets a collection of `InputExample`s for the dev set.""""""\n']","['self._create_examples', 'self._read_tsv']",2
source/models/atorch/util_transformer.py:BinaryProcessor:get_labels,BinaryProcessor:get_labels,method,1,3,3,15,5.0,0,0,['self'],[None],[None],79,"['        """"""Gets the list of labels for this data set.""""""\n']",[],0
source/models/atorch/util_transformer.py:BinaryProcessor:_create_examples,BinaryProcessor:_create_examples,method,8,23,22,192,8.35,1,0,"['self', 'lines', 'set_type']","[None, None, None]","[None, None, None]",113,"['        """"""Creates examples for the training and dev sets.""""""\n']","['enumerate', 'examples.append', 'InputExample']",3
source/models/atorch/util_transformer.py:TransformerDataReader:__init__,TransformerDataReader:__init__,method,5,8,8,199,24.88,0,0,"['self', '**args']","[None, None]","[None, None]",265,[],['path_norm'],1
source/models/atorch/util_transformer.py:TransformerDataReader:compute,TransformerDataReader:compute,method,6,11,11,201,18.27,0,0,"['self', 'input_tmp']","[None, None]","[None, None]",271,[],"['load_function', 'train_func', 'test_func']",3
source/models/atorch/util_transformer.py:TransformerDataReader:get_data,TransformerDataReader:get_data,method,2,2,2,15,7.5,0,0,['self'],[None],[None],280,[],[],0
source/models/repo/functions.py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],11,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/repo/functions.py:get_dataset,get_dataset,function,46,135,94,1508,11.17,7,2,"['state_num', 'time_len', 'signal_dimension', 'CNR', 'window_len', 'half_window_len']","[None, None, None, None, None, None]","['10', '50000', '15', '1', '11', '5']",21,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/repo/functions.py:get_model,get_model,function,57,197,148,2584,13.12,1,0,"['original_dim', 'class_num', 'intermediate_dim', 'intermediate_dim_2', 'latent_dim', 'batch_size', 'Lambda1', 'Lambda2', 'Alpha']","[None, None, None, None, None, None, None, None, None]","[None, '5', '64', '16', '3', '256', '1', '200', '0.075']",76,[],"['Input', 'Dense', 'Reshape', 'Lambda', 'Model', 'decoder', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log', 'vae.add_loss', 'vae.compile', 'vae.summary']",16
source/models/repo/functions.py:fit,fit,function,5,9,9,120,13.33,0,0,"['vae', 'x_train', 'epochs', 'batch_size']","[None, None, None, None]","[None, None, '1', '256']",176,[],"['np.ones', 'vae.fit']",2
source/models/repo/functions.py:save,save,function,1,1,1,38,38.0,0,0,['model'],[None],[None],184,[],['model.save_weights'],1
source/models/repo/functions.py:load,load,function,1,1,1,24,24.0,0,0,"['model', 'path']","[None, None]","[None, None]",187,[],['model.load_weights'],1
source/models/repo/functions.py:test,test,function,18,40,34,351,8.78,1,1,"['self', 'encoder', 'x_train', 'dummy_train', 'class_num', 'batch_size']","[None, None, None, None, None, None]","[None, None, None, None, '5', '256']",190,[],"['encoder.predict', 'np.zeros', 'range', 'np.max', 'np.argmax']",5
source/models/ztmp2/keras_widedeep_2.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],29,[],"['print', 'log2']",2
source/models/ztmp2/keras_widedeep_2.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],32,[],['print'],1
source/models/ztmp2/keras_widedeep_2.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",40,[],['Model'],1
source/models/ztmp2/keras_widedeep_2.py:Modelcustom,Modelcustom,function,22,64,49,1100,17.19,0,0,"['n_wide_cross', 'n_wide', 'n_deep', 'n_feat', 'm_EMBEDDING', 'loss', 'metric ']","[None, None, None, None, None, None, None]","[None, None, None, '8', '10', ""'mse'"", "" 'mean_squared_error'""]",48,[],"['layers.Input', 'layers.concatenate', 'layers.Dense', 'keras.Model', 'wide_model.compile', 'log2', 'layers.Embedding', 'layers.Flatten', 'deep_model.compile', 'model.compile']",10
source/models/ztmp2/keras_widedeep_2.py:get_dataset_tuple,get_dataset_tuple,function,12,43,31,329,7.65,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",84,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/ztmp2/keras_widedeep_2.py:get_dataset,get_dataset,function,31,136,91,1321,9.71,1,6,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",106,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",7
source/models/ztmp2/keras_widedeep_2.py:ModelCustom2,ModelCustom2,function,24,59,53,955,16.19,1,0,[],[],[],153,[],"['wide_and_deep_classifier', 'dnn_hidden_units.split', 'enumerate', 'model.compile', 'input_template_feed_keras', 'sparse.values', 'real.values']",7
source/models/ztmp2/keras_widedeep_2.py:input_template_feed_keras,input_template_feed_keras,function,33,144,84,1491,10.35,5,5,"['Xtrain', 'cols_type_received', 'cols_ref', '**kw']","[None, None, None, None]","[None, None, None, None]",181,"['    """"""\n', '       Create sparse data struccture in KERAS  To plug with MODEL:\n', '       No data, just virtual data\n', '    https://github.com/GoogleCloudPlatform/data-science-on-gcp/blob/master/09_cloudml/flights_model_tf2.ipynb\n', '\n', '    :return:\n', '    """"""\n']","['len', 'min', 'int', 'categorical_column_with_hash_bucket', 'numeric_column', 'crossed_column', 'np.linspace', 'bucketized_column', 'indicator_column', 'dict_sparse.items', 'embedding_column']",11
source/models/ztmp2/keras_widedeep_2.py:get_dataset_tuple_keras,get_dataset_tuple_keras,function,35,96,76,1307,13.61,1,2,"['pattern', 'batch_size', 'mode', 'truncate']","[None, None, None, None]","[None, None, 'tf.estimator.ModeKeys.TRAIN', 'None']",237,"['    """"""  ACTUAL Data reading :\n', '           Dataframe ---> TF Dataset  --> feed Keras model\n', '\n', '    """"""\n']","['pandas_to_dataset', 'print', 'tf.cast', 'load_dataset', 'features_and_labels', 'features.pop', 'dataset.map', 'dataset.shuffle', 'dataset.repeat', 'dataset.prefetch', 'dataset.take']",11
source/models/ztmp2/keras_widedeep_2.py:get_dataset2,get_dataset2,function,22,89,61,960,10.79,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",291,"['    """"""\n', '      return tuple of Tensoflow\n', '    """"""\n']","['data_pars.get', 'get_dataset_tuple_keras', 'log2', 'Exception']",4
source/models/ztmp2/keras_widedeep_2.py:fit,fit,function,22,49,46,554,11.31,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",361,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'ModelCheckpoint']",5
source/models/ztmp2/keras_widedeep_2.py:predict,predict,function,18,37,33,403,10.89,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",380,[],"['get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log2', 'compute_pars.get']",5
source/models/ztmp2/keras_widedeep_2.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],397,[],[],0
source/models/ztmp2/keras_widedeep_2.py:save,save,function,17,33,31,377,11.42,0,0,"['path', 'info']","[None, None]","['None', 'None']",402,[],"['os.makedirs', 'Model', 'pickle.dump', 'open']",4
source/models/ztmp2/keras_widedeep_2.py:load_model,load_model,function,17,30,28,310,10.33,0,0,['path'],[None],"['""""']",420,[],"['pickle.load', 'Model']",2
source/models/ztmp2/keras_widedeep_2.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",435,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/ztmp2/keras_widedeep_2.py:test,test,function,41,195,150,1973,10.12,2,0,['config'],[None],"[""''""]",448,"['    """"""\n', '        Group of columns for the input model\n', '           cols_input_group = [ ]\n', '          for cols in cols_input_group,\n', '\n', '    :param config:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 'range', 'train_test_split', 'log', 'colg_input.items', 'test_helper', 'Model', 'fit', 'predict', 'save', 'load_model']",11
source/models/ztmp2/keras_widedeep_2.py:test_helper,test_helper,function,12,49,39,644,13.14,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",521,[],"['Model', 'log', 'fit', 'predict', 'save', 'load_model']",6
source/models/ztmp2/keras_widedeep_2.py:Modelsparse2,Modelsparse2,function,54,188,131,2687,14.29,8,0,[],[],[],575,"['    """"""\n', '    https://github.com/GoogleCloudPlatform/data-science-on-gcp/blob/master/09_cloudml/flights_model_tf2.ipynb\n', '\n', '    :return:\n', '    """"""\n']","['real.keys', 'inputs.update', 'sparse.keys', 'np.linspace', 'disc.update', 'sparse.items', 'real.update', 'print', 'wide_and_deep_classifier', 'dnn_hidden_units.split', 'enumerate', 'model.compile', 'sparse.values', 'real.values']",14
source/models/ztmp2/keras_widedeep_2.py:Model,Model,class,19,40,37,734,18.35,0,1,[],[],[],336,[],[],0
source/models/ztmp2/keras_widedeep_2.py:Model:__init__,Model:__init__,method,18,35,32,666,19.03,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",337,[],"['log2', 'len', 'Modelcustom']",3
source/models/ztmp2/keras_widedeep_old.py:log,log,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],27,[],['print'],1
source/models/ztmp2/keras_widedeep_old.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",35,[],['Model'],1
source/models/ztmp2/keras_widedeep_old.py:Modelcustom,Modelcustom,function,22,64,48,1103,17.23,0,0,"['n_wide_cross', 'n_wide', 'n_feat', 'm_EMBEDDING', 'loss', 'metric ']","[None, None, None, None, None, None]","[None, None, '8', '10', ""'mse'"", "" 'mean_squared_error'""]",41,[],"['layers.Input', 'layers.concatenate', 'layers.Dense', 'keras.Model', 'wide_model.compile', 'print', 'layers.Embedding', 'layers.Flatten', 'deep_model.compile', 'model.compile']",10
source/models/ztmp2/keras_widedeep_old.py:fit,fit,function,15,31,28,274,8.84,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",91,"['    """"""\n', '    """"""\n']","['get_dataset', 'compute_pars.get']",2
source/models/ztmp2/keras_widedeep_old.py:eval,eval,function,15,40,39,474,11.85,0,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",106,"['    """"""\n', '       Return metrics of the model when fitted.\n', '    """"""\n']","['get_dataset', 'predict', 'compute_pars.get', 'mpars.get', 'scorer']",5
source/models/ztmp2/keras_widedeep_old.py:predict,predict,function,13,26,22,253,9.73,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', '{}', '{}', '{}', None]",134,[],"['get_dataset', 'compute_pars.get']",2
source/models/ztmp2/keras_widedeep_old.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],151,[],[],0
source/models/ztmp2/keras_widedeep_old.py:save,save,function,7,11,11,121,11.0,0,0,['path'],[None],['None'],156,[],['os.makedirs'],1
source/models/ztmp2/keras_widedeep_old.py:load_model,load_model,function,7,13,11,117,9.0,0,0,['path'],[None],"['""""']",165,[],[],0
source/models/ztmp2/keras_widedeep_old.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",174,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/ztmp2/keras_widedeep_old.py:preprocess,preprocess,function,21,68,38,740,10.88,0,2,['prepro_pars'],[None],[None],185,[],"['make_classification', 'train_test_split', 'pd.read_csv']",3
source/models/ztmp2/keras_widedeep_old.py:get_dataset,get_dataset,function,25,118,64,1196,10.14,4,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",216,"['    """"""\n', '      return tuple of dataframes\n', '    """"""\n']","['data_pars.get', 'Xtuple_train.append', 'Xtuple_test.append', 'Exception']",4
source/models/ztmp2/keras_widedeep_old.py:test,test,function,40,147,121,1496,10.18,2,0,['config'],[None],"[""''""]",277,"['    """"""\n', '        Group of columns for the input model\n', '           cols_input_group = [\n', '\n', '          ]\n', '          for cols in cols_input_group,\n', '\n', '\n', '    :param config:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 'range', 'train_test_split', 'EarlyStopping', 'ModelCheckpoint', 'colg_input.items', 'test_helper']",7
source/models/ztmp2/keras_widedeep_old.py:get_dataset2,get_dataset2,function,28,121,65,1240,10.25,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",355,"['    """"""\n', '      ""ram""  :\n', '      ""file"" :\n', '\n', ""    n_wide_features = data_pars.get('n_wide_features', None)\n"", ""    n_deep_features = data_pars.get('n_deep_features', None)\n"", '\n', '    Xtrain_A, Xtrain_B, Xtrain_C = Xtrain[:, :n_wide_features], Xtrain[:, -n_deep_features:], Xtrain[:, -n_deep_features:]\n', '    Xtest_A, Xtest_B, Xtest_C    = Xtest[:, :n_wide_features], Xtest[:, -n_deep_features:], Xtest[:, -n_deep_features:]\n', '\n', '\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/ztmp2/keras_widedeep_old.py:get_params_sklearn,get_params_sklearn,function,2,2,2,39,19.5,0,0,['deep'],[None],['False'],414,[],[],0
source/models/ztmp2/keras_widedeep_old.py:get_params,get_params,function,15,34,30,377,11.09,0,1,['deep'],[None],['False'],418,[],"['get_params', 'json.load', 'Exception']",3
source/models/ztmp2/keras_widedeep_old.py:test_helper,test_helper,function,12,59,44,779,13.2,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",435,[],"['Model', 'log', 'fit', 'predict', 'save', 'load_model']",6
source/models/ztmp2/keras_widedeep_old.py:test2,test2,function,32,93,85,1045,11.24,0,0,['config'],[None],"[""''""]",468,[],"['train_test_split', 'EarlyStopping', 'ModelCheckpoint', 'test_helper']",4
source/models/ztmp2/keras_widedeep_old.py:Model,Model,class,17,32,29,375,11.72,0,2,[],[],[],78,[],[],0
source/models/ztmp2/keras_widedeep_old.py:Model:__init__,Model:__init__,method,16,27,24,307,11.37,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",79,[],"['globals', 'Modelcustom', 'log']",3
source/models/ztmp2/modelsVaem.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],33,[],"['print', 'log2']",2
source/models/ztmp2/modelsVaem.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],36,[],['print'],1
source/models/ztmp2/modelsVaem.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",44,[],['Model'],1
source/models/ztmp2/modelsVaem.py:load_data,load_data,function,68,132,119,2478,18.77,1,2,"['filePath', 'categories', 'cat_col', 'num_cols', 'discrete_cols', 'targetCol', 'nsample', 'delimiter']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",62,[],"['pd.read_csv', 'print', 'dataframe.copy', 'process.encode_catrtogrial_column', 'np.array', 'len', 'np.concatenate', 'np.zeros', 'np.ones', 'Data_sub.min', 'process.data_preprocess', 'train_test_split', 'Data_train_decompressed.copy', 'Data_test_decompressed.copy', 'process.noisy_transform']",15
source/models/ztmp2/modelsVaem.py:encode2,encode2,function,32,134,73,2332,17.4,0,2,"['data_decode', 'list_discrete', 'records_d', 'fast_plot']","[None, None, None, None]","[None, None, None, None]",144,[],"['p_vae_active_learning', 'tf.reset_default_graph', 'vae.get_imputation', 'process.compress_data', 'x_real.min', 'pd.DataFrame', 'sns.pairplot', 'g.map_diag', 'g.set', 'g.map_upper', 'g.map_lower', 'process.invert_noise']",12
source/models/ztmp2/modelsVaem.py:decode2,decode2,function,41,73,71,1403,19.22,0,1,"['data_decode', 'scaling_factor', 'list_discrete', 'records_d', 'plot']","[None, None, None, None, None]","[None, None, None, None, 'False']",202,[],"['params.Params', 'print', 'p_vae_active_learning', 'np.load', 'plt.figure', 'plt.subplots', 'ax1.plot', 'ax1.errorbar', 'plt.xlabel', 'plt.ylabel', 'plt.xticks', 'plt.yticks', 'ax1.legend', 'ax1.ticklabel_format', 'plt.show']",15
source/models/ztmp2/modelsVaem.py:save_model2,save_model2,function,1,1,1,22,22.0,0,0,"['model', 'output_dir']","[None, None]","[None, None]",241,[],['model.save'],1
source/models/ztmp2/modelsVaem.py:p_vae_active_learning,p_vae_active_learning,function,138,540,271,4958,9.18,8,6,"['Data_train_compressed', 'Data_train', 'mask_train', 'Data_test', 'mask_test_compressed', 'mask_test', 'cat_dims', 'dim_flt', 'dic_var_type', 'args', 'list_discrete', 'records_d', 'estimation_method']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None, '1']",244,[],"['len', 'np.zeros', 'range', 'reward.lindley', 'tf.reset_default_graph', 'train_p_vae', 'np.tile', 'random.shuffle', 'vae.predictive_loss', 'print', 'np.eye', 'np.savez', 'Data_train_compressed.reshape', 'np.ones', 'np.where', 'reward_estimation.R_lindley_chain', 'reward_estimation.completion']",17
source/models/ztmp2/modelsVaem.py:train_p_vae,train_p_vae,function,87,362,189,4013,11.09,10,8,"['stage', 'x_train', 'Data_train', 'mask_train', 'epochs', 'latent_dim', 'cat_dims', 'dim_flt', 'batch_size', 'p', 'K', 'iteration', 'list_discrete', 'records_d', 'args']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]",422,[],"['np.arange', 'np.minimum', 'encoders.vaem_encoders', 'decoders.vaem_decoders', 'vae_model.partial_vaem', 'int', 'np.zeros', 'range', 'sample', 'bernoulli.rvs', 'np.sum', 'mask_drop.reshape', 'vae.update', 'vae.full_batch_loss', 'print', 'vae.save_generator', 'vae.save_encoder']",17
source/models/ztmp2/modelsVaem.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],577,[],[],0
source/models/ztmp2/modelsVaem.py:save,save,function,1,1,1,22,22.0,0,0,"['model', 'output_dir']","[None, None]","[None, None]",581,"['        """"""\n', '        This Function will load the data and fit the preprocessing technique into it\n', '        params:\n', '            filePath: CSV File path relative to file\n', '            categories: Categorical feature list\n', '            cat_cols: category_columns\n', '            nums_cols: Numerical Columns list\n', '            discrete_cols: discrete cols list\n', '            target_cols: Column name of Target Variable\n', '            nsample: No of Sample for Encoding-Decoding\n', ""            delimiter: Delimiter in CSV File,Default=','\n"", '\n', '        """"""\n']",['model.save'],1
source/models/ztmp2/modelsVaem.py:load_model,load_model,function,18,30,28,353,11.77,0,0,['path'],[None],"['""""']",599,[],"['pickle.load', 'Model', 'get_model', 'log']",4
source/models/ztmp2/modelsVaem.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",620,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/ztmp2/modelsVaem.py:Model,Model,class,59,112,92,1809,16.15,3,0,[],[],[],632,[],[],0
source/models/ztmp2/modelsVaem.py:Model:__init__,Model:__init__,method,1,5,5,48,9.6,0,0,['self'],[None],[None],633,[],['print'],1
source/models/ztmp2/modelsVaem.py:Model:fit,Model:fit,method,37,47,44,765,16.28,1,0,"['self,filePath, categories,cat_cols,num_cols,discrete_cols,targetCol,nsample ']",[None],"["" -1,delimiter=',',plot=False""]",642,"['        """"""\n', '        This Function will load the data and fit the preprocessing technique into it\n', '        params:\n', '            filePath: CSV File path relative to file\n', '            categories: Categorical feature list\n', '            cat_cols: category_columns\n', '            nums_cols: Numerical Columns list\n', '            discrete_cols: discrete cols list\n', '            target_cols: Column name of Target Variable\n', '            nsample: No of Sample for Encoding-Decoding\n', ""            delimiter: Delimiter in CSV File,Default=','\n"", '\n', '        """"""\n']","['load_data', 'encode2', 'decode2']",3
source/models/ztmp2/modelsVaem.py:Model:decode,Model:decode,method,14,25,25,415,16.6,1,0,['self'],[None],[None],679,[],"['range', 'print', 'log2', 'pa.table', 'pq.write_table']",5
source/models/ztmp2/modelsVaem.py:Model:encode,Model:encode,method,14,25,25,413,16.52,1,0,['self'],[None],[None],698,[],"['range', 'print', 'log2', 'pa.table', 'pq.write_table']",5
source/models/ztmp2/model_vaem.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],38,[],"['print', 'log2']",2
source/models/ztmp2/model_vaem.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],41,[],['print'],1
source/models/ztmp2/model_vaem.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",49,[],['Model'],1
source/models/ztmp2/model_vaem.py:fit,fit,function,12,25,25,212,8.48,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",116,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy', 'compute_pars.get']",3
source/models/ztmp2/model_vaem.py:predict,predict,function,12,24,23,249,10.38,0,1,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",128,[],"['get_dataset', 'get_dataset_tuple', 'log2']",3
source/models/ztmp2/model_vaem.py:get_dataset_tuple,get_dataset_tuple,function,12,43,31,329,7.65,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",146,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/ztmp2/model_vaem.py:get_dataset,get_dataset,function,31,136,91,1321,9.71,1,6,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",168,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",7
source/models/ztmp2/model_vaem.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],215,[],[],0
source/models/ztmp2/model_vaem.py:save,save,function,17,33,31,377,11.42,0,0,"['path', 'info']","[None, None]","['None', 'None']",220,[],"['os.makedirs', 'Model', 'pickle.dump', 'open']",4
source/models/ztmp2/model_vaem.py:load_model,load_model,function,17,30,28,310,10.33,0,0,['path'],[None],"['""""']",237,[],"['pickle.load', 'Model']",2
source/models/ztmp2/model_vaem.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",252,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/ztmp2/model_vaem.py:test,test,function,41,195,150,1973,10.12,2,0,['config'],[None],"[""''""]",265,"['    """"""\n', '        Group of columns for the input model\n', '           cols_input_group = [ ]\n', '          for cols in cols_input_group,\n', '\n', '    :param config:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 'range', 'train_test_split', 'log', 'colg_input.items', 'test_helper', 'Model', 'fit', 'predict', 'save', 'load_model']",11
source/models/ztmp2/model_vaem.py:test_helper,test_helper,function,12,49,39,644,13.14,0,0,"['model_pars', 'data_pars', 'compute_pars']","[None, None, None]","[None, None, None]",338,[],"['Model', 'log', 'fit', 'predict', 'save', 'load_model']",6
source/models/ztmp2/model_vaem.py:load_dataset,load_dataset,function,2,2,2,44,22.0,0,0,"[')seed ', '[""job""])matrix1', '[""marital""])matrix1', '[""education""])matrix1', '[""default""])matrix1', '[""housing""])matrix1', '[""loan""])matrix1', '[""contact""])matrix1', '[""month""])matrix1', '[""day_of_week""])matrix1', '[""poutcome""])matrix1', '[""y""])(matrix1.values).astype(float))[0', '', '1', '2', '3', '4', '5', '6', '7])[8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20])[8', '9])np.in1d(list_flt', 'list_discrete).nonzero()[0])list_cat)list_flt)>0 and len(list_cat)>0']","[None, None, None, None, None, None, None, None, None, None, None, '', ']max_Data ', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, '']","[' 3000""./data/bank/bankmarketing_train.csv"")bank_raw.info())label_column=""y"")matrix1', None, None, None, None, None, None, None, None, None, None, None, ' 0.7min_Data = 0.3[0', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]",374,[],['np.concatenate'],1
source/models/ztmp2/model_vaem.py:Model,Model,class,15,31,29,369,11.9,0,1,[],[],[],99,[],[],0
source/models/ztmp2/model_vaem.py:Model:__init__,Model:__init__,method,14,26,24,301,11.58,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",100,[],"['log2', 'Modelcustom']",2
source/models/ztmp2/model_vaem3.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],37,[],"['print', 'log2']",2
source/models/ztmp2/model_vaem3.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],40,[],['print'],1
source/models/ztmp2/model_vaem3.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",48,[],['Model'],1
source/models/ztmp2/model_vaem3.py:load_data,load_data,function,67,129,117,2198,17.04,1,2,"['filePath', 'categories', 'cat_col', 'num_cols', 'discrete_cols', 'targetCol', 'nsample', 'delimiter']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",64,"[""    '''\n"", '        Data will be loaded from repo/VAEM/data/bank and then preprocessing starts\n', ""    '''\n""]","['pd.read_csv', 'print', 'dataframe.copy', 'process.encode_catrtogrial_column', 'np.array', 'len', 'np.concatenate', 'np.zeros', 'np.ones', 'Data_sub.min', 'process.data_preprocess', 'train_test_split', 'Data_train_decomp.copy', 'Data_test_decomp.copy', 'process.noisy_transform']",15
source/models/ztmp2/model_vaem3.py:encode2,encode2,function,32,134,73,2298,17.15,0,2,"['data_decode', 'list_discrete', 'records_d', 'fast_plot']","[None, None, None, None]","[None, None, None, None]",143,[],"['p_vae_active_learning', 'process.compress_data', 'tf.reset_default_graph', 'vae.get_imputation', 'x_real.min', 'pd.DataFrame', 'sns.pairplot', 'g.map_diag', 'g.set', 'g.map_upper', 'g.map_lower', 'process.invert_noise']",12
source/models/ztmp2/model_vaem3.py:decode2,decode2,function,39,71,69,1336,18.82,0,1,"['data_decode', 'scaling_factor', 'list_discrete', 'records_d', 'plot', 'args']","[None, None, None, None, None, None]","[None, None, None, None, 'False', 'None']",202,[],"['print', 'p_vae_active_learning', 'np.load', 'plt.figure', 'plt.subplots', 'ax1.plot', 'ax1.errorbar', 'plt.xlabel', 'plt.ylabel', 'plt.xticks', 'plt.yticks', 'ax1.legend', 'ax1.ticklabel_format', 'plt.show']",14
source/models/ztmp2/model_vaem3.py:save_model2,save_model2,function,1,1,1,22,22.0,0,0,"['model', 'output_dir']","[None, None]","[None, None]",241,[],['model.save'],1
source/models/ztmp2/model_vaem3.py:p_vae_active_learning,p_vae_active_learning,function,137,540,271,4912,9.1,8,6,"['Data_train_comp', 'Data_train', 'mask_train', 'Data_test', 'mask_test_comp', 'mask_test', 'cat_dims', 'dim_flt', 'dic_var_type', 'args', 'list_discrete', 'records_d', 'estimation_method']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None, '1']",244,[],"['len', 'np.zeros', 'range', 'reward.lindley', 'tf.reset_default_graph', 'train_p_vae', 'np.tile', 'random.shuffle', 'vae.predictive_loss', 'print', 'np.eye', 'np.savez', 'Data_train_comp.reshape', 'np.ones', 'np.where', 'reward_estimation.R_lindley_chain', 'reward_estimation.completion']",17
source/models/ztmp2/model_vaem3.py:train_p_vae,train_p_vae,function,87,382,195,4013,10.51,10,8,"['stage', 'x_train', 'Data_train', 'mask_train', 'epochs', 'latent_dim', 'cat_dims', 'dim_flt', 'batch_size', 'p', 'K', 'iteration', 'list_discrete', 'records_d', 'args']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]",421,[],"['np.arange', 'np.minimum', 'encoders.vaem_encoders', 'decoders.vaem_decoders', 'vae_model.partial_vaem', 'int', 'np.zeros', 'range', 'sample', 'bernoulli.rvs', 'np.sum', 'mask_drop.reshape', 'vae.update', 'vae.full_batch_loss', 'print', 'vae.save_generator', 'vae.save_encoder']",17
source/models/ztmp2/model_vaem3.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],578,[],[],0
source/models/ztmp2/model_vaem3.py:save,save,function,1,1,1,22,22.0,0,0,"['model', 'output_dir']","[None, None]","[None, None]",582,"['        """"""\n', '        This Function will load the data and fit the preprocessing technique into it\n', '        params:\n', '            filePath: CSV File path relative to file\n', '            categories: Categorical feature list\n', '            cat_cols: category_columns\n', '            nums_cols: Numerical Columns list\n', '            discrete_cols: discrete cols list\n', '            target_cols: Column name of Target Variable\n', '            nsample: No of Sample for Encoding-Decoding\n', ""            delimiter: Delimiter in CSV File,Default=','\n"", '\n', '        """"""\n']",['model.save'],1
source/models/ztmp2/model_vaem3.py:load_model,load_model,function,18,30,28,353,11.77,0,0,['path'],[None],"['""""']",600,[],"['pickle.load', 'Model', 'get_model', 'log']",4
source/models/ztmp2/model_vaem3.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",621,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/ztmp2/model_vaem3.py:Model,Model,class,37,71,60,1027,14.46,1,0,[],[],[],632,[],[],0
source/models/ztmp2/model_vaem3.py:Model:__init__,Model:__init__,method,1,5,5,48,9.6,0,0,['self'],[None],[None],633,[],['print'],1
source/models/ztmp2/model_vaem3.py:Model:fit,Model:fit,method,22,35,35,539,15.4,1,0,"['self', 'p']","[None, None]","[None, None]",642,"['        """"""\n', '        This Function will load the data and fit the preprocessing technique into it\n', '        params:\n', '            filePath: CSV File path relative to file\n', '            categories: Categorical feature list\n', '            cat_cols: category_columns\n', '            nums_cols: Numerical Columns list\n', '            discrete_cols: discrete cols list\n', '            target_cols: Column name of Target Variable\n', '            nsample: No of Sample for Encoding-Decoding\n', ""            delimiter: Delimiter in CSV File,Default=','\n"", '\n', '        """"""\n']",['load_data'],1
source/models/ztmp2/model_vaem3.py:Model:encode,Model:encode,method,8,10,10,175,17.5,0,0,"['self', 'plot', 'args']","[None, None, None]","[None, 'False', 'None']",678,"[""        '''\n"", '        This function will be using encode2 function defined above for encoding data.\n', '        This Model will automatically saved as Generator in saved_weights folder.\n', '\n', '        params:\n', '            plot: To Plot the Graph of Encoder Training\n', ""        '''\n""]",['encode2'],1
source/models/ztmp2/model_vaem3.py:Model:decode,Model:decode,method,5,9,8,151,16.78,0,0,"['self', 'plot', 'args']","[None, None, None]","[None, 'False', 'None']",692,"[""        '''\n"", '        This function will be using decoder2 function defined above for decoding data.\n', '        This Model will automatically saved as Generator in saved_weights folder.\n', '        params:\n', '            plot: To Plot the Graph of Decoder Training\n', ""        '''\n""]",['decode2'],1
source/models/ztmp2/torch_rvae2.py:log,log,function,3,8,7,65,8.12,0,1,['*s'],[None],[None],33,[],"['print', 'log2']",2
source/models/ztmp2/torch_rvae2.py:log2,log2,function,2,5,5,31,6.2,0,1,['*s'],[None],[None],36,[],['print'],1
source/models/ztmp2/torch_rvae2.py:init,init,function,3,8,7,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",43,[],['Model'],1
source/models/ztmp2/torch_rvae2.py:fit,fit,function,16,43,38,444,10.33,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",67,"['    """"""\n', '    """"""\n']","['copy.deepcopy', 'compute_pars.get', 'get_dataset', 'pd.concat']",4
source/models/ztmp2/torch_rvae2.py:predict,predict,function,19,40,36,461,11.53,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, ' dict', ' dict', ' dict', None]","['None', '{}', '{}', '{}', None]",88,[],"['get_dataset', 'data_pars.get', 'get_dataset_tuple', 'pd.concat', 'compute_pars.get']",5
source/models/ztmp2/torch_rvae2.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],108,[],[],0
source/models/ztmp2/torch_rvae2.py:save,save,function,11,32,29,360,11.25,0,0,"['path', 'info']","[None, None]","['None', 'None']",112,"['    """""" Custom saving\n', '    """"""\n']","['os.makedirs', 'pickle.dump', 'open']",3
source/models/ztmp2/torch_rvae2.py:load_model,load_model,function,18,31,29,348,11.23,0,0,['path'],[None],"['""""']",128,[],"['pickle.load', 'Model', 'TabularModel.load_from_checkpoint']",3
source/models/ztmp2/torch_rvae2.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",146,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/ztmp2/torch_rvae2.py:get_dataset_tuple,get_dataset_tuple,function,12,42,30,330,7.86,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",159,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3) OR single dataframe\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/ztmp2/torch_rvae2.py:get_dataset,get_dataset,function,31,135,90,1322,9.79,1,6,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",185,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",7
source/models/ztmp2/torch_rvae2.py:test_dataset_1,test_dataset_1,function,2,5,5,19,3.8,0,0,['nrows'],[None],['1000'],231,[],[],0
source/models/ztmp2/torch_rvae2.py:test,test,function,39,305,202,3224,10.57,1,1,['nrows'],[None],['1000'],240,[],"['test', 'test_dataset_covtype', 'log', 'np.sum', 'train_test_split', 'len', 'reset', 'Model', 'fit', 'predict', 'save', 'load_model', 'test2']",13
source/models/ztmp2/torch_rvae2.py:test2,test2,function,2,3,3,19,6.33,0,0,['nrow'],[None],['10000'],373,"['    """"""\n', '       python source/models/torch_tabular.py test\n', '\n', '    """"""\n']",[],0
source/models/ztmp2/torch_rvae2.py:Model,Model,class,18,34,30,339,9.97,0,2,[],[],[],48,[],[],0
source/models/ztmp2/torch_rvae2.py:Model:__init__,Model:__init__,method,17,29,25,271,9.34,0,2,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",49,[],['log'],1
source/models/ztmp2/torch_tabular2.py:log,log,function,2,5,4,54,10.8,0,0,['*s'],[None],[None],66,[],"['print', 'log2']",2
source/models/ztmp2/torch_tabular2.py:log2,log2,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],69,[],['print'],1
source/models/ztmp2/torch_tabular2.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",75,[],['Model'],1
source/models/ztmp2/torch_tabular2.py:fit,fit,function,16,44,38,456,10.36,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",130,"['    """"""\n', '    """"""\n']","['log2', 'get_dataset', 'copy.deepcopy', 'compute_pars.get', 'pd.concat']",5
source/models/ztmp2/torch_tabular2.py:predict,predict,function,19,40,36,461,11.53,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, ' dict', ' dict', ' dict', None]","['None', '{}', '{}', '{}', None]",153,[],"['get_dataset', 'data_pars.get', 'get_dataset_tuple', 'pd.concat', 'compute_pars.get']",5
source/models/ztmp2/torch_tabular2.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],173,[],[],0
source/models/ztmp2/torch_tabular2.py:save,save,function,11,32,29,360,11.25,0,0,"['path', 'info']","[None, None]","['None', 'None']",177,"['    """""" Custom saving\n', '    """"""\n']","['os.makedirs', 'pickle.dump', 'open']",3
source/models/ztmp2/torch_tabular2.py:load_model,load_model,function,18,31,29,348,11.23,0,0,['path'],[None],"['""""']",193,[],"['pickle.load', 'Model', 'TabularModel.load_from_checkpoint']",3
source/models/ztmp2/torch_tabular2.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",211,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/ztmp2/torch_tabular2.py:get_dataset2,get_dataset2,function,7,49,33,415,8.47,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",222,"['    """"""\n', '      ""ram""  :\n', '      ""file"" :\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/ztmp2/torch_tabular2.py:get_dataset_tuple,get_dataset_tuple,function,9,31,25,272,8.77,1,1,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",251,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3) OR single dataframe\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/ztmp2/torch_tabular2.py:get_dataset,get_dataset,function,7,49,33,415,8.47,0,4,"['data_pars', 'task_type', '**kw']","[None, None, None]","['None', '""train""', None]",274,"['    """"""\n', '      return tuple of dataframes\n', '    """"""\n']","['data_pars.get', 'Exception']",2
source/models/ztmp2/torch_tabular2.py:test_dataset_covtype,test_dataset_covtype,function,23,76,73,1113,14.64,0,1,['nrows'],[None],['1000'],320,[],"['log', 'Path.home', 'BASE_DIR.joinpath', 'datafile.exists', 'wget.download', 'datafile.as_posix', 'pd.read_csv']",7
source/models/ztmp2/torch_tabular2.py:test,test,function,82,471,338,5648,11.99,1,2,['nrows'],[None],['1000'],354,[],"['log', 'Path.home', 'BASE_DIR.joinpath', 'datafile.exists', 'wget.download', 'datafile.as_posix', 'pd.read_csv', 'test', 'test_dataset_covtype', 'np.sum', 'train_test_split', 'len', 'post_process_fun', 'int', 'pre_process_fun', 'Model', 'log2', 'fit', 'predict', 'save', 'load_model', 'reset', 'test3', 'test2', 'df.head', 'DataConfig', 'CategoryEmbeddingModelConfig', 'TrainerConfig', 'ExperimentConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'test.drop', 'tabular_model.predict']",35
source/models/ztmp2/torch_tabular2.py:test3,test3,function,0,1,1,4,4.0,0,0,[],[],[],509,[],[],0
source/models/ztmp2/torch_tabular2.py:test2,test2,function,32,63,60,1175,18.65,0,0,['nrows'],[None],['10000'],513,"['    """"""\n', '       python source/models/torch_tabular.py test\n', '\n', '    """"""\n']","['test_dataset_covtype', 'df.head', 'train_test_split', 'len', 'DataConfig', 'CategoryEmbeddingModelConfig', 'TrainerConfig', 'ExperimentConfig', 'OptimizerConfig', 'TabularModel', 'tabular_model.fit', 'tabular_model.evaluate', 'log', 'test.drop', 'tabular_model.predict']",15
source/models/ztmp2/torch_tabular2.py:Model,Model,class,35,122,102,1317,10.8,0,3,[],[],[],81,[],[],0
source/models/ztmp2/torch_tabular2.py:Model:__init__,Model:__init__,method,34,117,97,1249,10.68,0,3,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",82,[],"['DataConfig', 'model_pars.get', 'log2', 'model_class', 'TrainerConfig', 'OptimizerConfig', 'TabularModel', 'log']",8
data/docs/encoder/cat_string_encoder/column_encoder.py:test_MDVEncoder,test_MDVEncoder,function,9,51,29,354,6.94,0,0,[],[],[],590,[],"['np.array', 'MDVEncoder', 'encoder.fit', 'np.array_equal']",4
data/docs/encoder/cat_string_encoder/column_encoder.py:OneHotEncoderRemoveOne,OneHotEncoderRemoveOne,class,18,32,31,397,12.41,0,0,[],[],[],30,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramNaiveFisherKernel,NgramNaiveFisherKernel,class,111,399,243,4030,10.1,11,9,[],[],[],53,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:PretrainedFastText,PretrainedFastText,class,33,82,68,779,9.5,3,2,[],[],[],240,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:MinHashEncoder,MinHashEncoder,class,34,145,92,1256,8.66,6,3,[],[],[],280,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocIndependentPDF,AdHocIndependentPDF,class,49,102,82,1048,10.27,4,0,[],[],[],339,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramsMultinomialMixture,NgramsMultinomialMixture,class,105,288,192,3456,12.0,4,7,[],[],[],375,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocNgramsMultinomialMixture,AdHocNgramsMultinomialMixture,class,70,195,129,2132,10.93,3,3,[],[],[],487,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:MDVEncoder,MDVEncoder,class,27,96,56,715,7.45,4,5,[],[],[],557,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:PasstroughEncoder,PasstroughEncoder,class,10,20,17,225,11.25,0,0,[],[],[],600,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:ColumnEncoder,ColumnEncoder,class,110,604,347,8393,13.9,1,15,[],[],[],617,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:DimensionalityReduction,DimensionalityReduction,class,26,118,93,1180,10.0,0,5,[],[],[],941,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:OneHotEncoderRemoveOne:__init__,OneHotEncoderRemoveOne:__init__,method,13,13,13,186,14.31,0,0,"['self', 'n_values', 'categorical_features', 'categories', 'sparse', 'dtype', 'handle_unknown', '']","[None, None, None, None, None, None, None, None]","[None, 'None', 'None', '""auto""', 'True', 'np.float64', '""error""', None]",31,[],['super'],1
data/docs/encoder/cat_string_encoder/column_encoder.py:OneHotEncoderRemoveOne:transform,OneHotEncoderRemoveOne:transform,method,4,5,5,43,8.6,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",48,[],['super'],1
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramNaiveFisherKernel:__init__,NgramNaiveFisherKernel:__init__,method,17,17,17,241,14.18,0,0,"['self', '2', '4)', 'categories', 'dtype', 'handle_unknown', 'hashing_dim', 'n_prototypes', 'random_state', 'n_jobs', '']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, '""auto""', 'np.float64', '""ignore""', 'None', 'None', 'None', 'None', None]",31,[],['super'],1
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramNaiveFisherKernel:fit,NgramNaiveFisherKernel:fit,method,33,145,111,1372,9.46,2,7,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['self._check_X', 'ValueError', 'isinstance', 'np.all', 'np.array', 'list', 'check_random_state', 'range', 'np.unique', 'get_kmeans_prototypes', 'np.in1d']",11
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramNaiveFisherKernel:transform,NgramNaiveFisherKernel:transform,method,38,75,65,658,8.77,2,2,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['self._check_X', 'range', 'np.in1d', 'np.all', 'np.unique', 'ValueError', 'sum', 'np.empty', 'enumerate', 'self._ngram_presence_fisher_kernel', 'len']",11
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramNaiveFisherKernel:_ngram_presence_fisher_kernel,NgramNaiveFisherKernel:_ngram_presence_fisher_kernel,method,46,138,75,1486,10.77,7,0,"['self', 'strings', 'cats']","[None, None, None]","[None, None, None]",178,"['        """""" given to arrays of strings, returns the\n', '        encoding matrix of size\n', '        len(strings) x len(cats)\n', '        kernel fisher with p\n', '        where p is the presence vector\n', '        """"""\n']","['np.unique', 'sum', 'theta.sum', 'CountVectorizer', 'vectorizer.fit_transform', 'vectorizer.transform', 'enumerate', 'np.ones', 'similarity.append', 'gamma_inv.sum', 'sparse.vstack', 'similarity.reshape', 'np.empty', 'len', 'np.nan_to_num', '_ngram_presence_fisher_kernel2', 'np.zeros', 'np.array']",18
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramNaiveFisherKernel:_ngram_presence_fisher_kernel2,NgramNaiveFisherKernel:_ngram_presence_fisher_kernel2,method,34,65,51,672,10.34,4,0,"['self', 'strings', 'cats']","[None, None, None]","[None, None, None]",210,"['        """""" given to arrays of strings, returns the\n', '        encoding matrix of size\n', '        len(strings) x len(cats)\n', '        kernel fisher with p\n', '        where p is the presence vector\n', '        """"""\n']","['np.unique', 'sum', 'CountVectorizer', 'vectorizer.fit_transform', 'vectorizer.transform', 'enumerate', 'np.zeros', 'similarity.append', 'np.array', 'np.empty', 'len', 'np.nan_to_num']",12
data/docs/encoder/cat_string_encoder/column_encoder.py:PretrainedFastText:__init__,PretrainedFastText:__init__,method,4,4,4,53,13.25,0,0,"['self', 'n_components', 'language']","[None, None, None]","[None, None, '""english""']",245,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:PretrainedFastText:fit,PretrainedFastText:fit,method,7,29,27,316,10.9,0,1,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['dict', 'path_dict.keys', 'AttributeError', 'load_model']",4
data/docs/encoder/cat_string_encoder/column_encoder.py:PretrainedFastText:transform,PretrainedFastText:transform,method,21,38,32,312,8.21,3,1,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['X.ravel', 'np.unique', 'dict', 'enumerate', 'x.find', 'np.empty', 'zip']",7
data/docs/encoder/cat_string_encoder/column_encoder.py:MinHashEncoder:__init__,MinHashEncoder:__init__,method,4,4,4,59,14.75,0,0,"['self', 'n_components', 'ngram_range', '4']","[None, None, None, None]","[None, None, '(2', None]",285,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:MinHashEncoder:get_unique_ngrams,MinHashEncoder:get_unique_ngrams,method,8,34,26,231,6.79,2,0,"['self', 'string', 'ngram_range']","[None, None, None]","[None, None, None]",289,"['        """"""\n', '        Return a list of different n-grams in a string\n', '        """"""\n']","['range', 'list']",2
data/docs/encoder/cat_string_encoder/column_encoder.py:MinHashEncoder:minhash,MinHashEncoder:minhash,method,9,37,31,353,9.54,1,1,"['self', 'string', 'n_components', 'ngram_range']","[None, None, None, None]","[None, None, None, None]",301,[],"['np.ones', 'self.get_unique_ngrams', 'len', 'np.array', 'range', 'np.minimum']",6
data/docs/encoder/cat_string_encoder/column_encoder.py:MinHashEncoder:fit,MinHashEncoder:fit,method,7,20,18,170,8.5,1,1,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['enumerate', 'self.minhash']",2
data/docs/encoder/cat_string_encoder/column_encoder.py:MinHashEncoder:transform,MinHashEncoder:transform,method,10,29,21,247,8.52,2,1,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['np.zeros', 'enumerate', 'self.minhash']",3
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocIndependentPDF:__init__,AdHocIndependentPDF:__init__,method,8,9,9,162,18.0,0,0,"['self', 'fisher_kernel', 'dtype', 'ngram_range', '4']","[None, None, None, None, None]","[None, 'True', 'np.float64', '(2', None]",340,[],['CountVectorizer'],1
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocIndependentPDF:fit,AdHocIndependentPDF:fit,method,11,16,15,201,12.56,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['np.unique', 'sum']",2
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocIndependentPDF:transform,AdHocIndependentPDF:transform,method,29,64,50,565,8.83,4,0,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['np.unique', 'len', 'enumerate', 'np.ones', 'inv_beta.transpose', 'inv_beta.sum', 'np.zeros', 'np.nan_to_num']",8
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramsMultinomialMixture:__init__,NgramsMultinomialMixture:__init__,method,16,18,18,280,15.56,0,0,"['self', 'n_topics', 'max_iters', 'fisher_kernel', 'beta_init_type', 'max_mean_change_tol', '2', '4)', '']","[None, None, None, None, None, None, None, None, None]","[None, '10', '100', 'True', 'None', '1e-5', None, None, None]",31,"['    """"""\n', '    Fisher kernel for a simple n-gram probability distribution\n', '    For the moment, the default implementation uses the most-frequent\n', '    prototypes\n', '    """"""\n']",['CountVectorizer'],1
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramsMultinomialMixture:_get_most_frequent,NgramsMultinomialMixture:_get_most_frequent,method,12,15,13,235,15.67,0,0,"['self', 'X']","[None, None]","[None, None]",400,[],"['np.unique', 'np.argsort']",2
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramsMultinomialMixture:_max_mean_change,NgramsMultinomialMixture:_max_mean_change,method,3,5,4,76,15.2,0,0,"['self', 'last_beta', 'beta']","[None, None, None]","[None, None, None]",408,[],['max'],1
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramsMultinomialMixture:_e_step,NgramsMultinomialMixture:_e_step,method,24,52,40,734,14.12,2,0,"['self', 'D', 'unqD', 'X', 'unqX', 'theta', 'beta']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",412,[],"['enumerate', 'np.log', 'np.array', 'range', 'logsumexp', 'np.zeros', 'np.exp']",7
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramsMultinomialMixture:_m_step,NgramsMultinomialMixture:_m_step,method,9,21,20,232,11.05,0,0,"['self', 'D', '_doc_topic_posterior']","[None, None, None]","[None, None, None]",432,[],"['np.dot', 'D.toarray', 'np.divide', 'np.sum']",4
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramsMultinomialMixture:fit,NgramsMultinomialMixture:fit,method,43,97,71,1194,12.31,2,4,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['np.unique', 'self._get_most_frequent', 'protoD.sum', 'np.ones', 'np.zeros', 'range', 'print', 'self._e_step', 'self._m_step', 'self._max_mean_change']",10
data/docs/encoder/cat_string_encoder/column_encoder.py:NgramsMultinomialMixture:transform,NgramsMultinomialMixture:transform,method,14,43,31,361,8.4,0,3,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['np.unique', 'type', 'TypeError', 'self._e_step']",4
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocNgramsMultinomialMixture:__init__,AdHocNgramsMultinomialMixture:__init__,method,8,9,9,162,18.0,0,0,"['self', 'n_iters', 'fisher_kernel', 'ngram_range', '4']","[None, None, None, None, None]","[None, '10', 'True', '(2', None]",495,[],['CountVectorizer'],1
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocNgramsMultinomialMixture:_e_step,AdHocNgramsMultinomialMixture:_e_step,method,24,52,38,638,12.27,2,0,"['self', 'D', 'unqD', 'X', 'unqX', 'theta', 'beta']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",412,[],"['enumerate', 'np.array', 'range', 'P_dz_thetabeta.sum', 'np.zeros']",5
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocNgramsMultinomialMixture:_m_step,AdHocNgramsMultinomialMixture:_m_step,method,9,21,20,232,11.05,0,0,"['self', 'D', '_doc_topic_posterior']","[None, None, None]","[None, None, None]",432,[],"['np.dot', 'D.toarray', 'np.divide', 'np.sum']",4
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocNgramsMultinomialMixture:fit,AdHocNgramsMultinomialMixture:fit,method,26,45,38,539,11.98,1,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['np.unique', 'len', 'sparse.csr_matrix', 'unqD.sum', 'range', 'self._e_step', 'self._m_step']",7
data/docs/encoder/cat_string_encoder/column_encoder.py:AdHocNgramsMultinomialMixture:transform,AdHocNgramsMultinomialMixture:transform,method,14,43,31,361,8.4,0,3,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['np.unique', 'type', 'TypeError', 'self._e_step']",4
data/docs/encoder/cat_string_encoder/column_encoder.py:MDVEncoder:__init__,MDVEncoder:__init__,method,2,2,2,22,11.0,0,0,"['self', 'clf_type']","[None, None]","[None, None]",558,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:MDVEncoder:fit,MDVEncoder:fit,method,18,53,36,408,7.7,3,2,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['np.unique', 'enumerate']",2
data/docs/encoder/cat_string_encoder/column_encoder.py:MDVEncoder:transform,MDVEncoder:transform,method,10,31,21,210,6.77,1,3,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']","['np.zeros', 'len', 'enumerate']",3
data/docs/encoder/cat_string_encoder/column_encoder.py:PasstroughEncoder:__init__,PasstroughEncoder:__init__,method,2,2,2,28,14.0,0,0,"['self', 'passthrough']","[None, None]","[None, 'True']",601,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:PasstroughEncoder:fit,PasstroughEncoder:fit,method,4,6,6,83,13.83,0,0,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']",['FunctionTransformer'],1
data/docs/encoder/cat_string_encoder/column_encoder.py:PasstroughEncoder:transform,PasstroughEncoder:transform,method,2,2,2,31,15.5,0,0,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']",[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:ColumnEncoder:__init__,ColumnEncoder:__init__,method,52,246,162,4816,19.58,0,0,"['self', 'encoder_name', 'reduction_method', '2', '4)', 'categories', 'dtype', 'handle_unknown', 'clf_type', 'n_components', '']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'None', None, None, '""auto""', 'np.float64', '""ignore""', 'None', 'None', None]",31,[],"['OneHotEncoder', 'OneHotEncoderRemoveOne', 'SimilarityEncoder', 'NgramNaiveFisherKernel', 'CountVectorizer', 'TfidfVectorizer', 'TargetEncoder', 'MDVEncoder', 'cat_enc.BackwardDifferenceEncoder', 'cat_enc.BinaryEncoder', 'cat_enc.HashingEncoder', 'cat_enc.HelmertEncoder', 'cat_enc.SumEncoder', 'cat_enc.PolynomialEncoder', 'cat_enc.BaseNEncoder', 'cat_enc.LeaveOneOutEncoder', 'Pipeline', 'LatentDirichletAllocation', 'NMF', 'NgramsMultinomialMixture', 'AdHocNgramsMultinomialMixture', 'AdHocIndependentPDF', 'gamma_poisson_factorization.OnlineGammaPoissonFactorization', 'MinHashEncoder', 'PretrainedFastText', 'FunctionTransformer', 'PasstroughEncoder']",27
data/docs/encoder/cat_string_encoder/column_encoder.py:ColumnEncoder:_get_most_frequent,ColumnEncoder:_get_most_frequent,method,13,42,41,404,9.62,0,1,"['self', 'X']","[None, None]","[None, None]",400,[],"['np.unique', 'len', 'warnings.warn', 'unqX.ravel', 'np.argsort', 'np.sort']",6
data/docs/encoder/cat_string_encoder/column_encoder.py:ColumnEncoder:get_feature_names,ColumnEncoder:get_feature_names,method,5,9,7,120,13.33,0,0,['self'],[None],[None],816,[],[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:ColumnEncoder:fit,ColumnEncoder:fit,method,40,267,151,2602,9.75,1,13,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['ValueError', 'np.all', 'np.array', 'LabelEncoder', 'np.in1d', 'X.reshape', 'len', 'warnings.warn', 'Pipeline', 'DimensionalityReduction']",10
data/docs/encoder/cat_string_encoder/column_encoder.py:ColumnEncoder:transform,ColumnEncoder:transform,method,9,15,14,169,11.27,0,1,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']",[],0
data/docs/encoder/cat_string_encoder/column_encoder.py:DimensionalityReduction:__init__,DimensionalityReduction:__init__,method,9,26,25,417,16.04,0,0,"['self', 'method_name', 'n_components', 'column_names']","[None, None, None, None]","[None, 'None', 'None', 'None']",942,[],"['FunctionTransformer', 'GaussianRandomProjection', 'TruncatedSVD', 'PCA']",4
data/docs/encoder/cat_string_encoder/column_encoder.py:DimensionalityReduction:fit,DimensionalityReduction:fit,method,13,69,53,557,8.07,0,4,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",81,"['        """"""Fit the CategoricalEncoder to X.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature.\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['ValueError', 'warnings.warn']",2
data/docs/encoder/cat_string_encoder/column_encoder.py:DimensionalityReduction:transform,DimensionalityReduction:transform,method,5,11,9,87,7.91,0,1,"['self', 'X']","[None, None]","[None, None]",140,"['        """"""Transform X using specified encoding scheme.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape [n_samples, n_features]\n', '            The data to encode.\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_features_new]\n', '            Transformed input.\n', '        """"""\n']",['Xout.reshape'],1
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:write_json,write_json,function,3,10,10,83,8.3,0,0,"['data', 'file']","[None, None]","[None, None]",32,[],"['open', 'json.dump']",2
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:read_json,read_json,function,5,9,8,51,5.67,0,0,['file'],[None],[None],37,[],"['open', 'json.load']",2
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:array2list,array2list,function,10,34,23,202,5.94,2,2,['d'],[None],[None],43,"['    """"""\n', '    For a dictionary d, it transforms tuple/array elements to list elements\n', '    """"""\n']","['type', 'array2list', 'list', 'enumerate']",4
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:method2str,method2str,function,12,37,26,203,5.49,2,1,['iterable'],[None],[None],59,"['    """"""\n', '    Like array2list but, if there is a method, it transforms it into str.\n', '    """"""\n']","['type', 'method2str', 'list', 'enumerate', 'str']",5
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:verify_if_exists,verify_if_exists,function,11,30,22,234,7.8,2,2,"['results_path', 'results_dict']","[None, None]","[None, None]",78,[],"['array2list', 'glob.glob', 'read_json']",3
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:get_score_metric,get_score_metric,function,8,24,15,273,11.38,0,3,['clf_type'],[None],[None],90,[],[],0
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:instanciate_estimators,instanciate_estimators,function,59,616,201,9365,15.2,2,3,"['clf_type', 'classifiers', 'clf_seed', 'y', '**kw']","[None, None, None, None, None]","[None, None, None, 'None', None]",103,[],"['get_score_metric', 'print', 'len', 'list', 'linear_model.LogisticRegressionCV', 'ensemble.GradientBoostingClassifier', 'GridSearchCV', 'estimator=LGBMClassifier', 'estimator=XGBClassifier', 'MLPClassifier', 'estimator=MLPClassifier', 'FKC_EigenPro', 'estimator=FKC_EigenPro', 'estimator=Pipeline', 'Nystroem', 'max', 'set', 'num_class=len', 'linear_model.RidgeCV', 'ensemble.GradientBoostingRegressor', 'estimator=LGBMRegressor', 'estimator=XGBRegressor', 'MLPRegressor', 'estimator=MLPRegressor', 'FKR_EigenPro', 'estimator=FKR_EigenPro', 'ValueError', 'clf.get_params', 'clf.set_params']",29
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:select_cross_val,select_cross_val,function,15,39,31,465,11.92,0,3,"['clf_type', 'n_splits', 'test_size', 'custom_cv', 'col_name', 'random_state']","[None, None, None, None, None, None]","[None, None, None, 'None', 'None', 'shuffle_seed']",587,[],"['ShuffleSplit', 'StratifiedShuffleSplit']",2
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:select_scaler,select_scaler,function,3,4,3,65,16.25,0,0,[],[],[],607,[],['preprocessing.StandardScaler'],1
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:choose_nrows,choose_nrows,function,3,28,26,375,13.39,0,1,['dataset_name'],[None],[None],612,[],[],0
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:get_column_action,get_column_action,function,7,36,30,592,16.44,1,0,"['col_action', 'xcols', 'encoder', 'reduction_method', 'n_components', 'clf_type']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",637,[],['ColumnEncoder'],1
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:fit_predict_fold,fit_predict_fold,function,46,121,95,1243,10.27,0,1,"['data', 'scaler', 'column_action', 'clf', 'encoder', 'reduction_method', 'n_components', 'fold', 'n_splits', 'train_index', 'test_index', '']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None]",666,"['    """"""\n', '    fits and predicts a X with y given multiple parameters.\n', '    """"""\n']","['time.time', 'ColumnTransformer', 'transformer.fit_transform', 'scaler.fit_transform', 'clf.fit', 'transformer.transform', 'scaler.transform', 'get_score_metric', 'clf.predict', 'clf.predict_proba', 'clf.decision_function', 'score_metric', 'print']",13
data/docs/encoder/cat_string_encoder/fit_predict_categorical_encoding.py:fit_predict_categorical_encoding,fit_predict_categorical_encoding,function,83,336,221,3763,11.2,7,9,"['datasets', 'str_preprocess', 'encoders', 'classifiers', 'reduction_methods', 'n_components', 'test_size', 'n_splits', 'n_jobs', 'results_path', 'model_path', 'custom_cv', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, 'None', 'None', None]",732,"['    """"""\n', '    Learning with dirty categorical variables.\n', '    """"""\n']","['get_data_path', 'os.makedirs', 'choose_nrows', 'print', 'Data', 'data.preprocess', 'type', 'enumerate', 'select_cross_val', 'select_scaler', 'instanciate_estimators', 'str', 'scaler.get_params', 'clf.get_params', 'verify_if_exists', 'time.time', 'get_column_action', 'Parallel', 'delayed', 'cv.split', 'np.array', 'list', 'socket.gethostname', 'c.isdigit', 'array2list', 'method2str', 'write_json']",27
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:_rescale_W,_rescale_W,function,8,13,11,51,3.92,0,0,"['W', 'A', 'B']","[None, None, None]","[None, None, None]",404,[],['W.sum'],1
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:_multiplicative_update_w,_multiplicative_update_w,function,12,30,23,166,5.53,0,1,"['Vt', 'W', 'A', 'B', 'Ht', 'rescale_W', 'rho']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",411,[],"['safe_sparse_dot', 'Vt.multiply', 'Ht.sum', 'np.divide', '_rescale_W']",5
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:_rescale_h,_rescale_h,function,11,20,18,123,6.15,0,0,"['V', 'H']","[None, None]","[None, None]",422,[],"['np.maximum', 'V.sum', 'H.sum']",3
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:_multiplicative_update_h,_multiplicative_update_h,function,32,73,55,480,6.58,2,2,"['Vt', 'W', 'Ht', 'epsilon', 'max_iter', 'rescale_W', 'gamma_shape_prior', 'gamma_scale_prior', '']","[None, None, None, None, None, None, None, None, None]","[None, None, None, '1e-3', '10', 'False', '1.1', '1.0', None]",429,[],"['np.sum', 'WT1.reshape', 'zip', 'range', 'np.dot']",5
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:batch_lookup,batch_lookup,function,7,18,18,154,8.56,1,0,"['lookup', 'n']","[None, None]","[None, '1']",463,[],"['len', 'range', 'lookup[slice', 'min', 'np.unique']",5
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:get_kmeans_prototypes,get_kmeans_prototypes,function,18,30,29,511,17.03,0,1,"['X', 'n_prototypes', 'hashing_dim', '2', '4)', 'sparse', 'sample_weight', 'random_state', '']","[None, None, None, None, None, None, None, None, None]","[None, None, '128', None, None, 'False', 'None', 'None', None]",471,"['    """"""\n', '    Computes prototypes based on:\n', '      - dimensionality reduction (via hashing n-grams)\n', '      - k-means clustering\n', '      - nearest neighbor\n', '    """"""\n']","['HashingVectorizer', 'vectorizer.transform', 'projected.toarray', 'KMeans', 'kmeans.fit', 'NearestNeighbors', 'neighbors.fit', 'np.unique', 'np.sort']",9
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization,OnlineGammaPoissonFactorization,class,159,602,335,7272,12.08,8,21,[],[],[],14,[],[],0
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:__init__,OnlineGammaPoissonFactorization:__init__,method,40,71,58,973,13.7,0,3,"['self', 'n_topics', 'batch_size', 'gamma_shape_prior', 'gamma_scale_prior', 'r', 'rho', 'hashing', 'hashing_n_features', 'init', 'tol', 'min_iter', 'max_iter', '2', '4)', 'analizer', 'add_words', 'random_state', 'rescale_W', 'max_iter_e_step', '']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, '10', '512', '1.1', '1.0', '0.7', '0.99', 'False', '2 ** 12', '""k-means++""', '1e-4', '2', '5', None, None, '""char""', 'False', 'None', 'True', '20', None]",72,[],"['check_random_state', 'HashingVectorizer', 'CountVectorizer']",3
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:_update_H_dict,OnlineGammaPoissonFactorization:_update_H_dict,method,4,8,7,34,4.25,1,0,"['self', 'X', 'H']","[None, None, None]","[None, None, None]",135,[],['zip'],1
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:_init_vars,OnlineGammaPoissonFactorization:_init_vars,method,30,64,52,722,11.28,0,4,"['self', 'X']","[None, None]","[None, None]",139,[],"['np.unique', 'sparse.hstack', 'np.concatenate', 'self._init_w', '_rescale_h', 'np.ones', 'dict', 'self._update_H_dict', 'len']",9
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:_get_H,OnlineGammaPoissonFactorization:_get_H,method,8,13,12,100,7.69,1,0,"['self', 'X']","[None, None]","[None, None]",166,[],"['np.empty', 'zip']",2
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:_init_w,OnlineGammaPoissonFactorization:_init_w,method,24,88,62,901,10.24,0,3,"['self', 'V', 'X']","[None, None, None]","[None, None, None]",172,[],"['_k_init', 'row_norms', 'get_kmeans_prototypes', 'np.hstack', 'np.concatenate', 'AttributeError', 'W.sum', 'np.ones', 'A.copy']",9
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:fit,OnlineGammaPoissonFactorization:fit,method,24,75,65,777,10.36,2,3,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",216,"['        """"""Fit the OnlineGammaPoissonFactorization to X.\n', '\n', '        Parameters\n', '        ----------\n', '        X : string aself.rrray-like, shape [n_samples, n_features]\n', '            The data to determine the categories of each feature\n', '        Returns\n', '        -------\n', '        self\n', '        """"""\n']","['self._init_vars', 'self._get_H', 'range', 'enumerate', '_multiplicative_update_h', '_multiplicative_update_w', 'self._update_H_dict']",7
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:get_feature_names,OnlineGammaPoissonFactorization:get_feature_names,method,21,37,30,490,13.24,1,0,"['self', 'n_top']","[None, None]","[None, '3']",260,[],"['CountVectorizer', 'vectorizer.fit', 'np.array', 'self.transform', 'abs', 'np.sum', 'range', 'topic_labels.append']",8
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:score,OnlineGammaPoissonFactorization:score,method,19,43,40,685,15.93,1,1,"['self', 'X']","[None, None]","[None, None]",276,"['        """"""\n', '        Returns the Kullback-Leibler divergence.\n', '\n', '        Parameters\n', '        ----------\n', '        X : array-like (str), shape [n_samples,]\n', '            The data to encode.\n', '\n', '        Returns\n', '        -------\n', '        kl_divergence : float.\n', '            Transformed input.\n', '        """"""\n']","['np.unique', 'sparse.hstack', 'self._add_unseen_keys_to_H_dict', 'self._get_H', 'gen_batches', '_multiplicative_update_h', '_beta_divergence']",7
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:partial_fit,OnlineGammaPoissonFactorization:partial_fit,method,34,86,74,1086,12.63,1,4,"['self', 'X', 'y']","[None, None, None]","[None, None, 'None']",315,[],"['hasattr', 'np.unique', 'sparse.hstack', 'np.setdiff1d', 'np.array', '_rescale_h', 'np.ones', 'zip', 'self._init_vars', 'self._get_H', '_multiplicative_update_h', 'self._update_H_dict', '_multiplicative_update_w']",13
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:_add_unseen_keys_to_H_dict,OnlineGammaPoissonFactorization:_add_unseen_keys_to_H_dict,method,12,23,21,349,15.17,0,2,"['self', 'X']","[None, None]","[None, None]",357,[],"['np.setdiff1d', 'np.array', 'sparse.hstack', '_rescale_h', 'np.ones', 'self._update_H_dict']",6
data/docs/encoder/cat_string_encoder/gamma_poisson_factorization.py:OnlineGammaPoissonFactorization:transform,OnlineGammaPoissonFactorization:transform,method,17,35,34,568,16.23,1,1,"['self', 'X']","[None, None]","[None, None]",368,"['        """"""Transform X using the trained matrix W.\n', '\n', '        Parameters\n', '        ----------\n', '        X : array-like (str), shape [n_samples,]\n', '            The data to encode.\n', '\n', '        Returns\n', '        -------\n', '        X_new : 2-d array, shape [n_samples, n_topics]\n', '            Transformed input.\n', '        """"""\n']","['np.unique', 'sparse.hstack', 'self._add_unseen_keys_to_H_dict', 'self._get_H', 'gen_batches', '_multiplicative_update_h', 'self._update_H_dict']",7
data/docs/encoder/cat_string_encoder/get_data.py:preprocess_data,preprocess_data,function,9,23,20,159,6.91,1,0,"['df', 'cols']","[None, None]","[None, None]",22,[],"['string_normalize', 'str', 'print']",3
data/docs/encoder/cat_string_encoder/get_data.py:get_data_path,get_data_path,function,5,31,21,316,10.19,0,1,[],[],[],34,[],['socket.gethostname'],1
data/docs/encoder/cat_string_encoder/get_data.py:create_folder,create_folder,function,3,7,7,93,13.29,0,1,"['path', 'folder']","[None, None]","[None, None]",47,[],['os.makedirs'],1
data/docs/encoder/cat_string_encoder/get_data.py:print_unique_values,print_unique_values,function,3,8,8,89,11.12,1,0,['df'],[None],[None],53,[],['print'],1
data/docs/encoder/cat_string_encoder/get_data.py:check_nan_percentage,check_nan_percentage,function,4,28,27,220,7.86,0,1,"['df', 'col_name']","[None, None]","[None, None]",60,[],['warnings.warn'],1
data/docs/encoder/cat_string_encoder/get_data.py:Data,Data,class,137,917,437,11661,12.72,13,53,[],[],[],70,[],[],0
data/docs/encoder/cat_string_encoder/get_data.py:Data:__init__,Data:__init__,method,19,295,115,4525,15.34,0,21,"['self', 'name']","[None, None]","[None, None]",74,[],['create_folder'],1
data/docs/encoder/cat_string_encoder/get_data.py:Data:preprocess,Data:preprocess,method,33,120,76,1297,10.81,4,6,"['self', 'n_rows', 'str_preprocess', 'clf_type']","[None, None, None, None]","[None, '-1', 'True', 'None']",252,[],"['check_nan_percentage', 'preprocess_data', 'LabelEncoder', 'le.fit', 'le.transform', 'np.all']",6
data/docs/encoder/cat_string_encoder/get_data.py:Data:get_df,Data:get_df,method,97,492,280,5731,11.65,9,26,['self'],[None],[None],298,[],"['getattr', 'pd.read_csv', 'glob.glob', 'csv_files_.append', 'pd.DataFrame', 'df.drop_duplicates', 'pd.concat', 'print', 'tuple', 'set', 'df_dfd.drop_duplicates', 'df_dfd.to_csv', 'filename.split', 'df_mod.set_index', 'df_mod.dropna', 'df_veh.set_index', 'df_cas.set_index', 'df_cas.join', 'df.dropna', 'MSSubClass.items', 'np.log']",21
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH,LSH,class,98,329,211,3488,10.6,17,8,[],[],[],8,[],[],0
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:__init__,LSH:__init__,method,2,4,4,37,9.25,0,0,['self'],[None],[None],10,[],[],0
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:set_sparse_matrix,LSH:set_sparse_matrix,method,2,2,2,32,16.0,0,0,"['self', 'sparse_matrix']","[None, None]","[None, None]",14,"['        """"""Set the sparse matrix in which the lsh algorithm has run.""""""\n']",[],0
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:set_dimensionality_reduction,LSH:set_dimensionality_reduction,method,2,2,2,10,5.0,0,0,"['self', 'dr']","[None, None]","[None, None]",19,"['        """"""Set the reduced dimensionality of sparse matrix for operations.""""""\n']",[],0
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:_get_larger_prime_number,LSH:_get_larger_prime_number,method,9,23,19,218,9.48,1,1,['self'],[None],[None],24,[],"['len', 'self._crivo_algo']",2
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:_set_larger_prime_number,LSH:_set_larger_prime_number,method,2,2,2,49,24.5,0,0,['self'],[None],[None],39,[],['self._get_larger_prime_number'],1
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:_crivo_algo,LSH:_crivo_algo,method,10,24,19,149,6.21,2,1,"['self', 'limit']","[None, None]","[None, None]",42,[],['range'],1
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:_get_permutation_matrix,LSH:_get_permutation_matrix,method,19,40,30,436,10.9,3,0,['self'],[None],[None],55,[],"['len', 'self._set_larger_prime_number', 'range', 'shuffle', 'self._get_permuted_row', 'permutation_matrix.append']",6
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:_get_permuted_row,LSH:_get_permuted_row,method,7,15,14,167,11.13,1,0,"['self', 'n_cols', 'a_coeff', 'b_coeff']","[None, None, None, None]","[None, None, None, None]",77,[],"['range', 'self.minhash', 'permuted_row.append']",3
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:minhash,LSH:minhash,method,2,8,7,62,7.75,0,0,"['self', 'a_coeff', 'b_coeff', 'col_number', 'prime']","[None, None, None, None, None]","[None, None, None, None, None]",86,[],[],0
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:_get_signature_matrix,LSH:_get_signature_matrix,method,28,75,50,711,9.48,5,4,"['self', 'permutation_matrix']","[None, None]","[None, None]",90,[],"['len', 'range', 'enumerate', 'sorted', 'sum']",5
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:_get_similarity_matrix,LSH:_get_similarity_matrix,method,13,50,34,441,8.82,5,0,"['self', 'sig_matrix']","[None, None]","[None, None]",129,[],"['len', 'range', 'set', 'self._get_jaccard_similarity']",4
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:_get_jaccard_similarity,LSH:_get_jaccard_similarity,method,4,10,9,78,7.8,0,0,"['self', 'setA', 'setB', 'n_hashes']","[None, None, None, None]","[None, None, None, None]",152,[],"['len', 'round']",2
data/docs/encoder/LSH-semantic-similarity/LSH.py:LSH:get_similarity,LSH:get_similarity,method,17,31,29,550,17.74,0,2,"['self', 'matrix', 'language']","[None, None, None]","[None, 'None', 'None']",157,[],"['print', 'self.set_sparse_matrix', 'TFIDF', 'tfidf.get_sparse_matrix', 'int', 'self.set_dimensionality_reduction', 'self._get_permutation_matrix', 'self._get_signature_matrix', 'self._get_similarity_matrix']",9
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF,TFIDF,class,76,253,174,2772,10.96,6,2,[],[],[],15,[],[],0
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:__init__,TFIDF:__init__,method,10,17,17,197,11.59,0,0,"['self', 'texts', 'language']","[None, None, None]","[None, None, None]",17,[],"['pd.DataFrame', 'range', 'print']",3
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_to_lower_case,TFIDF:_to_lower_case,method,3,4,3,28,7.0,0,0,"['self', 'text']","[None, None]","[None, None]",30,[],['text.lower'],1
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_remove_stopwords,TFIDF:_remove_stopwords,method,9,19,18,239,12.58,0,0,"['self', 'text']","[None, None]","[None, None]",48,[],"['get_stop_words', 'RegexpTokenizer', 'list']",3
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_tf,TFIDF:_tf,method,2,3,3,44,14.67,0,0,"['self', 'word', 'blob']","[None, None, None]","[None, None, None]",59,[],['len'],1
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_n_containing,TFIDF:_n_containing,method,1,7,7,55,7.86,0,0,"['self', 'word', 'bloblist']","[None, None, None]","[None, None, None]",62,[],['len'],1
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_idf,TFIDF:_idf,method,3,7,7,85,12.14,0,0,"['self', 'word', 'bloblist']","[None, None, None]","[None, None, None]",65,[],"['len', 'math.log', 'self._n_containing']",3
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_tfidf,TFIDF:_tfidf,method,2,5,5,50,10.0,0,0,"['self', 'word', 'blob', 'bloblist']","[None, None, None, None]","[None, None, None, None]",69,[],"['self._tf', 'self._idf']",2
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_get_text_radicals,TFIDF:_get_text_radicals,method,7,15,14,138,9.2,0,0,"['self', 'text']","[None, None]","[None, None]",72,[],"['PorterStemmer', 'tb']",2
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_get_important_radicals,TFIDF:_get_important_radicals,method,10,22,21,248,11.27,1,0,"['self', 'text']","[None, None]","[None, None]",82,[],"['tb', 'self._tfidf', 'np.median', 'list', 'filter', 'scores.items']",6
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_extract_tokens,TFIDF:_extract_tokens,method,16,49,40,738,15.06,0,0,['self'],[None],[None],91,[],"['print', 'self._to_lower_case', 'self._remove_stopwords', 'self._get_text_radicals', 'list', 'self._get_important_radicals', 'important_radicals.apply', 'pd.DataFrame']",8
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:_conc_my_tokens,TFIDF:_conc_my_tokens,method,3,6,6,50,8.33,0,1,"['self', 'tokens', 'i']","[None, None, None]","[None, None, None]",119,[],['type'],1
data/docs/encoder/LSH-semantic-similarity/TFIDF.py:TFIDF:get_sparse_matrix,TFIDF:get_sparse_matrix,method,17,58,40,503,8.67,5,1,['self'],[None],[None],123,[],"['self._extract_tokens', 'list', 'print', 'range', 'self._conc_my_tokens', 'len', 'enumerate']",7
data/docs/encoder/MinHashComparison_Python/MinHash.py:MinHash,MinHash,class,49,178,123,1838,10.33,5,5,[],[],[],4,[],[],0
data/docs/encoder/MinHashComparison_Python/MinHash.py:MinHash:__init__,MinHash:__init__,method,15,53,40,576,10.87,1,2,"['self', 'tokens_in_word', 'num_hash_functions']","[None, ' int', ' int']","[None, None, None]",27,[],"['Exception', 'range', 'randint']",3
data/docs/encoder/MinHashComparison_Python/MinHash.py:MinHash:compute_sketch,MinHash:compute_sketch,method,22,49,39,605,12.35,3,2,"['self', 'tokens']","[None, ' list']","[None, None]",44,"['        """"""Compute the MinHash Sketch from an array of tokens.\n', '\n', '        Update the hash tables according to the min values of the sketch.\n', '        """"""\n']","['len', 'range', 'word_builder.append', 'hash', 'hash_function.calculate_hash', 'min']",6
data/docs/encoder/MinHashComparison_Python/MinHash.py:MinHash:compare_sketches,MinHash:compare_sketches,method,7,15,13,168,11.2,1,1,"['self', 'first_min_hash_sketch', 'second_min_hash_sketch']","[None, ' list', ' list']","[None, None, None]",77,"['        """"""Compare two MinHash sketches.""""""\n']",['range'],1
data/docs/encoder/MinHashComparison_Python/MinHashSimilarity.py:MinHashSimilarity,MinHashSimilarity,class,38,133,96,1510,11.35,4,6,[],[],[],4,[],[],0
data/docs/encoder/MinHashComparison_Python/MinHashSimilarity.py:MinHashSimilarity:__init__,MinHashSimilarity:__init__,method,15,36,32,417,11.58,0,2,"['self', 'threshold', 'tokens_in_word', 'num_hash_functions', 'bands', 'rows']","[None, ' float', ' int', ' int', ' int', ' int']","[None, None, '5', '400', '20', '20']",7,[],"['Exception', 'MinHash']",2
data/docs/encoder/MinHashComparison_Python/MinHashSimilarity.py:MinHashSimilarity:clear_documents,MinHashSimilarity:clear_documents,method,1,1,1,21,21.0,0,0,['self'],[None],[None],21,"['        """"""Clear all history of documents.""""""\n']",[],0
data/docs/encoder/MinHashComparison_Python/MinHashSimilarity.py:MinHashSimilarity:look_for_similar_documents,MinHashSimilarity:look_for_similar_documents,method,16,52,32,664,12.77,3,4,"['self', 'doc']","[None, ' str']","[None, None]",25,"['        """"""Given a string document, look whether a similar document was already seen.""""""\n']","['set', 'range', 'band_hashes.append', 'compared_sketches.add']",4
data/docs/encoder/MinHashComparison_Python/MinHashSimilarity.py:MinHashSimilarity:_compute_band_hash,MinHashSimilarity:_compute_band_hash,method,5,16,14,163,10.19,1,0,"['self', 'min_hashes', 'i']","[None, ' list', ' int']","[None, None, None]",52,"['        """"""Compute a hash for quick bucket match search.""""""\n']","['range', 'band_hash_list.append']",2
data/docs/features/data_exploration/explore.py:get_dtypes,get_dtypes,function,14,47,32,504,10.72,2,3,"['data', 'drop_col']","[None, None]","[None, '[]']",11,"['    """"""Return the dtypes for each column of a pandas Dataframe\n', '\n', '    Parameters\n', '    ----------\n', '    data : pandas Dataframe\n', '\n', '    drop_col : columns to omit in a list\n', '\n', '    Returns\n', '    -------\n', '    str_var_list, num_var_list, all_var_list\n', '    \n', '    """"""\n']","['list', 'name_of_col.copy', 'str_var_list.remove', 'num_var_list.append', 'num_var_list.remove', 'all_var_list.extend']",6
data/docs/features/data_exploration/explore.py:describe,describe,function,8,16,15,178,11.12,0,1,"['data', 'output_path']","[None, None]","[None, 'None']",50,"['    """"""output the general description of a  pandas Dataframe\n', '       into a csv file\n', '    \n', '    """"""\n']","['data.describe', 'result.to_csv', 'print', 'str']",4
data/docs/features/data_exploration/explore.py:discrete_var_barplot,discrete_var_barplot,function,7,15,15,205,13.67,0,1,"['x', 'y', 'data', 'output_path']","[None, None, None, None]","[None, None, None, 'None']",64,"['    """"""draw the barplot of a discrete variable x against y(target variable). \n', '    By default the bar shows the mean value of y.\n', '\n', '    Parameters\n', '    ----------\n', '\n', '\n', '    Returns\n', '    -------\n', '    figure save as PNG\n', '    """"""\n']","['plt.figure', 'sns.barplot', 'plt.savefig', 'print', 'str']",5
data/docs/features/data_exploration/explore.py:discrete_var_countplot,discrete_var_countplot,function,7,14,14,194,13.86,0,1,"['x', 'data', 'output_path']","[None, None, None]","[None, None, 'None']",85,"['    """"""draw the countplot of a discrete variable x.\n', '\n', '    Parameters\n', '    ----------\n', '\n', '\n', '    Returns\n', '    -------\n', '    figure save as PNG\n', '    """"""    \n']","['plt.figure', 'sns.countplot', 'plt.savefig', 'print']",4
data/docs/features/data_exploration/explore.py:discrete_var_boxplot,discrete_var_boxplot,function,7,14,14,205,14.64,0,1,"['x', 'y', 'data', 'output_path']","[None, None, None, None]","[None, None, None, 'None']",105,"['    """"""draw the boxplot of a discrete variable x against y.\n', '\n', '    Parameters\n', '    ----------\n', '\n', '\n', '    Returns\n', '    -------\n', '    figure save as PNG\n', '    """"""    \n']","['plt.figure', 'sns.boxplot', 'plt.savefig', 'print']",4
data/docs/features/data_exploration/explore.py:continuous_var_distplot,continuous_var_distplot,function,6,12,12,207,17.25,0,1,"['x', 'output_path', 'bins']","[None, None, None]","[None, 'None', 'None']",125,"['    """"""draw the distplot of a continuous variable x.\n', '\n', '    Parameters\n', '    ----------\n', '\n', '\n', '    Returns\n', '    -------\n', '    figure save as PNG\n', '    """"""    \n']","['plt.figure', 'sns.distplot', 'plt.savefig', 'print']",4
data/docs/features/data_exploration/explore.py:scatter_plot,scatter_plot,function,7,13,13,224,17.23,0,1,"['x', 'y', 'data', 'output_path']","[None, None, None, None]","[None, None, None, 'None']",147,"['    """"""draw the scatter-plot of two variables.\n', '\n', '    Parameters\n', '    ----------\n', '\n', '\n', '    Returns\n', '    -------\n', '    figure save as PNG\n', '    """"""    \n']","['plt.figure', 'sns.scatterplot', 'plt.savefig', 'print']",4
data/docs/features/data_exploration/explore.py:correlation_plot,correlation_plot,function,12,18,18,258,14.33,0,1,"['data', 'output_path']","[None, None]","[None, 'None']",167,"['    """"""draw the correlation plot between variables.\n', '\n', '    Parameters\n', '    ----------\n', '\n', '\n', '    Returns\n', '    -------\n', '    figure save as PNG\n', '    """"""    \n']","['data.corr', 'plt.subplots', 'fig.set_size_inches', 'sns.heatmap', 'plt.savefig', 'print']",6
data/docs/features/data_exploration/explore.py:heatmap,heatmap,function,10,16,16,241,15.06,0,1,"['data', 'output_path', 'fmt']","[None, None, None]","[None, 'None', ""'d'""]",189,"['    """"""draw the heatmap between 2 variables.\n', '\n', '    Parameters\n', '    ----------\n', '\n', '\n', '    Returns\n', '    -------\n', '    figure save as PNG\n', '    """"""    \n']","['plt.subplots', 'fig.set_size_inches', 'sns.heatmap', 'plt.savefig', 'print']",5
data/docs/features/feature_cleaning/missing_data.py:check_missing,check_missing,function,7,18,16,268,14.89,0,1,"['data', 'output_path']","[None, None]","[None, 'None']",8,"['    """"""\n', '    check the total number & percentage of missing values\n', '    per variable of a pandas Dataframe\n', '    """"""\n']","['pd.concat', 'result.rename', 'result.to_csv', 'print']",4
data/docs/features/feature_cleaning/missing_data.py:drop_missing,drop_missing,function,4,6,4,98,16.33,0,0,"['data', 'axis']","[None, None]","[None, '0']",22,"['    """"""\n', '    Listwise deletion:\n', '    excluding all cases (listwise) that have missing values\n', '\n', '    Parameters\n', '    ----------\n', '    axis: drop cases(0)/columns(1),default 0\n', '\n', '    Returns\n', '    -------\n', '    Pandas dataframe with missing cases/columns dropped\n', '    """"""    \n']","['data.copy', 'data_copy.dropna']",2
data/docs/features/feature_cleaning/missing_data.py:add_var_denote_NA,add_var_denote_NA,function,9,21,20,193,9.19,1,1,"['data', 'NA_col']","[None, None]","[None, '[]']",41,"['    """"""\n', '    creating an additional variable indicating whether the data \n', '    was missing for that observation (1) or not (0).\n', '    """"""\n']","['data.copy', 'np.where', 'warn']",3
data/docs/features/feature_cleaning/missing_data.py:impute_NA_with_arbitrary,impute_NA_with_arbitrary,function,8,21,20,204,9.71,1,1,"['data', 'impute_value', 'NA_col']","[None, None, None]","[None, None, '[]']",57,"['    """"""\n', '    replacing NA with arbitrary values. \n', '    """"""\n']","['data.copy', 'warn']",2
data/docs/features/feature_cleaning/missing_data.py:impute_NA_with_avg,impute_NA_with_avg,function,12,30,27,395,13.17,1,2,"['data', 'strategy', 'NA_col']","[None, None, None]","[None, ""'mean'"", '[]']",71,"['    """"""\n', '    replacing the NA with mean/median/most frequent values of that variable. \n', '    Note it should only be performed over training set and then propagated to test set.\n', '    """"""\n']","['data.copy', 'warn']",2
data/docs/features/feature_cleaning/missing_data.py:impute_NA_with_end_of_distribution,impute_NA_with_end_of_distribution,function,9,20,19,219,10.95,1,1,"['data', 'NA_col']","[None, None]","[None, '[]']",91,"['    """"""\n', '    replacing the NA by values that are at the far end of the distribution of that variable\n', '    calculated by mean + 3*std\n', '    """"""\n']","['data.copy', 'warn']",2
data/docs/features/feature_cleaning/missing_data.py:impute_NA_with_random,impute_NA_with_random,function,13,28,27,391,13.96,1,1,"['data', 'NA_col', 'random_state']","[None, None, None]","[None, '[]', '0']",106,"['    """"""\n', '    replacing the NA with random sampling from the pool of available observations of the variable\n', '    """"""\n']","['data.copy', 'str', 'warn']",3
data/docs/features/feature_cleaning/outlier.py:outlier_detect_arbitrary,outlier_detect_arbitrary,function,8,19,16,304,16.0,0,0,"['data', 'col', 'upper_fence', 'lower_fence']","[None, None, None, None]","[None, None, None, None]",7,"[""    '''\n"", '    identify outliers based on arbitrary boundaries passed to the function.\n', ""    '''\n""]","['pd.concat', 'tmp.any', 'print']",3
data/docs/features/feature_cleaning/outlier.py:outlier_detect_IQR,outlier_detect_IQR,function,12,29,23,464,16.0,0,0,"['data', 'col', 'threshold']","[None, None, None]","[None, None, '3']",21,"[""    '''\n"", ""    outlier detection by Interquartile Ranges Rule, also known as Tukey's test. \n"", '    calculate the IQR ( 75th quantile - 25th quantile) \n', '    and the 25th 75th quantile. \n', '    Any value beyond:\n', '        upper bound = 75th quantile + IQR * threshold\n', '        lower bound = 25th quantile - IQR * threshold   \n', '    are regarded as outliers. Default threshold is 3.\n', ""    '''\n""]","['pd.concat', 'tmp.any', 'print']",3
data/docs/features/feature_cleaning/outlier.py:outlier_detect_mean_std,outlier_detect_mean_std,function,11,28,22,414,14.79,0,0,"['data', 'col', 'threshold']","[None, None, None]","[None, None, '3']",43,"[""    '''\n"", '    outlier detection by Mean and Standard Deviation Method.\n', '    If a value is a certain number(called threshold) of standard deviations away \n', '    from the mean, that data point is identified as an outlier. \n', '    Default threshold is 3.\n', '\n', '    This method can fail to detect outliers because the outliers increase the standard deviation. \n', '    The more extreme the outlier, the more the standard deviation is affected.\n', ""    '''\n""]","['pd.concat', 'tmp.any', 'print']",3
data/docs/features/feature_cleaning/outlier.py:outlier_detect_MAD,outlier_detect_MAD,function,10,32,23,407,12.72,0,0,"['data', 'col', 'threshold']","[None, None, None]","[None, None, '3.5']",64,"['    """"""\n', '    outlier detection by Median and Median Absolute Deviation Method (MAD)\n', '    The median of the residuals is calculated. Then, the difference is calculated between each historical value and this median. \n', '    These differences are expressed as their absolute values, and a new median is calculated and multiplied by \n', '    an empirically derived constant to yield the median absolute deviation (MAD). \n', '    If a value is a certain number of MAD away from the median of the residuals, \n', '    that value is classified as an outlier. The default threshold is 3 MAD.\n', '    \n', '    This method is generally more effective than the mean and standard deviation method for detecting outliers, \n', '    but it can be too aggressive in classifying values that are not really extremely different. \n', '    Also, if more than 50% of the data points have the same value, MAD is computed to be 0, \n', '    so any value different from the residual median is classified as an outlier.\n', '    """"""\n']","['np.median', 'pd.Series', 'np.abs', 'print']",4
data/docs/features/feature_cleaning/outlier.py:impute_outlier_with_arbitrary,impute_outlier_with_arbitrary,function,7,10,9,94,9.4,1,0,"['data', 'outlier_index', 'value', 'col']","[None, None, None, None]","[None, None, None, '[]']",89,"['    """"""\n', '    impute outliers with arbitrary value\n', '    """"""\n']",['data.copy'],1
data/docs/features/feature_cleaning/outlier.py:windsorization,windsorization,function,8,23,15,311,13.52,0,1,"['data', 'col', 'para', 'strategy']","[None, None, None, None]","[None, None, None, ""'both'""]",100,"['    """"""\n', '    top-coding & bottom coding (capping the maximum of a distribution at an arbitrarily set value,vice versa)\n', '    """"""\n']",['data.copy'],1
data/docs/features/feature_cleaning/outlier.py:drop_outlier,drop_outlier,function,3,4,3,46,11.5,0,0,"['data', 'outlier_index']","[None, None]","[None, None]",116,"['    """"""\n', '    drop the cases that are outliers\n', '    """"""\n']",[],0
data/docs/features/feature_cleaning/outlier.py:impute_outlier_with_avg,impute_outlier_with_avg,function,8,17,13,282,16.59,0,1,"['data', 'col', 'outlier_index', 'strategy']","[None, None, None, None]","[None, None, None, ""'mean'""]",125,"['    """"""\n', '    impute outlier with mean/median/most frequent values of that variable.\n', '    """"""\n']",['data.copy'],1
data/docs/features/feature_cleaning/rare_values.py:GroupingRareValues,GroupingRareValues,class,33,144,105,1144,7.94,3,3,[],[],[],8,[],[],0
data/docs/features/feature_cleaning/rare_values.py:ModeImputation,ModeImputation,class,35,146,107,1192,8.16,3,3,[],[],[],140,[],[],0
data/docs/features/feature_cleaning/rare_values.py:GroupingRareValues:__init__,GroupingRareValues:__init__,method,7,8,8,75,9.38,0,0,"['self', 'mapping', 'cols', 'threshold']","[None, None, None, None]","[None, 'None', 'None', '0.01']",17,[],[],0
data/docs/features/feature_cleaning/rare_values.py:GroupingRareValues:fit,GroupingRareValues:fit,method,7,14,14,151,10.79,0,0,"['self', 'X', 'y', '**kwargs']","[None, None, None, None]","[None, None, 'None', None]",24,"['        """"""Fit encoder according to X and y.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape = [n_samples, n_features]\n', '            Training vectors, where n_samples is the number of samples\n', '            and n_features is the number of features.\n', '        y : array-like, shape = [n_samples]\n', '            Target values.\n', '        Returns\n', '        -------\n', '        self : encoder\n', '            Returns self.\n', '        """"""\n']",['self.grouping'],1
data/docs/features/feature_cleaning/rare_values.py:GroupingRareValues:transform,GroupingRareValues:transform,method,8,39,37,288,7.38,0,2,"['self', 'X']","[None, None]","[None, None]",51,"['        """"""Perform the transformation to new categorical data.\n', '        Will use the mapping (if available) and the column list to encode the\n', '        data.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape = [n_samples, n_features]\n', '        Returns\n', '        -------\n', '        X : Transformed values with encoding applied.\n', '        """"""\n']","['ValueError', 'self.grouping']",2
data/docs/features/feature_cleaning/rare_values.py:GroupingRareValues:grouping,GroupingRareValues:grouping,method,20,63,48,459,7.29,3,1,"['self', 'X_in', 'threshold', 'mapping', 'cols']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",80,"['        """"""\n', ""        Grouping the observations that show rare labels into a unique category ('rare')\n"", '\n', '        """"""\n']","['X_in.copy', 'i.get', 'pd.Series', 'mapping_out.append']",4
data/docs/features/feature_cleaning/rare_values.py:ModeImputation:__init__,ModeImputation:__init__,method,7,8,8,75,9.38,0,0,"['self', 'mapping', 'cols', 'threshold']","[None, None, None, None]","[None, 'None', 'None', '0.01']",17,[],[],0
data/docs/features/feature_cleaning/rare_values.py:ModeImputation:fit,ModeImputation:fit,method,7,14,14,159,11.36,0,0,"['self', 'X', 'y', '**kwargs']","[None, None, None, None]","[None, None, 'None', None]",24,"['        """"""Fit encoder according to X and y.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape = [n_samples, n_features]\n', '            Training vectors, where n_samples is the number of samples\n', '            and n_features is the number of features.\n', '        y : array-like, shape = [n_samples]\n', '            Target values.\n', '        Returns\n', '        -------\n', '        self : encoder\n', '            Returns self.\n', '        """"""\n']",['self.impute_with_mode'],1
data/docs/features/feature_cleaning/rare_values.py:ModeImputation:transform,ModeImputation:transform,method,8,39,37,296,7.59,0,2,"['self', 'X']","[None, None]","[None, None]",51,"['        """"""Perform the transformation to new categorical data.\n', '        Will use the mapping (if available) and the column list to encode the\n', '        data.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape = [n_samples, n_features]\n', '        Returns\n', '        -------\n', '        X : Transformed values with encoding applied.\n', '        """"""\n']","['ValueError', 'self.impute_with_mode']",2
data/docs/features/feature_cleaning/rare_values.py:ModeImputation:impute_with_mode,ModeImputation:impute_with_mode,method,22,65,50,483,7.43,3,1,"['self', 'X_in', 'threshold', 'mapping', 'cols']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",212,"['        """"""\n', ""        Grouping the observations that show rare labels into a unique category ('rare')\n"", '\n', '        """"""\n']","['X_in.copy', 'i.get', 'pd.Series', 'mapping_out.append']",4
data/docs/features/feature_engineering/discretization.py:ChiMerge,ChiMerge,class,62,512,208,4768,9.31,4,7,[],[],[],11,[],[],0
data/docs/features/feature_engineering/discretization.py:DiscretizeByDecisionTree,DiscretizeByDecisionTree,class,51,180,135,1801,10.01,1,4,[],[],[],198,[],[],0
data/docs/features/feature_engineering/discretization.py:ChiMerge:__init__,ChiMerge:__init__,method,9,10,10,104,10.4,0,0,"['self', 'col', 'bins', 'confidenceVal', 'num_of_bins']","[None, None, None, None, None]","[None, 'None', 'None', '3.841', '10']",27,[],[],0
data/docs/features/feature_engineering/discretization.py:ChiMerge:fit,ChiMerge:fit,method,7,15,15,160,10.67,0,0,"['self', 'X', 'y', '**kwargs']","[None, None, None, None]","[None, None, None, None]",35,"['        """"""Fit encoder according to X and y.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape = [n_samples, n_features]\n', '            Training vectors, where n_samples is the number of samples\n', '            and n_features is the number of features.\n', '        y : array-like, shape = [n_samples]\n', '            Target values.\n', '        Returns\n', '        -------\n', '        self : encoder\n', '            Returns self.\n', '        """"""\n']",['self.chimerge'],1
data/docs/features/feature_engineering/discretization.py:ChiMerge:transform,ChiMerge:transform,method,8,38,36,259,6.82,0,2,"['self', 'X']","[None, None]","[None, None]",63,"['            """"""Perform the transformation to new data.\n', '            Will use the tree model and the column list to discretize the\n', '            column.\n', '            Parameters\n', '            ----------\n', '            X : array-like, shape = [n_samples, n_features]\n', '            Returns\n', '            -------\n', '            X : new dataframe with discretized new column.\n', '            """"""\n']","['ValueError', 'self.chimerge']",2
data/docs/features/feature_engineering/discretization.py:ChiMerge:chimerge,ChiMerge:chimerge,method,47,426,146,4034,9.47,4,5,"['self', 'X_in', 'y', 'confidenceVal', 'num_of_bins', 'col', 'bins']","[None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None', 'None']",90,"['        """"""\n', '        discretize a variable using ChiMerge\n', '\n', '        """"""\n']","['X_in.copy', 'pd.cut', 'print', 'X.groupby', 'pd.DataFrame', 'pd.merge', 'regroup.reset_index', 'regroup.drop', 'np.array', 'np.delete', 'np.arange', 'np.append', 'min', 'np.argwhere', 'str', 'bins.append', 'tmp.append', 'bins.sort']",18
data/docs/features/feature_engineering/discretization.py:DiscretizeByDecisionTree:__init__,DiscretizeByDecisionTree:__init__,method,7,8,8,79,9.88,0,0,"['self', 'col', 'max_depth', 'tree_model']","[None, None, None, None]","[None, 'None', 'None', 'None']",219,[],[],0
data/docs/features/feature_engineering/discretization.py:DiscretizeByDecisionTree:fit,DiscretizeByDecisionTree:fit,method,7,15,15,158,10.53,0,0,"['self', 'X', 'y', '**kwargs']","[None, None, None, None]","[None, None, None, None]",35,"['        """"""Fit encoder according to X and y.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape = [n_samples, n_features]\n', '            Training vectors, where n_samples is the number of samples\n', '            and n_features is the number of features.\n', '        y : array-like, shape = [n_samples]\n', '            Target values.\n', '        Returns\n', '        -------\n', '        self : encoder\n', '            Returns self.\n', '        """"""\n']",['self.discretize'],1
data/docs/features/feature_engineering/discretization.py:DiscretizeByDecisionTree:transform,DiscretizeByDecisionTree:transform,method,8,38,36,273,7.18,0,2,"['self', 'X']","[None, None]","[None, None]",63,"['            """"""Perform the transformation to new data.\n', '            Will use the tree model and the column list to discretize the\n', '            column.\n', '            Parameters\n', '            ----------\n', '            X : array-like, shape = [n_samples, n_features]\n', '            Returns\n', '            -------\n', '            X : new dataframe with discretized new column.\n', '            """"""\n']","['ValueError', 'self.discretize']",2
data/docs/features/feature_engineering/discretization.py:DiscretizeByDecisionTree:discretize,DiscretizeByDecisionTree:discretize,method,39,98,78,1107,11.3,1,2,"['self', 'X_in', 'y', 'max_depth', 'tree_model', 'col']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",281,"['        """"""\n', '        discretize a variable using DecisionTreeClassifier\n', '\n', '        """"""\n']","['X_in.copy', 'tree_model.predict_proba', 'isinstance', 'DecisionTreeClassifier', 'tree_model.fit', 'len', 'cross_val_score', 'score_ls.append', 'score_std_ls.append', 'pd.concat', 'pd.Series', 'print', 'ValueError']",13
data/docs/features/feature_engineering/encoding.py:MeanEncoding,MeanEncoding,class,29,124,90,983,7.93,2,3,[],[],[],5,[],[],0
data/docs/features/feature_engineering/encoding.py:MeanEncoding:__init__,MeanEncoding:__init__,method,5,6,6,50,8.33,0,0,"['self', 'mapping', 'cols']","[None, None, None]","[None, 'None', 'None']",14,[],[],0
data/docs/features/feature_engineering/encoding.py:MeanEncoding:fit,MeanEncoding:fit,method,7,14,14,133,9.5,0,0,"['self', 'X', 'y', '**kwargs']","[None, None, None, None]","[None, None, 'None', None]",21,"['        """"""Fit encoder according to X and y.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape = [n_samples, n_features]\n', '            Training vectors, where n_samples is the number of samples\n', '            and n_features is the number of features.\n', '        y : array-like, shape = [n_samples]\n', '            Target values.\n', '        Returns\n', '        -------\n', '        self : encoder\n', '            Returns self.\n', '        """"""\n']",['self.mean_encoding'],1
data/docs/features/feature_engineering/encoding.py:MeanEncoding:transform,MeanEncoding:transform,method,8,38,36,267,7.03,0,2,"['self', 'X']","[None, None]","[None, None]",49,"['        """"""Perform the transformation to new categorical data.\n', '        Will use the mapping (if available) and the column list to encode the\n', '        data.\n', '        Parameters\n', '        ----------\n', '        X : array-like, shape = [n_samples, n_features]\n', '        Returns\n', '        -------\n', '        X : Transformed values with encoding applied.\n', '        """"""\n']","['ValueError', 'self.mean_encoding']",2
data/docs/features/feature_engineering/encoding.py:MeanEncoding:mean_encoding,MeanEncoding:mean_encoding,method,18,47,38,375,7.98,2,1,"['self', 'X_in', 'y', 'mapping', 'cols']","[None, None, None, None, None]","[None, None, 'None', 'None', 'None']",78,"['        """"""\n', ""        Grouping the observations that show rare labels into a unique category ('rare')\n"", '\n', '        """"""\n']","['X_in.copy', 'i.get', 'pd.Series', 'mapping_out.append']",4
data/docs/features/feature_engineering/transformation.py:diagnostic_plots,diagnostic_plots,function,5,12,10,147,12.25,0,0,"['df', 'variable']","[None, None]","[None, None]",9,[],"['plt.figure', 'plt.subplot', 'stats.probplot', 'plt.show']",4
data/docs/features/feature_engineering/transformation.py:log_transform,log_transform,function,9,18,16,173,9.61,1,0,"['data', 'cols']","[None, None]","[None, '[]']",23,"['    """"""\n', '    Logarithmic transformation\n', '    """"""\n']","['data.copy', 'np.log', 'print', 'diagnostic_plots']",4
data/docs/features/feature_engineering/transformation.py:reciprocal_transform,reciprocal_transform,function,8,18,16,181,10.06,1,0,"['data', 'cols']","[None, None]","[None, '[]']",36,"['    """"""\n', '    Reciprocal transformation\n', '    """"""\n']","['data.copy', 'print', 'diagnostic_plots']",3
data/docs/features/feature_engineering/transformation.py:square_root_transform,square_root_transform,function,8,18,16,188,10.44,1,0,"['data', 'cols']","[None, None]","[None, '[]']",49,"['    """"""\n', '    square root transformation\n', '    """"""\n']","['data.copy', 'print', 'diagnostic_plots']",3
data/docs/features/feature_engineering/transformation.py:exp_transform,exp_transform,function,8,18,16,171,9.5,1,0,"['data', 'coef', 'cols']","[None, None, None]","[None, None, '[]']",62,"['    """"""\n', '    exp transformation\n', '    """"""\n']","['data.copy', 'print', 'diagnostic_plots']",3
data/docs/features/feature_selection/embedded_method.py:rf_importance,rf_importance,function,23,63,54,778,12.35,1,0,"['X_train', 'y_train', 'max_depth', 'class_weight', 'top_n', 'n_estimators', 'random_state']","[None, None, None, None, None, None, None]","[None, None, '10', 'None', '15', '50', '0']",13,[],"['RandomForestClassifier', 'model.fit', 'np.argsort', 'np.std', 'print', 'range', 'plt.figure', 'plt.title', 'plt.bar', 'plt.xticks', 'plt.xlim', 'plt.show']",12
data/docs/features/feature_selection/embedded_method.py:gbt_importance,gbt_importance,function,23,62,53,748,12.06,1,0,"['X_train', 'y_train', 'max_depth', 'top_n', 'n_estimators', 'random_state']","[None, None, None, None, None, None]","[None, None, '10', '15', '50', '0']",47,[],"['GradientBoostingClassifier', 'model.fit', 'np.argsort', 'np.std', 'print', 'range', 'plt.figure', 'plt.title', 'plt.bar', 'plt.xticks', 'plt.xlim', 'plt.show']",12
data/docs/features/feature_selection/feature_shuffle.py:feature_shuffle_rf,feature_shuffle_rf,function,25,43,41,843,19.6,1,0,"['X_train', 'y_train', 'max_depth', 'class_weight', 'top_n', 'n_estimators', 'random_state']","[None, None, None, None, None, None, None]","[None, None, 'None', 'None', '15', '50', '0']",11,[],"['RandomForestClassifier', 'model.fit', 'roc_auc_score', 'X_train.copy', 'y_train.copy', 'pd.Series', 'auc_drop.sort_values']",7
data/docs/features/feature_selection/filter_method.py:constant_feature_detect,constant_feature_detect,function,10,27,25,367,13.59,1,1,"['data', 'threshold']","[None, None]","[None, '0.98']",11,"['    """""" detect features that show the same value for the \n', '    majority/all of the observations (constant/quasi-constant features)\n', '    \n', '    Parameters\n', '    ----------\n', '    data : pd.Dataframe\n', '    threshold : threshold to identify the variable as constant\n', '        \n', '    Returns\n', '    -------\n', '    list of variables names\n', '    """"""\n']","['data.copy', 'np.float', 'len', 'quasi_constant_feature.append', 'print']",5
data/docs/features/feature_selection/filter_method.py:corr_feature_detect,corr_feature_detect,function,20,53,40,616,11.62,1,1,"['data', 'threshold']","[None, None]","[None, '0.8']",36,"['    """""" detect highly-correlated features of a Dataframe\n', '    Parameters\n', '    ----------\n', '    data : pd.Dataframe\n', '    threshold : threshold to identify the variable correlated\n', '        \n', '    Returns\n', '    -------\n', '    pairs of correlated variables\n', '    """"""\n']","['data.corr', 'corrmat.abs', 'corrmat.sort_values', 'pd.DataFrame', 'list', 'correlated_groups.append']",6
data/docs/features/feature_selection/filter_method.py:mutual_info,mutual_info,function,8,30,24,299,9.97,0,1,"['X', 'y', 'select_k']","[None, None, None]","[None, None, '10']",72,[],"['SelectKBest', 'SelectPercentile', 'ValueError']",3
data/docs/features/feature_selection/filter_method.py:chi_square_test,chi_square_test,function,8,31,24,269,8.68,0,1,"['X', 'y', 'select_k']","[None, None, None]","[None, None, '10']",94,"['    """"""\n', '    Compute chi-squared stats between each non-negative feature and class.\n', '    This score should be used to evaluate categorical variables in a classification task\n', '    """"""\n']","['SelectKBest', 'SelectPercentile', 'ValueError']",3
data/docs/features/feature_selection/filter_method.py:univariate_roc_auc,univariate_roc_auc,function,16,36,33,489,13.58,1,0,"['X_train', 'y_train', 'X_test', 'y_test', 'threshold']","[None, None, None, None, None]","[None, None, None, None, None]",112,"['    """"""\n', '    First, it builds one decision tree per feature, to predict the target\n', '    Second, it makes predictions using the decision tree and the mentioned feature\n', '    Third, it ranks the features according to the machine learning metric (roc-auc or mse)\n', '    It selects the highest ranked features\n', '\n', '    """"""\n']","['DecisionTreeClassifier', 'clf.fit', 'clf.predict_proba', 'roc_values.append', 'pd.Series', 'print', 'len']",7
data/docs/features/feature_selection/filter_method.py:univariate_mse,univariate_mse,function,16,35,32,482,13.77,1,0,"['X_train', 'y_train', 'X_test', 'y_test', 'threshold']","[None, None, None, None, None]","[None, None, None, None, None]",135,"['    """"""\n', '    First, it builds one decision tree per feature, to predict the target\n', '    Second, it makes predictions using the decision tree and the mentioned feature\n', '    Third, it ranks the features according to the machine learning metric (roc-auc or mse)\n', '    It selects the highest ranked features\n', '\n', '    """"""\n']","['DecisionTreeRegressor', 'clf.fit', 'clf.predict', 'mse_values.append', 'pd.Series', 'print', 'len']",7
data/docs/features/feature_selection/hybrid.py:recursive_feature_elimination_rf,recursive_feature_elimination_rf,function,23,117,75,1383,11.82,2,2,"['X_train', 'y_train', 'X_test', 'y_test', 'tol', 'max_depth', 'class_weight', 'top_n', 'n_estimators', 'random_state']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, '0.001', 'None', 'None', '15', '50', '0']",10,[],"['RandomForestClassifier', 'model_all_features.fit', 'model_all_features.predict_proba', 'roc_auc_score', 'print', 'len', 'model.fit', 'model.predict_proba', 'X_test.drop', 'features_to_remove.append']",10
data/docs/features/feature_selection/hybrid.py:recursive_feature_addition_rf,recursive_feature_addition_rf,function,20,100,67,1298,12.98,1,1,"['X_train', 'y_train', 'X_test', 'y_test', 'tol', 'max_depth', 'class_weight', 'top_n', 'n_estimators', 'random_state']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, '0.001', 'None', 'None', '15', '50', '0']",73,[],"['RandomForestClassifier', 'model_one_feature.fit', 'model_one_feature.predict_proba', 'roc_auc_score', 'print', 'len', 'model.fit', 'model.predict_proba', 'features_to_keep.append']",9
data/docs/timeseries/rnn_timeseries/cocob.py:COCOB,COCOB,class,39,147,103,2114,14.38,1,0,[],[],[],32,[],[],0
data/docs/timeseries/rnn_timeseries/cocob.py:COCOB:__init__,COCOB:__init__,method,3,5,5,62,12.4,0,0,"['self', 'alpha', 'use_locking', 'name']","[None, None, None, None]","[None, '100', 'False', ""'COCOB'""]",33,"[""        '''\n"", '        constructs a new COCOB optimizer\n', ""        '''\n""]",['super'],1
data/docs/timeseries/rnn_timeseries/cocob.py:COCOB:_create_slots,COCOB:_create_slots,method,12,46,30,727,15.8,1,0,"['self', 'var_list']","[None, None]","[None, None]",40,[],"['ops.colocate_with', 'constant_op.constant', 'self._get_or_make_slot']",3
data/docs/timeseries/rnn_timeseries/cocob.py:COCOB:_apply_dense,COCOB:_apply_dense,method,26,70,60,1050,15.0,0,0,"['self', 'grad', 'var']","[None, None, None]","[None, None, None]",59,[],"['self.get_slot', 'tf.maximum', 'tf.abs', 'state_ops.assign', 'control_flow_ops.group']",5
data/docs/timeseries/rnn_timeseries/cocob.py:COCOB:_apply_sparse,COCOB:_apply_sparse,method,2,3,3,33,11.0,0,0,"['self', 'grad', 'var']","[None, None, None]","[None, None, None]",89,[],['self._apply_dense'],1
data/docs/timeseries/rnn_timeseries/cocob.py:COCOB:_resource_apply_dense,COCOB:_resource_apply_dense,method,2,3,3,36,12.0,0,0,"['self', 'grad', 'handle']","[None, None, None]","[None, None, None]",92,[],['self._apply_dense'],1
data/docs/timeseries/rnn_timeseries/extractor.py:extract,extract,function,31,77,64,727,9.44,1,4,['source'],[None],[None],14,"['    """"""\n', '    Extracts features from url. Features: agent, site, country, term, marker\n', '    :param source: urls\n', '    :return: DataFrame, one column per feature\n', '    """"""\n']","['isinstance', 'np.full_like', 'range', 'pat.fullmatch', 'match.group', 'term_pat.match', 'term_match.group', 'pd.DataFrame']",8
data/docs/timeseries/rnn_timeseries/feeder.py:_meta_file,_meta_file,function,2,3,3,42,14.0,0,0,['path'],[None],[None],11,[],[],0
data/docs/timeseries/rnn_timeseries/feeder.py:VarFeeder,VarFeeder,class,64,203,142,2049,10.09,4,5,[],[],[],15,[],[],0
data/docs/timeseries/rnn_timeseries/feeder.py:FeederVars,FeederVars,class,16,37,32,400,10.81,1,2,[],[],[],104,[],[],0
data/docs/timeseries/rnn_timeseries/feeder.py:VarFeeder:__init__,VarFeeder:__init__,method,43,111,85,1141,10.28,3,4,"['self', 'path', 'tensor_vars', 'Union[pd.DataFrame', 'pd.Series', 'np.ndarray]] ', 'plain_vars', 'Any] ']","[None, ' str', ' Dict[str', None, None, None, ' Dict[str', None]","[None, None, None, None, None, ' None', None, ' None']",21,"['        """"""\n', '        :param path: dir to store data\n', '        :param tensor_vars: Variables to save as Tensors (pandas DataFrames/Series or numpy arrays)\n', '        :param plain_vars: Variables to save as Python objects\n', '        """"""\n']","['dict', 'get_values', 'hasattr', 'isinstance', 'np.array', 'v.astype', 'tensor_vars.values', 'list', 'os.mkdir', 'open', 'pickle.dump', 'tf.Graph', 'self._build_vars', 'zip', 'tf.global_variables_initializer', 'tf.Session', 'sess.run', 'saver.save']",18
data/docs/timeseries/rnn_timeseries/feeder.py:VarFeeder:_var_dict,VarFeeder:_var_dict,method,4,9,8,54,6.0,1,0,"['self', 'variables']","[None, None]","[None, None]",69,[],['zip'],1
data/docs/timeseries/rnn_timeseries/feeder.py:VarFeeder:_build_vars,VarFeeder:_build_vars,method,13,43,34,417,9.7,0,1,['self'],[None],[None],72,[],"['make_tensor', 'tf.as_dtype', 'tf.constant', 'tf.get_local_variable', 'tf.device', 'tf.name_scope', 'zip']",7
data/docs/timeseries/rnn_timeseries/feeder.py:VarFeeder:create_vars,VarFeeder:create_vars,method,2,4,4,78,19.5,0,0,['self'],[None],[None],89,"['        """"""\n', '        Builds variable list to use in current graph. Should be called during graph building stage\n', '        :return: variable list with additional restore and create_saver methods\n', '        """"""\n']",['FeederVars'],1
data/docs/timeseries/rnn_timeseries/feeder.py:VarFeeder:read_vars,VarFeeder:read_vars,method,8,12,12,119,9.92,0,0,['path'],[None],[None],97,[],"['open', 'pickle.load', 'feeder.create_vars']",3
data/docs/timeseries/rnn_timeseries/feeder.py:FeederVars:__init__,FeederVars:__init__,method,12,22,19,245,11.14,1,2,"['self', 'tensors', 'plain_vars', 'path']","[None, ' dict', ' dict', None]","[None, None, None, None]",105,[],"['dict', 'variables.update', 'super']",3
data/docs/timeseries/rnn_timeseries/feeder.py:FeederVars:restore,FeederVars:restore,method,2,5,5,75,15.0,0,0,"['self', 'session']","[None, None]","[None, None]",116,"['        """"""\n', '        Restores variable content\n', '        :param session: current session\n', '        :return: variable list\n', '        """"""\n']",[],0
data/docs/timeseries/rnn_timeseries/hparams.py:build_hparams,build_hparams,function,2,2,2,32,16.0,0,0,['params'],[None],['def_params'],191,[],['training.HParams'],1
data/docs/timeseries/rnn_timeseries/hparams.py:build_from_set,build_from_set,function,2,2,2,35,17.5,0,0,['set_name'],[None],[None],195,[],['build_hparams'],1
data/docs/timeseries/rnn_timeseries/input_pipe.py:page_features,page_features,function,1,9,9,128,14.22,0,0,['inp'],[' VarFeeder'],[None],307,[],[],0
data/docs/timeseries/rnn_timeseries/input_pipe.py:ModelMode,ModelMode,class,3,6,6,25,4.17,0,0,[],[],[],10,[],[],0
data/docs/timeseries/rnn_timeseries/input_pipe.py:Split,Split,class,9,18,17,198,11.0,0,0,[],[],[],16,[],[],0
data/docs/timeseries/rnn_timeseries/input_pipe.py:Splitter,Splitter,class,47,138,113,1614,11.7,0,0,[],[],[],24,[],[],0
data/docs/timeseries/rnn_timeseries/input_pipe.py:FakeSplitter,FakeSplitter,class,17,65,52,677,10.42,0,1,[],[],[],72,[],[],0
data/docs/timeseries/rnn_timeseries/input_pipe.py:InputPipe,InputPipe,class,137,436,322,5266,12.08,0,6,[],[],[],93,[],[],0
data/docs/timeseries/rnn_timeseries/input_pipe.py:Split:__init__,Split:__init__,method,8,8,8,99,12.38,0,0,"['self', 'test_set', 'train_set', 'test_size', 'train_size']","[None, ' List[tf.Tensor]', ' List[tf.Tensor]', ' int', ' int']","[None, None, None, None, None]",17,[],[],0
data/docs/timeseries/rnn_timeseries/input_pipe.py:Splitter:cluster_pages,Splitter:cluster_pages,method,10,18,16,236,13.11,0,0,"['self', 'cluster_idx']","[None, ' tf.Tensor']","[None, None]",25,"['        """"""\n', '        Shuffles pages so all user_agents of each unique pages stays together in a shuffled list\n', '        :param cluster_idx: Tensor[uniq_pages, n_agents], each value is index of pair (uniq_page, agent) in other page tensors\n', '        :return: list of page indexes for use in a global page tensors\n', '        """"""\n']","['tf.random_shuffle', 'tf.gather', 'tf.boolean_mask']",3
data/docs/timeseries/rnn_timeseries/input_pipe.py:Splitter:__init__,Splitter:__init__,method,37,106,87,1210,11.42,0,0,"['self', 'tensors', 'cluster_indexes', 'n_splits', 'seed', 'train_sampling', 'test_sampling']","[None, ' List[tf.Tensor]', ' tf.Tensor', None, None, None, None]","[None, None, None, None, None, '1.0', '1.0']",39,[],"['self.cluster_pages', 'tf.shape', 'tf.assert_equal', 'tf.control_dependencies', 'int', 'tf.split', 'range', 'mk_name', 'prepare_split', 'name=mk_name', 'Split']",11
data/docs/timeseries/rnn_timeseries/input_pipe.py:FakeSplitter:__init__,FakeSplitter:__init__,method,16,58,46,602,10.38,0,1,"['self', 'tensors', 'n_splits', 'seed', 'test_sampling']","[None, ' List[tf.Tensor]', None, None, None]","[None, None, None, None, '1.0']",73,[],"['int', 'mk_name', 'prepare_split', 'tf.random_shuffle', 'name=mk_name', 'Split', 'range']",7
data/docs/timeseries/rnn_timeseries/input_pipe.py:InputPipe:cut,InputPipe:cut,method,45,129,98,1667,12.92,0,1,"['self', 'hits', 'start', 'end']","[None, None, None, None]","[None, None, None, None]",94,"['        """"""\n', '        Cuts [start:end] diapason from input data\n', '        :param hits: hits timeseries\n', '        :param start: start index\n', '        :param end: end index\n', '        :return: tuple (train_hits, test_hits, dow, lagged_hits)\n', '        """"""\n']","['tf.concat', 'tf.fill', 'tf.cast', 'tf.maximum', 'tf.gather', 'tf.zeros_like', 'tf.where', 'tf.is_nan', 'tf.split', 'cut_train', 'pd.Timedelta', 'print', 'tf.random_uniform', 'self.cut', 'cut_eval']",15
data/docs/timeseries/rnn_timeseries/input_pipe.py:InputPipe:cut_train,InputPipe:cut_train,method,22,67,50,902,13.46,0,1,"['self', 'hits', '*args']","[None, None, None]","[None, None, None]",129,"['        """"""\n', '        Cuts a segment of time series for training. Randomly chooses starting point.\n', '        :param hits: hits timeseries\n', '        :param args: pass-through data, will be appended to result\n', '        :return: result of cut() + args\n', '        """"""\n']","['pd.Timedelta', 'print', 'tf.random_uniform', 'self.cut']",4
data/docs/timeseries/rnn_timeseries/input_pipe.py:InputPipe:cut_eval,InputPipe:cut_eval,method,6,9,9,107,11.89,0,0,"['self', 'hits', '*args']","[None, None, None]","[None, None, None]",155,"['        """"""\n', '        Cuts segment of time series for evaluation.\n', '        Always cuts train_window + predict_window length segment beginning at start_offset point\n', '        :param hits: hits timeseries\n', '        :param args: pass-through data, will be appended to result\n', '        :return: result of cut() + args\n', '        """"""\n']",['self.cut'],1
data/docs/timeseries/rnn_timeseries/input_pipe.py:InputPipe:reject_filter,InputPipe:reject_filter,method,7,19,16,198,10.42,0,1,"['self', 'x_hits', 'y_hits', '*args']","[None, None, None, None]","[None, None, None, None]",166,"['        """"""\n', '        Rejects timeseries having too many zero datapoints (more than self.max_train_empty)\n', '        """"""\n']","['print', 'tf.reduce_sum']",2
data/docs/timeseries/rnn_timeseries/input_pipe.py:InputPipe:make_features,InputPipe:make_features,method,32,74,59,902,12.19,0,0,"['self', 'x_hits', 'y_hits', 'dow', 'lagged_hits', 'pf_agent', 'pf_country', 'pf_site', 'page_ix', 'page_popularity', 'year_autocorr', 'quarter_autocorr']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None, None, None]",176,"['        """"""\n', '        Main method. Assembles input data into final tensors\n', '        """"""\n']","['tf.split', 'tf.reduce_mean', 'tf.sqrt', 'tf.stack', 'tf.concat', 'tf.expand_dims', 'tf.tile']",7
data/docs/timeseries/rnn_timeseries/input_pipe.py:InputPipe:__init__,InputPipe:__init__,method,62,162,135,1864,11.51,0,4,"['self', 'inp', 'features', 'n_pages', 'mode', 'n_epoch', 'batch_size', 'runs_in_burst', 'verbose', 'predict_window', 'train_window', 'train_completeness_threshold', 'predict_completeness_threshold', 'back_offset', 'train_skip_first', 'rand_seed']","[None, ' VarFeeder', ' Iterable[tf.Tensor]', ' int', ' ModelMode', None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, 'None', '127', '1', 'True', '60', '500', '1', '1', '0', '0', 'None']",222,"['        """"""\n', '        Create data preprocessing pipeline\n', '        :param inp: Raw input data\n', '        :param features: Features tensors (subset of data in inp)\n', '        :param n_pages: Total number of pages\n', '        :param mode: Train/Predict/Eval mode selector\n', '        :param n_epoch: Number of epochs. Generates endless data stream if None\n', '        :param batch_size:\n', '        :param runs_in_burst: How many batches can be consumed at short time interval (burst). Multiplicator for prefetch()\n', '        :param verbose: Print additional information during graph construction\n', '        :param predict_window: Number of days to predict\n', '        :param train_window: Use train_window days for traning\n', '        :param train_completeness_threshold: Percent of zero datapoints allowed in train timeseries.\n', '        :param predict_completeness_threshold: Percent of zero datapoints allowed in test/predict timeseries.\n', ""        :param back_offset: Don't use back_offset days at the end of timeseries\n"", ""        :param train_skip_first: Don't use train_skip_first days at the beginning of timeseries\n"", '        :param rand_seed:\n', '\n', '        """"""\n']","['print', 'pd.Timedelta', 'int', 'batch.make_initializable_iterator']",4
data/docs/timeseries/rnn_timeseries/input_pipe.py:InputPipe:load_vars,InputPipe:load_vars,method,1,1,1,25,25.0,0,0,"['self', 'session']","[None, None]","[None, None]",300,[],[],0
data/docs/timeseries/rnn_timeseries/input_pipe.py:InputPipe:init_iterator,InputPipe:init_iterator,method,1,1,1,38,38.0,0,0,"['self', 'session']","[None, None]","[None, None]",303,[],['session.run'],1
data/docs/timeseries/rnn_timeseries/make_features.py:read_cached,read_cached,function,9,27,22,228,8.44,1,2,['name'],[None],[None],13,"['    """"""\n', ""    Reads csv file (maybe zipped) from data directory and caches it's content as a pickled DataFrame\n"", '    :param name: file name without extension\n', '    :return: file content\n', '    """"""\n']","['pd.read_pickle', 'pd.read_csv', 'df.to_pickle']",3
data/docs/timeseries/rnn_timeseries/make_features.py:read_all,read_all,function,14,29,24,415,14.31,0,1,[],[],[],31,"['    """"""\n', '    Reads source data for training/prediction\n', '    """"""\n']","['read_file', 'read_cached', 'pd.read_pickle', 'df.sort_index', 'df.to_pickle']",5
data/docs/timeseries/rnn_timeseries/make_features.py:make_holidays,make_holidays,function,11,31,27,428,13.81,0,0,"['tagged', 'start', 'end']","[None, None, None]","[None, None, None]",59,[],"['read_df', 'pd.read_pickle', 'pd.DataFrame', 'pd.DatetimeIndex']",4
data/docs/timeseries/rnn_timeseries/make_features.py:read_x,read_x,function,12,21,15,180,8.57,0,1,"['start', 'end']","[None, None]","[None, None]",71,"['    """"""\n', '    Gets source data from start to end date. Any date can be None\n', '    """"""\n']",['read_all'],1
data/docs/timeseries/rnn_timeseries/make_features.py:single_autocorr,single_autocorr,function,13,28,23,191,6.82,0,0,"['series', 'lag']","[None, None]","[None, None]",89,"['    """"""\n', '    Autocorrelation for single data series\n', '    :param series: traffic series\n', '    :param lag: lag, days\n', '    :return:\n', '    """"""\n']","['np.mean', 'np.sqrt', 'np.sum']",3
data/docs/timeseries/rnn_timeseries/make_features.py:batch_autocorr,batch_autocorr,function,28,56,42,504,9.0,1,1,"['data', 'lag', 'starts', 'ends', 'threshold', 'backoffset']","[None, None, None, None, None, None]","[None, None, None, None, None, '0']",107,"['    """"""\n', '    Calculate autocorrelation for batch (many time series at once)\n', '    :param data: Time series, shape [n_pages, n_days]\n', '    :param lag: Autocorrelation lag\n', '    :param starts: Start index for each series\n', '    :param ends: End index for each series\n', '    :param threshold: Minimum support (ratio of time series length to lag) to calculate meaningful autocorrelation.\n', '    :param backoffset: Offset from the series end, days.\n', '    :return: autocorrelation, shape [n_series]. If series is too short (support less than threshold),\n', '    autocorrelation value is NaN\n', '    """"""\n']","['np.empty', 'range', 'min', 'single_autocorr']",4
data/docs/timeseries/rnn_timeseries/make_features.py:find_start_end,find_start_end,function,16,54,33,383,7.09,3,2,['data'],[' np.ndarray'],[None],142,"['    """"""\n', '    Calculates start and end of real traffic data. Start is an index of first non-zero, non-NaN value,\n', '     end is index of last non-zero, non-NaN value\n', '    :param data: Time series, shape [n_pages, n_days]\n', '    :return:\n', '    """"""\n']","['np.full', 'range', 'np.isnan']",3
data/docs/timeseries/rnn_timeseries/make_features.py:prepare_data,prepare_data,function,13,31,30,286,9.23,0,0,"['start', 'end', 'valid_threshold']","[None, None, None]","[None, None, None]",167,"['    """"""\n', '    Reads source data, calculates start and end of each series, drops bad series, calculates log1p(series)\n', '    :param start: start date of effective time interval, can be None to start from beginning\n', '    :param end: end date of effective time interval, can be None to return all data\n', '    :param valid_threshold: minimal ratio of series real length to entire (end-start) interval. Series dropped if\n', '    ratio is less than threshold\n', '    :return: tuple(log1p(series), nans, series start, series end)\n', '    """"""\n']","['read_x', 'find_start_end', 'print', 'len', 'pd.isnull', 'np.log1p']",6
data/docs/timeseries/rnn_timeseries/make_features.py:lag_indexes,lag_indexes,function,8,24,21,252,10.5,0,0,"['begin', 'end']","[None, None]","[None, None]",187,"['    """"""\n', '    Calculates indexes for 3, 6, 9, 12 months backward lag for the given date range\n', '    :param begin: start of date range\n', '    :param end: end of date range\n', '    :return: List of 4 Series, one for each lag. For each Series, index is date in range(begin, end), value is an index\n', '     of target (lagged) date in a same Series. If target date is out of (begin,end) range, index is -1\n', '    """"""\n']","['pd.date_range', 'pd.Series', 'len', 'lag']",4
data/docs/timeseries/rnn_timeseries/make_features.py:make_page_features,make_page_features,function,6,9,9,124,13.78,0,0,['pages'],[' np.ndarray'],[None],206,"['    """"""\n', '    Calculates page features (site, country, agent, etc) from urls\n', '    :param pages: Source urls\n', '    :return: DataFrame with features as columns and urls as index\n', '    """"""\n']","['extractor.extract', 'tagged.drop']",2
data/docs/timeseries/rnn_timeseries/make_features.py:uniq_page_map,uniq_page_map,function,22,49,46,517,10.55,1,1,['pages'],['Collection'],[None],218,"['    """"""\n', '    Finds agent types (spider, desktop, mobile, all) for each unique url, i.e. groups pages by agents\n', '    :param pages: all urls (must be presorted)\n', '    :return: array[num_unique_urls, 4], where each column corresponds to agent type and each row corresponds to unique url.\n', '     Value is an index of page in source pages array. If agent is missing, value is -1\n', '    """"""\n']","['np.full', 're.compile', 'enumerate', 'pat.fullmatch', 'match.group']",5
data/docs/timeseries/rnn_timeseries/make_features.py:encode_page_features,encode_page_features,function,4,18,16,177,9.83,0,0,['df'],[None],[None],244,"['    """"""\n', '    Applies one-hot encoding to page features and normalises result\n', '    :param df: page features DataFrame (one column per feature)\n', '    :return: dictionary feature_name:encoded_values. Encoded values is [n_pages,n_values] array\n', '    """"""\n']","['encode', 'pd.get_dummies', 'one_hot.mean', 'one_hot.std']",4
data/docs/timeseries/rnn_timeseries/make_features.py:normalize,normalize,function,1,4,4,43,10.75,0,0,['values'],[' np.ndarray'],[None],258,[],"['values.mean', 'np.std']",2
data/docs/timeseries/rnn_timeseries/make_features.py:run,run,function,53,172,148,2522,14.66,0,0,[],[],[],262,[],"['argparse.ArgumentParser', 'parser.add_argument', 'parser.parse_args', 'prepare_data', 'pd.Timedelta', 'print', 'uniq_page_map', 'batch_autocorr', 'np.sum', 'int', 'len', 'normalize', 'make_page_features', 'encode_page_features', 'pd.date_range', 'np.stack', 'np.sin', 'df.median', 'page_popularity.mean', 'page_popularity.std', 'dict', 'features_days=len', 'data_days=len', 'n_pages=len', 'VarFeeder']",25
data/docs/timeseries/rnn_timeseries/model.py:default_init,default_init,function,2,5,5,94,18.8,0,0,['seed'],[None],[None],16,[],['layers.variance_scaling_initializer'],1
data/docs/timeseries/rnn_timeseries/model.py:selu,selu,function,6,15,13,162,10.8,0,0,['x'],[None],[None],24,"['    """"""\n', '    SELU activation\n', '    https://arxiv.org/abs/1706.02515\n', '    :param x:\n', '    :return:\n', '    """"""\n']","['tf.name_scope', 'tf.where']",2
data/docs/timeseries/rnn_timeseries/model.py:make_encoder,make_encoder,function,17,53,40,641,12.09,0,3,"['time_inputs', 'encoder_features_depth', 'is_train', 'hparams', 'seed', 'transpose_output']","[None, None, None, None, None, None]","[None, None, None, None, None, 'True']",37,"['    """"""\n', '    Builds encoder, using CUDA RNN\n', '    :param time_inputs: Input tensor, shape [batch, time, features]\n', '    :param encoder_features_depth: Static size for features dimension\n', '    :param is_train:\n', '    :param hparams:\n', '    :param seed:\n', '    :param transpose_output: Transform RNN output to batch-first shape\n', '    :return:\n', '    """"""\n']","['build_rnn', 'RNN', 'tf.transpose', 'cuda_model']",4
data/docs/timeseries/rnn_timeseries/model.py:compressed_readout,compressed_readout,function,6,17,17,254,14.94,0,1,"['rnn_out', 'hparams', 'dropout', 'seed']","[None, None, None, None]","[None, None, None, None]",72,"['    """"""\n', '    FC compression layer, reduces RNN output depth to hparams.attention_depth\n', '    :param rnn_out:\n', '    :param hparams:\n', '    :param dropout:\n', '    :param seed:\n', '    :return:\n', '    """"""\n']",[],0
data/docs/timeseries/rnn_timeseries/model.py:make_fingerprint,make_fingerprint,function,25,93,62,1291,13.88,0,1,"['x', 'is_train', 'fc_dropout', 'seed']","[None, None, None, None]","[None, None, None, None]",91,"['    """"""\n', ""    Calculates 'fingerprint' of timeseries, to feed into attention layer\n"", '    :param x:\n', '    :param is_train:\n', '    :param fc_dropout:\n', '    :param seed:\n', '    :return:\n', '    """"""\n']","['tf.variable_scope', 'tf.reshape']",2
data/docs/timeseries/rnn_timeseries/model.py:attn_readout_v3,attn_readout_v3,function,23,65,53,701,10.78,1,0,"['readout', 'attn_window', 'attn_heads', 'page_features', 'seed']","[None, None, None, None, None]","[None, None, None, None, None]",125,[],"['tf.transpose', 'kernel_initializer=default_init', 'tf.reshape', 'tf.reduce_sum', 'tf.squeeze', 'range', 'tf.concat']",7
data/docs/timeseries/rnn_timeseries/model.py:calc_smape_rounded,calc_smape_rounded,function,13,28,25,319,11.39,0,0,"['true', 'predicted', 'weights']","[None, None, None]","[None, None, None]",164,"['    """"""\n', '    Calculates SMAPE on rounded submission values. Should be close to official SMAPE in competition\n', '    :param true:\n', '    :param predicted:\n', '    :param weights: Weights mask to exclude some values\n', '    :return:\n', '    """"""\n']","['tf.reduce_sum', 'tf.round', 'tf.maximum', 'tf.abs', 'tf.where', 'tf.zeros_like']",6
data/docs/timeseries/rnn_timeseries/model.py:smape_loss,smape_loss,function,19,31,30,289,9.32,0,0,"['true', 'predicted', 'weights']","[None, None, None]","[None, None, None]",182,"['    """"""\n', '    Differentiable SMAPE loss\n', '    :param true: Truth values\n', '    :param predicted: Predicted values\n', '    :param weights: Weights mask to exclude some values\n', '    :return:\n', '    """"""\n']","['tf.expm1', 'tf.maximum', 'tf.abs']",3
data/docs/timeseries/rnn_timeseries/model.py:decode_predictions,decode_predictions,function,6,12,8,170,14.17,0,0,"['decoder_readout', 'inp']","[None, ' InputPipe']","[None, None]",198,"['    """"""\n', '    Converts normalized prediction values to log1p(pageviews), e.g. reverts normalization\n', '    :param decoder_readout: Decoder output, shape [n_days, batch]\n', '    :param inp: Input tensors\n', '    :return:\n', '    """"""\n']","['tf.transpose', 'tf.expand_dims']",2
data/docs/timeseries/rnn_timeseries/model.py:calc_loss,calc_loss,function,15,30,27,397,13.23,0,1,"['predictions', 'true_y', 'additional_mask']","[None, None, None]","[None, None, 'None']",212,"['    """"""\n', '    Calculates losses, ignoring NaN true values (assigning zero loss to them)\n', '    :param predictions: Predicted values\n', '    :param true_y: True values\n', '    :param additional_mask:\n', '    :return: MAE loss, differentiable SMAPE loss, competition SMAPE loss\n', '    """"""\n']","['tf.is_finite', 'tf.where', 'tf.zeros_like', 'tf.to_float', 'tf.expand_dims', 'smape_loss', 'calc_smape_rounded', 'tf.size']",8
data/docs/timeseries/rnn_timeseries/model.py:make_train_op,make_train_op,function,35,60,48,803,13.38,1,4,"['loss', 'ema_decay', 'prefix']","[None, None, None]","[None, 'None', 'None']",234,[],"['tf.get_collection', 'optimizer.compute_gradients', 'zip', 'tf.clip_by_global_norm', 'optimizer.apply_gradients', 'ema.apply', 'tf.control_dependencies', 'tf.group']",8
data/docs/timeseries/rnn_timeseries/model.py:convert_cudnn_state_v2,convert_cudnn_state_v2,function,14,47,39,582,12.38,0,2,"['h_state', 'hparams', 'seed', 'c_state', 'dropout']","[None, None, None, None, None]","[None, None, None, 'None', '1.0']",265,"['    """"""\n', '    Converts RNN state tensor from cuDNN representation to TF RNNCell compatible representation.\n', '    :param h_state: tensor [num_layers, batch_size, depth]\n', '    :param c_state: LSTM additional state, should be same shape as h_state\n', '    :return: TF cell representation matching RNNCell.state_size structure for compatible cell\n', '    """"""\n']","['squeeze', 'tuple', 'len', 'wrap_dropout', 'nest.map_structure', 'tf.unstack', 'range']",7
data/docs/timeseries/rnn_timeseries/model.py:rnn_stability_loss,rnn_stability_loss,function,5,12,10,132,11.0,0,1,"['rnn_output', 'beta']","[None, None]","[None, None]",296,"['    """"""\n', '    REGULARIZING RNNS BY STABILIZING ACTIVATIONS\n', '    https://arxiv.org/pdf/1511.08400.pdf\n', '    :param rnn_output: [time, batch, features]\n', '    :return: loss value\n', '    """"""\n']","['tf.sqrt', 'tf.reduce_mean']",2
data/docs/timeseries/rnn_timeseries/model.py:rnn_activation_loss,rnn_activation_loss,function,3,8,6,59,7.38,0,1,"['rnn_output', 'beta']","[None, None]","[None, None]",311,"['    """"""\n', '    REGULARIZING RNNS BY STABILIZING ACTIVATIONS\n', '    https://arxiv.org/pdf/1511.08400.pdf\n', '    :param rnn_output: [time, batch, features]\n', '    :return: loss value\n', '    """"""\n']",[],0
data/docs/timeseries/rnn_timeseries/model.py:Model,Model,class,123,347,254,4723,13.61,1,11,[],[],[],323,[],[],0
data/docs/timeseries/rnn_timeseries/model.py:Model:__init__,Model:__init__,method,56,129,105,1954,15.15,1,5,"['self', 'inp', 'hparams', 'is_train', 'seed', 'graph_prefix', 'asgd_decay', 'loss_mask']","[None, ' InputPipe', None, None, None, None, None, None]","[None, None, None, None, None, 'None', 'None', 'None']",324,"['        """"""\n', '        Encoder-decoder prediction model\n', '        :param inp: Input tensors\n', '        :param hparams:\n', '        :param is_train:\n', '        :param seed:\n', '        :param graph_prefix: Subgraph prefix for multi-model graph\n', '        :param asgd_decay: Decay for SGD averaging\n', '        :param loss_mask: Additional mask for losses calculation (one value for each prediction day), shape=[predict_window]\n', '        """"""\n']","['make_encoder', 'rnn_stability_loss', 'rnn_activation_loss', 'convert_cudnn_state_v2', 'compressed_readout', 'tf.concat', 'tf.expand_dims', 'make_fingerprint', 'attn_readout_v3', 'self.decoder', 'decode_predictions', 'tf.get_collection', 'calc_loss', 'make_train_op']",14
data/docs/timeseries/rnn_timeseries/model.py:Model:default_init,Model:default_init,method,2,3,3,38,12.67,0,0,"['self', 'seed_add']","[None, None]","[None, '0']",395,[],['default_init'],1
data/docs/timeseries/rnn_timeseries/model.py:Model:decoder,Model:decoder,method,68,196,142,2516,12.84,0,6,"['self', 'encoder_state', 'attn_features', 'prediction_inputs', 'previous_y']","[None, None, None, None, None]","[None, None, None, None, None]",398,"['        """"""\n', '        :param encoder_state: shape [batch_size, encoder_rnn_depth]\n', '        :param prediction_inputs: features for prediction days, tensor[batch_size, time, input_depth]\n', '        :param previous_y: Last day pageviews, shape [batch_size]\n', '        :param attn_features: Additional features from attention layer, shape [batch, predict_window, readout_depth*n_heads]\n', '        :return: decoder rnn output\n', '        """"""\n']","['build_cell', 'tf.variable_scope', 'rnn.GRUBlockCell', 'rnn.DropoutWrapper', 'range', 'rnn.MultiRNNCell', 'nest.assert_same_structure', 'tf.transpose', 'cond_fn', 'project_output', 'loop_fn', 'tf.concat', 'cell', 'array_outputs.write', 'array_targets.write', 'tf.expand_dims', 'tf.TensorArray', 'tf.constant', 'tf.while_loop', 'targets_ta.stack', 'tf.squeeze', 'outputs_ta.stack']",22
data/docs/timeseries/rnn_timeseries/trainer.py:train,train,function,154,600,379,7701,12.84,4,32,"['name', 'hparams', 'multi_gpu', 'n_models', 'train_completeness_threshold', 'seed', 'logdir', 'max_epoch', 'patience', 'train_sampling', 'eval_sampling', 'eval_memsize', 'gpu', 'gpu_allow_growth', 'save_best_model', 'forward_split', 'write_summaries', 'verbose', 'asgd_decay', 'tqdm', 'side_split', 'max_steps', 'save_from_step', 'do_eval', 'predict_window']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'False', '1', '0.01', 'None', ""'data/logs'"", '100', '2', '1.0', '1.0', '5', '0', 'False', 'False', 'False', 'False', 'False', 'None', 'True', 'True', 'None', 'None', 'True', '63']",399,[],"['int', 'tf.reset_default_graph', 'tf.set_random_seed', 'tf.device', 'VarFeeder.read_vars', 'Splitter', 'FakeSplitter', 'tf.assign_add', 'create_model', 'tf.variable_scope', 'InputPipe', 'inp_scope.reuse_variables', 'Model', 'scope.reuse_variables', 'eval_stages.append', 'shutil.rmtree', 'ModelTrainerV2', 'tf.get_variable_scope', 'range', 'all_models.append', 'MultiModelTrainer', 'os.makedirs', 'ema_vars', 'dict', 'tf.global_variables_initializer', 'trainer.metric', 'DummyMetric', 'ema_eval_stages.append', 'tf.Session', 'sess.run', 'inp.restore', 'model.init', 'trange', 'trainer.train_step', 'trainer.eval_step', 'saver.save', 'ema_saver.save', 'ema_loader.restore', 'ema_saver.restore', 'tqr.set_postfix', 'trainer.has_active', 'tqr.close', 'trainer.end_epoch', 'best_epoch_smape.append', 'print', 'np.mean']",46
data/docs/timeseries/rnn_timeseries/trainer.py:predict,predict,function,74,212,150,2441,11.51,4,10,"['checkpoints', 'hparams', 'return_x', 'verbose', 'predict_window', 'back_offset', 'n_models', 'target_model', 'asgd', 'seed', 'batch_size']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'False', 'False', '6', '0', '1', '0', 'False', '1', '1024']",663,[],"['tf.variable_scope', 'tf.device', 'VarFeeder.read_vars', 'InputPipe', 'page_features', 'Model', 'range', 'models.append', 'list', 'var.endswith', 'var.startswith', 'tf.Session', 'pipe.load_vars', 'pipe.init_iterator', 'saver.restore', 'sess.run', 'pd.DataFrame', 'pred_buffer.append', 'x_buffer.append', 'print', 'pd.concat', 'len', 'pd.Timedelta', 'pd.date_range']",24
data/docs/timeseries/rnn_timeseries/trainer.py:Ema,Ema,class,11,38,29,250,6.58,0,1,[],[],[],22,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:Metric,Metric,class,30,93,64,885,9.52,0,5,[],[],[],39,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:AggMetric,AggMetric,class,13,54,30,475,8.8,0,0,[],[],[],87,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:DummyMetric,DummyMetric,class,8,30,12,239,7.97,0,0,[],[],[],116,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:Stage,Stage,class,5,10,10,63,6.3,0,0,[],[],[],142,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainerV2,ModelTrainerV2,class,45,156,109,1704,10.92,6,4,[],[],[],150,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:MultiModelTrainer,MultiModelTrainer,class,59,182,121,1766,9.7,6,5,[],[],[],210,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer,ModelTrainer,class,89,297,194,3663,12.33,1,7,[],[],[],270,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:Ema:__init__,Ema:__init__,method,4,6,6,37,6.17,0,0,"['self', 'k']","[None, None]","[None, '0.99']",23,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:Ema:__call__,Ema:__call__,method,7,25,19,152,6.08,0,1,"['self', '*args', '**kwargs']","[None, None, None]","[None, None, None]",28,[],['min'],1
data/docs/timeseries/rnn_timeseries/trainer.py:Metric:__init__,Metric:__init__,method,13,22,22,188,8.55,0,0,"['self', 'name', 'op', 'smoothness']","[None, ' str', None, ' float ']","[None, None, None, ' None']",40,[],['Ema'],1
data/docs/timeseries/rnn_timeseries/trainer.py:Metric:avg_epoch,Metric:avg_epoch,method,2,2,2,32,16.0,0,0,['self'],[None],[None],52,[],['np.mean'],1
data/docs/timeseries/rnn_timeseries/trainer.py:Metric:best_epoch,Metric:best_epoch,method,2,2,2,31,15.5,0,0,['self'],[None],[None],56,[],['np.min'],1
data/docs/timeseries/rnn_timeseries/trainer.py:Metric:last,Metric:last,method,4,6,6,56,9.33,0,1,['self'],[None],[None],60,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:Metric:top,Metric:top,method,1,2,2,25,12.5,0,0,['self'],[None],[None],64,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:Metric:update,Metric:update,method,12,34,28,351,10.32,0,4,"['self', 'value', 'epoch', 'step']","[None, None, None, None]","[None, None, None, None]",68,[],"['self.smoother', 'len', 'heapq.heappushpop', 'heapq.heappush']",4
data/docs/timeseries/rnn_timeseries/trainer.py:AggMetric:__init__,AggMetric:__init__,method,2,2,2,20,10.0,0,0,"['self', 'metrics']","[None, ' List[Metric]']","[None, None]",88,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:AggMetric:_mean,AggMetric:_mean,method,2,6,6,51,8.5,0,0,"['self', 'fun']","[None, None]","[None, None]",91,[],['np.mean'],1
data/docs/timeseries/rnn_timeseries/trainer.py:AggMetric:avg_epoch,AggMetric:avg_epoch,method,2,4,4,37,9.25,0,0,['self'],[None],[None],52,[],['self._mean'],1
data/docs/timeseries/rnn_timeseries/trainer.py:AggMetric:best_epoch,AggMetric:best_epoch,method,2,4,4,38,9.5,0,0,['self'],[None],[None],56,[],['self._mean'],1
data/docs/timeseries/rnn_timeseries/trainer.py:AggMetric:last,AggMetric:last,method,2,4,4,32,8.0,0,0,['self'],[None],[None],60,[],['self._mean'],1
data/docs/timeseries/rnn_timeseries/trainer.py:AggMetric:top,AggMetric:top,method,2,4,4,31,7.75,0,0,['self'],[None],[None],64,[],['self._mean'],1
data/docs/timeseries/rnn_timeseries/trainer.py:AggMetric:improved,AggMetric:improved,method,2,6,6,54,9.0,0,0,['self'],[None],[None],112,[],['np.any'],1
data/docs/timeseries/rnn_timeseries/trainer.py:DummyMetric:avg_epoch,DummyMetric:avg_epoch,method,2,2,2,12,6.0,0,0,['self'],[None],[None],52,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:DummyMetric:best_epoch,DummyMetric:best_epoch,method,2,2,2,12,6.0,0,0,['self'],[None],[None],56,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:DummyMetric:last,DummyMetric:last,method,2,2,2,12,6.0,0,0,['self'],[None],[None],60,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:DummyMetric:top,DummyMetric:top,method,2,2,2,12,6.0,0,0,['self'],[None],[None],64,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:DummyMetric:improved,DummyMetric:improved,method,1,2,2,11,5.5,0,0,['self'],[None],[None],112,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:DummyMetric:metrics,DummyMetric:metrics,method,1,2,2,8,4.0,0,0,['self'],[None],[None],138,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainerV2:__init__,ModelTrainerV2:__init__,method,30,64,56,685,10.7,3,1,"['self', 'train_model', 'eval', 'Model]]', 'model_no', 'patience', 'stop_metric', 'summary_writer']","[None, ' Model', ' List[Tuple[Stage', None, None, None, None, None]","[None, None, None, None, '0', 'None', 'None', 'None']",151,[],"['zip', 'std_metrics', 'Metric']",3
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainerV2:init,ModelTrainerV2:init,method,2,6,6,82,13.67,1,0,"['self', 'sess']","[None, None]","[None, None]",174,[],['list'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainerV2:metrics,ModelTrainerV2:metrics,method,2,2,2,19,9.5,0,0,['self'],[None],[None],138,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainerV2:train_ops,ModelTrainerV2:train_ops,method,4,7,7,62,8.86,0,0,['self'],[None],[None],183,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainerV2:metric_ops,ModelTrainerV2:metric_ops,method,3,6,6,46,7.67,1,0,"['self', 'key']","[None, None]","[None, None]",187,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainerV2:process_metrics,ModelTrainerV2:process_metrics,method,9,17,16,222,13.06,1,0,"['self', 'key', 'run_results', 'epoch', 'step']","[None, None, None, None, None]","[None, None, None, None, None]",190,[],"['zip', 'metric.update', 'summaries.append']",3
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainerV2:end_epoch,ModelTrainerV2:end_epoch,method,6,23,17,276,12.0,0,3,['self'],[None],[None],198,[],['self.stop_metric'],1
data/docs/timeseries/rnn_timeseries/trainer.py:MultiModelTrainer:__init__,MultiModelTrainer:__init__,method,9,10,10,125,12.5,0,0,"['self', 'trainers', 'inc_step_op', 'misc_global_ops']","[None, ' List[ModelTrainerV2]', None, None]","[None, None, None, 'None']",211,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:MultiModelTrainer:active,MultiModelTrainer:active,method,4,9,9,60,6.67,1,1,['self'],[None],[None],218,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:MultiModelTrainer:_metric_step,MultiModelTrainer:_metric_step,method,36,73,60,699,9.58,2,3,"['self', 'stage', 'initial_ops', 'sess', 'epoch', 'step', 'repeats', 'summary_every']","[None, None, None, ' tf.Session', ' int', None, None, None]","[None, None, None, None, None, 'None', '1', '1']",221,[],"['self.active', 'offsets.append', 'trainer.metric_ops', 'lengths.append', 'ops.extend', 'np.stack', 'range', 'np.mean', 'sess.run', 'zip', 'trainer.process_metrics', 'tf.Summary']",12
data/docs/timeseries/rnn_timeseries/trainer.py:MultiModelTrainer:train_step,MultiModelTrainer:train_step,method,9,16,16,180,11.25,1,0,"['self', 'sess', 'epoch']","[None, ' tf.Session', ' int']","[None, None, None]",246,[],"['self.active', 'ops.extend', 'self._metric_step']",3
data/docs/timeseries/rnn_timeseries/trainer.py:MultiModelTrainer:eval_step,MultiModelTrainer:eval_step,method,5,19,18,144,7.58,1,1,"['self', 'sess', 'epoch', 'step', 'n_batches', 'stages']","[None, ' tf.Session', ' int', None, None, 'List[Stage]']","[None, None, None, None, None, 'None']",254,[],['self._metric_step'],1
data/docs/timeseries/rnn_timeseries/trainer.py:MultiModelTrainer:metric,MultiModelTrainer:metric,method,2,6,6,77,12.83,0,0,"['self', 'stage', 'name']","[None, None, None]","[None, None, None]",259,[],['AggMetric'],1
data/docs/timeseries/rnn_timeseries/trainer.py:MultiModelTrainer:end_epoch,MultiModelTrainer:end_epoch,method,3,5,5,46,9.2,1,0,['self'],[None],[None],198,[],"['self.active', 'trainer.end_epoch']",2
data/docs/timeseries/rnn_timeseries/trainer.py:MultiModelTrainer:has_active,MultiModelTrainer:has_active,method,1,2,2,24,12.0,0,0,['self'],[None],[None],266,[],['len'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:__init__,ModelTrainer:__init__,method,28,38,34,486,12.79,0,0,"['self', 'train_model', 'eval_model', 'model_no', 'summary_writer', 'keep_best', 'patience']","[None, None, None, None, None, None, None]","[None, None, None, '0', 'None', '5', 'None']",271,[],['Ema'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:train_ops,ModelTrainer:train_ops,method,8,9,9,116,12.89,0,0,['self'],[None],[None],183,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:process_train_results,ModelTrainer:process_train_results,method,14,21,21,292,13.9,0,1,"['self', 'run_results', 'offset', 'global_step', 'write_summary']","[None, None, None, None, None]","[None, None, None, None, None]",297,[],"['self.smooth_train_mae', 'self.smooth_train_smape', 'self.smooth_grad', 'np.array']",4
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:eval_ops,ModelTrainer:eval_ops,method,4,5,5,51,10.2,0,0,['self'],[None],[None],306,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:eval_len,ModelTrainer:eval_len,method,1,2,2,24,12.0,0,0,['self'],[None],[None],311,[],['len'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:train_len,ModelTrainer:train_len,method,1,2,2,25,12.5,0,0,['self'],[None],[None],315,[],['len'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:best_top_loss,ModelTrainer:best_top_loss,method,1,2,2,44,22.0,0,0,['self'],[None],[None],319,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:best_epoch_mae,ModelTrainer:best_epoch_mae,method,2,6,6,51,8.5,0,0,['self'],[None],[None],323,[],['min'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:mean_epoch_mae,ModelTrainer:mean_epoch_mae,method,2,6,6,55,9.17,0,0,['self'],[None],[None],327,[],['np.mean'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:mean_epoch_smape,ModelTrainer:mean_epoch_smape,method,2,6,6,59,9.83,0,0,['self'],[None],[None],331,[],['np.mean'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:best_epoch_smape,ModelTrainer:best_epoch_smape,method,2,6,6,55,9.17,0,0,['self'],[None],[None],335,[],['min'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:remember_for_epoch,ModelTrainer:remember_for_epoch,method,6,12,11,142,11.83,0,1,"['self', 'epoch', 'mae', 'smape']","[None, None, None, None]","[None, None, None, None]",338,[],[],0
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:best_epoch_metrics,ModelTrainer:best_epoch_metrics,method,2,3,3,59,19.67,0,0,['self'],[None],[None],347,[],['np.array'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:mean_epoch_metrics,ModelTrainer:mean_epoch_metrics,method,2,3,3,59,19.67,0,0,['self'],[None],[None],351,[],['np.array'],1
data/docs/timeseries/rnn_timeseries/trainer.py:ModelTrainer:process_eval_results,ModelTrainer:process_eval_results,method,37,118,95,1468,12.44,1,5,"['self', 'run_results', 'offset', 'global_step', 'epoch']","[None, None, None, None, None]","[None, None, None, None, None]",354,[],"['np.zeros', 'np.array', 'len', 'tf.Summary', 'self.smooth_eval_mae', 'self.smooth_eval_smape', 'self.remember_for_epoch', 'np.mean', 'log.debug', 'heapq.heappushpop', 'heapq.heappush']",11
data/input/adfraud/zold/fraud0.py:pd_colall_preprocess,pd_colall_preprocess,function,38,129,74,2127,16.49,1,0,"['df', 'col', 'pars']","[None, None, None]","[None, None, None]",24,[],"['copy.deepcopy', 'df.drop', 'pd.to_datetime', 'zip']",4
data/input/adfraud/zold/fraud0.py:category_encoding,category_encoding,function,12,19,15,436,22.95,0,0,['encoder'],[None],[None],186,[],"['encoder.fit', 'train_X.join', 'encoder.transform', 'val_X.join']",4
data/input/adfraud/zold/fraud0.py:pd_col_genetic_transform,pd_col_genetic_transform,function,18,62,43,954,15.39,0,0,"['df', 'col', 'pars']","[None, None, None]","['None', 'None', 'None']",318,[],"['SymbolicTransformer', 'gp.fit_transform', 'pd.DataFrame', 'range', 'gp.transform']",5
data/input/adfraud/zold/fraud1.py:get_data_splits,get_data_splits,function,11,17,16,218,12.82,0,0,"['dataframe', 'valid_fraction']","[None, None]","[None, '0.1']",42,[],"['dataframe.sort_values', 'int']",2
data/input/adfraud/zold/fraud1.py:train_model,train_model,function,21,79,70,953,12.06,0,2,"['train', 'valid', 'test', 'feature_cols', 'valid_name_model']","[None, None, None, None, None]","[None, None, 'None', 'None', ""'Baseline Model'""]",53,[],"['lgb.Dataset', 'print', 'lgb.train', 'bst.predict', 'metrics.roc_auc_score']",5
data/input/adfraud/zold/fraud1.py:my_own_train_plot_model,my_own_train_plot_model,function,14,20,19,305,15.25,0,0,"['clicks', 'valid_name_model', 'my_own_metrics']","[None, None, None]","[None, None, None]",83,[],"['print', 'get_data_splits', 'train_model', 'plot_model_information']",4
data/input/adfraud/zold/fraud1.py:plot_model_information,plot_model_information,function,15,56,48,670,11.96,1,0,"['bst', 'validation_metrics', 'my_own_metrics']","[None, None, None]","[None, None, None]",100,[],"['print', 'bst.num_trees', 'lgb.plot_metric', 'plt.show', 'lgb.plot_importance', 'plot_my_own_metrics', 'x=list', 'y=list', 'plt.barh', 'enumerate', 'plt.text', 'str', 'lgb.plot_tree']",13
data/input/adfraud/zold/fraud1.py:count_past_events,count_past_events,function,6,10,9,180,18.0,0,0,['series'],[None],[None],284,[],"['pd.Series', 'print', 'new_series.rolling']",3
data/input/adfraud/zold/fraud1.py:time_diff,time_diff,function,3,4,3,82,20.5,0,0,['series'],[None],[None],315,"['    """"""Returns a series with the time since the last timestamp in seconds.""""""\n']",['series.diff'],1
data/input/adfraud/zold/fraud1.py:previous_attributions,previous_attributions,function,4,7,6,119,17.0,0,0,['series'],[None],[None],339,"['    """"""Returns a series with the number of times an app has been downloaded.""""""\n']","['print', 'series.expanding']",2
data/input/adfraud/zold/fraud2.py:pd_colall_preprocess,pd_colall_preprocess,function,36,304,100,3917,12.88,2,0,"['df', 'col', 'pars']","[None, None, None]","[None, 'None', 'None']",10,[],"['copy.deepcopy', 'df.drop', 'pd.to_datetime', 'zip', 'print', 'float', 'len']",7
data/input/adfraud/zold/fraud2.py:pd_col_genetic_transform,pd_col_genetic_transform,function,18,62,43,954,15.39,0,0,"['df', 'col', 'pars']","[None, None, None]","['None', 'None', 'None']",229,[],"['SymbolicTransformer', 'gp.fit_transform', 'pd.DataFrame', 'range', 'gp.transform']",5
data/input/adfraud/zold/fraud2.py:category_encoding,category_encoding,function,12,19,15,436,22.95,0,0,['encoder'],[None],[None],303,[],"['encoder.fit', 'train_X.join', 'encoder.transform', 'val_X.join']",4
data/input/adfraud/zold/fraud5.py:get_data_splits,get_data_splits,function,11,17,16,218,12.82,0,0,"['dataframe', 'valid_fraction']","[None, None]","[None, '0.1']",42,[],"['dataframe.sort_values', 'int']",2
data/input/adfraud/zold/fraud5.py:train_model,train_model,function,21,79,70,953,12.06,0,2,"['train', 'valid', 'test', 'feature_cols', 'valid_name_model']","[None, None, None, None, None]","[None, None, 'None', 'None', ""'Baseline Model'""]",53,[],"['lgb.Dataset', 'print', 'lgb.train', 'bst.predict', 'metrics.roc_auc_score']",5
data/input/adfraud/zold/fraud5.py:my_own_train_plot_model,my_own_train_plot_model,function,14,20,19,305,15.25,0,0,"['clicks', 'valid_name_model', 'my_own_metrics']","[None, None, None]","[None, None, None]",83,[],"['print', 'get_data_splits', 'train_model', 'plot_model_information']",4
data/input/adfraud/zold/fraud5.py:plot_model_information,plot_model_information,function,15,56,48,670,11.96,1,0,"['bst', 'validation_metrics', 'my_own_metrics']","[None, None, None]","[None, None, None]",100,[],"['print', 'bst.num_trees', 'lgb.plot_metric', 'plt.show', 'lgb.plot_importance', 'plot_my_own_metrics', 'x=list', 'y=list', 'plt.barh', 'enumerate', 'plt.text', 'str', 'lgb.plot_tree']",13
data/input/adfraud/zold/fraud5.py:count_past_events,count_past_events,function,6,10,9,180,18.0,0,0,['series'],[None],[None],284,[],"['pd.Series', 'print', 'new_series.rolling']",3
data/input/adfraud/zold/fraud5.py:time_diff,time_diff,function,3,4,3,82,20.5,0,0,['series'],[None],[None],315,"['    """"""Returns a series with the time since the last timestamp in seconds.""""""\n']",['series.diff'],1
data/input/adfraud/zold/fraud5.py:previous_attributions,previous_attributions,function,4,7,6,119,17.0,0,0,['series'],[None],[None],339,"['    """"""Returns a series with the number of times an app has been downloaded.""""""\n']","['print', 'series.expanding']",2
data/input/adfraud/zold/pastclick_duration.py:generateAggregateFeatures,generateAggregateFeatures,function,10,41,36,414,10.1,1,0,"['df', 'aggregateFeatures']","[None, None]","[None, None]",17,[],"['print', 'df.merge', 'gc.collect']",3
data/input/adfraud/zold/pastclick_duration.py:generatePastClickFeatures,generatePastClickFeatures,function,7,14,14,235,16.79,1,0,"['df', 'pastClickAggregateFeatures']","[None, None]","[None, None]",43,[],['x.diff'],1
data/input/adfraud/ztmp/Adfraud2.py:pd_colall_preprocess,pd_colall_preprocess,function,35,302,99,3877,12.84,2,0,"['df', 'col', 'pars']","[None, None, None]","[None, 'None', 'None']",18,[],"['copy.deepcopy', 'pd.to_datetime', 'zip', 'print', 'float', 'len']",6
data/input/adfraud/ztmp/Adfraud2.py:category_encoding,category_encoding,function,12,19,15,436,22.95,0,0,['encoder'],[None],[None],216,[],"['encoder.fit', 'train_X.join', 'encoder.transform', 'val_X.join']",4
data/input/adfraud/ztmp/Adfraud2.py:pd_col_genetic_transform,pd_col_genetic_transform,function,18,62,43,954,15.39,0,0,"['df', 'col', 'pars']","[None, None, None]","['None', 'None', 'None']",300,[],"['SymbolicTransformer', 'gp.fit_transform', 'pd.DataFrame', 'range', 'gp.transform']",5
data/input/online_intention/clustering/clustering_user_behaviour.py:div,div,function,4,9,9,82,9.11,0,1,"['numerator', 'denominator']","[None, None]","[None, None]",121,[],['float'],1
data/input/tseries_m5/code/util_model.py:import_,import_,function,7,23,22,243,10.57,0,1,"['abs_module_path', 'class_name']","[None, None]","[None, 'None']",63,[],"['import_module', 'print', 'getattr']",3
data/input/tseries_m5/code/util_model.py:pd_dim_reduction,pd_dim_reduction,function,16,45,34,410,9.11,0,4,"['df', 'colname', 'colprefix', 'method', 'dimpca', 'model_pretrain', 'return_val', '']","[None, None, None, None, None, None, None, None]","[None, None, '""colsvd""', '""svd""', '2', 'None', '""dataframe,param""', None]",101,"['    """"""\n', '       Dimension reduction technics\n', '       dftext_svd, svd = pd_dim_reduction(dfcat_test, None,colprefix=""colsvd"",\n', '                         method=""svd"", dimpca=2, return_val=""dataframe,param"")\n', '    :param df:\n', '    :param colname:\n', '    :param colprefix:\n', '    :param method:\n', '    :param dimpca:\n', '    :param return_val:\n', '    :return:\n', '    """"""\n']","['list', 'TruncatedSVD', 'svd.fit', 'copy.deepcopy', 'svd.transform', 'pd.DataFrame', 'str']",7
data/input/tseries_m5/code/util_model.py:model_lightgbm_kfold,model_lightgbm_kfold,function,25,75,69,940,12.53,1,1,"['df', 'colname', 'num_folds', 'stratified', 'colexclude', 'debug']","[None, None, None, None, None, None]","[None, 'None', '2', 'False', 'None', 'False']",142,[],"['StratifiedKFold', 'KFold', 'np.zeros', 'pd.DataFrame', 'enumerate', 'lgb.Dataset', 'lgb.train', 'regs.append']",8
data/input/tseries_m5/code/util_model.py:model_catboost_classifier,model_catboost_classifier,function,21,50,47,585,11.7,0,2,"['Xtrain', 'Ytrain', 'Xcolname', 'pars={""learning_rate""', '""iterations""', '""random_seed""', '""loss_function""', '}', 'isprint', '']","[None, None, None, '', ' 1000', ' 0', ' ""MultiClass""', None, None, None]","[None, None, 'None', '{""learning_rate"": 0.1', None, None, None, None, '0', None]",192,"['    """"""\n', '  from catboost import Pool, CatBoostClassifier\n', ""TRAIN_FILE = '../data/cloudness_small/train_small'\n"", ""TEST_FILE = '../data/cloudness_small/test_small'\n"", ""CD_FILE = '../data/cloudness_small/train.cd'\n"", '# Load data from files to Pool\n', 'train_pool = Pool(TRAIN_FILE, column_description=CD_FILE)\n', 'test_pool = Pool(TEST_FILE, column_description=CD_FILE)\n', '# Initialize CatBoostClassifier\n', ""model = CatBoostClassifier(iterations=2, learning_rate=1, depth=2, loss_function='MultiClass')\n"", '# Fit model\n', 'model.fit(train_pool)\n', '# Get predicted classes\n', 'preds_class = model.predict(test_pool)\n', '# Get predicted probabilities for each class\n', 'preds_proba = model.predict_proba(test_pool)\n', '# Get predicted RawFormulaVal\n', ""  preds_raw = model.predict(test_pool, prediction_type='RawFormulaVal')\n"", '  https://tech.yandex.com/catboost/doc/dg/concepts/python-usages-examples-docpage/\n', '  """"""\n']","['dict2', 'range', 'pd.DataFrame', 'catboost.CatBoostClassifier', 'clf.fit', 'clf.predict', 'cm.astype', 'cm.sum', 'print']",9
data/input/tseries_m5/code/util_model.py:sk_score_get,sk_score_get,function,10,19,15,223,11.74,0,2,['name'],[None],"['""r2""']",253,[],[],0
data/input/tseries_m5/code/util_model.py:sk_params_search_best,sk_params_search_best,function,27,50,41,690,13.8,0,2,"['clf', 'X', 'y', '0', '1', '5)}', 'method', 'param_search={""scorename""', '""cv""', '""population_size""', '""generations_number""', '']","[None, None, None, None, None, None, None, '', ' 5', ' 5', ' 3}', None]","[None, None, None, None, None, None, '""gridsearch""', '{""scorename"": ""r2""', None, None, None, None]",263,"['    """"""\n', '   Genetic: population_size=5, ngene_mutation_prob=0.10,,gene_crossover_prob=0.5, tournament_size=3,  generations_number=3\n', '    :param X:\n', '    :param y:\n', '    :param clf:\n', '    :param param_grid:\n', '    :param method:\n', '    :param param_search:\n', '    :return:\n', '  """"""\n']","['sk_score_get', 'GridSearchCV', 'grid.fit', 'EvolutionaryAlgorithmSearchCV', 'cv=StratifiedKFold', 'cv.fit']",6
data/input/tseries_m5/code/util_model.py:sk_error,sk_error,function,13,39,31,327,8.38,0,2,"['ypred', 'ytrue', 'method', 'sample_weight', 'multioutput']","[None, None, None, None, None]","[None, None, '""r2""', 'None', 'None']",313,[],"['np.sqrt', 'len', 'print', 'np.std', 'r2_score', 'np.sign']",6
data/input/tseries_m5/code/util_model.py:sk_cluster,sk_cluster,function,43,130,101,1055,8.12,2,8,"['Xmat', 'method', ')', 'kwds={""metric""', '""min_cluster_size""', '""min_samples""', 'isprint', 'preprocess={""norm""', '']","[None, None, None, '', ' 150', ' 3}', None, '', None]","[None, '""kmode""', None, '{""metric"": ""euclidean""', None, None, '1', '{""norm"": False}', None]",327,"['    """"""\n', ""   'hdbscan',(), kwds={'metric':'euclidean', 'min_cluster_size':150, 'min_samples':3 }\n"", ""   'kmodes',(), kwds={ n_clusters=2, n_init=5, init='Huang', verbose=1 }\n"", ""   'kmeans',    kwds={ n_clusters= nbcluster }\n"", '   Xmat[ Xcluster== 5 ]\n', '   # HDBSCAN Clustering\n', '   Xcluster_hdbscan= da.sk_cluster_algo_custom(Xtrain_d, hdbscan.HDBSCAN, (),\n', ""                  {'metric':'euclidean', 'min_cluster_size':150, 'min_samples':3})\n"", '   print len(np.unique(Xcluster_hdbscan))\n', '   Xcluster_use =  Xcluster_hdbscan\n', '# Calculate Distribution for each cluster\n', ""kde= da.plot_distribution_density(Y[Xcluster_use== 2], kernel='gaussian', N=200, bandwith=1 / 500.)\n"", 'kde.sample(5)\n', '   """"""\n']","['km.fit_predict', 'hdbscan.HDBSCAN', 'print', 'len', 'np.std', 'np.mean', 'Xmat.reshape', 'KMeans', 'kmeans.fit', 'range', 'plt.plot', 'plt.show']",12
data/input/tseries_m5/code/util_model.py:sk_model_ensemble_weight,sk_model_ensemble_weight,function,14,26,24,237,9.12,1,0,"['model_list', 'acclevel', 'maxlevel']","[None, None, None]","[None, None, '0.88']",451,[],"['min', 'len', 'np.empty', 'range', 'estww.append', 'np.log', 'np.array']",7
data/input/tseries_m5/code/util_model.py:sk_model_votingpredict,sk_model_votingpredict,function,17,38,30,269,7.08,2,1,"['estimators', 'voting', 'ww', 'X_test']","[None, None, None, None]","[None, None, None, None]",468,[],"['np.sum', 'np.zeros', 'enumerate', 'clf.predict_proba', 'range', 'len']",6
data/input/tseries_m5/code/util_model.py:sk_showconfusion,sk_showconfusion,function,11,22,20,209,9.5,0,1,"['Y', 'Ypred', 'isprint']","[None, None, None]","[None, None, 'True']",488,[],"['cm.astype', 'cm.sum', 'print']",3
data/input/tseries_m5/code/util_model.py:sk_showmetrics,sk_showmetrics,function,21,81,70,799,9.86,0,1,"['y_test', 'ytest_pred', 'ytest_proba', 'target_names', '""1""]', 'return_stat']","[None, None, None, None, None, None]","[None, None, None, '[""0""', None, '0']",498,[],"['sk_showconfusion', 'roc_auc_score', 'accuracy_score', 'print', 'str', 'roc_curve', 'plt.plot', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.show']",11
data/input/tseries_m5/code/util_model.py:sk_metric_roc_optimal_cutoff,sk_metric_roc_optimal_cutoff,function,13,21,21,236,11.24,0,0,"['ytest', 'ytest_proba']","[None, None]","[None, None]",536,"['    """""" Find the optimal probability cutoff point for a classification model related to event rate\n', '    Parameters\n', '    ----------\n', '    ytest : Matrix with dependent or target data, where rows are observations\n', '    ytest_proba : Matrix with predicted data, where rows are observations\n', '    # Find prediction to the dataframe applying threshold\n', ""    data['pred'] = data['pred_proba'].map(lambda x: 1 if x > threshold else 0)\n"", '    # Print confusion Matrix\n', '    from sklearn.metrics import confusion_matrix\n', ""    confusion_matrix(data['admit'], data['pred'])\n"", '    # array([[175,  98],\n', '    #        [ 46,  81]])\n', '    Returns: with optimal cutoff value\n', '    """"""\n']","['roc_curve', 'np.arange', 'pd.DataFrame', 'pd.Series']",4
data/input/tseries_m5/code/util_model.py:sk_metric_roc_auc,sk_metric_roc_auc,function,45,188,132,1791,9.53,2,2,"['y_test', 'ytest_pred', 'ytest_proba']","[None, None, None]","[None, None, None]",560,[],"['print', 'roc_auc_score', 'roc_curve', 'classification_report', 'sk_metric_roc_auc_multiclass', 'dict', 'np.array', 'range', 'list', 'np.concatenate', 'plt.figure', 'plt.plot', 'plt.xlim', 'plt.ylim', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.legend', 'plt.show']",19
data/input/tseries_m5/code/util_model.py:sk_metric_roc_auc_multiclass,sk_metric_roc_auc_multiclass,function,37,140,112,1333,9.52,2,1,"['n_classes', 'y_test', 'y_test_pred', 'y_predict_proba']","[None, None, None, None]","['3', 'None', 'None', 'None']",577,[],"['print', 'dict', 'np.array', 'range', 'list', 'np.concatenate', 'roc_curve', 'plt.figure', 'plt.plot', 'plt.xlim', 'plt.ylim', 'plt.xlabel', 'plt.ylabel', 'plt.title', 'plt.legend', 'plt.show']",16
data/input/tseries_m5/code/util_model.py:sk_model_eval_regression,sk_model_eval_regression,function,10,44,36,538,12.23,0,1,"['clf', 'istrain', 'Xtrain', 'ytrain', 'Xval', 'yval']","[None, None, None, None, None, None]","[None, '1', 'None', 'None', 'None', 'None']",636,[],"['clf.fit', 'print', 'CV_score.mean', 'CV_score.std', 'clf.predict', 'mean_absolute_error']",6
data/input/tseries_m5/code/util_model.py:sk_model_eval_classification,sk_model_eval_classification,function,12,32,29,456,14.25,0,1,"['clf', 'istrain', 'Xtrain', 'ytrain', 'Xtest', 'ytest']","[None, None, None, None, None, None]","[None, '1', 'None', 'None', 'None', 'None']",656,[],"['print', 'clf.fit', 'clf.predict_proba', 'clf.predict', 'sk_showmetrics']",5
data/input/tseries_m5/code/util_model.py:sk_metrics_eval,sk_metrics_eval,function,14,34,28,317,9.32,2,0,"['clf', 'Xtest', 'ytest', 'cv', 'metrics', '""accuracy""', '""precision_macro""', '""recall_macro""]']","[None, None, None, None, None, None, None, None]","[None, None, None, '1', '[""f1_macro""', None, None, None]",672,[],"['cross_val_score', 'enumerate', 'entries.append', 'pd.DataFrame']",4
data/input/tseries_m5/code/util_model.py:sk_model_eval,sk_model_eval,function,17,82,65,1089,13.28,0,2,"['clf', 'istrain', 'Xtrain', 'ytrain', 'Xval', 'yval']","[None, None, None, None, None, None]","[None, '1', 'None', 'None', 'None', 'None']",684,"['    """"""\n', '       Feature importance with colname\n', '    :param clf:  model or colnum with weights\n', '    :param colname:\n', '    :return:\n', '    """"""\n']","['clf.fit', 'print', 'CV_score.mean', 'CV_score.std', 'clf.predict', 'mean_absolute_error', 'sk_model_eval_classification', 'clf.predict_proba', 'sk_showmetrics']",9
data/input/tseries_m5/code/util_model.py:sk_feature_impt,sk_feature_impt,function,18,50,43,583,11.66,1,2,"['clf', 'colname', 'model_type']","[None, None, None]","[None, None, '""logistic""']",696,"['    """"""\n', '       Feature importance with colname\n', '    :param clf:  model or colnum with weights\n', '    :param colname:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 'np.abs', 'np.arange', 'len', 'isinstance', 'np.argsort', 'range']",7
data/input/tseries_m5/code/util_model.py:sk_feature_selection,sk_feature_selection,function,25,48,38,422,8.79,1,3,"['clf', 'method', 'colname', 'kbest', 'Xtrain', 'ytrain']","[None, None, None, None, None, None]","[None, '""f_classif""', 'None', '50', 'None', 'None']",726,[],"['SelectKBest', 'clf_best.get_support', 'zip', 'new_features.append']",4
data/input/tseries_m5/code/util_model.py:sk_feature_evaluation,sk_feature_evaluation,function,28,63,58,696,11.05,2,1,"['clf', 'df', 'kbest', 'colname_best', 'dfy']","[None, None, None, None, None]","[None, None, '30', 'None', 'None']",744,[],"['copy.deepcopy', 'train_test_split', 'print', 'range', 'len', 'clf.fit', 'clf.predict_proba', 'clf.predict', 'sk_showmetrics', 'pd.DataFrame']",10
data/input/tseries_m5/code/util_model.py:sk_feature_prior_shift,sk_feature_prior_shift,function,0,0,0,0,0.0,0,0,[],[],[],777,"['    """"""\n', '     Label is drifting\n', '    https://dkopczyk.quantee.co.uk/covariate_shift/\n', '    Parameters\n', '    ----------\n', '    df : TYPE\n', '        DESCRIPTION.\n', '    Returns\n', '    -------\n', '    None.\n', '    """"""\n']",[],0
data/input/tseries_m5/code/util_model.py:sk_feature_concept_shift,sk_feature_concept_shift,function,0,1,1,4,4.0,0,0,['df'],[None],[None],792,"['    """"""\n', '       (X,y) distribution relation is shifting.\n', '    https://dkopczyk.quantee.co.uk/covariate_shift/\n', '    Parameters\n', '    ----------\n', '    df : TYPE\n', '        DESCRIPTION.\n', '    Returns\n', '    -------\n', '    None.\n', '    """"""\n']",[],0
data/input/tseries_m5/code/util_model.py:sk_feature_covariate_shift,sk_feature_covariate_shift,function,25,58,49,582,10.03,1,3,"['dftrain', 'dftest', 'colname', 'nsample']","[None, None, None, None]","[None, None, None, '10000']",807,"['    """"""\n', '      X is drifting\n', '    Parameters\n', '    ----------\n', '    dftrain : TYPE\n', '        DESCRIPTION.\n', '    dftest : TYPE\n', '        DESCRIPTION.\n', '    colname : TYPE\n', '        DESCRIPTION.\n', '    nsample : TYPE, optional\n', '        DESCRIPTION. The default is 10000.\n', '    Returns\n', '    -------\n', '    drop_list : TYPE\n', '        DESCRIPTION.\n', '    """"""\n']","['len', 'train.append', 'combi.drop', 'RandomForestClassifier', 'cross_val_score', 'pd.DataFrame', 'np.mean', 'drop_list.append', 'print']",9
data/input/tseries_m5/code/util_model.py:sk_model_eval_classification_cv,sk_model_eval_classification_cv,function,25,62,43,570,9.19,2,1,"['clf', 'X', 'y', 'test_size', 'ncv', 'method']","[None, None, None, None, None, None]","[None, None, None, '0.5', '1', '""random""']",853,"['    """"""\n', '    :param clf:\n', '    :param X:\n', '    :param y:\n', '    :param test_size:\n', '    :param ncv:\n', '    :param method:\n', '    :return:\n', '    """"""\n']","['StratifiedKFold', 'enumerate', 'print', 'sk_model_eval_classification', 'range', 'train_test_split']",6
data/input/tseries_m5/code/util_model.py:dict2,dict2,class,3,5,5,36,7.2,0,0,[],[],[],93,[],[],0
data/input/tseries_m5/code/util_model.py:model_template1,model_template1,class,41,135,81,1190,8.81,0,3,[],[],[],398,[],[],0
data/input/tseries_m5/code/util_model.py:dict2:__init__,dict2:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",94,[],[],0
data/input/tseries_m5/code/util_model.py:model_template1:__init__,model_template1:__init__,method,13,16,15,166,10.38,0,0,"['self', 'alpha', 'low_y_cut', 'high_y_cut', 'ww0']","[None, None, None, None, None]","[None, '0.5', '-0.09', '0.09', '0.95']",399,[],['Ridge'],1
data/input/tseries_m5/code/util_model.py:model_template1:fit,model_template1:fit,method,19,44,37,466,10.59,0,1,"['self', 'X', 'Y']","[None, None, None]","[None, None, 'None']",406,[],"['len', 'print', 'np.median']",3
data/input/tseries_m5/code/util_model.py:model_template1:predict,model_template1:predict,method,8,25,17,165,6.6,0,1,"['self', 'X', 'y', 'ymedian']","[None, None, None, None]","[None, None, 'None', 'None']",425,[],['Y.clip'],1
data/input/tseries_m5/code/util_model.py:model_template1:score,model_template1:score,method,12,30,23,215,7.17,0,1,"['self', 'X', 'Ytrue', 'ymedian']","[None, None, None, None]","[None, None, 'None', 'None']",437,[],"['Y.clip', 'r2_score']",2
data/input/tseries_m5_gluon/gluon/cov19_forecast_mlmodels.py:min_max_scale,min_max_scale,function,9,16,15,120,7.5,1,0,['lst'],[None],[None],130,[],"['min', 'max', 'range', 'new.append']",4
data/input/tseries_m5_gluon/gluon/cov19_forecast_mlmodels.py:gluonts_create_dataset,gluonts_create_dataset,function,34,117,64,820,7.01,1,6,"['timeseries_list', 'start_dates_list', 'feat_dynamic_list', 'feat_static_list', 'feat_static_real_list', 'freq']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', '""D""']",216,[],"['len', 'zip', 'target.tolist', 'fdr.tolist', 'train_ds.append']",5
data/input/tseries_m5_gluon/gluon/cov19_forecast_mlmodels.py:to_gluonts_json,to_gluonts_json,function,12,23,23,258,11.22,1,0,"['path', 'data']","['Path', ' List[Dict]']","[None, None]",256,[],"['print', 'os.makedirs', 'open', 'fp.write']",4
data/input/tseries_m5_gluon/gluon/cov19_forecast_mlmodels.py:gluonts_to_pandas,gluonts_to_pandas,function,71,238,139,2740,11.51,5,15,"['dataset_path', 'data_type']","[None, None]","[None, None]",355,[],"['load_datasets', 'deepcopy', 'json.load', 'k.lstrip', 'd.items', 'print', 'np.transpose', 'all_static_Real.append', 'all_static.append', 'all_targets.append', 'len', 'df_static_real.astype', 'df_dynamic.rename', 'df_dynamic.astype', 'df_timeseries.rename', 'df_timeseries.astype', 'df_static.rename', 'df_static.astype']",18
data/input/tseries_m5_gluon/gluon/cov19_forecast_mlmodels.py:pd_difference,pd_difference,function,23,77,62,799,10.38,0,5,"['df1', 'df2', 'is_real']","[None, None, None]","[None, None, 'False']",471,"['  """"""Identify differences between two pandas DataFrames\n', '    \n', '  """"""\n']","['print', 'any', 'df2.astype', 'df1.equals', 'df2.isnull', 'diff_mask.stack', 'np.where', 'pd.DataFrame']",8
data/input/tseries_m5_gluon/gluon/cov19_forecast_mlmodels.py:gluonts_json_check,gluonts_json_check,function,8,23,18,390,16.96,0,0,[],[],[],508,[],"['Path', 'gluonts_to_pandas', 'print']",3
data/input/tseries_m5_gluon/gluon/cov19_forecast_mlmodels.py:CustomEvaluator,CustomEvaluator,class,23,46,38,710,15.43,0,0,[],[],[],753,[],[],0
data/input/tseries_m5_gluon/gluon/cov19_forecast_mlmodels.py:CustomEvaluator:get_metrics_per_ts,CustomEvaluator:get_metrics_per_ts,method,15,29,23,460,15.86,0,0,"['self', 'time_series', 'forecast']","[None, None, None]","[None, None, None]",755,[],"['np.diff', 'np.mean', 'super']",3
data/input/tseries_m5_gluon/gluon/cov19_forecast_mlmodels.py:CustomEvaluator:get_aggregate_metrics,CustomEvaluator:get_aggregate_metrics,method,8,10,9,153,15.3,0,0,"['self', 'metric_per_ts']","[None, None]","[None, None]",768,[],['super'],1
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:generate_dataframes,generate_dataframes,function,18,35,30,522,14.91,1,0,"['dataset_path', 'index']","[None, None]","['None', 'None']",52,[],"['np.transpose', 'all_dynamic.append', 'print']",3
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:gluonts_create_dynamic,gluonts_create_dynamic,function,2,6,6,65,10.83,0,0,"['df_dynamic', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, 'True', '28', '10', '1', '1']",117,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']",[],0
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:gluonts_create_static,gluonts_create_static,function,17,24,23,473,19.71,1,0,"['df_static', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, '1', '28', '10', '1', '1']",143,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']","['static_cat_list.append', 'np.unique', 'static_cat_cardinalities.append', 'np.concatenate', 'static_cat.reshape', 'len']",6
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:gluonts_static_cardinalities,gluonts_static_cardinalities,function,11,17,17,284,16.71,1,0,"['df_static', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, '1', '28', '10', '1', '1']",171,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']","['np.unique', 'static_cat_cardinalities.append']",2
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:gluonts_create_timeseries,gluonts_create_timeseries,function,9,25,18,332,13.28,1,1,"['df_timeseries', 'submission', 'single_pred_length', 'submission_pred_length', 'n_timeseries', 'transpose']","[None, None, None, None, None, None]","[None, '1', '28', '10', '1', '1']",195,"['    """"""\n', '        N_cat x N-timseries\n', '    """"""\n']","['np.ones', 'train_target_values.copy']",2
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:create_startdate,create_startdate,function,4,8,7,71,8.88,1,0,"['date', 'freq', 'n_timeseries']","[None, None, None]","['""2011-01-29""', '""1D""', '1']",214,[],['range'],1
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:gluonts_create_dataset,gluonts_create_dataset,function,9,60,46,471,7.85,0,0,"['train_timeseries_list', 'start_dates_list', 'train_dynamic_list', 'train_static_list', 'freq']","[None, None, None, None, None]","[None, None, None, None, '""D""']",219,[],"['target.tolist', 'fdr.tolist', 'fsc.tolist', 'zip']",4
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:gluonts_save_to_file,gluonts_save_to_file,function,12,23,23,258,11.22,1,0,"['path', 'data']","['Path', ' List[Dict]']","[None, None]",238,[],"['print', 'os.makedirs', 'open', 'fp.write']",4
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:pandas_to_gluonts_multiseries,pandas_to_gluonts_multiseries,function,31,90,67,1532,17.02,0,2,"['df_timeseries', 'df_dynamic', 'df_static', ""pars={'submission'"", ""'single_pred_length'"", ""'submission_pred_length'"", ""'n_timeseries'"", ""'start_date'"", ""'freq'"", 'path_save', 'return_df']","[None, None, None, '', '28', '10', '1', '""2011-01-29""', '""D""}', None, None]","[None, None, None, ""{'submission':True"", None, None, None, None, None, 'None', 'False']",250,[],"['gluonts_create_dynamic', 'gluonts_create_static', 'gluonts_create_timeseries', 'create_startdate', 'gluonts_create_dataset', 'gluonts_save_to_file', 'open', 'f.write', 'json.dumps']",9
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:cols_remove,cols_remove,function,4,13,10,37,2.85,1,1,"['col1', 'col_remove']","[None, None]","[None, None]",300,[],[],0
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:create_timeseries1d,create_timeseries1d,function,16,25,23,279,11.16,1,0,"['start_date ', 'end_date ', 'periods ', 'freq ', 'weight ', 't', 'expression ']","[None, None, None, None, None, None, None]","["" '1/1/2000'"", ' None', ' 720', "" 'D'"", ' 1', 'None', "" 'math.cos(index""]",421,[],"['pd.date_range', 'pd.DataFrame', 'range', 'data.append', 'pd.to_datetime']",5
data/input/tseries_m5_gluon/gluon/synthetic_ts_with_deepar.py:plot_prob_forecasts,plot_prob_forecasts,function,12,35,35,387,11.06,0,0,"['ts_entry', 'forecast_entry']","[None, None]","[None, None]",522,[],"['plt.subplots', 'forecast_entry.plot', 'plt.grid', 'plt.legend', 'plt.show']",5
data/input/tseries_m5_gluon/gluon/util_armdn.py:timeseries_to_supervised,timeseries_to_supervised,function,7,12,11,151,12.58,0,0,"['data', 'lag']","[None, None]","[None, '1']",38,[],"['pd.DataFrame', 'range', 'pd.concat']",3
data/input/tseries_m5_gluon/gluon/util_armdn.py:difference,difference,function,10,10,10,131,13.1,1,0,"['dataset', 'interval']","[None, None]","[None, '1']",47,[],"['list', 'range', 'len', 'pd.Series']",4
data/input/tseries_m5_gluon/gluon/util_armdn.py:inverse_difference,inverse_difference,function,3,3,3,29,9.67,0,0,"['history', 'yhat', 'interval']","[None, None, None]","[None, None, '1']",55,[],[],0
data/input/tseries_m5_gluon/gluon/util_armdn.py:scale,scale,function,13,13,13,280,21.54,0,0,"['train', 'test']","[None, None]","[None, None]",59,[],"['MinMaxScaler', 'scaler.fit', 'train.reshape', 'scaler.transform', 'test.reshape']",5
data/input/tseries_m5_gluon/gluon/util_armdn.py:invert_scale,invert_scale,function,10,13,13,150,11.54,1,0,"['scaler', 'X', 'value']","[None, None, None]","[None, None, None]",72,[],"['np.array', 'array.reshape', 'len', 'scaler.inverse_transform']",4
data/input/tseries_m5_gluon/gluon/util_armdn.py:fit_lstm,fit_lstm,function,18,42,40,566,13.48,1,0,"['train', 'batch_size', 'nb_epoch', 'lstm_neurons', 'timesteps', 'dense_neurons', 'mdn_output', 'mdn_Nmixes']","[None, None, None, None, None, None, None, None]","[None, '1', '5', '1', '1', '1', '1', '1']",80,[],"['X.reshape', 'Sequential', 'model.add', 'Adam', 'model.compile', 'print', 'range', 'model.fit', 'model.reset_states']",9
data/input/tseries_m5_gluon/gluon/util_armdn.py:forecast_lstm,forecast_lstm,function,6,7,7,93,13.29,0,0,"['model', 'batch_size', 'timesteps', 'X']","[None, None, None, None]","[None, None, None, None]",99,[],"['X.reshape', 'len', 'model.predict']",3
data/input/tseries_m5_gluon/gluon/util_armdn.py:test_supervised,test_supervised,function,0,1,1,4,4.0,0,0,[],[],[],110,[],[],0
data/input/tseries_m5_gluon/gluon/util_armdn.py:myfun,myfun,function,1,4,4,32,8.0,0,0,['t'],[None],[None],126,[],[],0
data/input/tseries_m5_gluon/gluon/util_keras.py:timeseries_to_supervised,timeseries_to_supervised,function,7,12,11,151,12.58,0,0,"['data', 'lag']","[None, None]","[None, '1']",12,[],"['pd.DataFrame', 'range', 'pd.concat']",3
data/input/tseries_m5_gluon/gluon/util_keras.py:difference,difference,function,10,10,10,131,13.1,1,0,"['dataset', 'interval']","[None, None]","[None, '1']",21,[],"['list', 'range', 'len', 'pd.Series']",4
data/input/tseries_m5_gluon/gluon/util_keras.py:inverse_difference,inverse_difference,function,3,3,3,29,9.67,0,0,"['history', 'yhat', 'interval']","[None, None, None]","[None, None, '1']",29,[],[],0
data/input/tseries_m5_gluon/gluon/util_keras.py:scale,scale,function,13,13,13,280,21.54,0,0,"['train', 'test']","[None, None]","[None, None]",33,[],"['MinMaxScaler', 'scaler.fit', 'train.reshape', 'scaler.transform', 'test.reshape']",5
data/input/tseries_m5_gluon/gluon/util_keras.py:invert_scale,invert_scale,function,10,13,13,150,11.54,1,0,"['scaler', 'X', 'value']","[None, None, None]","[None, None, None]",46,[],"['np.array', 'array.reshape', 'len', 'scaler.inverse_transform']",4
data/input/tseries_m5_gluon/gluon/util_keras.py:fit_lstm,fit_lstm,function,18,42,40,566,13.48,1,0,"['train', 'batch_size', 'nb_epoch', 'lstm_neurons', 'timesteps', 'dense_neurons', 'mdn_output', 'mdn_Nmixes']","[None, None, None, None, None, None, None, None]","[None, '1', '5', '1', '1', '1', '1', '1']",57,[],"['X.reshape', 'Sequential', 'model.add', 'Adam', 'model.compile', 'print', 'range', 'model.fit', 'model.reset_states']",9
data/input/tseries_m5_gluon/gluon/util_keras.py:forecast_lstm,forecast_lstm,function,6,7,7,93,13.29,0,0,"['model', 'batch_size', 'timesteps', 'X']","[None, None, None, None]","[None, None, None, None]",76,[],"['X.reshape', 'len', 'model.predict']",3
data/input/tseries_m5_gluon/gluon/util_keras.py:zgenerate_pivot_unit,zgenerate_pivot_unit,function,19,64,41,497,7.77,1,1,"['df', 'keyref', '**kw']","[None, None, None]","[None, 'None', None]",85,"['    """"""item_id X Date  : Unit\n', '       generate__file(    )\n', '    """"""\n']","['df.pivot_table', 'gc.collect', 'dfp.join', 'key_all.set_index', 'str']",5
data/input/tseries_m5_gluon/gluon/util_keras.py:zgenerate_pivot_gluonts,zgenerate_pivot_gluonts,function,48,184,136,1672,9.09,3,0,"['path_input', 'path_export', 'folder_list', 'cols', 'prefix_col', 'prefix_file', 'shop_list', '17]', 'verbose', '**kw']","[None, None, None, None, None, None, None, None, None, None]","['""/data/pos/""', 'None', 'None', 'None', '""""', '""pivot-gluonts""', '[16', None, '1', None]",109,[],"['isint', 'int', 'enumerate', 'log', 'pd_read_file2', 'log_pd', 'dfp2.join', 'dfpi.set_index', 'gc.collect', 'sorted', 'pd.DataFrame', 'from_timekey', 'generate_X_date', 'dfp.reset_index', 'pd_to_file', 'str', 'len', 'pandas_to_gluonts_multiseries']",18
data/input/tseries_online/zold/clean2.py:generate_train,generate_train,function,36,307,99,3917,12.76,2,0,"['df', 'col', 'pars']","[None, None, None]","[None, 'None', 'None']",8,[],"['copy.deepcopy', 'df.drop', 'pd.to_datetime', 'zip', 'print', 'float', 'len']",7
data/input/tseries_online/zold/clean2.py:generateAggregateFeatures,generateAggregateFeatures,function,13,75,59,740,9.87,1,0,['df'],[None],[None],165,[],"['pd.DataFrame', 'float', 'len', 'print', 'df2.merge', 'gc.collect']",6
data/input/tseries_online/zold/clean2.py:generatePastClickFeatures,generatePastClickFeatures,function,8,28,24,357,12.75,1,0,['df'],[None],[None],192,[],"['pd.DataFrame', 'x.diff']",2
source/bin/reinforcement_feature_CAFEM/src/cafem.py:generate_trajectories,generate_trajectories,function,42,88,78,1055,11.99,3,2,['task'],[None],[None],63,[],"['load', 'Env', 'Model', 'tf.ConfigProto', 'tf.Session', 'localsess.run', 'range', 'print', 'env.reset', 'np.copy', 'ma.masked_array', 'env.step', 'tmp_buffer.append']",13
source/bin/reinforcement_feature_CAFEM/src/cafem.py:main,main,function,83,201,153,2695,13.41,7,1,[],[],[],112,[],"['Model', 'Tasks', 'tf.Session', 'sess.run', 'saver.save', 'tf.trainable_variables', 'updateTargetGraph', 'tqdm', 'tasks.sample', 'Pool', 'np.array', 'pool.close', 'pool.join', 'enumerate', 'generate_trajectories', 'task_buff.sample', 'np.split', 'np.reshape', 'np.clip', 'inputsa.append', 'labela.append', 'inputsb.append', 'labelb.append', 'actiona.append', 'actionb.append', 'print', 'updateTarget', 'open', 'f.write', 'f.close']",30
source/bin/reinforcement_feature_CAFEM/src/cafem.py:Tasks,Tasks,class,27,51,43,612,12.0,3,2,[],[],[],39,[],[],0
source/bin/reinforcement_feature_CAFEM/src/cafem.py:Tasks:__init__,Tasks:__init__,method,22,42,36,501,11.93,3,2,"['self', 'datasetids', 'buffer_size']","[None, None, None]","[None, None, '100']",40,[],"['pd.read_csv', 'open', 'tqdm', 'load', 'enumerate', 'f.write', 'f.close']",7
source/bin/reinforcement_feature_CAFEM/src/cafem.py:Tasks:sample,Tasks:sample,method,3,4,3,45,11.25,0,0,"['self', 'n']","[None, None]","[None, None]",58,[],['random.sample'],1
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:one_mse_func,one_mse_func,function,9,15,13,230,15.33,0,0,[],[],[],26,[],"['one_relative_abs', 'mean_absolute_error', 'np.mean', 'np.abs', 'make_scorer']",5
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:load,load,function,24,38,33,411,10.82,1,3,['f_path'],[None],[None],108,[],"['LabelEncoder', 'loadarff', 'np.array', 'meta.names', 'meta.types', 'enumerate', 'le.fit_transform', 'dataset.astype']",8
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:updateTargetGraph,updateTargetGraph,function,7,25,20,215,8.6,1,0,"['tfVars', 'tau']","[None, None]","[None, None]",794,[],"['len', 'enumerate', 'op_holder.append']",3
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:updateTarget,updateTarget,function,10,32,26,279,8.72,2,0,"['tfVars', 'tau']","[None, None]","[None, None]",802,[],"['len', 'enumerate', 'op_holder.append', 'updateTarget', 'sess.run']",5
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:performance,performance,function,20,43,36,459,10.67,3,0,"['did', 'maxfeat', 'step']","[None, None, None]","[None, None, None]",969,[],"['load', 'Env', 'range', 'pd.read_csv', 'actions.append', 'trfs.append', 'env.batch_perform']",7
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Evaluater,Evaluater,class,63,147,105,1785,12.14,1,6,[],[],[],36,[],[],0
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Env,Env,class,191,927,394,10896,11.75,19,47,[],[],[],131,[],[],0
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Buffer,Buffer,class,12,32,25,438,13.69,0,2,[],[],[],574,[],[],0
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Model,Model,class,137,468,288,6412,13.7,7,3,[],[],[],593,[],[],0
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Evaluater:__init__,Evaluater:__init__,method,19,52,33,727,13.98,0,4,"['self', 'cv', 'stratified', 'n_jobs', 'tasktype', 'evaluatertype', 'n_estimators', '100000))']","[None, None, None, None, None, None, None, '']","[None, '5', 'True', '1', '""C""', '""rf""', '20', None]",38,[],"['StratifiedKFold', 'KFold', 'RandomForestClassifier', 'RandomForestRegressor', 'LogisticRegression', 'Lasso']",6
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Evaluater:CV,Evaluater:CV,method,29,46,42,524,11.39,1,1,"['self', 'X', 'y']","[None, None, None]","[None, None, None]",66,[],"['np.nan_to_num', 'np.clip', 'one_mse_func', 'cross_val_score', 'abs', 'CV2', 'res.append', 'np.array']",8
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Evaluater:CV2,Evaluater:CV2,method,19,26,25,306,11.77,1,0,"['self', 'X', 'y']","[None, None, None]","[None, None, None]",78,[],"['res.append', 'np.array']",2
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Evaluater:metrics,Evaluater:metrics,method,15,32,26,346,10.81,0,1,"['self', 'y_true', 'y_pred']","[None, None, None]","[None, None, None]",95,[],"['f1_score', 'roc_auc_score', 'log_loss', 'mean_absolute_error', 'np.mean', 'mean_squared_error']",6
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Env:__init__,Env:__init__,method,46,82,71,1211,14.77,3,4,"['self', 'dataset', 'feature', 'globalreward', 'maxdepth', 'evalcount', 'binsize', 'opt_type', 'tasktype', 'evaluatertype', '\\100000)', 'historysize', 'pretransform', 'n_jobs=1)']","[None, None, None, None, None, None, None, None, None, None, None, None, None, '']","[None, None, None, 'True', '5', '10', '100', ""'o1'"", '""C""', ""'rf'"", None, '5', 'None', '1):']",132,[],"['np.array', 'len', 'range', 'print', 'self.fe', 'value.append', 'Evaluater', 'np.copy', 'self._init']",9
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Env:_init,Env:_init,method,24,40,37,735,18.38,0,0,['self'],[None],[None],185,[],"['np.copy', 'self._QSA', 'np.concatenate', 'np.array']",4
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Env:node2root,Env:node2root,method,8,17,15,188,11.06,2,0,"['self', 'adict', 'node']","[None, None, None]","[None, None, None]",214,[],"['apath.append', 'range']",2
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Env:step,Env:step,method,100,399,218,4709,11.8,7,23,"['self', 'action']","[None, None]","[None, None]",223,[],"['set', 'getattr', 'feature.min', 'np.log', 'np.sqrt', 'MinMaxScaler', 'mmn.fit_transform', 'np.var', 'stats.zscore', 'np.nan_to_num', 'np.clip', 'math.sqrt', 'np.insert', 'self.node2root', 'np.concatenate', 'len', 'np.copy', 'self._QSA', 'range', 'abs', 'np.array', 'allperf.argmax', 'allperf.max']",23
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Env:_QSA,Env:_QSA,method,27,146,60,1570,10.75,1,7,['self'],[None],[None],423,[],"['np.median', 'feat_0.min', 'abs', 'np.arange', 'np.bincount', 'len', 'feat_1.min', 'np.concatenate', 'range', 'feat_0.max', 'feat_1.max', 'QSA.append']",12
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Env:reset,Env:reset,method,5,5,5,75,15.0,0,0,['self'],[None],[None],477,[],['self._init'],1
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Env:fe,Env:fe,method,39,221,104,2054,9.29,6,13,"['self', 'operators', 'feat_id']","[None, None, None]","[None, None, None]",481,[],"['type', 'set', 'getattr', 'feature.min', 'np.log', 'np.sqrt', 'MinMaxScaler', 'mmn.fit_transform', 'np.var', 'stats.zscore', 'len', 'np.nan_to_num', 'np.clip', 'math.sqrt', 'np.insert', 'np.delete', 'range']",17
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Buffer:__init__,Buffer:__init__,method,3,4,4,43,10.75,0,0,"['self', 'buffer_size ']","[None, None]","[None, ' 50000']",575,[],[],0
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Buffer:add,Buffer:add,method,3,8,8,129,16.12,0,1,"['self', 'experience']","[None, None]","[None, None]",579,[],['len'],1
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Buffer:sample,Buffer:sample,method,5,11,10,180,16.36,0,1,"['self', 'size']","[None, None]","[None, None]",584,[],"['len', 'np.copy']",2
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Model:__init__,Model:__init__,method,35,60,43,1002,16.7,0,1,"['self', 'opt_size', 'input_size', 'name', 'meta', 'update_lr', 'meta_lr', 'num_updates', 'maml', 'qsasize']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, 'False', '1e-3', '0.001', '1', 'True', '200']",594,[],"['tf.placeholder', 'self.construct_fc_weights', 'self.network', 'self.construct_model', 'tf.global_variables_initializer']",5
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Model:mse,Model:mse,method,2,3,3,45,15.0,0,0,"['self', 'y_pred', 'y_true']","[None, None, None]","[None, None, None]",623,[],['tf.reduce_sum'],1
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Model:construct_fc_weights,Model:construct_fc_weights,method,14,83,55,1161,13.99,1,1,['self'],[None],[None],626,[],"['tf.Variable', 'range', 'len', 'str', 'tf.truncated_normal']",5
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Model:forward,Model:forward,method,14,114,47,1237,10.85,3,1,"['self', 'inp', 'weights', 'reuse']","[None, None, None, None]","[None, None, None, 'False']",656,[],"['normalize', 'range', 'len', 'str', 'scope=str', 'tf.matmul']",6
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Model:L2loss,Model:L2loss,method,5,11,9,97,8.82,1,0,"['self', 'weights', 'reg']","[None, None, None]","[None, None, None]",690,[],[],0
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Model:network,Model:network,method,12,19,19,376,19.79,0,0,['self'],[None],[None],696,[],"['self.forward', 'tf.one_hot', 'tf.reduce_sum', 'self.loss_func']",4
source/bin/reinforcement_feature_CAFEM/src/MLFE.py:Model:construct_model,Model:construct_model,method,67,155,129,2194,14.15,2,0,['self'],[None],[None],707,[],"['tf.variable_scope', 'training_scope.reuse_variables', 'forward_Q', 'tf.one_hot', 'tf.reduce_sum', 'task_metalearn', 'self.loss_func', 'tf.gradients', 'list', 'dict', 'zip', 'weights.keys', 'task_outputbs.append', 'task_lossesb.append', 'range', 'fast_weights.keys', 'tf.map_fn', 'tf.to_float', 'optimizer.compute_gradients', 'optimizer.apply_gradients']",20
source/bin/reinforcement_feature_CAFEM/src/single_afem.py:simulate,simulate,function,35,60,54,956,15.93,1,4,['inp'],[None],[None],8,[],"['Model', 'tf.ConfigProto', 'tf.Session', 'saver.restore', 'env.reset', 'range', 'np.copy', 'localsess.run', 'ma.masked_array', 'env.step', 'tmp_buffer.append', 'localsess.close']",12
source/bin/reinforcement_feature_CAFEM/src/single_afem.py:main,main,function,150,493,333,5851,11.87,7,15,[],[],[],55,[],"['os.mkdir', 'load', 'Model', 'tf.ConfigProto', 'tf.Session', 'tqdm', 'Buffer', 'len', 'print', 'saver.restore', 'saver.save', 'sess.run', 'Env', 'open', 'f.write', 'f.close', 'tf.trainable_variables', 'updateTargetGraph', 'Pool', 'pool.map', 'range', 'pool.close', 'pool.join', 'buff.add', 'best_seq.append', 'best_pfm.append', 'simulate', 'buff.sample', 'np.split', 'np.array', 'np.reshape', 'enumerate', 'np.clip', 'mean_loss.append', 'updateTarget', 'np.around', 'ma.masked_array', 'pretransform.append', 'max', 'np.copy', 'env.step', 'pretransform_test.append', 'test_pfm.append']",43
source/bin/reinforcement_feature_CAFEM/src/utils.py:normalize,normalize,function,2,2,2,21,10.5,0,0,"['inp', 'activation', 'reuse', 'scope', 'norm']","[None, None, None, None, None]","[None, None, None, None, None]",22,[],['activation'],1
source/bin/reinforcement_feature_CAFEM/src/utils.py:load_pretransform,load_pretransform,function,12,19,16,445,23.42,0,0,['fdir'],[None],[None],34,[],"['pd.read_csv', 'transform.fillna', 'np.argmax', 'print', 'int']",5
source/bin/reinforcement_feature_CAFEM/src/utils.py:get_result,get_result,function,17,36,31,742,20.61,1,1,"['mark', 'did', 'plot']","[None, None, None]","[None, None, 'True']",47,[],"['pd.read_csv', 'score_te.drop_duplicates', 'res.append', 'plt.plot', 'plt.show', 'print', 'pd.DataFrame']",7
source/bin/reinforcement_feature_CAFEM/src/utils.py:plot,plot,function,42,133,106,1586,11.92,3,1,"['fpath1', 'fpath2', 'size', 'name']","[None, None, None, None]","[None, None, '30', ""''""]",70,[],"['pd.read_csv', 'plt.figure', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'plt.xticks', 'plt.plot', 'len', 'range', 'plt.legend', 'plt.show', 'plot2', 'np.arange', 'plt.subplots', 'ax.bar', 'plt.yticks', 'zip', 'plt.text', 'ax.set_ylabel', 'ax.set_title', 'ax.legend']",21
source/bin/reinforcement_feature_CAFEM/src/utils.py:plot2,plot2,function,29,96,77,959,9.99,2,0,[],[],[],90,[],"['np.arange', 'plt.subplots', 'ax.bar', 'plt.yticks', 'zip', 'plt.text', 'ax.set_ylabel', 'ax.set_title', 'plt.xticks', 'ax.legend', 'plt.show']",11
source/models/repo/autorec/edlae_rec.py:get_count,get_count,function,5,6,5,85,14.17,0,0,"['tp', 'id']","[None, None]","[None, None]",71,[],['playcount_groupbyid.size'],1
source/models/repo/autorec/edlae_rec.py:filter_triplets,filter_triplets,function,11,31,21,300,9.68,0,2,"['tp', 'min_uc', 'min_sc']","[None, None, None]","[None, '5', '0']",76,[],['get_count'],1
source/models/repo/autorec/edlae_rec.py:split_train_test_proportion,split_train_test_proportion,function,20,48,44,548,11.42,1,2,"['data', 'test_prop']","[None, None]","[None, '0.2']",131,[],"['data.groupby', 'list', 'enumerate', 'len', 'np.zeros', 'size=int', 'tr_list.append', 'te_list.append', 'print', 'pd.concat']",10
source/models/repo/autorec/edlae_rec.py:numerize,numerize,function,5,17,15,163,9.59,0,0,['tp'],[None],[None],170,[],"['map', 'pd.DataFrame', 'list']",3
source/models/repo/autorec/edlae_rec.py:load_train_data,load_train_data,function,11,18,17,190,10.56,0,0,['csv_file'],[None],[None],208,[],"['pd.read_csv', 'sparse.csr_matrix']",2
source/models/repo/autorec/edlae_rec.py:load_tr_te_data,load_tr_te_data,function,18,41,33,535,13.05,0,0,"['csv_file_tr', 'csv_file_te']","[None, None]","[None, None]",226,[],"['pd.read_csv', 'min', 'max', 'sparse.csr_matrix']",4
source/models/repo/autorec/edlae_rec.py:NDCG_binary_at_k_batch,NDCG_binary_at_k_batch,function,21,38,34,474,12.47,1,0,"['X_pred', 'heldout_batch', 'k']","[None, None, None]","[None, None, '100']",252,"[""    '''\n"", '    normalized discounted cumulative gain@k for binary relevance\n', ""    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n"", ""    '''\n""]","['bn.argpartition', 'np.argsort', 'np.log2', 'np.array', 'heldout_batch.getnnz']",5
source/models/repo/autorec/edlae_rec.py:Recall_at_k_batch,Recall_at_k_batch,function,13,28,26,367,13.11,0,0,"['X_pred', 'heldout_batch', 'k']","[None, None, None]","[None, None, '100']",274,[],"['bn.argpartition', 'np.zeros_like', 'np.minimum', 'X_true_binary.sum']",4
source/models/repo/autorec/edlae_rec.py:evaluate,evaluate,function,28,73,61,1067,14.62,1,1,"['BB', 'test_data_tr ', 'test_data_te ']","[None, None, None]","[None, ' test_data_tr', ' test_data_te']",287,[],"['print', 'range', 'enumerate', 'min', 'sparse.isspmatrix', 'X.toarray', 'X.astype', 'X.dot', 'n100_list.append', 'r20_list.append', 'r50_list.append', 'np.concatenate', 'np.std', 'np.sqrt']",14
source/models/repo/autorec/edlae_rec.py:learn_EDLAE_fullrank,learn_EDLAE_fullrank,function,9,12,11,118,9.83,0,0,"['pdrop', 'L2const', 'XtX ', 'XtXdiag ', 'iidiag ']","[None, None, None, None, None]","[None, None, ' XtX', ' XtXdiag', ' ii_diag']",328,[],[],0
source/models/repo/autorec/edlae_rec.py:learn_EDLAE_analytic,learn_EDLAE_analytic,function,39,103,69,1048,10.17,1,0,"['pdrop', 'L2const', 'omega', 'hidden_dim', 'train_epochs', 'init_scale ', 'XtX ', 'XtXdiag ', 'iidiag ']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, ' 0.0001', ' XtX', ' XtXdiag', ' ii_diag']",336,[],"['np.zeros', 'range', 'print', 'HH.dot', 'VVt.dot', 'UU.dot', 'np.diag']",7
source/models/repo/autorec/edlae_rec.py:learn_EDLAE_approx,learn_EDLAE_approx,function,33,93,66,877,9.43,1,3,"['zeroDiagConstraint', 'pdrop', 'L2const', 'hidden_dim', 'train_epochs', 'init_scale ', 'XtX ', 'XtXdiag ', 'iidiag ']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, ' 0.0001', ' XtX', ' XtXdiag', ' ii_diag']",382,[],"['print', 'range', 'VVt.dot', 'np.diag', 'HH.dot']",5
source/models/repo/autorec/edlae_rec.py:learn_DAE_stochastic,learn_DAE_stochastic,function,68,151,133,1846,12.23,2,1,"['pdrop', 'L2constAdd', 'L2constProd', 'hidden_dim', 'train_epochs', 'bsize ', 'X ']","[None, None, None, None, None, None, None]","[None, None, None, None, None, '4096', ' train_data']",422,[],"['tf.reset_default_graph', 'tf.placeholder', 'tf.get_variable', 'tf.matmul', 'l2_regularizer', 'apply_regularization', 'tf.reduce_mean', 'tf.square', 'optimizer.minimize', 'np.arange', 'tf.Session', 'tf.global_variables_initializer', 'sess.run', 'list', 'range', 'enumerate', 'min', 'sparse.isspmatrix', 'inp.toarray', 'loss_epoch.append', 'mse_epoch.append', 'loss_list.append', 'mse_list.append', 'print', 'np.sqrt']",25
source/models/repo/autorec/edlae_rec.py:calc_cosineSimilarity,calc_cosineSimilarity,function,15,27,26,311,11.52,1,0,"['EE', 'ixSims']","[None, None]","[None, None]",580,[],"['np.sqrt', 'EEn.dot', 'np.ones', 'range', 'np.median']",5
source/models/repo/autorec/edlae_rec.py:MyClock,MyClock,class,7,21,17,173,8.24,0,0,[],[],[],316,[],[],0
source/models/repo/autorec/edlae_rec.py:MyClock:tic,MyClock:tic,method,2,2,2,26,13.0,0,0,['self'],[None],[None],318,[],['time.time'],1
source/models/repo/autorec/edlae_rec.py:MyClock:toc,MyClock:toc,method,3,13,12,96,7.38,0,0,['self'],[None],[None],320,[],"['time.time', 'print']",2
source/models/repo/autorec/process_ml_20m_data.py:get_count,get_count,function,5,7,6,100,14.29,0,0,"['tp', 'id']","[None, None]","[None, None]",86,[],['playcount_groupbyid.size'],1
source/models/repo/autorec/process_ml_20m_data.py:filter_triplets,filter_triplets,function,11,31,21,325,10.48,0,2,"['tp', 'min_uc', 'min_sc']","[None, None, None]","[None, '5', '0']",91,[],['get_count'],1
source/models/repo/autorec/process_ml_20m_data.py:split_train_test_proportion,split_train_test_proportion,function,20,48,44,548,11.42,1,2,"['data', 'test_prop']","[None, None]","[None, '0.2']",146,[],"['data.groupby', 'list', 'enumerate', 'len', 'np.zeros', 'size=int', 'tr_list.append', 'te_list.append', 'print', 'pd.concat']",10
source/models/repo/autorec/process_ml_20m_data.py:numerize,numerize,function,5,17,15,166,9.76,0,0,['tp'],[None],[None],185,[],"['map', 'pd.DataFrame', 'list']",3
source/models/repo/keras_mmae/bregman_divergences.py:BregmanDivergence,BregmanDivergence,class,10,30,25,371,12.37,0,0,[],[],[],35,[],[],0
source/models/repo/keras_mmae/bregman_divergences.py:GaussianDivergence,GaussianDivergence,class,7,16,12,169,10.56,0,0,[],[],[],80,[],[],0
source/models/repo/keras_mmae/bregman_divergences.py:GammaDivergence,GammaDivergence,class,7,22,14,216,9.82,0,0,[],[],[],103,[],[],0
source/models/repo/keras_mmae/bregman_divergences.py:BernoulliDivergence,BernoulliDivergence,class,8,32,19,281,8.78,0,0,[],[],[],128,[],[],0
source/models/repo/keras_mmae/bregman_divergences.py:PoissonDivergence,PoissonDivergence,class,8,23,14,225,9.78,0,0,[],[],[],153,[],[],0
source/models/repo/keras_mmae/bregman_divergences.py:BinomialDivergence,BinomialDivergence,class,10,35,21,311,8.89,0,0,[],[],[],178,[],[],0
source/models/repo/keras_mmae/bregman_divergences.py:NegativeBinomialDivergence,NegativeBinomialDivergence,class,10,31,20,296,9.55,0,0,[],[],[],210,[],[],0
source/models/repo/keras_mmae/bregman_divergences.py:BregmanDivergence:__init__,BregmanDivergence:__init__,method,7,16,16,209,13.06,0,0,"['self', 'reduction', 'name']","[None, None, None]","[None, 'Reduction.SUM_OVER_BATCH_SIZE', 'None']",49,[],"['bregman_function', 'K.mean', 'self._phi', 'self._phi_gradient', 'super']",5
source/models/repo/keras_mmae/bregman_divergences.py:BregmanDivergence:_phi,BregmanDivergence:_phi,method,0,1,1,4,4.0,0,0,"['self', 'z']","[None, None]","[None, None]",64,"['        """"""\n', '        This is the phi function of the Bregman divergence.\n', '\n', '        """"""\n']",[],0
source/models/repo/keras_mmae/bregman_divergences.py:BregmanDivergence:_phi_gradient,BregmanDivergence:_phi_gradient,method,0,1,1,4,4.0,0,0,"['self', 'z']","[None, None]","[None, None]",72,"['        """"""\n', '        This is the gradient of the phi function of the Bregman divergence.\n', '\n', '        """"""\n']",[],0
source/models/repo/keras_mmae/bregman_divergences.py:GaussianDivergence:__init__,GaussianDivergence:__init__,method,1,2,2,50,25.0,0,0,"['self', 'name']","[None, None]","[None, ""'gaussian_divergence'""]",91,[],['super'],1
source/models/repo/keras_mmae/bregman_divergences.py:GaussianDivergence:_phi,GaussianDivergence:_phi,method,4,8,7,55,6.88,0,0,"['self', 'z']","[None, None]","[None, None]",64,"['        """"""\n', '        This is the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.square', '_phi_gradient']",2
source/models/repo/keras_mmae/bregman_divergences.py:GaussianDivergence:_phi_gradient,GaussianDivergence:_phi_gradient,method,2,2,2,7,3.5,0,0,"['self', 'z']","[None, None]","[None, None]",72,"['        """"""\n', '        This is the gradient of the phi function of the Bregman divergence.\n', '\n', '        """"""\n']",[],0
source/models/repo/keras_mmae/bregman_divergences.py:GammaDivergence:__init__,GammaDivergence:__init__,method,1,2,2,47,23.5,0,0,"['self', 'name']","[None, None]","[None, ""'gamma_divergence'""]",114,[],['super'],1
source/models/repo/keras_mmae/bregman_divergences.py:GammaDivergence:_phi,GammaDivergence:_phi,method,4,14,9,108,7.71,0,0,"['self', 'z']","[None, None]","[None, None]",64,"['        """"""\n', '        This is the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.maximum', 'K.epsilon', '_phi_gradient']",3
source/models/repo/keras_mmae/bregman_divergences.py:GammaDivergence:_phi_gradient,GammaDivergence:_phi_gradient,method,3,6,5,39,6.5,0,0,"['self', 'z']","[None, None]","[None, None]",72,"['        """"""\n', '        This is the gradient of the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.maximum', 'K.epsilon']",2
source/models/repo/keras_mmae/bregman_divergences.py:BernoulliDivergence:__init__,BernoulliDivergence:__init__,method,1,2,2,51,25.5,0,0,"['self', 'name']","[None, None]","[None, ""'bernoulli_divergence'""]",139,[],['super'],1
source/models/repo/keras_mmae/bregman_divergences.py:BernoulliDivergence:_phi,BernoulliDivergence:_phi,method,5,24,14,165,6.88,0,0,"['self', 'z']","[None, None]","[None, None]",64,"['        """"""\n', '        This is the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.clip', 'K.epsilon', 'K.log', '_phi_gradient']",4
source/models/repo/keras_mmae/bregman_divergences.py:BernoulliDivergence:_phi_gradient,BernoulliDivergence:_phi_gradient,method,4,9,9,65,7.22,0,0,"['self', 'z']","[None, None]","[None, None]",72,"['        """"""\n', '        This is the gradient of the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.clip', 'K.epsilon', 'K.log']",3
source/models/repo/keras_mmae/bregman_divergences.py:PoissonDivergence:__init__,PoissonDivergence:__init__,method,1,2,2,49,24.5,0,0,"['self', 'name']","[None, None]","[None, ""'poisson_divergence'""]",164,[],['super'],1
source/models/repo/keras_mmae/bregman_divergences.py:PoissonDivergence:_phi,PoissonDivergence:_phi,method,5,15,9,113,7.53,0,0,"['self', 'z']","[None, None]","[None, None]",64,"['        """"""\n', '        This is the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.maximum', 'K.epsilon', 'K.log', '_phi_gradient']",4
source/models/repo/keras_mmae/bregman_divergences.py:PoissonDivergence:_phi_gradient,PoissonDivergence:_phi_gradient,method,4,5,5,41,8.2,0,0,"['self', 'z']","[None, None]","[None, None]",72,"['        """"""\n', '        This is the gradient of the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.maximum', 'K.epsilon', 'K.log']",3
source/models/repo/keras_mmae/bregman_divergences.py:BinomialDivergence:__init__,BinomialDivergence:__init__,method,3,4,4,59,14.75,0,0,"['self', 'n', 'name']","[None, None, None]","[None, None, ""'binomial_divergence'""]",197,[],['super'],1
source/models/repo/keras_mmae/bregman_divergences.py:BinomialDivergence:_phi,BinomialDivergence:_phi,method,5,24,14,186,7.75,0,0,"['self', 'z']","[None, None]","[None, None]",64,"['        """"""\n', '        This is the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.clip', 'K.epsilon', 'K.log', '_phi_gradient']",4
source/models/repo/keras_mmae/bregman_divergences.py:BinomialDivergence:_phi_gradient,BinomialDivergence:_phi_gradient,method,4,9,9,73,8.11,0,0,"['self', 'z']","[None, None]","[None, None]",72,"['        """"""\n', '        This is the gradient of the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.clip', 'K.epsilon', 'K.log']",3
source/models/repo/keras_mmae/bregman_divergences.py:NegativeBinomialDivergence:__init__,NegativeBinomialDivergence:__init__,method,3,4,4,67,16.75,0,0,"['self', 'r', 'name']","[None, None, None]","[None, None, ""'negative_binomial_divergence'""]",229,[],['super'],1
source/models/repo/keras_mmae/bregman_divergences.py:NegativeBinomialDivergence:_phi,NegativeBinomialDivergence:_phi,method,5,20,12,154,7.7,0,0,"['self', 'z']","[None, None]","[None, None]",64,"['        """"""\n', '        This is the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.maximum', 'K.epsilon', 'K.log', '_phi_gradient']",4
source/models/repo/keras_mmae/bregman_divergences.py:NegativeBinomialDivergence:_phi_gradient,NegativeBinomialDivergence:_phi_gradient,method,4,7,7,57,8.14,0,0,"['self', 'z']","[None, None]","[None, None]",72,"['        """"""\n', '        This is the gradient of the phi function of the Bregman divergence.\n', '\n', '        """"""\n']","['K.maximum', 'K.epsilon', 'K.log']",3
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder,MultimodalAutoencoder,class,127,545,296,8198,15.04,12,16,[],[],[],54,[],[],0
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:__init__,MultimodalAutoencoder:__init__,method,34,88,69,1488,16.91,1,4,"['self', 'input_shapes', 'hidden_dims', 'output_activations', 'dropout_rates', 'activity_regularizers', '**unimodal_kwargs']","[None, None, None, None, None, None, None]","[None, None, None, ""'linear'"", 'None', 'None', None]",107,[],"['np.isscalar', 'isinstance', 'list', 'self._construct_input_layers', 'self._construct_unimodal_encoders', 'self._get_output_shapes', 'self._construct_multimodal_layer', 'self._construct_fusion_encoder', 'Input', 'self._construct_fusion_decoder', 'self._construct_unimodal_decoders', 'self._add_output_activations', 'super', 'Model']",14
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:compile,MultimodalAutoencoder:compile,method,10,16,15,419,26.19,0,0,"['self', 'optimizer', 'loss', 'loss_weights', 'sample_weight_mode', 'target_tensors']","[None, None, None, None, None, None]","[None, ""'adam'"", ""'gaussian_divergence'"", 'None', 'None', 'None']",164,"['        """"""\n', '        Sets the model configuration for training.\n', '\n', '        Parameters\n', '        ----------\n', '        optimizer : str, optional\n', ""            Name of optimization algorithm.  (Default: 'adam')\n"", '        loss : str, callable, dict or list, optional\n', '            Loss functions, including Bregman divergences, for each modality.\n', ""            (Default: 'gaussian_divergence')\n"", '        loss_weights : dict or list of floats, optional\n', '            Loss weights for each modality.  `None` corresponds to weight `1.0`\n', '            for each modality.  (Default: `None`)\n', '        sample_weight_mode : str, list or dict, optional\n', '            Sample weight mode for each modality.  Each mode can be `None`\n', ""            corresponding to sample-wise weighting or 'temporal' for\n"", '            timestep-wise weighting.  (Default: `None`)\n', '        target_tensors : tensor, list of tensors or dict, optional\n', '            Target tensors to be used instead of `data` arguments for training.\n', '            (Default: `None`)\n', '\n', '        """"""\n']","['self._replace_bregman_strings', 'self._rename_output_keys', 'super']",3
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:fit,MultimodalAutoencoder:fit,method,17,36,30,667,18.53,0,2,"['self', 'data', 'batch_size', 'epochs', 'verbose', 'callbacks', 'validation_split', 'validation_data', 'shuffle', 'sample_weight', 'validation_sample_weight', 'initial_epoch', 'steps_per_epoch', 'validation_steps']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, 'None', 'None', '1', '1', 'None', '0.0', 'None', 'True', 'None', 'None', '0', 'None', 'None']",202,"['        """"""\n', '        Fits the model to the given training data based on the reconstruction\n', '        loss.  If provided, also evaluates the reconstruction loss on the\n', '        validation data.\n', '\n', '        Parameters\n', '        ----------\n', '        data : array_like, optional\n', '            The training data.  Can be a NumPy array, a list of NumPy arrays\n', '            (one for each modality) or a dict with NumPy arrays as values where\n', '            the keys are the modality names.  (Default: `None`)\n', '        batch_size : int, optional\n', '            The number of samples used to calculate a gradient update.  If\n', '            `None` then the Keras default will be used.  (Default: `None`)\n', '        epochs : int, optional\n', '            Number of epochs to train.  (Default: 1)\n', '        verbose : int, optional\n', '            Verbosity mode.  One of 0, 1 or 2.  (Default: 1)\n', '        callbacks : list of Keras callbacks, optional\n', '            Callbacks to be called during training.  (Default: `None`)\n', '        validation_split : float, optional\n', '            The fraction as a value between 0 and 1 of `data` to be used as\n', '            validation data.  (Default: 0.0)\n', '        validation_data : array_like, optional\n', '            The validation data.  Can be a NumPy array, a list of NumPy arrays\n', '            (one for each modality) or a dict with NumPy arrays as values where\n', '            the keys are the modality names.  (Default: `None`)\n', '        shuffle : bool or str, optional\n', '            If true then the samples are shuffled before each epoch.  If set to\n', ""            the string 'batch' then samples are shuffled in chunks of\n"", '            `batch_size`.  (Default: `True`)\n', '        sample_weight : array_like, optional\n', '            Array of values for each sample in `data` to weight the loss\n', '            function for the corresponding sample.  (Default: `None`)\n', '        validation_sample_weight : array_like, optional\n', '            Array of values for each sample in `validation_data` to weight the\n', '            loss function for the corresponding sample.  (Default: `None`)\n', '        initial_epoch : int, optional\n', '            The epoch index at which to start training.  (Default: 0)\n', '        steps_per_epoch : int, optional\n', '            The number of batches used per epoch.  (Default: `None`)\n', '        validation_steps : int, optional\n', '            The number of batches used for validation.  (Default: `None`)\n', '\n', '        Returns\n', '        -------\n', '        history : Keras history\n', '            Performance values recorded for each training epoch.\n', '\n', '        """"""\n']","['self._rename_output_keys', 'super']",2
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:evaluate_reconstruction,MultimodalAutoencoder:evaluate_reconstruction,method,8,10,10,194,19.4,0,0,"['self', 'data', 'batch_size', 'verbose', 'sample_weight', 'steps']","[None, None, None, None, None, None]","[None, 'None', 'None', '1', 'None', 'None']",276,"['        """"""\n', '        Evaluates reconstruction loss for the given test data.\n', '\n', '        Parameters\n', '        ----------\n', '        data : array_like, optional\n', '            The test data to be evaluated.  Can be a NumPy array, a list of\n', '            NumPy arrays (one for each modality) or a dict with NumPy arrays as\n', '            values where the keys are the modality names.  (Default: `None`)\n', '        batch_size : int, optional\n', '            The number of samples used to calculate a gradient update.  If\n', '            `None` then the Keras default will be used.  (Default: `None`)\n', '        verbose : 0 or 1, optional\n', '            Verbosity mode.  (Default: 1)\n', '        sample_weight : array_like, optional\n', '            Array of values for each sample in `data` to weight the loss\n', '            function for the corresponding sample.  (Default: `None`)\n', '        steps : int, optional\n', '            The total number of batches used.  (Default: `None`)\n', '\n', '        Returns\n', '        -------\n', '        test_loss : list\n', '            A list of test reconstruction loss values for each modality.\n', '\n', '        """"""\n']","['self._rename_output_keys', 'super']",2
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:train_on_batch,MultimodalAutoencoder:train_on_batch,method,5,7,7,149,21.29,0,0,"['self', 'data', 'sample_weight']","[None, None, None]","[None, None, 'None']",310,"['        """"""\n', '        Applies a single batch update step using the provided training data.\n', '\n', '        Parameters\n', '        ----------\n', '        data : array_like\n', '            The training data.  Can be a NumPy array, a list of NumPy arrays\n', '            (one for each modality) or a dict with NumPy arrays as values where\n', '            the keys are the modality names.\n', '        sample_weight : array_like, optional\n', '            Array of values for each sample in `data` to weight the loss\n', '            function for the corresponding sample.  (Default: `None`)\n', '\n', '        Returns\n', '        -------\n', '        training_loss : list\n', '            A list of training reconstruction loss values for each modality.\n', '\n', '        """"""\n']","['self._rename_output_keys', 'super']",2
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:test_on_batch,MultimodalAutoencoder:test_on_batch,method,5,7,7,148,21.14,0,0,"['self', 'data', 'sample_weight']","[None, None, None]","[None, None, 'None']",335,"['        """"""\n', '        Evaluates reconstruction loss on a single batch of test data.\n', '\n', '        Parameters\n', '        ----------\n', '        data : array_like\n', '            The test data.  Can be a NumPy array, a list of NumPy arrays (one\n', '            for each modality) or a dict with NumPy arrays as values where the\n', '            keys are the modality names.\n', '        sample_weight : array_like, optional\n', '            Array of values for each sample in `data` to weight the loss\n', '            function for the corresponding sample.  (Default: `None`)\n', '\n', '        Returns\n', '        -------\n', '        test_loss : list\n', '            A list of test reconstruction loss values for each modality.\n', '\n', '        """"""\n']","['self._rename_output_keys', 'super']",2
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:encode,MultimodalAutoencoder:encode,method,2,5,5,85,17.0,0,0,"['self', 'data', 'batch_size', 'verbose', 'steps']","[None, None, None, None, None]","[None, None, 'None', '0', 'None']",360,"['        """"""\n', '        Encodes the input data to the latent representation.\n', '\n', '        Parameters\n', '        ----------\n', '        data : array_like\n', '            The data to be encoded.  Can be a NumPy array, a list of NumPy\n', '            arrays (one for each modality) or a dict with NumPy arrays as\n', '            values where the keys are the modality names.\n', '        batch_size : int, optional\n', '            The number of samples used to calculate a gradient update.  If\n', '            `None` then the Keras default will be used.  (Default: `None`)\n', '        verbose : 0 or 1, optional\n', '            Verbosity mode.  (Default: 0)\n', '        steps : int, optional\n', '            The total number of batches used.  (Default: `None`)\n', '\n', '        Returns\n', '        -------\n', '        latent_data : array_like\n', '            The latent representation of the input data.\n', '\n', '        """"""\n']",[],0
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:decode,MultimodalAutoencoder:decode,method,2,5,5,92,18.4,0,0,"['self', 'latent_data', 'batch_size', 'verbose', 'steps']","[None, None, None, None, None]","[None, None, 'None', '0', 'None']",387,"['        """"""\n', '        Decodes the input data from the latent representation.\n', '\n', '        Parameters\n', '        ----------\n', '        latent_data : array_like\n', '            The data to be decoded.\n', '        batch_size : int, optional\n', '            The number of samples used to calculate a gradient update.  If\n', '            `None` then the Keras default will be used.  (Default: `None`)\n', '        verbose : 0 or 1, optional\n', '            Verbosity mode.  (Default: 0)\n', '        steps : int, optional\n', '            The total number of batches used.  (Default: `None`)\n', '\n', '        Returns\n', '        -------\n', '        data : array_like\n', '            The full reconstructed representation of the latent data.  This is\n', '            either a NumPy array or a list of NumPy arrays (one for each\n', '            modality).\n', '\n', '        """"""\n']",[],0
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_replace_bregman_strings,MultimodalAutoencoder:_replace_bregman_strings,method,10,35,25,301,8.6,1,2,"['cls', 'loss']","[None, None]","[None, None]",415,"['        """"""\n', '        Replaces strings referring to Bregman divergences with the actual\n', '        divergence instances.\n', '\n', '        Parameters\n', '        ----------\n', '        loss : str, callable, dict or list\n', '            The loss functions.\n', '\n', '        Returns\n', '        -------\n', '        loss : str, callable, dict or list\n', '            The loss functions where the strings referring to Bregman\n', '            divergences were replaced.\n', '\n', '        """"""\n']","['isinstance', 'getattr', 'loss.items', 'cls._replace_bregman_strings']",4
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_construct_input_layers,MultimodalAutoencoder:_construct_input_layers,method,6,21,17,195,9.29,1,1,"['input_shapes', 'modality_names']","[None, None]","[None, 'None']",445,"['        """"""\n', '        Generates input layers for each modality.\n', '\n', '        Parameters\n', '        ----------\n', '        input_shapes : list of lists\n', '            Shape of input elements per modality.\n', '        modality_names : list of strings, optional\n', '            The names of the modalities.  If not `None` then this list must\n', '            have the same length as input_shapes.  (Default: `None`)\n', '\n', '        Returns\n', '        -------\n', '        input_layers : list of Keras tensor inputs\n', '            The input layers for each modality.  If modality names are\n', '            specified, then each input layer is named after the modality.\n', '\n', '        """"""\n']",['enumerate'],1
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_construct_unimodal_encoders,MultimodalAutoencoder:_construct_unimodal_encoders,method,3,4,3,38,9.5,0,0,"['input_layers', 'modality_names', '**kwargs']","[None, None, None]","[None, None, None]",472,"['        """"""\n', '        Returns the first argument.  Override this method to add unimodal\n', '        encoders between the input layers and the fusion network.\n', '\n', '        Parameters\n', '        ----------\n', '        input_layers : dict or list of Keras tensor inputs\n', '            The input layers for each modality.\n', '        modality_names : list of strings\n', '            The names of the modalities or `None`.\n', '\n', '        Returns\n', '        -------\n', '        um_layers : dict or list of Keras tensor outputs from layers\n', '            The unimodal encoder networks for each modality.\n', '\n', '        """"""\n']",[],0
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_construct_multimodal_layer,MultimodalAutoencoder:_construct_multimodal_layer,method,11,24,20,233,9.71,1,2,"['cls', 'um_layers']","[None, None]","[None, None]",494,"['        """"""\n', '        Constructs the first multimodal layer by flattening and concatenating\n', '        the unimodal input layers.\n', '\n', '        Parameters\n', '        ----------\n', '        um_layers : list of Keras tensor outputs from layers\n', '            The unimodal encoder networks for each modality.\n', '\n', '        Returns\n', '        -------\n', '        mm_layer : Keras tensor output from layer\n', '            The multimodel layer.\n', '\n', '        """"""\n']","['cls._get_output_shapes', 'enumerate', 'len', 'Flatten', 'concatenate']",5
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_construct_fusion_encoder,MultimodalAutoencoder:_construct_fusion_encoder,method,11,22,19,366,16.64,1,0,"['cls', 'mm_layer', 'hidden_dims', 'dropout_rates', 'activity_regularizers']","[None, None, None, None, None]","[None, None, None, None, None]",523,"['        """"""\n', '        Adds a densely connected fusion network for encoding to the latent\n', '        representation from a concatenated multimodal layer.\n', '\n', '        Parameters\n', '        ----------\n', '        mm_layer : Keras tensor output from layer\n', '            The multimodel layer.\n', '        hidden_dims : int or list of ints\n', '            Number of elements per layer of the encoder.  The last element of\n', '            the array represents the number of elements of the latent\n', '            representation.\n', '        dropout_rates : list of floats\n', '            Dropout rates for each encoder layer.\n', '        activity_regularizers : list of Keras regularizers\n', '            Activity regularizers for each encoder layer.\n', '\n', '        Returns\n', '        -------\n', '        encoder : Keras tensor output from layer\n', '            The encoder network.\n', '\n', '        """"""\n']","['cls._get_kernel_constraints', 'Dropout', 'enumerate', 'Dense', 'layer']",5
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_construct_fusion_decoder,MultimodalAutoencoder:_construct_fusion_decoder,method,22,43,35,692,16.09,2,0,"['cls', 'encoded_layer', 'encoder', 'hidden_dims', 'fusion_shapes', 'dropout_rates', 'activity_regularizers']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",560,"['        """"""\n', '        Adds a densely connected fusion network for decoding from the latent\n', '        representation.\n', '\n', '        Parameters\n', '        ----------\n', '        encoded_layer : Keras tensor input\n', '            The latent input layer.\n', '        encoder : Keras tensor output from layer\n', '            The encoder network.\n', '        hidden_dims : int or list of ints\n', '            Number of elements per layer of the decoder used in reverse order\n', '            from second-last element to first element.\n', '        fusion_shapes : list of lists\n', '            The shapes of the unimodal layers for each modality right before\n', '            fusion.\n', '        dropout_rates : list of floats\n', '            Dropout rates for each decoder layer, used in reverse order from\n', '            second-last element to first element.\n', '        activity_regularizers : list of Keras regularizers\n', '            Activity regularizers for each decoder layer, used in reverse order\n', '            from second-last element to first element.\n', '\n', '        Returns\n', '        -------\n', '        fusion_decoder : list of Keras tensor outputs from layers\n', '            The fusion decoder networks starting from the latent layer.\n', '        fusion_autoencoder : list of Keras tensor outputs from layers\n', '            The networks up to and including the fusion decoder.\n', '\n', '        """"""\n']","['cls._get_kernel_constraints', 'range', 'Dense', 'Dropout', 'dropout', 'len', 'enumerate', 'Reshape']",8
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_construct_unimodal_decoders,MultimodalAutoencoder:_construct_unimodal_decoders,method,6,7,6,107,15.29,0,0,"['fusion_decoder', 'fusion_autoencoder', 'output_shapes', 'modality_names', '**kwargs']","[None, None, None, None, None]","[None, None, None, None, None]",614,"['        """"""\n', '        Returns the first two arguments.  Override this method to add unimodal\n', '        decoders between the fusion network and the output layers.\n', '\n', '        Parameters\n', '        ----------\n', '        fusion_decoder : dict or list of Keras tensor outputs from layers\n', '            The fusion decoder networks starting from the latent layer.\n', '        fusion_autoencoder : dict or list of Keras tensor outputs from layers\n', '            The networks up to and including the fusion decoder.\n', '        output_shapes : dict or list of lists\n', '            Shapes of output elements for each modality.\n', '        modality_names : list of strings\n', '            The names of the modalities or `None`.\n', '\n', '        Returns\n', '        -------\n', '        output_decoder : dict or list of Keras tensor outputs from layers\n', '            The output decoder networks starting from the latent layer.\n', '        output_autoencoder : dict or list of Keras tensor outputs from layers\n', '            The networks up to and including the unimodal decoders.\n', '\n', '        """"""\n']",[],0
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_get_output_shapes,MultimodalAutoencoder:_get_output_shapes,method,2,8,7,70,8.75,0,0,['layers'],[None],[None],644,"['        """"""\n', '        Extracts the output shapes of the input.\n', '\n', '        Parameters\n', '        ----------\n', '        layers : list of Keras tensor outputs from layers\n', '            The network with the output shapes of interest.\n', '\n', '        Returns\n', '        -------\n', '        output_shapes : list of lists\n', '            The output shapes in a list.\n', '\n', '        """"""\n']",[],0
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_add_output_activations,MultimodalAutoencoder:_add_output_activations,method,17,65,39,864,13.29,4,4,"['cls', 'output_activations', 'output_decoder', 'output_autoencoder', 'modality_names']","[None, None, None, None, None]","[None, None, None, None, 'None']",663,"['        """"""\n', '        Adds output activations to the provided decoder and autoencoder.\n', '\n', '        Parameters\n', '        ----------\n', '        output_activations : str, callable, dict or list\n', '            Output activation functions for each modality.\n', '        output_decoder : list of Keras tensor outputs from layers\n', '            The output decoder networks starting from the latent layer.\n', '        output_autoencoder : list of Keras tensor outputs from layers\n', '            The networks up to and including the unimodal decoders.\n', '        modality_names : list of strings, optional\n', '            The names of the modalities.  (Default: `None`)\n', '\n', '        Returns\n', '        -------\n', '        output_decoder : list of Keras tensor outputs from layers\n', '            The output decoder networks starting from the latent layer with\n', '            the added output activations.\n', '        output_autoencoder : list of Keras tensor outputs from layers\n', '            The networks up to and including the unimodal decoders with the\n', '            added output activations.\n', '\n', '        """"""\n']","['isinstance', 'len', 'enumerate', 'activation']",4
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_get_kernel_constraints,MultimodalAutoencoder:_get_kernel_constraints,method,4,9,8,144,16.0,0,0,['dropout_rates'],[None],[None],717,"['        """"""\n', '        Constructs a kernel constraint for each dropout rate.\n', '\n', '        Parameters\n', '        ----------\n', '        dropout_rates : list of floats\n', '            Dropout rates for each encoder layer.\n', '\n', '        Returns\n', '        -------\n', '        kernel_constraints : array of Keras constraints\n', '            Constraints for layer kernels corresponding to the given dropout\n', '            rates.\n', '\n', '        """"""\n']","['np.empty', 'max_norm']",2
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_rename_output_keys,MultimodalAutoencoder:_rename_output_keys,method,6,13,12,120,9.23,1,1,"['cls', 'structure']","[None, None]","[None, None]",739,"['        """"""\n', '        Checks whether the argument is a dict and if so, appends output labels\n', '        to its keys.\n', '\n', '        Parameters\n', '        ----------\n', '        structure : object\n', '            An input object that is modified if it is a dict.\n', '\n', '        Returns\n', '        -------\n', '        output_structure : object\n', '            Either the unmodified argument or a dict with the same values and\n', '            keys with output labels appended.\n', '\n', '        """"""\n']","['isinstance', 'structure.items']",2
source/models/repo/keras_mmae/multimodal_autoencoder.py:MultimodalAutoencoder:_append_output_name,MultimodalAutoencoder:_append_output_name,method,2,3,3,28,9.33,0,0,['name'],[None],[None],762,"['        """"""\n', '        Appends an output name label to the argument.\n', '\n', '        Parameters\n', '        ----------\n', '        name : str\n', '            An input name.\n', '\n', '        Returns\n', '        -------\n', '        output_name : str\n', '            The input name with an output label appended.\n', '\n', '        """"""\n']",[],0
source/models/repo/TruncatedGaussianMixtureVAE/02 model_class.py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],11,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/repo/TruncatedGaussianMixtureVAE/02 model_class.py:VA_Model,VA_Model,class,155,454,339,5606,12.35,9,3,[],[],[],21,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/02 model_class.py:VA_Model:__init__,VA_Model:__init__,method,0,1,1,4,4.0,0,0,['self'],[None],[None],22,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/02 model_class.py:VA_Model:get_dataset,VA_Model:get_dataset,method,58,148,107,1697,11.47,7,2,"['self', 'state_num', 'time_len', 'signal_dimension', 'CNR', 'window_len', 'half_window_len']","[None, None, None, None, None, None, None]","[None, '10', '50000', '15', '1', '11', '5']",25,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/repo/TruncatedGaussianMixtureVAE/02 model_class.py:VA_Model:build_model,VA_Model:build_model,method,41,115,101,1644,14.3,0,0,['self'],[None],[None],87,[],"['Input', 'Dense', 'Reshape', 'Lambda', 'Model', 'self.decoder']",6
source/models/repo/TruncatedGaussianMixtureVAE/02 model_class.py:VA_Model:fit,VA_Model:fit,method,37,122,88,1528,12.52,1,0,"['self', 'epochs', 'batch_size', 'Lambda1', 'Lambda2', 'Alpha']","[None, None, None, None, None, None]","[None, None, '256', '1', '200', '0.075']",153,[],"['np.ones', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log']",8
source/models/repo/TruncatedGaussianMixtureVAE/02 model_class.py:VA_Model:save,VA_Model:save,method,1,1,1,43,43.0,0,0,['self'],[None],[None],215,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/02 model_class.py:VA_Model:load,VA_Model:load,method,1,1,1,29,29.0,0,0,"['self', 'path']","[None, None]","[None, None]",218,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/02 model_class.py:VA_Model:test,VA_Model:test,method,18,40,34,386,9.65,1,1,['self'],[None],[None],221,[],"['np.zeros', 'range', 'np.max', 'np.argmax']",4
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],54,[],"['print', 'log2']",2
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],57,[],['print'],1
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:init,init,function,3,5,5,38,7.6,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",65,[],['Model'],1
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],73,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:get_dataset,get_dataset,function,52,146,105,1707,11.69,7,2,['data_pars'],[None],[None],85,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:get_model,get_model,function,65,212,164,3036,14.32,1,0,['model_pars'],[None],[None],146,[],"['Input', 'Dense', 'Reshape', 'Lambda', 'Model', 'decoder', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log', 'vae.add_loss', 'vae.compile', 'vae.summary']",16
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:fit,fit,function,21,50,47,536,10.72,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",278,"['    """"""\n', '    """"""\n']","['copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'ModelCheckpoint', 'get_dataset']",5
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:predict,predict,function,18,36,32,396,11.0,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",318,[],"['get_dataset', 'get_dataset_tuple', 'log2', 'compute_pars.get']",4
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:save,save,function,1,3,3,48,16.0,0,0,['path'],[None],[None],337,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:load,load,function,4,7,7,82,11.71,0,0,['path'],[None],[None],341,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:get_label,get_label,function,18,40,34,351,8.78,1,1,"['encoder', 'x_train', 'dummy_train', 'class_num', 'batch_size']","[None, None, None, None, None]","[None, None, None, '5', '256']",345,[],"['encoder.predict', 'np.zeros', 'range', 'np.max', 'np.argmax']",5
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:Model,Model,class,16,32,30,386,12.06,0,1,[],[],[],256,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/03 model_functions.py:Model:__init__,Model:__init__,method,15,27,25,318,11.78,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",257,[],"['log2', 'get_model']",2
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],38,[],"['print', 'log2']",2
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],41,[],['print'],1
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",49,[],['Model'],1
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],57,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:get_model,get_model,function,58,196,147,2753,14.05,1,0,"['original_dim', 'class_num', 'intermediate_dim', 'intermediate_dim_2', 'latent_dim', 'Lambda1', 'batch_size', 'Lambda2', 'Alpha', '']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, None]",69,"['    """"""\n', ""    original_dim       = model_pars['original_dim']\n"", ""    class_num          = model_pars['class_num']\n"", ""    intermediate_dim   = model_pars['intermediate_dim']\n"", ""    intermediate_dim_2 = model_pars['intermediate_dim_2']\n"", ""    latent_dim         = model_pars['latent_dim']\n"", ""    Lambda1            = model_pars['Lambda1']\n"", ""    batch_size         = model_pars['batch_size']\n"", ""    Lambda2            = model_pars['Lambda2']\n"", ""    Alpha              = model_pars['Alpha']\n"", '    """"""\n']","['Input', 'Dense', 'Reshape', 'Lambda', 'decoder', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log', 'vae.add_loss', 'vae.compile', 'vae.summary']",15
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:fit,fit,function,28,75,75,900,12.0,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",215,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'ModelCheckpoint', 'model_pars.get', 'np.uint32']",7
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:predict,predict,function,18,36,32,396,11.0,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",252,[],"['get_dataset', 'get_dataset_tuple', 'log2', 'compute_pars.get']",4
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:get_dataset_tuple,get_dataset_tuple,function,12,43,31,329,7.65,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",273,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:get_dataset,get_dataset,function,36,148,98,1460,9.86,1,7,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",295,[],"['len', 'Xtuple_train.append', 'get_dataset', 'get_mydata_correl', 'np.ones', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",9
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:get_label,get_label,function,18,40,34,351,8.78,1,1,"['encoder', 'x_train', 'dummy_train', 'class_num', 'batch_size']","[None, None, None, None, None]","[None, None, None, '5', '256']",345,[],"['encoder.predict', 'np.zeros', 'range', 'np.max', 'np.argmax']",5
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],361,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:save,save,function,17,33,31,377,11.42,0,0,"['path', 'info']","[None, None]","['None', 'None']",365,[],"['os.makedirs', 'Model', 'pickle.dump', 'open']",4
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:load_model,load_model,function,17,30,28,310,10.33,0,0,['path'],[None],"['""""']",382,[],"['pickle.load', 'Model']",2
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",397,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:get_mydata_correl,get_mydata_correl,function,52,146,105,1707,11.69,7,2,['data_pars'],[None],[None],410,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:test,test,function,27,121,98,1473,12.17,0,0,[],[],[],471,[],"['get_mydata_correl', 'np.ones', 'np.uint32', 'init', 'Model', 'fit', 'predict', 'save', 'load_model']",9
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:Model,Model,class,21,55,53,662,12.04,0,1,[],[],[],181,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/04_model_functions.py:Model:__init__,Model:__init__,method,20,50,48,594,11.88,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",182,[],"['log2', 'model_pars.get', 'np.uint32', 'get_model']",4
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],54,[],"['print', 'log2']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],57,[],['print'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:init,init,function,3,5,5,38,7.6,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",65,[],['Model'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],73,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:get_dataset,get_dataset,function,52,146,105,1707,11.69,7,2,['data_pars'],[None],[None],85,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:get_model,get_model,function,65,212,164,3036,14.32,1,0,['model_pars'],[None],[None],146,[],"['Input', 'Dense', 'Reshape', 'Lambda', 'Model', 'decoder', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log', 'vae.add_loss', 'vae.compile', 'vae.summary']",16
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:fit,fit,function,21,50,47,536,10.72,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",278,"['    """"""\n', '    """"""\n']","['copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'ModelCheckpoint', 'get_dataset']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:predict,predict,function,18,36,32,396,11.0,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",318,[],"['get_dataset', 'get_dataset_tuple', 'log2', 'compute_pars.get']",4
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:save,save,function,1,3,3,48,16.0,0,0,['path'],[None],[None],337,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:load,load,function,4,7,7,82,11.71,0,0,['path'],[None],[None],341,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:get_label,get_label,function,18,40,34,351,8.78,1,1,"['encoder', 'x_train', 'dummy_train', 'class_num', 'batch_size']","[None, None, None, None, None]","[None, None, None, '5', '256']",345,[],"['encoder.predict', 'np.zeros', 'range', 'np.max', 'np.argmax']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:Model,Model,class,16,32,30,386,12.06,0,1,[],[],[],256,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions.py:Model:__init__,Model:__init__,method,15,27,25,318,11.78,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",257,[],"['log2', 'get_model']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],53,[],"['print', 'log2']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],56,[],['print'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:init,init,function,3,5,5,38,7.6,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",64,[],['Model'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],72,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:get_mydata_correl,get_mydata_correl,function,52,146,105,1707,11.69,7,2,['data_pars'],[None],[None],84,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:get_dataset_tuple,get_dataset_tuple,function,12,43,31,329,7.65,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",145,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:get_dataset,get_dataset,function,38,158,102,1564,9.9,1,8,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",167,[],"['len', 'Xtuple_train.append', 'get_dataset', 'get_mydata_correl', 'np.ones', 'get_mydata_mnist', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",10
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:get_label,get_label,function,18,40,34,351,8.78,1,1,"['encoder', 'x_train', 'dummy_train', 'class_num', 'batch_size']","[None, None, None, None, None]","[None, None, None, '5', '256']",225,[],"['encoder.predict', 'np.zeros', 'range', 'np.max', 'np.argmax']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:get_model,get_model,function,65,212,164,3075,14.5,1,0,['model_pars'],[None],[None],240,[],"['Input', 'Dense', 'Reshape', 'Lambda', 'decoder', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log', 'vae.add_loss', 'vae.compile', 'vae.summary']",15
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:fit,fit,function,22,51,46,549,10.76,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",383,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'ModelCheckpoint']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:predict,predict,function,18,36,32,396,11.0,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",423,[],"['get_dataset', 'get_dataset_tuple', 'log2', 'compute_pars.get']",4
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:save,save,function,1,3,3,48,16.0,0,0,['path'],[None],[None],442,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:load,load,function,4,7,7,82,11.71,0,0,['path'],[None],[None],446,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:test,test,function,18,65,57,877,13.49,0,0,[],[],[],455,[],"['np.uint32', 'Model', 'fit', 'get_mydata_correl', 'predict', 'save', 'load_model']",7
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:Model,Model,class,20,56,53,659,11.77,0,1,[],[],[],351,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_1.py:Model:__init__,Model:__init__,method,19,51,48,591,11.59,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",352,[],"['log2', 'np.uint32', 'model_pars.get', 'get_model']",4
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],52,[],"['print', 'log2']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],55,[],['print'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:init,init,function,3,5,5,38,7.6,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",63,[],['Model'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],71,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:get_mydata_correl,get_mydata_correl,function,52,146,105,1707,11.69,7,2,['data_pars'],[None],[None],83,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:get_dataset_tuple,get_dataset_tuple,function,12,43,31,329,7.65,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",144,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:get_dataset,get_dataset,function,36,148,98,1460,9.86,1,7,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",166,[],"['len', 'Xtuple_train.append', 'get_dataset', 'get_mydata_correl', 'np.ones', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",9
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:get_label,get_label,function,18,40,34,351,8.78,1,1,"['encoder', 'x_train', 'dummy_train', 'class_num', 'batch_size']","[None, None, None, None, None]","[None, None, None, '5', '256']",220,[],"['encoder.predict', 'np.zeros', 'range', 'np.max', 'np.argmax']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:get_model,get_model,function,65,212,164,3075,14.5,1,0,['model_pars'],[None],[None],235,[],"['Input', 'Dense', 'Reshape', 'Lambda', 'decoder', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log', 'vae.add_loss', 'vae.compile', 'vae.summary']",15
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:fit,fit,function,22,51,46,549,10.76,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",368,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'ModelCheckpoint']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:predict,predict,function,18,36,32,396,11.0,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",408,[],"['get_dataset', 'get_dataset_tuple', 'log2', 'compute_pars.get']",4
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:save,save,function,1,3,3,48,16.0,0,0,['path'],[None],[None],427,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:load_model,load_model,function,4,7,7,82,11.71,0,0,['path'],[None],[None],431,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:test,test,function,14,55,51,796,14.47,0,0,[],[],[],440,[],"['np.uint32', 'Model', 'fit', 'predict', 'save', 'load_model']",6
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:Model,Model,class,16,32,30,386,12.06,0,1,[],[],[],346,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_2.py:Model:__init__,Model:__init__,method,15,27,25,318,11.78,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",347,[],"['log2', 'get_model']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],38,[],"['print', 'log2']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],41,[],['print'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",49,[],['Model'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],57,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:get_model,get_model,function,66,214,165,3085,14.42,1,0,['model_pars'],[None],[None],68,[],"['Input', 'Dense', 'Reshape', 'Lambda', 'decoder', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log', 'vae.add_loss', 'vae.compile', 'vae.summary']",15
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:fit,fit,function,23,50,49,595,11.9,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",210,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'ModelCheckpoint']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:predict,predict,function,18,36,32,396,11.0,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",231,[],"['get_dataset', 'get_dataset_tuple', 'log2', 'compute_pars.get']",4
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:get_dataset_tuple,get_dataset_tuple,function,12,43,31,329,7.65,1,2,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",252,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:get_dataset,get_dataset,function,36,148,98,1460,9.86,1,7,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",274,[],"['len', 'Xtuple_train.append', 'get_dataset', 'get_mydata_correl', 'np.ones', 'data_pars.get', 'get_dataset_tuple', 'log2', 'Exception']",9
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:get_label,get_label,function,18,40,34,351,8.78,1,1,"['encoder', 'x_train', 'dummy_train', 'class_num', 'batch_size']","[None, None, None, None, None]","[None, None, None, '5', '256']",324,[],"['encoder.predict', 'np.zeros', 'range', 'np.max', 'np.argmax']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],340,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:save,save,function,17,33,31,377,11.42,0,0,"['path', 'info']","[None, None]","['None', 'None']",344,[],"['os.makedirs', 'Model', 'pickle.dump', 'open']",4
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:load_model,load_model,function,17,30,28,310,10.33,0,0,['path'],[None],"['""""']",361,[],"['pickle.load', 'Model']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",376,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:get_mydata_correl,get_mydata_correl,function,52,146,105,1707,11.69,7,2,['data_pars'],[None],[None],389,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:test,test,function,27,102,90,1269,12.44,0,0,[],[],[],450,[],"['get_mydata_correl', 'np.ones', 'np.uint32', 'init', 'Model', 'fit', 'predict', 'print', 'save', 'load_model']",10
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:Model,Model,class,20,55,53,655,11.91,0,1,[],[],[],179,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_3 (1).py:Model:__init__,Model:__init__,method,19,50,48,587,11.74,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",180,[],"['log2', 'np.uint32', 'model_pars.get', 'get_model']",4
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:log,log,function,3,9,8,70,7.78,0,1,['*s'],[None],[None],38,[],"['print', 'log2']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:log2,log2,function,2,6,6,36,6.0,0,1,['*s'],[None],[None],41,[],['print'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:init,init,function,4,8,8,58,7.25,0,0,"['*kw', '**kwargs']","[None, None]","[None, None]",49,[],['Model'],1
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:sampling,sampling,function,14,18,17,197,10.94,0,0,['args'],[None],[None],57,[],"['K.shape', 'K.int_shape', 'K.random_normal', 'K.random_uniform', 'K.exp']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:get_model,get_model,function,66,214,165,3085,14.42,1,0,['model_pars'],[None],[None],68,[],"['Input', 'Dense', 'Reshape', 'Lambda', 'decoder', 'mse', 'range', 'K.square', 'K.exp', 'K.mean', 'K.log', 'np.log', 'vae.add_loss', 'vae.compile', 'vae.summary']",15
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:fit,fit,function,24,51,50,590,11.57,1,0,"['data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None]","['None', 'None', 'None', None]",210,"['    """"""\n', '    """"""\n']","['get_dataset', 'copy.deepcopy', 'compute_pars.get', 'EarlyStopping', 'ModelCheckpoint', 'np.ones']",6
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:predict,predict,function,18,37,33,403,10.89,0,2,"['Xpred', 'data_pars', 'compute_pars', 'out_pars', '**kw']","[None, None, None, None, None]","['None', 'None', '{}', '{}', None]",235,[],"['get_dataset', 'data_pars.get', 'get_dataset_tuple', 'log2', 'compute_pars.get']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:get_dataset_tuple,get_dataset_tuple,function,9,31,25,272,8.77,1,1,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",256,"['    """"""  Split into Tuples to feed  Xyuple = (df1, df2, df3)\n', '    :param Xtrain:\n', '    :param cols_type_received:\n', '    :param cols_ref:\n', '    :return:\n', '    """"""\n']","['len', 'Xtuple_train.append']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:get_dataset,get_dataset,function,30,135,90,1380,10.22,1,6,"['Xtrain', 'cols_type_received', 'cols_ref']","[None, None, None]","[None, None, None]",276,[],"['len', 'Xtuple_train.append', 'get_dataset', 'data_pars.get', 'get_mydata_correl', 'get_dataset_tuple', 'log2', 'Exception']",8
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:get_label,get_label,function,18,40,34,351,8.78,1,1,"['encoder', 'x_train', 'dummy_train', 'class_num', 'batch_size']","[None, None, None, None, None]","[None, None, None, '5', '256']",325,[],"['encoder.predict', 'np.zeros', 'range', 'np.max', 'np.argmax']",5
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:reset,reset,function,3,7,6,43,6.14,0,0,[],[],[],341,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:save,save,function,17,33,31,377,11.42,0,0,"['path', 'info']","[None, None]","['None', 'None']",345,[],"['os.makedirs', 'Model', 'pickle.dump', 'open']",4
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:load_model,load_model,function,17,30,28,310,10.33,0,0,['path'],[None],"['""""']",362,[],"['pickle.load', 'Model']",2
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:load_info,load_info,function,13,25,23,172,6.88,1,1,['path'],[None],"['""""']",377,[],"['glob.glob', 'pickle.load', 'fp.split']",3
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:get_mydata_correl,get_mydata_correl,function,54,150,109,1753,11.69,7,2,['data_pars'],[None],[None],390,[],"['np.ones', 'range', 'np.zeros', 'np.floor', 'np.sum', 'np.matmul', 'np.uint32', 'np.corrcoef', 'np.squeeze']",9
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:test,test,function,29,127,111,1631,12.84,0,0,[],[],[],453,[],"['get_mydata_correl', 'np.uint32', 'test_helper', 'init', 'Model', 'log', 'fit', 'predict', 'save', 'load_model']",10
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:test_helper,test_helper,function,13,45,37,567,12.6,0,0,"['model_pars', 'data_pars', 'compute_pars', 'Xpred']","[None, None, None, None]","[None, None, None, None]",502,[],"['init', 'Model', 'log', 'fit', 'predict', 'save', 'load_model']",7
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:Model,Model,class,20,55,53,655,11.91,0,1,[],[],[],179,[],[],0
source/models/repo/TruncatedGaussianMixtureVAE/model_functions_4.py:Model:__init__,Model:__init__,method,19,50,48,587,11.74,0,1,"['self', 'model_pars', 'data_pars', 'compute_pars']","[None, None, None, None]","[None, 'None', 'None', 'None']",180,[],"['log2', 'np.uint32', 'model_pars.get', 'get_model']",4
