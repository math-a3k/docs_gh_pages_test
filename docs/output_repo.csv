uri,name,type,n_variable,n_words,n_words_unique,n_characters,avg_char_per_word,n_loop,n_ifthen,arg_name,arg_type,arg_value,line,docs,list_functions,n_functions
utilmy/ppandas.py:log,log,function,3,10,9,76,7.6,0,1,['*s'],[None],[None],6,[],"['print', 'log2']",2
utilmy/ppandas.py:log2,log2,function,2,6,6,32,5.33,0,1,"['*s', 'verbose']","[None, None]","[None, '1']",9,[],['print'],1
utilmy/ppandas.py:pd_random,pd_random,function,4,9,8,86,9.56,0,0,['nrows'],[None],['100'],15,[],"['pd.DataFrame', 'columns=list']",2
utilmy/ppandas.py:pd_merge,pd_merge,function,3,17,16,120,7.06,0,0,"['df1', 'df2', 'on', 'colkeep']","[None, None, None, None]","[None, None, 'None', 'None']",21,[],"['list', 'df1.join']",2
utilmy/ppandas.py:pd_plot_multi,pd_plot_multi,function,40,109,78,1103,10.12,2,3,"['df', 'plot_type', 'cols_axe1', 'cols_axe2', 'figsize', '4']","[None, None, 'list', 'list', None, None]","[None, 'None', '[]', '[]', '(8', None]",27,[],"['plt.figure', 'len', 'getattr', 'df.plot', 'plt.show', 'ax.set_ylabel', 'range', 'ax.get_legend_handles_labels', 'ax.twinx', 'ax_new.set_ylabel', 'ax_new.get_legend_handles_labels', 'ax.legend']",12
utilmy/ppandas.py:pd_filter,pd_filter,function,18,106,52,531,5.01,2,7,"['df', 'filter_dict', 'verbose']","[None, None, None]","[None, '""shop_id=11, l1_genre_id>600, l2_genre_id<80311,""', 'False']",76,"['    """"""\n', '     dfi = pd_filter2(dfa, ""shop_id=11, l1_genre_id>600, l2_genre_id<80311,"" )\n', '     dfi2 = pd_filter(dfa, {""shop_id"" : 11} )\n', '     ### Dilter dataframe with basic expr\n', '    """"""\n']","['isinstance', 'filter_dict.items', 'filter_dict.split', 'x_convert', 'str', 'dict', 'float', 'x.strip', 'print', 'len', 'x.split']",11
utilmy/ppandas.py:pd_to_file,pd_to_file,function,20,72,50,491,6.82,0,5,"['df', 'filei', 'check', 'verbose', 'show', '**kw']","[None, None, None, None, None, None]","[None, None, '0', 'True', ""'shape'"", None]",114,[],"['Path', 'os.makedirs', 'df.to_pickle', 'df.to_parquet', 'df.to_csv', 'log', 'gc.collect']",7
utilmy/ppandas.py:pd_sample_strat,pd_sample_strat,function,5,10,9,104,10.4,0,0,"['df', 'col', 'n']","[None, None, None]","[None, None, None]",143,[],"['df.groupby', 'x.sample']",2
utilmy/ppandas.py:pd_cartesian,pd_cartesian,function,12,22,18,154,7.0,0,0,"['df1', 'df2']","[None, None]","[None, None]",151,[],"['list', 'pd.merge']",2
utilmy/ppandas.py:pd_plot_histogram,pd_plot_histogram,function,24,71,45,428,6.03,0,4,"['dfi', 'path_save', 'nbin', 'q5', 'q95', 'nsample', 'show', 'clear']","[None, None, None, None, None, None, None, None]","[None, 'None', '20.0', '0.005', '0.995', ' -1', 'False', 'True']",163,[],"['dfi.quantile', 'dfi.hist', 'dfi.sample', 'plt.title', 'path_save.split', 'plt.show', 'os.makedirs', 'plt.savefig', 'print', 'plt.close']",10
utilmy/ppandas.py:pd_col_bins,pd_col_bins,function,8,19,18,128,6.74,0,0,"['df', 'col', 'nbins']","[None, None, None]","[None, None, '5']",188,[],"['pd.qcut', 'np.arange']",2
utilmy/ppandas.py:pd_dtype_reduce,pd_dtype_reduce,function,9,35,23,238,6.8,1,1,"['dfm', 'int0 ', 'float0 ']","[None, None, None]","[None, ""'int32'"", "" 'float32'""]",195,[],['np.dtype'],1
utilmy/ppandas.py:pd_dtype_count_unique,pd_dtype_count_unique,function,17,73,57,541,7.41,1,3,"['df', 'col_continuous']","[None, None]","[None, '[]']",204,"['    """"""Learns the number of categories in each variable and standardizes the data.\n', '        ----------\n', '        data: pd.DataFrame\n', '        continuous_ids: list of ints\n', '            List containing the indices of known continuous variables. Useful for\n', '            discrete data like age, which is better modeled as continuous.\n', '        Returns\n', '        -------\n', '        ncat:  number of categories of each variable. -1 if the variable is  continuous.\n', '    """"""\n']","['gef_is_continuous', 'np.sum', 'np.round', 'len', 'min', 'any', 'list', 'n=min']",8
utilmy/ppandas.py:pd_dtype_to_category,pd_dtype_to_category,function,15,36,29,350,9.72,1,3,"['df', 'col_exclude', 'treshold']","[None, None, None]","[None, None, '0.5']",243,"['  """"""\n', '    Convert string to category\n', '  """"""\n']","['isinstance', 'df.select_dtypes', 'len', 'float', 'pd.to_datetime', 'print']",6
utilmy/ppandas.py:pd_dtype_getcontinuous,pd_dtype_getcontinuous,function,10,43,29,261,6.07,1,2,"['df', 'cols_exclude', 'nsample']","[None, 'list', None]","[None, '[]', '-1']",262,[],"['len', 'str']",2
utilmy/ppandas.py:pd_del,pd_del,function,6,13,13,50,3.85,1,0,"['df', 'cols']","[None, 'list']","[None, None]",278,[],[],0
utilmy/ppandas.py:pd_add_noise,pd_add_noise,function,18,41,33,295,7.2,1,1,"['df', 'level', 'cols_exclude']","[None, None, 'list']","[None, '0.05', '[]']",287,[],"['pd.DataFrame', 'pd_dtype_getcontinuous', 'print']",3
utilmy/ppandas.py:pd_cols_unique_count,pd_cols_unique_count,function,10,43,29,261,6.07,1,2,"['df', 'cols_exclude', 'nsample']","[None, 'list', None]","[None, '[]', '-1']",301,[],"['len', 'str']",2
utilmy/ppandas.py:pd_show,pd_show,function,6,10,10,116,11.6,0,0,"['df', 'nrows', 'reader', '**kw']","[None, None, None, None]","[None, '100', ""'notepad.exe'"", None]",319,"['    """""" Show from Dataframe\n', '    """"""\n']",['os_makedirs'],1
utilmy/ppandas.py:to_dict,to_dict,function,2,2,2,8,4.0,0,0,['**kw'],[None],[None],339,[],[],0
utilmy/ppandas.py:to_timeunix,to_timeunix,function,2,16,12,187,11.69,0,2,['datex'],[None],"['""2018-01-16""']",344,[],"['isinstance', 'int', 'datex.timetuple']",3
utilmy/ppandas.py:to_datetime,to_datetime,function,5,8,8,45,5.62,0,0,['x'],[None],[None],352,[],"['pd.to_datetime', 'str']",2
utilmy/ppandas.py:np_list_intersection,np_list_intersection,function,4,10,8,24,2.4,1,1,"['l1', 'l2']","[None, None]","[None, None]",357,[],[],0
utilmy/ppandas.py:np_add_remove,np_add_remove,function,7,10,9,116,11.6,1,0,"['set_', 'to_remove', 'to_add']","[None, None, None]","[None, None, None]",361,[],"['set_.copy', 'result_temp.remove', 'result_temp.add']",3
utilmy/ppandas.py:to_float,to_float,function,1,8,6,46,5.75,0,0,['x'],[None],[None],370,[],['float'],1
utilmy/ppandas.py:to_int,to_int,function,1,8,6,44,5.5,0,0,['x'],[None],[None],377,[],"['int', 'float']",2
utilmy/ppandas.py:is_int,is_int,function,1,9,7,42,4.67,0,0,['x'],[None],[None],384,[],['int'],1
utilmy/ppandas.py:is_float,is_float,function,1,9,7,44,4.89,0,0,['x'],[None],[None],391,[],['float'],1
utilmy/ppandas.py:print_everywhere,print_everywhere,function,9,29,22,182,6.28,0,0,[],[],[],408,"['    """"""\n', '    https://github.com/alexmojaki/snoop\n', '    """"""\n']","['snoop.install', 'myfun', 'pp', 'print']",4
utilmy/ppandas.py:log10,log10,function,8,17,17,101,5.94,1,0,"['*s', 'nmax']","[None, None]","[None, '60']",427,"['    """""" Display variable name, type when showing,  pip install varname\n', '    \n', '    """"""\n']","['print', 'type', 'str']",3
utilmy/ppandas.py:log5,log5,function,5,7,7,37,5.29,0,0,['*s'],[None],[None],436,"['    """"""    ### Equivalent of print, but more :  https://github.com/gruns/icecream\n', '    pip install icrecream\n', '    ic()  --->  ic| example.py:4 in foo()\n', ""    ic(var)  -->   ic| d['key'][1]: 'one'\n"", '    \n', '    """"""\n']",['ic'],1
utilmy/ppandas.py:log_trace,log_trace,function,4,4,4,37,9.25,0,0,"['msg', 'dump_path', 'globs']","[None, None, None]","['""""', '""""', 'None']",447,[],"['print', 'pdb.set_trace']",2
utilmy/ppandas.py:dict_to_namespace,dict_to_namespace,class,3,5,5,36,7.2,0,0,[],[],[],333,[],[],0
utilmy/ppandas.py:dict_to_namespace:__init__,dict_to_namespace:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",335,[],[],0
utilmy/utils.py:log,log,function,10,39,25,357,9.15,0,0,['*s'],[None],[None],18,[],"['logger.info', 'log2', 'logger.debug', 'logw', 'logger.warning', 'loge', 'logger.error', 'logger_setup', 'logger.configure']",9
utilmy/utils.py:log2,log2,function,1,5,5,39,7.8,0,0,['*s'],[None],[None],22,[],['logger.debug'],1
utilmy/utils.py:logw,logw,function,1,5,5,41,8.2,0,0,['*s'],[None],[None],26,[],['logger.warning'],1
utilmy/utils.py:loge,loge,function,1,5,5,39,7.8,0,0,['*s'],[None],[None],30,[],['logger.error'],1
utilmy/utils.py:logger_setup,logger_setup,function,2,15,13,138,9.2,0,0,[],[],[],34,[],['logger.configure'],1
utilmy/utils.py:config_load,config_load,function,16,74,63,807,10.91,0,1,"['config_path', 'pathlib.Path]] ']","[' Optional[Union[str', None]","[None, ' None']",51,"['    """"""Load Config file into a dict\n', '    1) load config_path\n', '    2) If not, load in HOME USER\n', '    3) If not, create default one\n', ""    # config_default = yaml.load(os.path.join(os.path.dirname(__file__), 'config', 'config.yaml'))\n"", '\n', '    Args:\n', ""        config_path: path of config or 'default' tag value\n"", '    Returns: dict config\n', '    """"""\n']","['str', 'logw', 'log2', 'yaml.load', 'log', 'os.makedirs', 'open', 'json.dump']",8
utilmy/utils.py:dataset_donwload,dataset_donwload,function,9,19,17,246,12.95,0,0,"['url', 'path_target']","[None, None]","[None, None]",99,"['    """"""Donwload on disk the tar.gz file\n', '    Args:\n', '        url:\n', '        path_target:\n', '    Returns:\n', '\n', '    """"""\n']","['log', 'os.makedirs', 'wget.download', 'url.split', 'os_extract_archive', 'log2']",6
utilmy/utils.py:os_extract_archive,os_extract_archive,function,19,63,47,671,10.65,1,8,"['file_path', 'path', 'archive_format']","[None, None, None]","[None, '"".""', '""auto""']",117,"['    """"""Extracts an archive if it matches tar, tar.gz, tar.bz, or zip formats.\n', '    Args:\n', '        file_path: path to the archive file\n', '        path: path to extract the archive file\n', '        archive_format: Archive format to try for extracting the file.\n', ""            Options are 'auto', 'tar', 'zip', and None.\n"", ""            'tar' includes tar, tar.gz, and tar.bz files.\n"", ""            The default 'auto' is ['tar', 'zip'].\n"", '            None or an empty list will return no matches found.\n', '    Returns:\n', '        True if a match was found and an archive extraction was completed,\n', '        False otherwise.\n', '    """"""\n']","['isinstance', 'is_match_fn', 'open_fn', 'archive.extractall', 'os.remove', 'shutil.rmtree']",6
utilmy/utils.py:to_file,to_file,function,3,7,7,51,7.29,0,0,"['s', 'filep']","[None, None]","[None, None]",164,[],"['open', 'fp.write']",2
utilmy/keyvalue.py:os_environ_set,os_environ_set,function,1,2,2,7,3.5,0,0,"['name', 'value']","[None, None]","[None, None]",297,"['    """"""\n', '    https://stackoverflow.com/questions/716011/why-cant-environmental-variables-set-in-python-persist\n', '\n', '    """"""\n']",[],0
utilmy/keyvalue.py:os_path_size,os_path_size,function,14,28,23,197,7.04,2,1,['folder'],[None],['None'],308,"['    """"""\n', '       Get the size of a folder in bytes\n', '    """"""\n']","['os.getcwd', 'os.walk']",2
utilmy/keyvalue.py:db_init,db_init,function,8,25,21,207,8.28,1,0,"['db_dir', 'globs']","['str', None]","['""path""', 'None']",325,"['    """""" Initialize in the Global Space Name  globs= globals(), DB\n', '      db = Box({    \n', '          \'db_itemtag_items_path\'  :  f""{db_dir}/map_sampling_itemtag_siid.cache"",     \n', '          \'db_itemid_itemtag_path\' :  f""{db_dir}/map_itemid_itemtag.cache"",    \n', '      })\n', '\n', '    """"""\n']","['list', 'glob.glob', 'path.replace', 'path.split', 'log', 'len', 'diskcache_load']",7
utilmy/keyvalue.py:db_flush,db_flush,function,5,10,10,93,9.3,1,0,['db_dir'],[None],[None],345,"['    """"""\n', '     Flush wal files on the disk\n', '\n', '    """"""  \n']","['glob.glob', 'log', 'diskcache_config']",3
utilmy/keyvalue.py:db_size,db_size,function,4,13,13,105,8.08,1,0,['db_dir'],[None],[' None'],357,[],"['glob.glob', 'print', 'len', 'diskcache_load']",4
utilmy/keyvalue.py:db_merge,db_merge,function,12,46,40,370,8.04,0,1,[],[],[],366,[],"['fi2.append', 'len', 'log', 'pd_read_file', 'df.drop_duplicates', 'df.apply', 'pd_to_file']",7
utilmy/keyvalue.py:db_create_dict_pandas,db_create_dict_pandas,function,12,46,40,370,8.04,0,1,"['df', 'cols', 'colsu']","[None, None, None]","['None', 'None', 'None']",393,[],"['fi2.append', 'len', 'log', 'pd_read_file', 'df.drop_duplicates', 'df.apply', 'pd_to_file']",7
utilmy/keyvalue.py:db_load_dict,db_load_dict,function,12,35,28,260,7.43,0,2,"['df', 'colkey', 'colval', 'verbose']","[None, None, None, None]","[None, None, None, 'True']",423,[],"['isinstance', 'log', 'pd_read_file', 'df.drop_duplicates', 'df.set_index', 'len']",6
utilmy/keyvalue.py:diskcache_load,diskcache_load,function,10,27,23,214,7.93,0,2,"['db_path_or_object', 'size_limit', 'verbose']","[None, None, None]","['""""', '100000000000', 'True']",441,"['    """""" val = cache[mykey]\n', '    """"""\n']","['isinstance', 'dc.Cache', 'print', 'len']",4
utilmy/keyvalue.py:diskcache_save,diskcache_save,function,44,255,134,1712,6.71,4,5,"['df', 'colkey', 'colvalue', 'db_path', 'size_limit', 'timeout', 'shards', 'tbreak', '## Break during insert to prevent big WAL file**kw']","[None, None, None, None, None, None, None, None, None]","[None, None, None, '""./dbcache.db""', '100000000000', '10', '1', '1', None]",455,"['    """""" Create dict type on disk, < 100 Gb\n', '       shards>1 : disk spaced is BLOCKED in advance, so high usage\n', '    \n', '    """"""\n']","['dc.Cache', 'FanoutCache', 'print', 'range', 'time.sleep', 'len', 'diskcache_save2', 'diskcache_config', 'max', 'insert_key', 'enumerate', 'log', 'int', 'multithread_run', 'str']",15
utilmy/keyvalue.py:diskcache_save2,diskcache_save2,function,39,151,98,1028,6.81,3,3,"['df', 'colkey', 'colvalue', 'db_path', 'size_limit', 'timeout', 'shards', 'npool', 'sqlmode', 'verbose']","[None, None, None, None, None, None, None, None, None, None]","[None, None, None, '""./dbcache.db""', '100000000000', '10', '1', '10', "" 'fast'"", 'True']",488,"['    """""" Create dict type on disk, < 100 Gb\n', '       shards>1 : disk spaced is BLOCKED in advance, so high usage       \n', '       Issue, uses too much of DISK\n', '    """"""\n']","['dc.Cache', 'FanoutCache', 'print', 'diskcache_config', 'max', 'insert_key', 'enumerate', 'time.sleep', 'len', 'log', 'range', 'int', 'multithread_run', 'str']",14
utilmy/keyvalue.py:diskcache_getkeys,diskcache_getkeys,function,8,20,18,135,6.75,1,0,['cache'],[None],[None],543,[],"['diskcache_load', 'cache._sql']",2
utilmy/keyvalue.py:diskcache_keycount,diskcache_keycount,function,8,20,18,143,7.15,1,0,['cache'],[None],[None],550,[],"['diskcache_load', 'cache._sql', 'countt']",3
utilmy/keyvalue.py:diskcache_getall,diskcache_getall,function,8,26,23,159,6.12,1,0,"['cache', 'limit']","[None, None]","[None, '1000000000']",558,[],"['diskcache_load', 'cache._sql']",2
utilmy/keyvalue.py:diskcache_get,diskcache_get,function,8,20,18,135,6.75,1,0,['cache'],[None],[None],564,"['    """""" .open ""//e""    python prepro.py diskcache_config \n', '    https://sqlite.org/wal.html\n', '    \n', '    PRAGMA journal_mode = DELETE;   (You can switch it back afterwards.)\n', '    PRAGMA wal_checkpoint(TRUNCATE);\n', '    PRAGMA journal_mode = WAL;     \n', '    PRAGMA wal_checkpoint(FULL);\n', '    \n', '    """"""\n']","['diskcache_load', 'cache._sql']",2
utilmy/keyvalue.py:diskcache_config,diskcache_config,function,15,61,43,434,7.11,0,1,"['db_path', 'task']","[None, None]","['None', ""'commit'""]",573,"['    """""" .open ""//e""    python prepro.py diskcache_config \n', '    https://sqlite.org/wal.html\n', '    \n', '    PRAGMA journal_mode = DELETE;   (You can switch it back afterwards.)\n', '    PRAGMA wal_checkpoint(TRUNCATE);\n', '    PRAGMA journal_mode = WAL;     \n', '    PRAGMA wal_checkpoint(FULL);\n', '    \n', '    """"""\n']","['diskcache_load', 'diskcache_getkeys', 'log', 'cache.close', 'wal_checkpoint', 'open', 'fp.write', 'os.system']",8
utilmy/keyvalue.py:DBlist,DBlist,class,34,132,77,1180,8.94,5,4,[],[],[],204,[],[],0
utilmy/keyvalue.py:DBlist:__init__,DBlist:__init__,method,7,31,22,355,11.45,0,1,"['self', 'config_dict', 'config_path']","[None, None, None]","[None, 'None', 'None']",220,[],"['json.load', 'os.getcwd', 'config_dict']",3
utilmy/keyvalue.py:DBlist:add,DBlist:add,method,1,3,3,48,16.0,0,0,"['self', 'db_path']","[None, None]","[None, None]",236,[],['list'],1
utilmy/keyvalue.py:DBlist:remove,DBlist:remove,method,3,11,8,50,4.55,1,1,"['self', 'db_path']","[None, None]","[None, None]",239,[],[],0
utilmy/keyvalue.py:DBlist:list,DBlist:list,method,10,25,17,155,6.2,2,1,"['self', 'show']","[None, None]","[None, 'True']",242,[],"['glob.glob', 'os_path_size', 'print']",3
utilmy/keyvalue.py:DBlist:info,DBlist:info,method,11,22,19,217,9.86,1,1,"['self', '']","[None, None]","[None, None]",256,[],"['self.list', 'os_path_size', 'diskcache_load', 'diskcache_keycount', 'print']",5
utilmy/keyvalue.py:DBlist:clean,DBlist:clean,method,0,1,1,4,4.0,0,0,"['self', '']","[None, None]","[None, None]",270,[],[],0
utilmy/keyvalue.py:DBlist:check,DBlist:check,method,0,1,1,4,4.0,0,0,"['self', 'db_path']","[None, None]","[None, 'None']",275,"['        """"""\n', '           Check Sqlite cache.db if file is fine\n', '\n', '        """"""\n']",[],0
utilmy/keyvalue.py:DBlist:show,DBlist:show,method,8,14,14,121,8.64,1,0,"['self', 'db_path', 'n']","[None, None, None]","[None, 'None', '4']",283,"['        """"""\n', '           show content for each table\n', '\n', '        """"""\n']","['self.list', 'diskcache_load', 'diskcache_getall', 'log', 'str']",5
utilmy/utilmy.py:import_function,import_function,function,9,16,14,164,10.25,0,1,"['fun_name', 'module_name']","[None, None]","['None', 'None']",34,[],"['isinstance', 'importlib.import_module', 'getattr', 'globals']",4
utilmy/utilmy.py:help_create,help_create,function,16,34,25,218,6.41,2,1,"['modulename', 'prefixs']","[None, None]","[""'utilmy.nnumpy'"", 'None']",46,"['    """"""\n', '       Extract code source from test code\n', '    """"""\n']","['importlib.import_module', 'dir', 'import_function', 'help_get_codesource']",4
utilmy/utilmy.py:pd_random,pd_random,function,8,28,21,145,5.18,0,0,"['ncols', 'nrows']","[None, None]","['7', '100']",62,[],"['random.random', 'range', 'pd.DataFrame']",3
utilmy/utilmy.py:pd_generate_data,pd_generate_data,function,15,56,43,392,7.0,0,0,"['ncols', 'nrows']","[None, None]","['7', '100']",69,"['    """""" Generate sample data for function testing\n', '    categorical features for anova test\n', '    """"""\n']","['random.random', 'range', 'pd.DataFrame', 'np.where']",4
utilmy/utilmy.py:pd_getdata,pd_getdata,function,14,38,34,669,17.61,1,1,['verbose'],[None],['True'],83,"['    """"""data = test_get_data()\n', ""    df   = data['housing.csv']\n"", '    df.head(3)\n', '    https://github.com/szrlee/Stock-Time-Series-Analysis/tree/master/data\n', '    """"""\n']","['url.split', 'print', 'pd.read_csv']",3
utilmy/utilmy.py:git_repo_root,git_repo_root,function,7,23,19,142,6.17,0,1,[],[],[],262,[],"['os_system', 'mout.split', 'len']",3
utilmy/utilmy.py:git_current_hash,git_current_hash,function,6,12,10,123,10.25,0,0,['mode'],[None],"[""'full'""]",272,[],"['subprocess.check_output', 'label.decode']",2
utilmy/utilmy.py:save,save,function,5,10,10,140,14.0,0,0,"['dd', 'to_file', 'verbose']","[None, None, None]","[None, '""""', 'False']",352,[],"['os.makedirs', 'pickle.dump', 'open']",3
utilmy/utilmy.py:load,load,function,5,7,6,61,8.71,0,0,['to_file'],[None],"['""""']",359,[],['pickle.load'],1
utilmy/utilmy.py:Session,Session,class,42,178,114,1402,7.88,3,2,[],[],[],284,[],[],0
utilmy/utilmy.py:Session:__init__,Session:__init__,method,5,7,7,113,16.14,0,0,"['self', 'dir_session', '']","[None, None, None]","[None, '""ztmp/session/""', None]",290,[],"['os.makedirs', 'print']",2
utilmy/utilmy.py:Session:show,Session:show,method,5,7,7,62,8.86,0,0,['self'],[None],[None],296,[],"['glob.glob', 'print']",2
utilmy/utilmy.py:Session:save,Session:save,method,4,8,8,146,18.25,0,0,"['self', 'name', 'glob', 'tag']","[None, None, None, None]","[None, None, 'None', '""""']",301,[],"['os.makedirs', 'self.save_session']",2
utilmy/utilmy.py:Session:load,Session:load,method,4,9,9,126,14.0,0,0,"['self', 'name', 'glob', 'tag']","[None, None, 'dict', None]","[None, None, 'None', '""""']",307,[],"['print', 'self.load_session']",2
utilmy/utilmy.py:Session:save_session,Session:save_session,method,19,82,58,516,6.29,1,2,"['self', 'folder', 'globs', 'tag']","[None, None, None, None]","[None, None, None, '""""']",314,[],"['os.makedirs', 'globs.items', 'x.startswith', 'str', 'pd.to_pickle', 'save', 'print']",7
utilmy/utilmy.py:Session:load_session,Session:load_session,method,11,33,28,208,6.3,2,0,"['self', 'folder', 'globs']","[None, None, None]","[None, None, 'None']",337,"['      """"""\n', '      """"""\n']","['print', 'os.walk', 'x.replace', 'load']",4
utilmy/adatasets.py:log,log,function,3,11,7,84,7.64,0,2,['*s'],[None],[None],13,[],"['print', 'log2']",2
utilmy/adatasets.py:log2,log2,function,2,5,5,35,7.0,0,1,['*s'],[None],[None],16,[],['print'],1
utilmy/adatasets.py:dataset_classifier_XXXXX,dataset_classifier_XXXXX,function,10,22,19,118,5.36,0,0,"['nrows', '**kw']","[None, None]","['500', None]",21,"['    """"""\n', '\n', '    """"""\n']",[],0
utilmy/adatasets.py:pd_train_test_split,pd_train_test_split,function,18,62,33,696,11.23,0,0,"['df', 'coly']","[None, None]","[None, 'None']",36,[],"['df.drop', 'train_test_split', 'pd_train_test_split2', 'log2', 'np.sum', 'X.head', 'len']",7
utilmy/adatasets.py:pd_train_test_split2,pd_train_test_split2,function,17,35,28,392,11.2,0,0,"['df', 'coly']","[None, None]","[None, None]",44,[],"['log2', 'df.drop', 'np.sum', 'X.head', 'train_test_split', 'len']",6
utilmy/adatasets.py:dataset_classifier_pmlb,dataset_classifier_pmlb,function,13,18,17,159,8.83,0,0,"['name', 'return_X_y']","[None, None]","[""''"", 'False']",57,[],['fetch_data'],1
utilmy/adatasets.py:test_dataset_classifier_covtype,test_dataset_classifier_covtype,function,20,57,53,730,12.81,0,1,['nrows'],[None],['500'],67,[],"['log', 'os.getcwd', 'os_makedirs', 'Path', 'wget.download', 'pd.read_csv']",6
utilmy/adatasets.py:test_dataset_regression_fake,test_dataset_regression_fake,function,23,53,48,408,7.7,1,0,"['nrows', 'n_features']","[None, None]","['500', '17']",96,[],"['range', 'sklearn_datasets.make_regression', 'pd.DataFrame', 'y.reshape', 'len']",5
utilmy/adatasets.py:test_dataset_classification_fake,test_dataset_classification_fake,function,24,54,49,410,7.59,1,0,['nrows'],[None],['500'],113,[],"['range', 'sklearn_datasets.make_classification', 'pd.DataFrame', 'y.reshape', 'len']",5
utilmy/adatasets.py:test_dataset_classification_petfinder,test_dataset_classification_petfinder,function,31,79,71,940,11.9,0,1,['nrows'],[None],['1000'],131,[],"['os.makedirs', 'wget.download', 'zipfile.ZipFile', 'zip_ref.extractall', 'log', 'pd.read_csv', 'np.where', 'df.drop', 'shutil.rmtree', 'log2']",10
utilmy/adatasets.py:fetch_dataset,fetch_dataset,function,41,119,89,1131,9.5,0,4,"['url_dataset', 'path_target', 'file_target']","[None, None, None]","[None, 'None', 'None']",171,"['    """"""Fetch dataset from a given URL and save it.\n', '\n', '    Currently `github`, `gdrive` and `dropbox` are the only supported sources of\n', '    data. Also only zip files are supported.\n', '\n', '    :param url_dataset:   URL to send\n', '    :param path_target:   Path to save dataset\n', '    :param file_target:   File to save dataset\n', '\n', '    """"""\n']","['log', 'mkdtemp', 'pathlib.Path', 'mktemp', 'url_dataset.replace', 'urlx.replace', 'urlpath.split', 'os.makedirs', 'requests.Session', 's.get', 'print', 'open', 'f.write', 'res.raise_for_status']",14
utilmy/oos.py:log,log,function,3,10,9,76,7.6,0,1,['*s'],[None],[None],10,[],"['print', 'log2']",2
utilmy/oos.py:log2,log2,function,2,6,6,32,5.33,0,1,"['*s', 'verbose']","[None, None]","[None, '1']",13,[],['print'],1
utilmy/oos.py:help,help,function,3,3,3,17,5.67,0,0,[],[],[],16,[],['print'],1
utilmy/oos.py:to_dict,to_dict,function,2,2,2,8,4.0,0,0,['**kw'],[None],[None],29,[],[],0
utilmy/oos.py:to_timeunix,to_timeunix,function,2,16,12,187,11.69,0,2,['datex'],[None],"['""2018-01-16""']",34,[],"['isinstance', 'int', 'datex.timetuple']",3
utilmy/oos.py:to_datetime,to_datetime,function,5,8,8,45,5.62,0,0,['x'],[None],[None],42,[],"['pd.to_datetime', 'str']",2
utilmy/oos.py:np_list_intersection,np_list_intersection,function,4,10,8,24,2.4,1,1,"['l1', 'l2']","[None, None]","[None, None]",47,[],[],0
utilmy/oos.py:np_add_remove,np_add_remove,function,7,10,9,116,11.6,1,0,"['set_', 'to_remove', 'to_add']","[None, None, None]","[None, None, None]",51,[],"['set_.copy', 'result_temp.remove', 'result_temp.add']",3
utilmy/oos.py:to_float,to_float,function,1,8,6,46,5.75,0,0,['x'],[None],[None],60,[],['float'],1
utilmy/oos.py:to_int,to_int,function,1,8,6,44,5.5,0,0,['x'],[None],[None],67,[],"['int', 'float']",2
utilmy/oos.py:is_int,is_int,function,1,9,7,42,4.67,0,0,['x'],[None],[None],74,[],['int'],1
utilmy/oos.py:is_float,is_float,function,1,9,7,44,4.89,0,0,['x'],[None],[None],81,[],['float'],1
utilmy/oos.py:os_path_size,os_path_size,function,11,23,19,176,7.65,2,1,['path '],[None],"["" '.'""]",95,[],['os.walk'],1
utilmy/oos.py:os_path_split,os_path_split,function,10,27,21,215,7.96,0,2,['fpath'],['str'],"['""""']",108,[],"['fpath.replace', 'fpath.split']",2
utilmy/oos.py:os_file_replacestring,os_file_replacestring,function,13,35,30,416,11.89,2,0,"['findstr', 'replacestr', 'some_dir', 'pattern', 'dirlevel']","[None, None, None, None, None]","[None, None, None, '""*.*""', '1']",125,"['    """""" #fil_replacestring_files(""logo.png"", ""logonew.png"", r""D:/__Alpaca__details/aiportfolio"",\n', '        pattern=""*.html"", dirlevel=5  )\n', '    """"""\n']","['os_file_replacestring1', 'fileinput.FileInput', 'line.replace', 'file1.close', 'print', 'format', 'os_walk']",7
utilmy/oos.py:os_walk,os_walk,function,26,64,44,507,7.92,4,2,"['path', 'pattern', 'dirlevel']","[None, None, None]","[None, '""*""', '50']",147,"['    """""" dirlevel=0 : root directory\n', '        dirlevel=1 : 1 path below\n', '\n', '    """"""\n']","['path.replace', 'dir1.count', 'os.walk', 'root.replace', 'root.count', 'fnmatch.filter']",6
utilmy/oos.py:os_copy_safe,os_copy_safe,function,28,110,68,586,5.33,2,5,"['dirin', 'dirout', 'nlevel', 'nfile', 'cmd_fallback']","[None, None, None, None, None]","['None', 'None', '10', '100000', '""""']",173,"['    """""" Copy Safely/slowly between drive    \n', '    \n', '    """"""\n']","['range', 'glob.glob', 'len', 'log', 'fi.replace', 'time.sleep', 'os.makedirs', 'shutil.copy', 'os.system']",9
utilmy/oos.py:z_os_search_fast,z_os_search_fast,function,29,122,72,893,7.32,4,4,"['fname', 'texts', 'mode']","[None, None, None]","[None, 'None', '""regex/str""']",210,[],"['re.compile', 'enumerate', 're.search', 'line.decode', 'res.append', 'found.start', 'text.encode', 'line.find', 'print']",9
utilmy/oos.py:os_search_content,os_search_content,function,13,37,33,301,8.14,1,1,"['srch_pattern', 'mode', 'dir1', 'file_pattern', 'dirlevel']","[None, None, None, None, None]","['None', '""str""', '""""', '""*.*""', '1']",254,"['    """"""  search inside the files\n', '\n', '    """"""\n']","['os_walk', 'z_os_search_fast', 'pd.DataFrame']",3
utilmy/oos.py:os_get_function_name,os_get_function_name,function,6,31,18,199,6.42,0,0,[],[],[],270,[],"['str', 'socket.gethostname', 'sys._getframe']",3
utilmy/oos.py:os_variable_init,os_variable_init,function,3,12,10,45,3.75,1,0,"['ll', 'globs']","[None, None]","[None, None]",283,[],[],0
utilmy/oos.py:os_import,os_import,function,14,59,37,426,7.22,3,3,"['mod_name', 'globs', 'verbose']","[None, None, None]","['""myfile.config.model""', 'None', 'True']",291,[],"['__import__', 'hasattr', 'dir', 'name.startswith', 'all_names2.append', 'print', 'globs.update', 'getattr']",8
utilmy/oos.py:os_variable_exist,os_variable_exist,function,4,19,16,111,5.84,0,1,"['x', 'globs', 'msg']","[None, None, None]","[None, None, '""""']",318,[],"['str', 'log']",2
utilmy/oos.py:os_variable_check,os_variable_check,function,9,31,26,149,4.81,1,2,"['ll', 'globs', 'do_terminate']","[None, None, None]","[None, 'None', 'True']",328,[],"['Exception', 'log', 'sys.exit']",3
utilmy/oos.py:os_clean_memory,os_clean_memory,function,5,13,12,56,4.31,1,0,"['varlist', 'globx']","[None, None]","[None, None]",340,[],['gc.collect'],1
utilmy/oos.py:os_system_list,os_system_list,function,11,45,38,219,4.87,1,1,"['ll', 'logfile', 'sleep_sec']","[None, None, None]","[None, 'None', '10']",348,[],"['len', 'enumerate', 'log', 'os.system', 'time.sleep']",5
utilmy/oos.py:os_file_check,os_file_check,function,4,16,14,121,7.56,0,0,['fp'],[None],[None],370,[],"['log', 'os.stat', 'time.ctime']",3
utilmy/oos.py:os_to_file,os_to_file,function,3,7,7,52,7.43,0,0,"['txt', 'filename', 'mode']","[None, None, None]","['""""', '""ztmp.txt""', ""'a'""]",378,[],"['open', 'fp.write']",2
utilmy/oos.py:os_platform_os,os_platform_os,function,2,2,2,18,9.0,0,0,[],[],[],383,[],[],0
utilmy/oos.py:os_cpu,os_cpu,function,2,2,2,20,10.0,0,0,[],[],[],388,[],['os.cpu_count'],1
utilmy/oos.py:os_platform_ip,os_platform_ip,function,0,1,1,4,4.0,0,0,[],[],[],393,[],[],0
utilmy/oos.py:os_memory,os_memory,function,13,36,30,278,7.72,1,1,[],[],[],398,"['    """""" Get node total memory and memory usage in linux\n', '    """"""\n']","['open', 'i.split', 'str', 'int']",4
utilmy/oos.py:os_sleep_cpu,os_sleep_cpu,function,11,39,36,234,6.0,1,1,"['cpu_min', 'sleep', 'interval', 'msg', 'verbose']","[None, None, None, None, None]","['30', '10', '5', ' """"', 'True']",415,[],"['psutil.cpu_percent', 'log', 'time.sleep']",3
utilmy/oos.py:os_sizeof,os_sizeof,function,17,58,39,315,5.43,0,4,"['o', 'ids', 'hint', 'set(']","[None, None, None, None]","[None, None, '"" deep_getsizeof(df_pd', None]",427,"['    """""" deep_getsizeof(df_pd, set())\n', '    Find the memory footprint of a Python object\n', '    The sys.getsizeof function does a shallow size of only. It counts each\n', '    object inside a container as pointer only regardless of how big it\n', '    """"""\n']","['id', 'getsizeof', 'ids.add', 'isinstance', 'sum', 'd', 'o.items']",7
utilmy/oos.py:os_copy,os_copy,function,28,110,68,586,5.33,2,5,"['dirin', 'dirout', 'nlevel', 'nfile', 'cmd_fallback']","[None, None, None, None, None]","['None', 'None', '10', '100000', '""""']",458,"['    """"""  issues with no empty Folder\n', ""    # Delete everything reachable from the directory named in 'top',\n"", '    # assuming there are no symbolic links.\n', ""    # CAUTION:  This is dangerous!  For example, if top == '/', it could delete all your disk files.\n"", '    """"""\n']","['range', 'glob.glob', 'len', 'log', 'fi.replace', 'time.sleep', 'os.makedirs', 'shutil.copy', 'os.system']",9
utilmy/oos.py:os_removedirs,os_removedirs,function,12,48,32,294,6.12,3,1,['path'],[None],[None],470,"['    """"""  issues with no empty Folder\n', ""    # Delete everything reachable from the directory named in 'top',\n"", '    # assuming there are no symbolic links.\n', ""    # CAUTION:  This is dangerous!  For example, if top == '/', it could delete all your disk files.\n"", '    """"""\n']","['len', 'print', 'os.walk', 'os.remove', 'os.rmdir']",5
utilmy/oos.py:os_getcwd,os_getcwd,function,3,6,5,66,11.0,0,0,[],[],[],497,[],[],0
utilmy/oos.py:os_system,os_system,function,11,45,38,219,4.87,1,1,"['ll', 'logfile', 'sleep_sec']","[None, None, None]","[None, 'None', '10']",503,"['  """""" get values\n', '       os_system( f""   ztmp "",  doprint=True)\n', '  """"""\n']","['len', 'enumerate', 'log', 'os.system', 'time.sleep']",5
utilmy/oos.py:os_makedirs,os_makedirs,function,2,13,12,198,15.23,0,1,['dir_or_file'],[None],[None],520,[],"['dir_or_file.split', 'os.makedirs']",2
utilmy/oos.py:print_everywhere,print_everywhere,function,9,29,22,182,6.28,0,0,[],[],[],531,"['    """"""\n', '    https://github.com/alexmojaki/snoop\n', '    """"""\n']","['snoop.install', 'myfun', 'pp', 'print']",4
utilmy/oos.py:log10,log10,function,8,17,17,101,5.94,1,0,"['*s', 'nmax']","[None, None]","[None, '60']",550,"['    """""" Display variable name, type when showing,  pip install varname\n', '    \n', '    """"""\n']","['print', 'type', 'str']",3
utilmy/oos.py:log5,log5,function,5,7,7,37,5.29,0,0,['*s'],[None],[None],559,"['    """"""    ### Equivalent of print, but more :  https://github.com/gruns/icecream\n', '    pip install icrecream\n', '    ic()  --->  ic| example.py:4 in foo()\n', ""    ic(var)  -->   ic| d['key'][1]: 'one'\n"", '    \n', '    """"""\n']",['ic'],1
utilmy/oos.py:log_trace,log_trace,function,4,4,4,37,9.25,0,0,"['msg', 'dump_path', 'globs']","[None, None, None]","['""""', '""""', 'None']",570,[],"['print', 'pdb.set_trace']",2
utilmy/oos.py:profiler_start,profiler_start,function,6,9,9,82,9.11,0,0,[],[],[],576,[],"['Profiler', 'profiler.start']",2
utilmy/oos.py:profiler_stop,profiler_stop,function,3,5,5,83,16.6,0,0,[],[],[],584,[],"['profiler.stop', 'print']",2
utilmy/oos.py:dict_to_namespace,dict_to_namespace,class,3,5,5,36,7.2,0,0,[],[],[],23,[],[],0
utilmy/oos.py:dict_to_namespace:__init__,dict_to_namespace:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",25,[],[],0
utilmy/distributed.py:log2,log2,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],16,[],['print'],1
utilmy/distributed.py:log_mem,log_mem,function,3,8,8,76,9.5,0,0,['*s'],[None],[None],20,[],"['log2', 'str']",2
utilmy/distributed.py:test1_functions,test1_functions,function,3,15,14,181,12.07,0,0,[],[],[],33,"['    """"""Check that list function is working.\n', '    os_lock_releaseLock, os_lock_releaseLock, os_lock_execute\n', '\n', '    Basic test on only 1 thread\n', '    """"""\n']","['running', 'print', 'os_lock_execute']",3
utilmy/distributed.py:test2_funtions_thread,test2_funtions_thread,function,12,43,29,446,10.37,1,0,[],[],[],48,"['    """"""Check that list function is working.\n', '    os_lock_releaseLock, os_lock_releaseLock, os_lock_execute\n', '    Multi threads\n', '\n', '    How the test work.\n', '    - Create and run 5 threads. These threads try to access and use 1 function `running`\n', '    with os_lock_execute. So in one 1, only 1 thread can access and use this function.\n', '    """"""\n']","['running', 'print', 'time.sleep', 'thread_running', 'os_lock_execute', 'range', 'threading.Thread', 't.start']",8
utilmy/distributed.py:test3_index,test3_index,function,16,34,32,337,9.91,1,0,[],[],[],79,"['    """"""Check that class IndexLock is working\n', '    Multi threads\n', '\n', '    How the test work.\n', '    - The test will create the INDEX with the file using plock\n', '    - Create 100 threads that try to write data to this INDEX file lock\n', '    - This test will make sure with this INDEX file log\n', '        only 1 thread can access and put data to this file.\n', '        Others will waiting to acquire key after thread release it.\n', '    """"""\n']","['IndexLock', 'open', 'thread_running', 'print', 'INDEX.put', 'range', 'threading.Thread', 't.start']",8
utilmy/distributed.py:test_all,test_all,function,3,3,3,54,18.0,0,0,[],[],[],113,[],"['test1_funtions', 'test2_funtions_thread', 'test3_index']",3
utilmy/distributed.py:os_lock_acquireLock,os_lock_acquireLock,function,7,14,13,211,15.07,0,0,['plock'],['str'],"['""tmp/plock.lock""']",120,"[""    ''' acquire exclusive lock file access, return the locker\n"", '\n', ""    '''\n""]","['os.makedirs', 'open', 'fcntl.flock']",3
utilmy/distributed.py:os_lock_releaseLock,os_lock_releaseLock,function,3,4,4,61,15.25,0,0,['locked_file_descriptor'],[None],[None],131,"[""    ''' release exclusive lock file access '''\n""]",['fcntl.flock'],1
utilmy/distributed.py:os_lock_execute,os_lock_execute,function,9,27,24,182,6.74,1,0,"['fun_run', 'fun_args', 'ntry', 'plock']","[None, None, None, None]","[None, 'None', '5', '""tmp/plock.lock""']",138,"['    """""" Run a function in an atomic way :\n', '         Write on disk  exclusively on COMMON File.\n', '\n', '    """"""\n']","['os_lock_acquireLock', 'fun_run', 'os_lock_releaseLock', 'log2', 'time.sleep']",5
utilmy/distributed.py:date_now,date_now,function,11,14,12,174,12.43,0,0,"['fmt = ""%Y-%m-%d %H']",[''],"[' ""%Y-%m-%d %H:%M:%S %Z%z""']",204,[],"['datetime.now', 'now_utc.astimezone', 'now_pacific.strftime']",3
utilmy/distributed.py:time_sleep_random,time_sleep_random,function,4,6,6,52,8.67,0,0,['nmax'],[None],['5'],214,[],"['time.sleep', 'random.randrange']",2
utilmy/distributed.py:save,save,function,5,10,10,140,14.0,0,0,"['dd', 'to_file', 'verbose']","[None, None, None]","[None, '""""', 'False']",219,[],"['os.makedirs', 'pickle.dump', 'open']",3
utilmy/distributed.py:load,load,function,10,17,15,153,9.0,0,0,['to_file'],[None],"['""""']",226,[],"['pickle.load', 'load_serialize', 'log2', 'load']",4
utilmy/distributed.py:load_serialize,load_serialize,function,5,9,8,66,7.33,0,0,['name'],[None],[None],232,[],"['log2', 'load']",2
utilmy/distributed.py:save_serialize,save_serialize,function,3,7,7,56,8.0,0,0,"['name', 'value']","[None, None]","[None, None]",240,[],"['log2', 'save']",2
utilmy/distributed.py:IndexLock,IndexLock,class,21,76,55,519,6.83,1,1,[],[],[],158,[],[],0
utilmy/distributed.py:IndexLock:__init__,IndexLock:__init__,method,4,4,4,35,8.75,0,0,"['self', 'findex', 'plock']","[None, None, None]","[None, None, None]",165,[],[],0
utilmy/distributed.py:IndexLock:get,IndexLock:get,method,5,10,9,66,6.6,0,0,['self'],[None],[None],170,[],"['open', 'fp.readlines']",2
utilmy/distributed.py:IndexLock:put,IndexLock:put,method,14,50,41,317,6.34,1,1,"['self', 'val', 'ntry', 'plock']","[None, None, None, None]","[None, '""""', '100', '""tmp/plock.lock""']",176,[],"['os_lock_acquireLock', 'open', 'fp.readlines', 'set', 'fp.write', 'val.strip', 'os_lock_releaseLock', 'log2', 'time.sleep']",9
utilmy/parallel.py:log,log,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],13,[],['print'],1
utilmy/parallel.py:help,help,function,3,6,5,24,4.0,0,0,[],[],[],17,[],['print'],1
utilmy/parallel.py:pd_random,pd_random,function,2,16,16,97,6.06,0,0,"['nrows', 'ncols']","[None, None]","['1000', ' 5']",24,[],"['pd.DataFrame', 'str', 'range']",3
utilmy/parallel.py:test_fun_sum_inv,test_fun_sum_inv,function,6,22,16,97,4.41,1,1,"['group', 'name']","[None, None]","[None, 'None']",28,[],"['log', 'isinstance', 'sum']",3
utilmy/parallel.py:test_fun_sum,test_fun_sum,function,6,22,16,97,4.41,1,1,"['group', 'name']","[None, None]","[None, 'None']",32,[],"['log', 'isinstance', 'sum']",3
utilmy/parallel.py:test_fun_sum2,test_fun_sum2,function,6,22,16,97,4.41,1,1,"['list_vars', 'const', 'const2']","[None, None, None]","[None, '1', '1']",38,[],"['log', 'isinstance', 'sum']",3
utilmy/parallel.py:test_fun_run,test_fun_run,function,3,9,9,94,10.44,0,0,"['list_vars', 'const', 'const2']","[None, None, None]","[None, '1', '1']",47,[],['log'],1
utilmy/parallel.py:test_run_multithread,test_run_multithread,function,5,25,14,284,11.36,0,0,"['thread_name', 'num', 'string']","[None, None, None]","[None, None, None]",54,[],"['print', 'test_run_multithread2']",2
utilmy/parallel.py:test_run_multithread2,test_run_multithread2,function,3,11,9,113,10.27,0,0,"['thread_name', 'arg']","[None, None]","[None, None]",61,[],['print'],1
utilmy/parallel.py:test_sum,test_sum,function,2,3,3,19,6.33,0,0,['x'],[None],[None],67,[],[],0
utilmy/parallel.py:test0,test0,function,39,436,215,3726,8.55,5,0,[],[],[],71,[],"['pd_random', 'log', 'time.time', 'df.groupby', 'test_fun_sum_inv', 'df1.sort_values', 'list', 'pd_groupby_parallel', 'df2.sort_values', 'df1.equals', 'pd_groupby_parallel2', 'df.copy', 'df.apply', 'test_sum', 'pd_apply_parallel', 'df2.sort_index', 'df1.sort_index', 'multiproc_run', 'len', 'sum', 'range', 'n_pool=len', 'test_fun_run', 'multithread_run', 'multithread_run_list']",25
utilmy/parallel.py:test_pdreadfile,test_pdreadfile,function,22,122,75,1050,8.61,1,0,[],[],[],184,[],"['random.random', 'range', 'pd.DataFrame', 'len', 'os.makedirs', 'df.to_csv', 'pd_read_file', 'round']",8
utilmy/parallel.py:pd_read_file,pd_read_file,function,80,507,197,3547,7.0,7,30,"['path_glob', 'ignore_index', 'cols', 'verbose', 'nrows', 'nfile', 'concat_sort', 'n_pool', 'npool', 'drop_duplicates', 'col_filter', 'col_filter_val', 'dtype_reduce', 'fun_apply', '**kw']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","['""*.pkl""', 'True', 'None', 'False', '-1', '1000000', 'True', '1', 'None', 'None', 'None', 'None', 'None', 'None', None]",228,"['    """"""  Read file in parallel from disk : very Fast\n', '    :param path_glob: list of pattern, or sep by "";""\n', '    :return:\n', '    """"""\n']","['isinstance', 'log', 'print', 'path_glob.split', 'file_list.extend', 'sorted', 'glob.glob', 'file_list.append', 'fun_async', 'readers.get', 'pd_reader_obj', 'pd.DataFrame', 'dfi.drop_duplicates', 'dfi.apply', 'fun_apply', 'enumerate', 'futures.append', 'executor.submit', 'future.result', 'pd.concat', 'gc.collect', 'pd_read_file2', 'len', 'ThreadPool', 'range', 'job_list.append', 'pool.apply_async', 'pool.terminate', 'pool.join']",29
utilmy/parallel.py:pd_read_file2,pd_read_file2,function,69,285,159,1817,6.38,4,19,"['path_glob', 'ignore_index', 'cols', 'verbose', 'nrows', 'nfile', 'concat_sort', 'n_pool', 'npool', 'drop_duplicates', 'col_filter', 'col_filter_val', 'dtype_reduce', 'fun_apply', '**kw']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","['""*.pkl""', 'True', 'None', 'False', '-1', '1000000', 'True', '1', 'None', 'None', 'None', 'None', 'None', 'None', None]",297,"['    """"""  Read file in parallel from disk : very Fast\n', '    :param path_glob: list of pattern, or sep by "";""\n', '    :return:\n', '    """"""\n']","['isinstance', 'log', 'print', 'path_glob.split', 'file_list.extend', 'sorted', 'glob.glob', 'file_list.append', 'len', 'fun_async', 'readers.get', 'pd_reader_obj', 'dfi.drop_duplicates', 'dfi.apply', 'fun_apply', 'ThreadPool', 'pd.DataFrame', 'range', 'job_list.append', 'pool.apply_async', 'pd.concat', 'gc.collect', 'pool.terminate', 'pool.join']",24
utilmy/parallel.py:pd_groupby_parallel2,pd_groupby_parallel2,function,26,51,48,531,10.41,1,0,"['df', 'colsgroup', 'fun_apply', 'npool', '**kw', '']","[None, None, None, ' int ', None, None]","[None, 'None', 'None', ' 1', None, None]",388,"['    """"""Performs a Pandas groupby operation in parallel.\n', '    pd.core.groupby.DataFrameGroupBy\n', '    Example usage:\n', '        import pandas as pd\n', ""        df = pd.DataFrame({'A': [0, 1], 'B': [100, 200]})\n"", ""        df.groupby(df.groupby('A'), lambda row: row['B'].sum())\n"", '    Authors: Tamas Nagy and Douglas Myers-Turnbull\n', '    """"""\n']","['df.groupby', 'time.time', 'int', 'log', 'multiprocessing.Pool', 'multiprocessing.Manager', 'pool.starmap_async', 'itertools.cycle', 'result.ready', 'len', 'next', 'time.sleep', 'result.get', 'pd.concat']",14
utilmy/parallel.py:pd_groupby_parallel,pd_groupby_parallel,function,46,107,82,1079,10.08,3,1,"['df', 'colsgroup', 'fun_apply', 'npool', '**kw', '']","[None, None, None, ' int ', None, None]","[None, 'None', 'None', ' 1', None, None]",419,[],"['df.groupby', 'time.time', 'int', 'log', 'multiprocessing.Pool', 'multiprocessing.Manager', 'pool.starmap_async', 'itertools.cycle', 'result.ready', 'len', 'next', 'time.sleep', 'result.get', 'pd.concat', 'pd_groupby_parallel', 'isinstance', 'futures.append', 'gc.collect', 'pd.DataFrame', 'future.result']",20
utilmy/parallel.py:pd_apply_parallel,pd_apply_parallel,function,30,85,63,576,6.78,2,2,"['df', 'fun_apply', 'npool', 'verbose']","[None, None, None, None]","[None, 'None', '5', 'True']",446,"['    """""" Pandas parallel apply\n', '    """"""\n']","['f2', 'df.apply', 'int', 'range', 'futures.append', 'executor.submit', 'log', 'future.result', 'pd.concat']",9
utilmy/parallel.py:multiproc_run,multiproc_run,function,35,105,80,791,7.53,5,5,"['fun_async', 'input_list', 'n_pool', 'start_delay', 'verbose', 'input_fixed', 'npool', '**kw']","[None, ' list', None, None, None, 'dict', None, None]","[None, None, '5', '0.1', 'True', 'None', 'None', None]",481,"['    """"""  Multiprocessing execute\n', '    input is as list of tuples  [(x1,x2,x3), (y1,y2,y3) ]\n', '    def fun_async(xlist):\n', '      for x in xlist :\n', '            download.upload(x[0], x[1])\n', '          def f(i, n):\n', '       return i * i + 2*n\n', '    ..\n', '     from itertools import repeat\n', '     N = 10000\n', '     from pathos.pools import ProcessPool as Pool\n', '     pool = Pool()\n', '     ans = pool.map(f, xrange(1000), repeat(20))\n', '     ans[:10]\n', '    [40, 41, 44, 49, 56, 65, 76, 89, 104, 121]\n', '     # this also works\n', '     ans = pool.map(lambda x: f(x, 20), xrange(1000))\n', '     ans[:10]\n', '    [40, 41, 44, 49, 56, 65, 76, 89, 104, 121]\n', ""    input_fixed = {'const': 555}\n"", '    """"""\n']","['isinstance', 'len', 'functools.partial', 'range', 'enumerate', 'log', 'mp.Pool', 'time.sleep', 'job_list.append', 'res_list.append', 'pool.terminate', 'pool.join']",12
utilmy/parallel.py:multithread_run,multithread_run,function,32,101,76,779,7.71,5,5,"['fun_async', 'input_list', 'n_pool', 'start_delay', 'verbose', 'input_fixed', 'npool', '**kw']","[None, ' list', None, None, None, 'dict', None, None]","[None, None, '5', '0.1', 'True', 'None', 'None', None]",546,"['    """"""  input is as list of tuples  [(x1,x2,x3), (y1,y2,y3) ]\n', '    def fun_async(xlist):\n', '      for x in xlist :\n', '            hdfs.upload(x[0], x[1])\n', ""    input_fixed = {'const_var' : 1 }\n"", '    """"""\n']","['isinstance', 'len', 'functools.partial', 'range', 'enumerate', 'log', 'time.sleep', 'job_list.append', 'res_list.append', 'pool.terminate', 'pool.join']",11
utilmy/parallel.py:multiproc_tochunk,multiproc_tochunk,function,10,28,26,149,5.32,1,1,"['flist', 'npool']","[None, None]","[None, '2']",596,[],"['len', 'range', 'll.append', 'log', 'str']",5
utilmy/parallel.py:multithread_run_list,multithread_run_list,function,19,48,40,530,11.04,3,0,['**kwargs'],[None],[None],607,"['    """""" Creating n number of threads:  1 thread per function,    starting them and waiting for their subsequent completion\n', '    os_multithread(function1=(test_print, (""some text"",)),\n', '                          function2=(test_print, (""bbbbb"",)),\n', '                          function3=(test_print, (""ccccc"",)))\n', '    """"""\n']","['ThreadWithResult', '__init__', 'function', 'target', 'super', 'kwargs.values', 'list_of_threads.append', 'thread.start', 'zip', 'kwargs.keys', 'thread.join', 'results.append']",12
utilmy/parallel.py:z_pd_read_file3,z_pd_read_file3,function,66,255,143,1652,6.48,4,19,"['path_glob', 'ignore_index', 'cols', 'verbose', 'nrows', 'concat_sort', 'n_pool', 'npool', 'drop_duplicates', 'col_filter', 'col_filter_val', 'dtype_reduce', '**kw']","[None, None, None, None, None, None, None, None, None, None, None, None, None]","['""*.pkl""', 'True', 'None', 'False', '-1', 'True', '1', 'None', 'None', 'None', 'None', 'None', None]",661,"['    """"""  Read file in parallel from disk : very Fast\n', '    :param path_glob: list of pattern, or sep by "";""\n', '    :return:\n', '    """"""\n']","['isinstance', 'log', 'print', 'path_glob.split', 'file_list.extend', 'sorted', 'glob.glob', 'file_list.append', 'len', 'ThreadPool', 'pd.DataFrame', 'range', 'job_list.append', 'pool.apply_async', 'dfi.drop_duplicates', 'gc.collect', 'pd.concat', 'pool.terminate', 'pool.join']",19
utilmy/parallel.py:zz_pd_read_file3,zz_pd_read_file3,function,72,275,162,1887,6.86,7,16,"['path_glob', 'ignore_index', 'cols', 'nrows', 'concat_sort', 'n_pool', 'npool', 'drop_duplicates', 'col_filter', 'col_filter_val', 'dtype_reduce', 'fun_apply', 'max_file', '#### apply function for each subverbose', '**kw']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","['""*.pkl""', 'True', 'None', '-1', 'True', '1', 'None', 'None', 'None', 'None', 'None', 'None', '-1', 'False', None]",746,"['    """"""  Read file in parallel from disk : very Fast\n', '    :param path_glob: list of pattern, or sep by "";""\n', '    :return:\n', '    """"""\n']","['log', 'print', 'isinstance', 'path_glob.split', 'file_list.extend', 'sorted', 'glob.glob', 'file_list.append', 'len', 'fun_async', 'pd.DataFrame', 'readers.get', 'pd_reader_obj', 'dfi.drop_duplicates', 'dfi.apply', 'fun_apply', 'pd.concat', 'range', 'enumerate', 'tuple', 'time.sleep', 'ThreadPool', 'job_list.append', 'pool.apply_async', 'gc.collect', 'pool.terminate', 'pool.join']",27
utilmy/parallel.py:zz_pd_groupby_parallel5,zz_pd_groupby_parallel5,function,39,98,71,730,7.45,4,2,"['df', 'colsgroup', 'fun_apply', 'npool', 'verbose', '**kw']","[None, None, None, None, None, None]","[None, 'None', 'None', '5', 'False', None]",837,"['    """""" Pandas parallel groupby apply\n', '    """"""\n']","['f2', 'pd.DataFrame', 'dfi.apply', 'fun_apply', 'pd.concat', 'gc.collect', 'df.groupby', 'enumerate', 'log', 'len', 'job_list.append', 'range', 'pool.terminate', 'pool.join']",14
utilmy/parallel.py:ztest1,ztest1,function,20,140,98,956,6.83,2,0,[],[],[],880,[],"['fun_async', 'list.append', 'apply_func', 'multithread_run', 'log', 'pd.DataFrame', 'df.copy', 'pd_groupby_parallel', 'pd_groupby_parallel2']",9
utilmy/parallel.py:ztest2,ztest2,function,26,73,54,783,10.73,1,0,[],[],[],924,[],"['addition', 'addition1', 'fun_async', 'test_print', 'print', 'freeze_support', 'log', 'pd.DataFrame', 'pd_groupby_parallel', 'pd_groupby_parallel2', 'pd_apply_parallel', 'multithread_run', 'multithread_run_list']",13
utilmy/text.py:log,log,function,1,2,2,19,9.5,0,0,['*s'],[None],[None],14,[],['print'],1
utilmy/text.py:help,help,function,11,22,20,232,10.55,0,0,[],[],[],18,[],"['print', 'help_get_codesource', 'len', 'inspect.getsourcelines']",4
utilmy/text.py:help_get_codesource,help_get_codesource,function,7,15,14,177,11.8,0,0,['func'],[None],[None],24,"['    """""" Extract code source from func name""""""\n']","['len', 'inspect.getsourcelines']",2
utilmy/text.py:test,test,function,18,80,51,1001,12.51,0,0,[],[],[],36,[],"['pd.read_csv', 'print', 'pd_text_getcluster', 'df.head', 'log', 'pd_text_similarity', 'test_lsh', 'pd.DataFrame']",8
utilmy/text.py:test_lsh,test_lsh,function,10,28,27,222,7.93,0,0,[],[],[],65,[],"['pd.DataFrame', 'pd_text_getcluster', 'df.head', 'print']",4
utilmy/text.py:pd_text_hash_create_lsh,pd_text_hash_create_lsh,function,32,96,65,728,7.58,3,3,"['df', 'col', 'sep', 'threshold', 'num_perm', 'npool', 'chunk ']","[None, None, None, None, None, None, None]","[None, None, '"" ""', '0.7', '10', '1', ' 20000']",80,"[""    '''\n"", '    For each of the entry create a hash function\n', ""    '''\n""]","['len', 'range', 'MinHashLSH', 'multithread_run', 'hash_lines.extend', 'enumerate', 'sentence.split', 'MinHash', 'set', 'v.update', 'hash_lines.append', 'lsh.insert']",12
utilmy/text.py:pd_text_getcluster,pd_text_getcluster,function,27,80,61,626,7.83,1,3,"['df', 'col', 'threshold', 'num_perm', 'npool', 'chunk ']","['pd.DataFrame', 'str', None, 'int', None, None]","[None, ""'col'"", '0.5', '5', '1', ' 100000']",128,"[""    '''\n"", '    For each of the hash function find a cluster and assign unique id to the dataframe cluster_id\n', ""    '''\n""]","['len', 'range', 'multithread_run', 'pd.concat', 'pd_text_hash_create_lsh', 'enumerate', 'lsh.query', 'list']",8
utilmy/text.py:pd_text_similarity,pd_text_similarity,function,20,65,51,547,8.42,1,3,"['df', 'cols', 'algo']","[' pd.DataFrame', None, None]","[None, '[]', ""''""]",164,"[""    '''\n"", '        Return similarities between two columns with \n', ""        python's SequenceMatcher algorithm\n"", '\n', '        Args:\n', '            df (pd.DataFrame): Pandas Dataframe.\n', '            algo (String)    : rapidfuzz | editdistance \n', '            cols (list[str]) : List of of columns name (2 columns)\n', '\n', '        Returns:\n', '            pd.DataFrame\n', '\n', ""    '''\n""]","['len', 'Exception', 'find_similarity', 'fuzz.ratio', 'editdistance.eval', 'SequenceMatcher', 'df.apply']",7
utilmy/deeplearning.py:tensorboard_log,tensorboard_log,function,23,58,46,498,8.59,2,4,"['pars_dict', 'writer', 'verbose']","['dict', None, None]","['None', 'None', 'True']",6,"['    """"""\n', '    #### Usage 1 \n', ""    logdir = 'logs/params'\n"", '\n', '    from tensorboardX import SummaryWriter\n', '    # from tensorboard import SummaryWriter\n', '    tb_writer = SummaryWriter(logdir)\n', '    tensorboard_log(cc, writer= tb_writer)\n', '\n', '    %reload_ext tensorboard\n', '    %tensorboard --logdir logs/params/\n', '    """"""\n']","['dict_flatten', 'd.items', 'isinstance', 'items.extend', 'items.append', 'dict', 'print', 'flatten_box.items', 'writer.add_scalar', 'writer.add_text', 'str', 'writer.close']",12
utilmy/dates.py:log,log,function,1,1,1,9,9.0,0,0,['*s'],[None],[None],6,[],['print'],1
utilmy/dates.py:pd_date_split,pd_date_split,function,31,153,68,1005,6.57,1,2,"['df', 'coldate ', 'prefix_col ', 'verbose']","[None, None, None, None]","[None, ""  'time_key'"", '""""', 'False']",11,[],"['df.drop_duplicates', 'pd.to_datetime', 'x.weekday', 'date_weekmonth', 'date_weekmonth2', 'x.isocalendar', 'date_weekyear2', 'df.apply', 'int', 'merge1', 'date_is_holiday', 'log']",12
utilmy/dates.py:date_now,date_now,function,14,17,15,220,12.94,0,0,"['fmt=""%Y-%m-%d %H', 'add_days', 'timezone']","['', None, None]","['""%Y-%m-%d %H:%M:%S %Z%z""', '0', ""'Asia/Tokyo'""]",41,[],"['datetime.now', 'datetime.timedelta', 'now_new.astimezone', 'now_pacific.strftime']",4
utilmy/dates.py:date_is_holiday,date_is_holiday,function,8,23,21,146,6.35,0,0,['array'],[None],[None],53,"['    """"""\n', '      is_holiday([ pd.to_datetime(""2015/1/1"") ] * 10)\n', '\n', '    """"""\n']","['holidays.CountryHoliday', 'np.array', 'x.astype']",3
utilmy/dates.py:date_weekmonth2,date_weekmonth2,function,3,17,12,51,3.0,0,1,['d'],[None],[None],63,[],[],0
utilmy/dates.py:date_weekmonth,date_weekmonth,function,4,37,18,194,5.24,0,2,['d'],[None],[None],71,[],"['date_weekmonth', 'date_value.replace']",2
utilmy/dates.py:date_weekyear2,date_weekyear2,function,1,6,6,53,8.83,0,0,['dt'],[None],[None],80,[],['datetime.datetime'],1
utilmy/dates.py:date_weekday_excel,date_weekday_excel,function,7,17,15,98,5.76,0,1,['x'],[None],[None],84,[],"['arrow.get', 'str']",2
utilmy/dates.py:date_weekyear_excel,date_weekyear_excel,function,14,47,40,302,6.43,0,1,['x'],[None],[None],91,[],"['arrow.get', 'str', 'dd.isocalendar', 'date_weekday_excel', 'int']",5
utilmy/dates.py:date_generate,date_generate,function,8,17,16,174,10.24,0,0,"['start', 'ndays']","[None, None]","[""'2018-01-01'"", '100']",110,[],"['relativedelta', 'range']",2
utilmy/images.py:log,log,function,1,1,1,8,8.0,0,0,['*s'],[None],[None],13,[],['print'],1
utilmy/images.py:deps,deps,function,5,7,7,74,10.57,0,0,[],[],[],17,[],"['open', 'fp.readlines', 'print']",3
utilmy/images.py:read_image,read_image,function,18,50,37,703,14.06,0,4,"['filepath_or_buffer', 'io.BytesIO]']","[' typing.Union[str', None]","[None, None]",24,"['    """"""Read a file into an image object\n', '    Args:\n', '        filepath_or_buffer: The path to the file, a URL, or any object\n', '            with a `read` method (such as `io.BytesIO`)\n', '    """"""\n']","['isinstance', 'hasattr', 'np.asarray', 'cv2.imdecode', 'validators.url', 'read', 'filepath_or_buffer.endswith', 'tifffile.imread', 'cv2.imread', 'cv2.cvtColor']",10
utilmy/images.py:visualize_in_row,visualize_in_row,function,12,24,24,239,9.96,1,0,['**images'],[None],[None],53,"['    """"""Plot images in one row.""""""\n']","['len', 'plt.figure', 'enumerate', 'plt.subplot', 'plt.xticks', 'plt.yticks', 'plt.title', 'plt.imshow', 'plt.show']",9
utilmy/images.py:maintain_aspect_ratio_resize,maintain_aspect_ratio_resize,function,10,40,29,229,5.72,0,2,"['image', 'width', 'height', 'inter']","[None, None, None, None]","[None, 'None', 'None', 'cv2.INTER_AREA']",68,[],"['float', 'int', 'cv2.resize']",3
utilmy/data.py:log,log,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],21,[],['print'],1
utilmy/data.py:help,help,function,3,6,5,24,4.0,0,0,[],[],[],25,[],['print'],1
utilmy/tabular.py:log,log,function,1,1,1,8,8.0,0,0,['*s'],[None],[None],12,[],['print'],1
utilmy/tabular.py:y_adjustment,y_adjustment,function,9,25,23,264,10.56,0,1,[],[],[],18,"['    """"""\n', '       Adjustment of log, exp transfrmation for yt= y + error\n', '       https://www.inovex.de/de/blog/honey-i-shrunk-the-target-variable/\n', '       \n', '       log(y) = u =sigma**2\n', '    \n', '    """"""\n']","['adjuter_log', 'cost_func', 'error_func', 'RuntimeError']",4
utilmy/tabular.py:test_anova,test_anova,function,23,78,55,663,8.5,2,1,"['df', 'col1', 'col2']","[None, None, None]","[None, None, None]",41,"['    """"""\n', '    ANOVA test two categorical features\n', '    Input dfframe, 1st feature and 2nd feature\n', '    """"""\n']","['edu_frame.groupby', 'groups.keys', 'globals', 'lg.append', 'dfd=len', 'print', 'stats.f_oneway', 'dfn=len']",8
utilmy/tabular.py:test_normality2,test_normality2,function,19,122,59,890,7.3,1,6,"['df', 'column', 'test_type']","[None, None, None]","[None, None, None]",70,"['    """"""\n', '    Function to check Normal Distribution of a Feature by 3 methods\n', '    Input dfframe, feature name, and a test type\n', '    Three types of test\n', ""    1)'Shapiro'\n"", ""    2)'Normal'\n"", ""    3)'Anderson'\n"", '\n', '    output the statistical test score and result whether accept or reject\n', '    Accept mean the feature is Gaussain\n', '    Reject mean the feature is not Gaussain\n', '    """"""\n']","['shapiro', 'print', 'normaltest', 'anderson', 'range']",5
utilmy/tabular.py:test_plot_qqplot,test_plot_qqplot,function,15,29,25,338,11.66,0,0,"['df', 'col_name']","[None, None]","[None, None]",117,"['    """"""\n', '    Function to plot boxplot, histplot and qqplot for numerical feature analyze\n', '    """"""\n']","['plt.subplots', 'fig.suptitle', 'sns.boxplot', 'sns.histplot', 'sm.qqplot', 'print']",6
utilmy/tabular.py:test_heteroscedacity,test_heteroscedacity,function,27,64,58,557,8.7,0,0,"['y', 'y_pred', 'pred_value_only']","[None, None, None]","[None, None, '1']",134,[],"['Linear', 'pd.DataFrame', 'len', 'het_breuschpagan', 'het_white', 'dict']",6
utilmy/tabular.py:test_normality,test_normality,function,19,122,59,890,7.3,1,6,"['df', 'column', 'test_type']","[None, None, None]","[None, None, None]",155,"['    """"""\n', '       Test  Is Normal distribution\n', '       F pvalues < 0.01 : Rejected\n', '\n', '    """"""\n']","['shapiro', 'print', 'normaltest', 'anderson', 'range']",5
utilmy/tabular.py:test_mutualinfo,test_mutualinfo,function,11,18,18,245,13.61,0,0,"['error', 'Xtest', 'colname', 'bins']","[None, None, None, None]","[None, None, 'None', '5']",180,"['    """"""\n', '       Test  Error vs Input Variable Independance byt Mutual ifno\n', ""       sklearn.feature_selection.mutual_info_classif(X, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)\n"", '\n', '    """"""\n']","['pd.DataFrame', 'pd_colnum_tocat', 'mutual_info_classif', 'dict']",4
utilmy/tabular.py:test_hypothesis,test_hypothesis,function,8,26,24,227,8.73,0,1,"['df_obs', 'df_ref', 'method', '**kw']","[None, None, None, None]","[None, None, ""''"", None]",196,"['    """"""\n', '    https://github.com/aschleg/hypothetical/blob/master/tests/test_contingency.py\n', '\n', '    """"""\n']","['print', 'ChiSquareContingency']",2
utilmy/tabular.py:estimator_std_normal,estimator_std_normal,function,15,41,31,240,5.85,0,0,"['err', 'alpha', '']","[None, None, None]","[None, '0.05', None]",215,[],"['len', 'np.var', 'np.sqrt']",3
utilmy/tabular.py:estimator_boostrap_bayes,estimator_boostrap_bayes,function,8,13,10,89,6.85,0,0,"['err', 'alpha', '']","[None, None, None]","[None, '0.05', None]",227,[],['bayes_mvs'],1
utilmy/tabular.py:estimator_bootstrap,estimator_bootstrap,function,6,11,10,120,10.91,0,0,"['err', 'custom_stat', 'alpha', 'n_iter']","[None, None, None, None]","[None, 'None', '0.05', '10000']",233,"['    """"""\n', '      def custom_stat(values, axis=1):\n', '      # stat_val = np.mean(np.asmatrix(values),axis=axis)\n', '      # stat_val = np.std(np.asmatrix(values),axis=axis)p.mean\n', '      stat_val = np.sqrt(np.mean(np.asmatrix(values*values),axis=axis))\n', '      return stat_val\n', '    """"""\n']",['bs.bootstrap'],1
utilmy/tabular.py:pd_train_test_split_time,pd_train_test_split_time,function,23,72,52,497,6.9,1,3,"['df', 'test_period ', 'cols', 'coltime ', 'sort', 'minsize', 'n_sample', 'verbose']","[None, None, None, None, None, None, None, None]","[None, ' 40', 'None', '""time_key""', 'True', '5', '5', 'False']",248,[],"['list', 'df.sort_values', 'log', 'df.groupby', 'len']",5
utilmy/tabular.py:pd_to_scipy_sparse_matrix,pd_to_scipy_sparse_matrix,function,15,24,23,177,7.38,1,0,['df'],[None],[None],262,"['    """"""\n', '    Converts a sparse pandas data frame to sparse scipy csr_matrix.\n', '    :param df: pandas data frame\n', '    :return: csr_matrix\n', '    """"""\n']","['lil_matrix', 'enumerate', 'arr.tocsr']",3
utilmy/tabular.py:pd_stat_correl_pair,pd_stat_correl_pair,function,12,33,30,316,9.58,1,1,"['df', 'coltarget', 'colname']","[None, None, None]","[None, 'None', 'None']",278,"['    """"""\n', '      Genearte correletion between the column and target column\n', '      df represents the dataframe comprising the column and colname comprising the target column\n', '    :param df:\n', '    :param colname: list of columns\n', '    :param coltarget : target column\n', '    :return:\n', '    """"""\n']","['list', 'target_corr.append', 'pd.DataFrame', 'len']",4
utilmy/tabular.py:pd_stat_pandas_profile,pd_stat_pandas_profile,function,7,9,8,175,19.44,0,0,"['df', 'savefile', 'title']","[None, None, None]","[None, '""report.html""', '""Pandas Profile""']",299,"['    """""" Describe the tables\n', '        #Pandas-Profiling 2.0.0\n', '        df.profile_report()\n', '    """"""\n']","['print', 'df.profile_report', 'profile.to_file', 'profile.get_rejected_variables']",4
utilmy/tabular.py:pd_stat_distribution_colnum,pd_stat_distribution_colnum,function,23,84,72,646,7.69,1,3,"['df', 'nrows', 'verbose']","[None, None, None]","[None, '2000', 'False']",311,"['    """""" Stats the tables\n', '    """"""\n']","['df.sample', 'getstat', 'str', 'len', 'pd.DataFrame', 'pd.concat', 'print', 'np.arange']",8
utilmy/tabular.py:pd_stat_histogram,pd_stat_histogram,function,8,19,18,210,11.05,0,0,"['df', 'bins', 'coltarget']","[None, None, None]","[None, '50', '""diff""']",351,"['    """"""\n', '    :param df:\n', '    :param bins:\n', '    :param coltarget:\n', '    :return:\n', '    """"""\n']","['np.histogram', 'pd.DataFrame']",2
utilmy/tabular.py:np_col_extractname,np_col_extractname,function,7,37,23,207,5.59,1,5,['col_onehot'],[None],[None],366,"['    """"""\n', '    Column extraction from onehot name\n', '    :param col_onehotp\n', '    :return:\n', '    """"""\n']","['len', 'colnew.append']",2
utilmy/tabular.py:np_list_remove,np_list_remove,function,12,42,26,214,5.1,3,4,"['cols', 'colsremove', 'mode']","[None, None, None]","[None, None, '""exact""']",389,"['    """"""\n', '    """"""\n']","['cols.remove', 'cols3.append']",2
utilmy/tabular.py:pd_stat_shift_trend_changes,pd_stat_shift_trend_changes,function,20,41,35,660,16.1,0,0,"['df', 'feature', 'target_col', 'threshold']","[None, None, None, None]","[None, None, None, '0.03']",414,"['    """"""\n', '    Calculates number of times the trend of feature wrt target changed direction.\n', '    :param df: df_grouped dataset\n', '    :param feature: feature column name\n', '    :param target_col: target column\n', '    :param threshold: minimum % difference required to count as trend change\n', '    :return: number of trend chagnes for the feature\n', '    """"""\n']","['target_diffs.fillna', 'target_diffs.divide', 'target_diffs_norm.diff', 'target_diffs_lvl2.fillna', 'int']",5
utilmy/tabular.py:pd_stat_shift_trend_correlation,pd_stat_shift_trend_correlation,function,19,63,55,788,12.51,0,2,"['df', 'df_test', 'colname', 'target_col']","[None, None, None, None]","[None, None, None, None]",438,"['    """"""\n', '    Calculates correlation between train and test trend of colname wrt target.\n', '    :param df: train df data\n', '    :param df_test: test df data\n', '    :param colname: colname column name\n', '    :param target_col: target column name\n', '    :return: trend correlation between train and test\n', '    """"""\n']","['df.merge', 'pd.isnull', 'len', 'np.corrcoef', 'print']",5
utilmy/tabular.py:pd_stat_shift_changes,pd_stat_shift_changes,function,33,99,81,1236,12.48,1,4,"['df', 'target_col', 'features_list', 'bins', 'df_test']","[None, None, None, None, None]","[None, None, '0', '10', '0']",469,"['    """"""\n', '    Calculates trend changes and correlation between train/test for list of features\n', '    :param df: dfframe containing features and target columns\n', '    :param target_col: target column name\n', '    :param features_list: by default creates plots for all features. If list passed, creates plots of only those features.\n', '    :param bins: number of bins to be created from continuous colname\n', '    :param df_test: test df which has to be compared with input df for correlation\n', '    :return: dfframe with trend changes and trend correlation (if test df passed)\n', '    """"""\n']","['type', 'list', 'features_list.remove', 'ignored.append', 'pd_colnum_tocat_stat', 'pd_stat_shift_trend_correlation', 'pd_stat_shift_changes', 'stats_all.append', 'pd.DataFrame', 'len', 'print', 'str']",12
utilmy/tabular.py:np_conv_to_one_col,np_conv_to_one_col,function,5,12,11,141,11.75,0,0,"['np_array', 'sep_char']","[None, None]","[None, '""_""']",514,"['    """"""\n', '    converts string/numeric columns to one string column\n', '    :param np_array: the numpy array with more than one column\n', '    :param sep_char: the separator character\n', '    """"""\n']","['row2string', 'sep_char.join']",2
utilmy/debug.py:log,log,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],18,[],['print'],1
utilmy/debug.py:help,help,function,3,6,5,24,4.0,0,0,[],[],[],22,[],['print'],1
utilmy/debug.py:print_everywhere,print_everywhere,function,9,29,22,182,6.28,0,0,[],[],[],29,"['    """"""\n', '    https://github.com/alexmojaki/snoop\n', '    """"""\n']","['snoop.install', 'myfun', 'pp', 'print']",4
utilmy/debug.py:log10,log10,function,8,17,17,101,5.94,1,0,"['*s', 'nmax']","[None, None]","[None, '60']",48,"['    """""" Display variable name, type when showing,  pip install varname\n', '\n', '    """"""\n']","['print', 'type', 'str']",3
utilmy/debug.py:log5,log5,function,5,7,7,37,5.29,0,0,['*s'],[None],[None],57,"['    """"""    ### Equivalent of print, but more :  https://github.com/gruns/icecream\n', '    pip install icrecream\n', '    ic()  --->  ic| example.py:4 in foo()\n', ""    ic(var)  -->   ic| d['key'][1]: 'one'\n"", '\n', '    """"""\n']",['ic'],1
utilmy/debug.py:log_trace,log_trace,function,4,4,4,37,9.25,0,0,"['msg', 'dump_path', 'globs']","[None, None, None]","['""""', '""""', 'None']",68,[],"['print', 'pdb.set_trace']",2
utilmy/debug.py:profiler_start,profiler_start,function,6,9,9,82,9.11,0,0,[],[],[],74,[],"['Profiler', 'profiler.start']",2
utilmy/debug.py:profiler_stop,profiler_stop,function,3,5,5,83,16.6,0,0,[],[],[],82,[],"['profiler.stop', 'print']",2
utilmy/decorators.py:thread_decorator,thread_decorator,function,6,14,13,185,13.21,0,0,['func'],[None],[None],11,"['    """""" A decorator to run function in background on thread\n', '\tReturn:\n', '\t\tbackground_thread: ``Thread``\n', '    """"""\n']","['wrapper', 'Thread', 'background_thread.start']",3
utilmy/decorators.py:timeout_decorator,timeout_decorator,function,11,25,21,297,11.88,0,0,"['seconds', 'error_message']","[None, None]","['10', 'os.strerror(errno.ETIME']",36,"['    """"""Decorator to throw timeout error, if function doesnt complete in certain time\n', '    Args:\n', '        seconds:``int``\n', '            No of seconds to wait\n', '        error_message:``str``\n', '            Error message\n', '            \n', '    """"""\n']","['decorator', '_handle_timeout', '_TimeoutError', 'wrapper', 'signal.signal', 'signal.alarm', 'func', 'wraps']",8
utilmy/decorators.py:timer_decorator,timer_decorator,function,8,22,20,208,9.45,0,0,['func'],[None],[None],63,"['    """"""\n', '    Decorator to show the execution time of a function or a method in a class.\n', '    """"""\n']","['wrapper', 'time.perf_counter', 'func', 'print']",4
utilmy/decorators.py:profiler_context,profiler_context,function,9,20,20,191,9.55,0,0,[],[],[],81,"['    """"""\n', ""    Context Manager the will profile code inside it's bloc.\n"", '    And print the result of profiler.\n', '    Example:\n', '        with profiler_context():\n', '            # code to profile here\n', '    """"""\n']","['Profiler', 'profiler.start', 'profiler.stop', 'print']",4
utilmy/decorators.py:profiler_decorator,profiler_decorator,function,26,45,39,511,11.36,0,0,['func'],[None],[None],101,"['    """"""\n', '    A decorator that will profile a function\n', '    And print the result of profiler.\n', '    """"""\n']","['wrapper', 'Profiler', 'profiler.start', 'func', 'profiler.stop', 'print', 'profiler_decorator_base', 'inner', 'cProfile.Profile', 'pr.enable', 'fnc', 'pr.disable', 'io.StringIO', 'pstats.Stats', 'ps.print_stats']",15
utilmy/decorators.py:profiler_decorator_base,profiler_decorator_base,function,15,23,22,245,10.65,0,0,['fnc'],[None],[None],119,"['    """"""\n', '    A decorator that uses cProfile to profile a function\n', '    And print the result\n', '    """"""\n']","['inner', 'cProfile.Profile', 'pr.enable', 'fnc', 'pr.disable', 'io.StringIO', 'pstats.Stats', 'ps.print_stats', 'print']",9
utilmy/decorators.py:_TimeoutError,_TimeoutError,class,0,1,1,4,4.0,0,0,[],[],[],30,[],[],0
utilmy/templates/cli.py:run_cli,run_cli,function,9,30,23,332,11.07,0,2,[],[],[],5,"['    """""" USage\n', '    \n', '    template  copy  --repo_dir utilmy/\n', '    """"""\n']","['argparse.ArgumentParser', 'add', 'p.parse_args', 'template_show', 'template_copy']",5
utilmy/templates/cli.py:template_show,template_show,function,7,9,9,115,12.78,0,0,[],[],[],26,[],"['os.walk', 'print']",2
utilmy/templates/cli.py:template_copy,template_copy,function,12,26,22,249,9.58,0,0,"['name', 'out_dir']","[None, None]","[None, None]",33,[],"['os_copy', 'print', 'Path']",3
utilmy/zarchive/fast_parallel.py:task_summary,task_summary,function,4,17,17,142,8.35,1,0,['tasks'],[None],[None],76,[],"['print', 'enumerate', 't.get']",3
utilmy/zarchive/fast_parallel.py:task_progress,task_progress,function,8,26,23,161,6.19,1,1,['tasks'],[None],[None],83,"[""  ''' Monitor progress '''\n""]","['np.mean', 'print']",2
utilmy/zarchive/fast_parallel.py:task_find_best,task_find_best,function,3,11,11,80,7.27,0,0,"['tasks', 'n_top']","[None, None]","[None, '5']",94,"['    """"""Compute the best score of tasks_folder""""""\n']","['t.ready', 'sorted']",2
utilmy/zarchive/fast_parallel.py:task_parallel_job_01,task_parallel_job_01,function,19,33,32,336,10.18,0,0,"['name', 'param', 'datadict']","[None, None, None]","[None, None, None]",101,"[""   ''' Sample task run in Parallel '''\n""]","['os.chdir', 'util.a_run_ipython', 'str']",3
utilmy/zarchive/filelock.py:FileLock,FileLock,class,41,133,94,1246,9.37,2,6,[],[],[],36,[],[],0
utilmy/zarchive/filelock.py:FileLock:__init__,FileLock:__init__,method,11,27,23,279,10.33,1,1,"['self', 'protected_file_path', 'timeout', 'delay', 'lock_file_contents']","[None, None, None, None, None]","[None, None, 'None', '1', 'None']",53,"['        """""" Prepare the file locker. Specify the file to lock and optionally\n', '            the maximum timeout and the delay between each attempt to lock.\n', '        """"""\n']",[],0
utilmy/zarchive/filelock.py:FileLock:locked,FileLock:locked,method,2,2,2,20,10.0,0,0,['self'],[None],[None],67,"['        """"""\n', '        Returns True iff the file is owned by THIS FileLock instance.\n', '        (Even if this returns false, the file could be owned by another FileLock instance, possibly in a different thread or process).\n', '        """"""\n']",[],0
utilmy/zarchive/filelock.py:FileLock:available,FileLock:available,method,2,3,3,38,12.67,0,0,['self'],[None],[None],74,"['        """"""\n', '        Returns True iff the file is currently available to be locked.\n', '        """"""\n']",[],0
utilmy/zarchive/filelock.py:FileLock:acquire,FileLock:acquire,method,18,55,48,410,7.45,1,3,"['self', 'blocking']","[None, None]","[None, 'True']",80,"['        """""" Acquire the lock, if possible. If the lock is in use, and `blocking` is False, return False.\n', '            Otherwise, check again every `self.delay` seconds until it either gets the lock or\n', '            exceeds `timeout` number of seconds, in which case it raises an exception.\n', '        """"""\n']","['time.time', 'os.open', 'os.fdopen', 'f.write', 'FileLock.FileLockException', 'time.sleep']",6
utilmy/zarchive/filelock.py:FileLock:release,FileLock:release,method,2,3,3,45,15.0,0,0,['self'],[None],[None],106,"['        """""" Get rid of the lock by deleting the lockfile.\n', '            When working in a `with` statement, this gets automatically\n', '            called at the end.\n', '        """"""\n']",['os.unlink'],1
utilmy/zarchive/filelock.py:FileLock:__enter__,FileLock:__enter__,method,2,3,3,25,8.33,0,0,['self'],[None],[None],115,"['        """""" Activated when used in the with statement.\n', '            Should automatically acquire a lock to be used in the with block.\n', '        """"""\n']",['self.acquire'],1
utilmy/zarchive/filelock.py:FileLock:__exit__,FileLock:__exit__,method,1,1,1,14,14.0,0,0,"['self', 'type', 'value', 'traceback']","[None, None, None, None]","[None, None, None, None]",123,"['        """""" Activated at the end of the with statement.\n', ""            It automatically releases the lock if it isn't locked.\n"", '        """"""\n']",['self.release'],1
utilmy/zarchive/filelock.py:FileLock:__del__,FileLock:__del__,method,2,3,3,32,10.67,0,1,['self'],[None],[None],130,"['        """""" Make sure this ``FileLock`` instance doesn\'t leave a .lock file\n', '            lying around.\n', '        """"""\n']",['self.release'],1
utilmy/zarchive/filelock.py:FileLock:purge,FileLock:purge,method,3,7,6,70,10.0,0,1,['self'],[None],[None],137,"['        """"""\n', '        For debug purposes only.  Removes the lock file from the hard disk.\n', '        """"""\n']",['self.release'],1
utilmy/zarchive/excel.py:get_workbook_name,get_workbook_name,function,3,5,5,48,9.6,0,0,[],[],[],9,"['    """"""Writes the name of the Workbook into Range(""D3"") of Sheet 1""""""\n']","['Workbook.caller', 'Range']",2
utilmy/zarchive/excel.py:double_sum,double_sum,function,1,4,4,13,3.25,0,0,"['x', 'y']","[None, None]","[None, None]",16,"['    """"""Returns twice the sum of the two arguments""""""\n']",[],0
utilmy/zarchive/excel.py:add_one,add_one,function,5,11,9,40,3.64,2,0,['data'],[None],[None],23,"['    """"""Adds 1 to every cell in Range""""""\n']",[],0
utilmy/zarchive/excel.py:matrix_mult,matrix_mult,function,2,2,2,14,7.0,0,0,"['x', 'y']","[None, None]","[None, None]",31,"['    """"""Alternative implementation of Excel\'s MMULT, requires NumPy""""""\n']",['x.dot'],1
utilmy/zarchive/excel.py:npdot,npdot,function,0,0,0,0,0.0,0,0,[],[],[],43,[],[],0
utilmy/zarchive/util_ml.py:create_weight_variable,create_weight_variable,function,5,7,6,129,18.43,0,0,"['name', 'shape']","[None, None]","[None, None]",27,[],['tf.Variable'],1
utilmy/zarchive/util_ml.py:create_bias_variable,create_bias_variable,function,4,6,6,112,18.67,0,0,"['name', 'shape']","[None, None]","[None, None]",33,[],"['tf.constant_initializer', 'tf.Variable']",2
utilmy/zarchive/util_ml.py:create_adam_optimizer,create_adam_optimizer,function,2,3,3,70,23.33,0,0,"['learning_rate', 'momentum']","[None, None]","[None, None]",38,[],[],0
utilmy/zarchive/util_ml.py:tf_check,tf_check,function,32,91,51,915,10.05,0,0,[],[],[],44,[],"['print', 'tf.constant', 'tf.matmul', 'tf.Session', 'sess.run']",5
utilmy/zarchive/util_ml.py:parse_args,parse_args,function,11,26,24,196,7.54,1,1,"['ppa', 'args']","[None, None]","['None', ' {}']",75,[],"['argparse.ArgumentParser', 'args.items', 'ppa.add_argument', 'type=type', 'ppa.parse_args', 'parse_args2']",6
utilmy/zarchive/util_ml.py:parse_args2,parse_args2,function,5,133,98,1435,10.79,0,1,['ppa'],[None],['None'],86,[],"['argparse.ArgumentParser', 'e:print', 'ppa.add_argument']",3
utilmy/zarchive/util_ml.py:tf_global_variables_initializer,tf_global_variables_initializer,function,27,65,46,866,13.32,3,1,['sess'],[None],['None'],124,"['    """"""Initializes all uninitialized variables in correct order. Initializers\n', ""    are only run for uninitialized variables, so it's safe to run this multiple times.\n"", '    Args:   sess: session to use. Use default session if None.\n', '    """"""\n']","['make_initializer', 'f', 'tf.assign', 'make_noop', 'tf.no_op', 'make_safe_initializer', 'tf.cond', 'tf.get_default_session', 'tf.get_default_graph', 'tf.global_variables', 'g.get_operation_by_name', 'sess.run']",12
utilmy/zarchive/util_ml.py:visualize_result,visualize_result,function,21,142,111,1132,7.97,2,0,[],[],[],234,[],"['range', 'enumerate', 'pd.read_csv', 'plt.plot', 'train_loss.tolist', 'plt.legend', 'plt.xlabel', 'plt.ylabel', 'plt.show']",9
utilmy/zarchive/util_ml.py:TextLoader,TextLoader,class,65,159,120,2143,13.48,0,1,[],[],[],167,[],[],0
utilmy/zarchive/util_ml.py:TextLoader:__init__,TextLoader:__init__,method,15,34,31,491,14.44,0,1,"['self', 'data_dir', 'batch_size', 'seq_length']","[None, None, None, None]","[None, None, None, None]",168,[],"['print', 'self.preprocess', 'self.load_preprocessed', 'self.create_batches', 'self.reset_batch_pointer']",5
utilmy/zarchive/util_ml.py:TextLoader:preprocess,TextLoader:preprocess,method,17,34,31,404,11.88,0,0,"['self', 'input_file', 'vocab_file', 'tensor_file']","[None, None, None, None]","[None, None, None, None]",186,[],"['codecs.open', 'f.read', 'collections.Counter', 'sorted', 'list', 'len', 'dict', 'range', 'open', 'pickle.dump', 'np.array', 'np.save']",12
utilmy/zarchive/util_ml.py:TextLoader:load_preprocessed,TextLoader:load_preprocessed,method,10,19,19,245,12.89,0,0,"['self', 'vocab_file', 'tensor_file']","[None, None, None]","[None, None, None]",199,[],"['open', 'pickle.load', 'len', 'dict', 'range', 'np.load']",6
utilmy/zarchive/util_ml.py:TextLoader:create_batches,TextLoader:create_batches,method,20,40,33,665,16.62,0,0,['self'],[None],[None],207,[],"['np.copy', 'np.split', 'int', 'zip']",4
utilmy/zarchive/util_ml.py:TextLoader:next_batch,TextLoader:next_batch,method,6,10,8,87,8.7,0,0,['self'],[None],[None],223,[],[],0
utilmy/zarchive/util_ml.py:TextLoader:reset_batch_pointer,TextLoader:reset_batch_pointer,method,1,2,2,14,7.0,0,0,['self'],[None],[None],228,[],[],0
utilmy/zarchive/multiprocessfunc.py:multigbm_paralell_func,multigbm_paralell_func,function,34,117,81,913,7.8,1,4,"['nbsimul', 'ww', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'price', 'type1', 'strike', 'cp']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, '0', '0', '1']",20,[],"['np.zeros', 'range', 'np.dot', 'np.multiply', 'np.exp', 'np.cumprod', 'np.maximum', 'sum', 'sum1/float']",9
utilmy/zarchive/multiprocessfunc.py:func,func,function,6,10,10,56,5.6,1,0,"['val', 'lock']","[None, None]","[None, None]",73,[],"['range', 'time.sleep']",2
utilmy/zarchive/multiprocessfunc.py:multigbm_processfast7,multigbm_processfast7,function,24,48,44,419,8.73,3,0,"['nbsimul', 's0', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'price']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",150,[],"['range', 'np.dot', 'bm_generator', 'np.exp', 'sum', 'sum1/float']",6
utilmy/zarchive/multiprocessfunc.py:bm_generator,bm_generator,function,5,9,8,43,4.78,0,1,"['bm', 'dt', 'n', 'type1']","[None, None, None, None]","[None, None, None, None]",177,[],[],0
utilmy/zarchive/multiprocessfunc.py:merge,merge,function,6,20,17,106,5.3,1,1,['d2'],[None],[None],211,[],"['time.sleep', 'd2.keys']",2
utilmy/zarchive/multiprocessfunc.py:integratenp2,integratenp2,function,10,29,25,184,6.34,1,0,"['its', 'nchunk']","[None, None]","[None, None]",222,[],"['range', 'ne.evaluate']",2
utilmy/zarchive/multiprocessfunc.py:integratenp,integratenp,function,27,77,47,454,5.9,2,0,"['its', 'nchunk']","[None, None]","[None, None]",234,[],"['range', 'ne.evaluate', 'integratenp', 'np.sum', 'float']",5
utilmy/zarchive/multiprocessfunc.py:integratene,integratene,function,12,44,38,275,6.25,1,0,['its'],[None],[None],247,[],"['range', 'ne.evaluate', 'float']",3
utilmy/zarchive/multiprocessfunc.py:parzen_estimation,parzen_estimation,function,7,30,26,185,6.17,2,1,"['x_samples', 'point_x', 'h']","[None, None, None]","[None, None, None]",268,[],"['np.abs', 'len']",2
utilmy/zarchive/multiprocessfunc.py:init2,init2,function,2,4,4,13,3.25,0,0,['d'],[None],[None],282,[],[],0
utilmy/zarchive/multiprocessfunc.py:init_global1,init_global1,function,2,2,2,37,18.5,0,0,"['l', 'r']","[None, None]","[None, None]",286,[],[],0
utilmy/zarchive/multiprocessfunc.py:np_sin,np_sin,function,2,2,2,19,9.5,0,0,['value'],[None],[None],291,[],['np.sin'],1
utilmy/zarchive/multiprocessfunc.py:ne_sin,ne_sin,function,2,2,2,27,13.5,0,0,['x'],[None],[None],294,[],['ne.evaluate'],1
utilmy/zarchive/multiprocessfunc.py:res_shared2,res_shared2,function,4,16,14,94,5.88,0,0,[],[],[],297,[],['float'],1
utilmy/zarchive/multiprocessfunc.py:list_append,list_append,function,3,4,4,54,13.5,1,0,"['count', 'id', 'out_list']","[None, None, None]","[None, None, None]",261,"['\t""""""Creates an empty list and then appends a random number to the list \'count\' number of times. A CPU-heavy operation!""""""\n']",['range'],1
utilmy/zarchive/function_custom.py:mapping_calc_risk_elvis_v03,mapping_calc_risk_elvis_v03,function,4,9,9,63,7.0,0,0,"['ss', 'tr', 't', 'riskout']","[None, None, None, None]","[None, None, None, None]",18,"[""     ''' ss: state,  tr:trigger level,  risk0: previous risk value '''\n""]",['mapping_calc_risk_v01'],1
utilmy/zarchive/function_custom.py:mapping_calc_risk_v02,mapping_calc_risk_v02,function,18,190,75,704,3.71,0,2,"['ss', 'tr', 't', 'risk0']","[None, None, None, None]","[None, None, None, None]",31,"[""     ''' ss: state,  tr:trigger level,  risk0: previous risk value '''\n""]",[],0
utilmy/zarchive/function_custom.py:mapping_calc_risk_v01,mapping_calc_risk_v01,function,19,130,56,532,4.09,0,2,"['ss', 'tr', 't', 'risk0']","[None, None, None, None]","[None, None, None, None]",72,"[""     ''' ss: state,  tr:trigger level,  risk0: previous risk value '''\n""]",[],0
utilmy/zarchive/function_custom.py:mapping_risk_ww_v01,mapping_risk_ww_v01,function,12,56,29,245,4.38,0,1,"['risk', 'wwmat', 'ww2']","[None, None, None]","[None, None, None]",98,[],"['np.sum', 'np.array']",2
utilmy/zarchive/function_custom.py:mapping_calc_risk_v00,mapping_calc_risk_v00,function,16,68,45,286,4.21,0,1,"['self', 'ss', 'tr', 't', 'risk0']","[None, None, None, None, None]","[None, None, None, None, None]",107,[],[],0
utilmy/zarchive/function_custom.py:getweight,getweight,function,7,13,12,70,5.38,0,1,"['ww', 'size', '3']","[None, None, None]","[None, '(9', None]",133,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/function_custom.py:fun_obj,fun_obj,function,6,11,11,120,10.91,0,0,"['vv', 'ext']","[None, None]","[None, None]",139,[],"['pf.folio_volta', 'int', 'pf.volhisto_fromprice', 'len']",4
utilmy/zarchive/datanalysis.py:pd_filter_column,pd_filter_column,function,12,25,22,218,8.72,1,2,"['df_client_product', 'filter_val', 'iscol']","[None, None, None]","[None, '[]', '1']",73,"[""   '''\n"", '   # Remove Columns where Index Value is not in the filter_value\n', ""   # filter1= X_client['client_id'].values\n"", '   :param df_client_product:\n', '   :param filter_val:\n', '   :param iscol:\n', '   :return:\n', ""   '''\n""]",['col_delete1.append'],1
utilmy/zarchive/datanalysis.py:pd_missing_show,pd_missing_show,function,0,1,1,4,4.0,0,0,[],[],[],90,[],[],0
utilmy/zarchive/datanalysis.py:pd_describe,pd_describe,function,22,86,65,578,6.72,1,2,['df'],[None],[None],119,"[""   ''' Describe the tables\n"", '        \n', '       \n', ""   '''\n""]","['getstat', 'list', 'str', 'len', 'pd.Series', 'pd.DataFrame', 'pd.concat']",7
utilmy/zarchive/datanalysis.py:pd_stack_dflist,pd_stack_dflist,function,8,29,24,157,5.41,1,1,['df_list'],[None],[None],158,[],"['enumerate', 'df0.append', 'print']",3
utilmy/zarchive/datanalysis.py:pd_validation_struct,pd_validation_struct,function,0,1,1,4,4.0,0,0,[],[],[],168,[],[],0
utilmy/zarchive/datanalysis.py:pd_checkpoint,pd_checkpoint,function,0,1,1,4,4.0,0,0,[],[],[],178,[],[],0
utilmy/zarchive/datanalysis.py:xl_setstyle,xl_setstyle,function,34,82,50,743,9.06,2,0,['file1'],[None],[None],237,"["" '''\n"", '   http://openpyxl.readthedocs.io/en/default/styles.html#cell-styles-and-named-styles\n', '  import openpyxl.styles.builtins  as bi\n', '  import openpyxl.styles.builtins\n', '\n', ""  col = ws.column_dimensions['A']\n"", '  col.font = Font(bold=True)\n', '\n', ""  for cell in ws['A'] + ws[1]:\n"", ""    cell.style = 'data01'\n"", '\n', '  bd = Side(style=\'thick\', color=""000000"")\n', '  highlight.border = BORDER_NONE\n', '  from openpyxl.styles import\n', "" '''\n""]","['load_workbook', 'print', 'named_styles.NamedStyle', 'Font', 'wb.get_sheet_names', 'wb.get_sheet_by_name', 'range', 'wb.save']",8
utilmy/zarchive/datanalysis.py:xl_val,xl_val,function,7,14,13,108,7.71,0,0,"['ws', 'colj', 'rowi']","[None, None, None]","[None, None, None]",282,[],"['ws[gcol', 'str']",2
utilmy/zarchive/datanalysis.py:isnull,isnull,function,4,6,6,20,3.33,0,0,['x'],[None],[None],289,[],[],0
utilmy/zarchive/datanalysis.py:xl_get_rowcol,xl_get_rowcol,function,8,44,37,220,5.0,1,1,"['ws', 'i0', 'j0', 'imax', 'jmax']","[None, None, None, None, None]","[None, None, None, None, None]",292,[],"['range', 'isnull', 'rmat.append']",3
utilmy/zarchive/datanalysis.py:xl_getschema,xl_getschema,function,71,250,174,1902,7.61,4,8,"['dirxl', 'filepattern', 'dirlevel', 'outfile']","[None, None, None, None]","['""""', ""'*.xlsx'"", '1', ""'.xlsx'""]",302,"["" '''Take All excel in a folder and provide Table, Column Schema, type into master file\n"", "" '''\n""]","['xl_is_data_block_start', 'isnull', 'xl_find_start_block', 'range', '_xl_getschema', 'util.os_file_getname', 'load_workbook', 'print', 'wb.get_sheet_names', 'wb.get_sheet_by_name', 'gcol', 'str', 'np.array', 'np.column_stack', 'df_list.append', 'pd_stack_dflist', 'util.os_file_listall', 'enumerate', 'df0.append', 'util.pd_toexcel']",20
utilmy/zarchive/datanalysis.py:str_to_unicode,str_to_unicode,function,4,13,11,63,4.85,0,1,"['x', 'encoding']","[None, None]","[None, ""'utf-8'""]",381,[],"['isinstance', 'str']",2
utilmy/zarchive/datanalysis.py:csv_dtypes_getdict,csv_dtypes_getdict,function,6,20,16,122,6.1,0,1,"['df', 'csvfile']","[None, None]","['None', 'None']",389,[],['pd.read_csv'],1
utilmy/zarchive/datanalysis.py:csv_fast_processing,csv_fast_processing,function,0,0,0,0,0.0,0,0,[],[],[],396,"[""   '''\n"", '   http://word.bitly.com/post/74069870671/optimizing-text-processing\n', '\n', 'import sys\n', 'from collections import defaultdict\n', 'OUT_FILES = defaultdict(dict)\n', '\n', 'open_outfiles()  # open all files I could possibly need\n', '\n', 'for line in sys.stdin:\n', '    # 1. parse line for account_id and metric_type\n', ""    key = line.split(',')\n"", '    account_id = key[ACCOUNT_ID_INDEX][1:] # strip leading quote\n', '\n', '    # 2. write to appropriate file for account_id and metric_type\n', '    OUT_FILES[account_id][key[METRIC_TYPE_INDEX]].write(line)\n', '\n', '   close_outfiles()  # close all the files we opened\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/datanalysis.py:csv_col_schema_toexcel,csv_col_schema_toexcel,function,87,291,205,2615,8.99,3,8,"['dircsv', 'filepattern', 'outfile', 'returntable', 'maxrow', 'maxcol_pertable', 'maxstrlen']","[None, None, None, None, None, None, None]","['""""', ""'*.csv'"", ""'.xlsx'"", '1', '5000000', '90', ""'U80'""]",418,"["" '''Take All csv in a folder and provide Table, Column Schema, type\n"", ' str(df[col].dtype)  USE str always, otherwise BIG Issue\n', '\n', 'METHOD FOR Unicode / ASCII issue\n', ""1. Decode early:  Decode to <type 'unicode'> ASAP\n"", ""    df['PREF_NAME']=       df['PREF_NAME'].apply(to_unicode)\n"", '2. Unicode everywhere\n', ""3. Encode late :f = open('/tmp/ivan_out.txt','w')\n"", ""                f.write(ivan_uni.encode('utf-8'))\n"", "" '''\n""]","['util.os_file_listall', 'len', 'np.array', 'enumerate', 'print', 'pd.read_csv', 'str', 'util.pd_toexcel', 'df.apply', 'np.zeros', 'range', 'float', 'gc.collect', 'pd.DataFrame', 'util.save', 'outfile.replace']",16
utilmy/zarchive/datanalysis.py:csv_col_get_dict_categoryfreq,csv_col_get_dict_categoryfreq,function,39,80,64,553,6.91,4,4,"['dircsv', 'filepattern', 'category_cols', 'maxline', 'fileencoding']","[None, None, None, None, None]","[None, '""*.csv""', '[]', '-1', '""utf-8""']",510,"[""  ''' Find Category Freq in large CSV Transaction Column   '''\n""]","['datetime.now', 'defaultdict', 'util.os_file_listall', 'enumerate', 'line.split', 'print']",6
utilmy/zarchive/datanalysis.py:csv_row_reduce_line,csv_row_reduce_line,function,22,80,63,477,5.96,1,5,"['fromfile', 'tofile', 'condfilter', 'catval_tokeep', 'header', 'maxline']","[None, None, None, None, None, None]","[None, None, None, None, 'True', '-1']",536,"[""  ''' Reduce Data Row by filtering on some Category\n"", '    file_category=  in1+ ""offers.csv""\n', '    ncol= 8\n', '    catval_tokeep=[ {} for i in xrange(0, ncol)]\n', '    for i, line in enumerate(open(file_category)):\n', '      ll=  line.split("","")\n', '      catval_tokeep[3][  ll[1] ]  = 1  # Offer_file_col1 --> Transact_file_col_4\n', '      catval_tokeep[4][  ll[3] ] =  1  # Offer_file_col3 --> Transact_file_col_4\n', '\n', '  def condfilter(colk, catval_tokeep) :\n', '    if colk[3] in catval_tokeep[3] or colk[4] in catval_tokeep[4]: return True\n', '    else: return False\n', ""  '''\n""]","['datetime.now', 'open', 'next', 'outfile.write', 'enumerate', 'line.split', 'condfilter', 'print']",8
utilmy/zarchive/datanalysis.py:csv_analysis,csv_analysis,function,0,0,0,0,0.0,0,0,[],[],[],576,"[""   '''\n"", '   https://csvkit.readthedocs.io/en/540/tutorial/1_getting_started.html\n', '\n', '   sudo pip install csvkit\n', '\n', '   :return:\n', ""   '''\n""]",[],0
utilmy/zarchive/datanalysis.py:csv_row_reduce_line_manual,csv_row_reduce_line_manual,function,25,77,54,499,6.48,2,4,"['file_category', 'file_transact', 'file_reduced']","[None, None, None]","[None, None, None]",587,"[""  ''' Reduce Data by filtering on some Category '''\n""]","['datetime.now', 'enumerate', 'line.split', 'open', 'outfile.write', 'print']",6
utilmy/zarchive/datanalysis.py:csv_row_mapreduce,csv_row_mapreduce,function,17,29,26,282,9.72,2,0,"['dircsv', 'outfile', 'type_mapreduce', 'nrow', 'chunk']","[None, None, None, None, None]","['""""', '""""', ""'sum'"", '1000000', ' 5000000']",617,"["" '''Take All csv in a folder and provide Table, Column Schema'''\n""]","['util.os_file_listall', 'int', 'pd.DataFrame', 'enumerate', 'range', 'pd.read_csv']",6
utilmy/zarchive/datanalysis.py:csv_pivotable,csv_pivotable,function,13,57,38,576,10.11,1,4,"['dircsv', 'filepattern', 'fileh5', 'leftX', 'topY', 'centerZ', 'mapreduce', 'chunksize', 'tablename']","[None, None, None, None, None, None, None, None, None]","['""""', ""'*.csv'"", ""'.h5'"", ""'col0'"", ""'col2'"", ""'coli'"", ""'sum'"", ' 500000', ""'df'""]",634,"["" ''' return df Pivot Table from series of csv file (transfer to d5 temporary)\n"", '\n', 'Edit: you can groupby/sum from the store iteratively since this ""map-reduces"" over the chunks:\n', '\n', 'reduce(lambda x, y: x.add(y, fill_value=0),\n', ""       (df.groupby().sum() for df in store.select('df', chunksize=50000)))\n"", '\n', "" '''\n""]","['util.pd_h5_fromcsv_tohdfs', 'pd.HDFStore', 'store.select', 'pd.concat']",4
utilmy/zarchive/datanalysis.py:csv_bigcompute,csv_bigcompute,function,0,1,1,4,4.0,0,0,[],[],[],664,[],[],0
utilmy/zarchive/datanalysis.py:db_getdata,db_getdata,function,0,1,1,4,4.0,0,0,[],[],[],671,[],[],0
utilmy/zarchive/datanalysis.py:db_sql,db_sql,function,0,1,1,4,4.0,0,0,[],[],[],674,[],[],0
utilmy/zarchive/datanalysis.py:db_meta_add,db_meta_add,function,26,85,61,761,8.95,1,4,"['metadb', 'dbname', 'new_table', '[]']","[None, None, None, None]","[None, None, ""(''"", None]",677,"[""   ''' Create Meta database to store infos on the tables : csv, zip, HFS, Postgres\n"", ""ALL_DB['japancoupon']= {}\n"", ""ALL_DB['japancoupon']['schema']=    df_schema\n"", ""ALL_DB['japancoupon']['df_table_uri']= df_schema_dictionnary\n"", ""ALL_DB['japancoupon']['df_table_columns']= df_schema_dict\n"", '        DBname, db_schema, db_table_uri, db_table_columns(dict_table->colum_list),\n', ""   '''\n""]","['pd_df_todict', 'df.drop_duplicates', 'range', 'dict0.setdefault']",4
utilmy/zarchive/datanalysis.py:db_meta_find,db_meta_find,function,22,86,67,617,7.17,3,4,"['ALLDB', 'query', 'filter_db', 'filter_table', 'filter_column']","[None, None, None, None, None]","[None, ""''"", '[]', '[]', '[]']",714,"[""  ''' Find string in all the meta table name, column\n"", ""  db_meta_find(ALLDB, query='bottler', filter_db=['cokeon'],   filter_table=['table'], filter_column=['table'] )\n"", '  dbname: should be exact name\n', '  fitler_table: partial match is ok\n', '  fitler_column : partial name is ok\n', '  return   (dbname, meta_table_name,  meta_table_filtered_by_row_containing query)\n', ""  '''\n""]","['list', 'len', 'isinstance', 'util.str_match_fuzzy', 'util.find_fuzzy', 'util.pd_find', 'rs.append']",7
utilmy/zarchive/datanalysis.py:col_study_getcategorydict_freq,col_study_getcategorydict_freq,function,16,26,25,408,15.69,1,0,['catedict'],[None],[None],745,"[""  ''' Generate Frequency of category : Id, Freq, Freqin%, CumSum%, ZScore\n"", '      given a dictionnary of category parsed previously\n', ""  '''\n""]","['list', 'util.pd_array_todataframe', 'df.sort_values', 'np.arange', 'catlist.append']",5
utilmy/zarchive/datanalysis.py:col_feature_importance,col_feature_importance,function,0,1,1,4,4.0,0,0,"['Xcol', 'Ytarget']","[None, None]","[None, None]",760,"[""   ''' random forest for column importance '''\n""]",[],0
utilmy/zarchive/datanalysis.py:pd_col_study_distribution_show,pd_col_study_distribution_show,function,22,96,71,1000,10.42,3,3,"['df', 'col_include', 'col_exclude', ""pars={'binsize'""]","[None, None, None, '']","[None, 'None', 'None', ""{'binsize':20}""]",764,"["" '''  Perfom Full Study of the pandas columns'''\n""]","['np.count_nonzero', 'sorted', 'print', 'len', 'np.max', 'np.min', 'np.median', 'np.mean', 'np.std', 'plot_Y', 'plt.title', 'plt.plot', 'plt.hist', 'plt.show', 'plt.figure', 'np.percentile', 'list']",17
utilmy/zarchive/datanalysis.py:col_study_summary,col_study_summary,function,18,57,53,606,10.63,1,3,"['Xmat', '0.0]', 'Xcolname', ""'col2']"", 'Xcolselect', '9]', 'isprint']","[None, None, None, None, None, None, None]","['[0.0', None, ""['col1'"", None, '[9', None, '0']",794,[],"['np.arange', 'len', 'print', 'np.min', 'np.max', 'np.median', 'np.mean', 'np.std', 'colanalysis.append', 'pd.DataFrame', 'np.shape']",11
utilmy/zarchive/datanalysis.py:pd_col_pair_plot,pd_col_pair_plot,function,14,34,32,261,7.68,1,1,"['dfX', 'Xcolname_selectlist', 'dfY', 'Ycolname']","[None, None, None, None]","[None, 'None', 'None', 'None']",811,[],"['str', 'plt.scatter', 'plt.autoscale', 'plt.title', 'plt.show']",5
utilmy/zarchive/datanalysis.py:col_pair_correl,col_pair_correl,function,0,1,1,4,4.0,0,0,"['Xcol', 'Ytarget']","[None, None]","[None, None]",825,[],[],0
utilmy/zarchive/datanalysis.py:col_pair_interaction,col_pair_interaction,function,0,1,1,4,4.0,0,0,"['Xcol', 'Ytarget']","[None, None]","[None, None]",828,"[""   ''' random forest for pairwise interaction '''\n""]",[],0
utilmy/zarchive/datanalysis.py:plot_col_pair,plot_col_pair,function,1,4,4,69,17.25,0,0,"['dfX', 'Xcolname_selectlist', 'dfY', 'Ycolname']","[None, None, None, None]","[None, 'None', 'None', 'None']",832,[],['pd_col_pair_plot'],1
utilmy/zarchive/datanalysis.py:tf_transform_catlabel_toint,tf_transform_catlabel_toint,function,13,18,18,218,12.11,1,0,['Xmat'],[None],[None],838,"[""   '''\n"", '     # [""paris"", ""paris"", ""tokyo"", ""amsterdam""]  --> 2 ,5,6\n', '     # np.array(le.inverse_transform([2, 2, 1]))\n', '     le = preprocessing.LabelEncoder()\n', '     le.fit([""paris"", ""paris"", ""tokyo"", ""amsterdam""])\n', 'LabelEncoder()\n', 'list(le.classes_)\n', ""['amsterdam', 'paris', 'tokyo']\n"", 'le.transform([""tokyo"", ""tokyo"", ""paris""])\n', 'array([2, 2, 1]...)\n', 'list(le.inverse_transform([2, 2, 1]))\n', ""['tokyo', 'tokyo', 'paris']\n"", ""   '''\n""]","['range', 'le.get_params']",2
utilmy/zarchive/datanalysis.py:tf_transform_pca,tf_transform_pca,function,3,4,4,78,19.5,0,0,"['Xmat', 'dimpca', 'whiten']","[None, None, None]","[None, '2', 'True']",863,"[""   '''Project ndim data into dimpca sub-space  '''\n""]","['pca=PCA', 'pca.transform']",2
utilmy/zarchive/datanalysis.py:plot_distance_heatmap,plot_distance_heatmap,function,19,30,29,363,12.1,0,0,"['Xmat_dist', 'Xcolname']","[None, None]","[None, None]",872,[],"['pd.DataFrame', 'plt.figure', 'fig.add_subplot', 'ax.imshow', 'pyplot.get_cmap', 'ax.set_xlabel', 'ax.set_ylabel', 'ax.set_title', 'plt.colorbar']",9
utilmy/zarchive/datanalysis.py:plot_cluster_2D,plot_cluster_2D,function,10,20,20,237,11.85,1,0,"['X_2dim', 'target_class', 'target_names']","[None, None, None]","[None, None, None]",888,"[""   ''' Plot 2d of Clustering Class,\n"", '       X2d: Nbsample x 2 dim  (projection on 2D sub-space)\n', ""   '''\n""]","['target_ids=range', 'len', 'pl.figure', 'zip', 'pl.scatter', 'pl.legend', 'pl.show']",7
utilmy/zarchive/datanalysis.py:plot_cluster_tsne,plot_cluster_tsne,function,23,53,46,590,11.13,0,3,"['Xmat', 'Xcluster_label', 'metric', 'perplexity', 'ncomponent', 'savefile', 'isprecompute', 'returnval']","[None, None, None, None, None, None, None, None]","[None, 'None', ""'euclidean'"", '50', '2', ""''"", 'False', 'True']",901,"["" '''Plot High dimemnsionnal State using TSNE method\n"", ""   'euclidean, 'minkowski', 'cityblock', 'seuclidean', 'sqeuclidean, 'cosine, 'correlation, 'hamming, 'jaccard, 'chebyshev,\n"", ""   'canberra, 'braycurtis, 'mahalanobis', VI=None) 'yule, 'matching, 'dice, 'kulsinski, 'rogerstanimoto, 'russellrao, 'sokalmichener, 'sokalsneath,\n"", '\n', '   Xtsne= da.plot_cluster_tsne(Xtrain_dist, Xcluster_label=None, perplexity=40, ncomponent=2, isprecompute=True)\n', '\n', '   Xtrain_dist= sci.spatial.distance.squareform(sci.spatial.distance.pdist(Xtrain_d,\n', ""               metric='cityblock', p=2, w=None, V=None, VI=None))\n"", ""   '''\n""]","['np.set_printoptions', 'model.fit_transform', 'np.arange', 'plot_XY']",4
utilmy/zarchive/datanalysis.py:plot_cluster_pca,plot_cluster_pca,function,22,51,44,480,9.41,0,3,"['Xmat', 'Xcluster_label', 'metric', 'dimpca', 'whiten', 'isprecompute', 'savefile', 'doreturn']","[None, None, None, None, None, None, None, None]","[None, 'None', ""'euclidean'"", '2', 'True', 'False', ""''"", '1']",930,[],"['PCA', 'model.fit_transform', 'np.zeros', 'plot_XY']",4
utilmy/zarchive/datanalysis.py:plot_cluster_hiearchy,plot_cluster_hiearchy,function,30,87,78,957,11.0,1,3,"['Xmat_dist', 'p', 'truncate_mode', 'color_threshold', 'get_leaves', 'orientation', 'labels', 'count_sort', 'distance_sort', 'show_leaf_counts', 'do_plot', 'no_labels', 'leaf_font_size', 'leaf_rotation', 'leaf_label_func', 'show_contracted', 'link_color_func', 'ax', 'above_threshold_color', 'annotate_above']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, '30', 'None', 'None', 'True', ""'top'"", 'None', 'False', 'False', 'True', '1', 'False', 'None', 'None', 'None', 'False', 'None', 'None', ""'b'"", '0']",948,[],"['dendrogram', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'zip', 'sum', 'plt.plot', 'plt.annotate', 'plt.axhline']",9
utilmy/zarchive/datanalysis.py:plot_distribution_density,plot_distribution_density,function,28,58,52,631,10.88,0,0,"['Xsample', 'kernel', 'N', 'bandwith']","[None, None, None, None]","[None, ""'gaussian'"", '10', '1 / 10.0']",978,[],"['plt.subplots', 'len', 'np.min', 'np.max', 'np.linspace', 'np.ones_like', 'np.ones', 'ax.hist', 'kde.score_samples', 'np.log', 'ax.plot', 'np.exp', 'ax.set_xlim', 'plt.show']",14
utilmy/zarchive/datanalysis.py:plot_Y,plot_Y,function,4,7,7,88,12.57,0,0,"['Yval', 'typeplot', 'tsize', 'labels', 'title', 'xlabel', 'ylabel', 'zcolor_label', 'figsize', '6']","[None, None, None, None, None, None, None, None, None, None]","[None, ""'.b'"", 'None', 'None', ""''"", ""''"", ""''"", ""''"", '(8', None]",1021,[],"['plt.figure', 'plt.title', 'plt.plot', 'plt.show']",4
utilmy/zarchive/datanalysis.py:plot_XY,plot_XY,function,72,197,147,1805,9.16,0,7,"['xx', 'yy', 'zcolor', 'tsize', 'labels', 'title', 'xlabel', 'ylabel', 'zcolor_label', 'figsize', '6']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', ""''"", ""''"", ""''"", ""''"", '(8', None]",1028,"[""  '''\n"", '      labels= numpy array, ---> Generate HTML File with the labels interactives\n', '      Color: Plasma\n', ""  '''\n""]","['isinstance', 'np.array', 'np.abs', 'plt.get_cmap', 'np.min', 'plt.subplots', 'fig.set_size_inches', 'fig.set_dpi', 'fig.tight_layout', 'ax1.scatter', 'ax1.set_xlabel', 'ax1.set_ylabel', 'ax1.set_title', 'ax1.grid', 'np.max', 'scatter.set_clim', 'fig.colorbar', 'cb.set_label', 'list', 'mpld3.save_html', 'plt.show', 'util.os_folder_create', 'plt.savefig', 'plot_XY_plotly', 'go.Scatter', 'py.iplot', 'py.plot', 'plot_XY_seaborn', 'sns.set_context', 'sns.set_color_codes', 'sns.color_palette', 'np.unique', 'plt.scatter', 'plt.gca', 'plt.title']",35
utilmy/zarchive/datanalysis.py:plot_XY_plotly,plot_XY_plotly,function,15,31,26,261,8.42,0,1,"['xx', 'yy', 'towhere']","[None, None, None]","[None, None, ""'url'""]",1095,"[""  ''' Create Interactive Plotly   '''\n""]","['go.Scatter', 'py.iplot', 'py.plot']",3
utilmy/zarchive/datanalysis.py:plot_XY_seaborn,plot_XY_seaborn,function,14,44,42,400,9.09,0,1,"['X', 'Y', 'Zcolor']","[None, None, None]","[None, None, 'None']",1118,[],"['sns.set_context', 'sns.set_color_codes', 'sns.color_palette', 'np.unique', 'plt.scatter', 'plt.gca', 'plt.title']",7
utilmy/zarchive/datanalysis.py:optim_is_pareto_efficient,optim_is_pareto_efficient,function,15,35,31,304,8.69,1,2,"['Xmat_cost', 'epsilon', 'ret_boolean']","[None, None, None]","[None, ' 0.01', '1']",1160,"['    """""" Calculate Pareto Frontier of Multi-criteria Optimization program\n', '    c1, c2  has to be minimized : -Sharpe, -Perf, +Drawdown\n', '    :param Xmat_cost: An (n_points, k_costs) array\n', '    :return: A (n_points, ) boolean array, indicating whether each point is Pareto efficient\n', '    """"""\n']","['np.ones', 'enumerate', 'np.any']",3
utilmy/zarchive/datanalysis.py:sk_catboost_classifier,sk_catboost_classifier,function,22,54,51,581,10.76,0,2,"['Xtrain', 'Ytrain', 'Xcolname', 'pars= {""learning_rate""', '""iterations""', '""random_seed""', '""loss_function""', 'isprint']","[None, None, None, '', '1000', '0', ' ""MultiClass"" }', None]","[None, None, 'None', ' {""learning_rate"":0.1', None, None, None, '0']",1245,"[""  '''\n"", '  from catboost import Pool, CatBoostClassifier\n', '\n', ""TRAIN_FILE = '../data/cloudness_small/train_small'\n"", ""TEST_FILE = '../data/cloudness_small/test_small'\n"", ""CD_FILE = '../data/cloudness_small/train.cd'\n"", '# Load data from files to Pool\n', 'train_pool = Pool(TRAIN_FILE, column_description=CD_FILE)\n', 'test_pool = Pool(TEST_FILE, column_description=CD_FILE)\n', '# Initialize CatBoostClassifier\n', ""model = CatBoostClassifier(iterations=2, learning_rate=1, depth=2, loss_function='MultiClass')\n"", '# Fit model\n', 'model.fit(train_pool)\n', '# Get predicted classes\n', 'preds_class = model.predict(test_pool)\n', '# Get predicted probabilities for each class\n', 'preds_proba = model.predict_proba(test_pool)\n', '# Get predicted RawFormulaVal\n', ""  preds_raw = model.predict(test_pool, prediction_type='RawFormulaVal')  \n"", '  \n', '  \n', '  https://tech.yandex.com/catboost/doc/dg/concepts/python-usages-examples-docpage/\n', '  \n', ""  '''\n""]","['dict2', 'str', 'range', 'pd.DataFrame', 'catboost.CatBoostClassifier', 'clf.fit', 'clf.predict', 'cm.astype', 'cm.sum', 'print']",10
utilmy/zarchive/datanalysis.py:sk_catboost_regressor,sk_catboost_regressor,function,0,1,1,4,4.0,0,0,[],[],[],1291,[],[],0
utilmy/zarchive/datanalysis.py:sk_model_auto_tpot,sk_model_auto_tpot,function,21,41,36,562,13.71,0,1,"['Xmat', 'y', 'outfolder', 'model_type', 'train_size', 'generation', 'population_size', 'verbosity']","[None, None, None, None, None, None, None, None]","[None, None, ""'aaserialize/'"", ""'regressor/classifier'"", '0.5', '1', '5', '2']",1308,"[""  ''' Automatic training of Xmat--->Y, Generate SKlearn code in outfile\n"", '      Very Slow Process, use lower number of Sample\n', '  :param Xmat:\n', '  :param y:\n', '  :param outfolder:\n', '  :param model_type:\n', '  :param train_size:\n', '  :param generation: \n', '  :param population_size:\n', '  :param verbosity:\n', '  :return:\n', ""  '''\n""]","['train_test_split', 'TPOTRegressor', 'TPOTClassifier', 'tpot.fit', 'print', 'str', 'tpot.export']",7
utilmy/zarchive/datanalysis.py:sk_params_search_best,sk_params_search_best,function,30,61,50,840,13.77,0,3,"['Xmat', 'Ytarget', 'model1', ""param_grid={'alpha'"", '1', '5) }', 'method', ""param_search= {'scoretype'"", ""'cv'"", ""'population_size'"", ""'generations_number'""]","[None, None, None, '', None, None, None, '', '5', '5', '3 }']","[None, None, None, ""{'alpha':  np.linspace(0"", None, None, ""'gridsearch'"", "" {'scoretype':'r2'"", None, None, None]",1336,"[""  '''\n"", '   genetic: population_size=5, ngene_mutation_prob=0.10,,gene_crossover_prob=0.5, tournament_size=3,  generations_number=3\n', '\n', '  :param Xmat:\n', '  :param Ytarget:\n', '  :param model1:\n', '  :param param_grid:\n', '  :param method:\n', '  :param param_search:\n', '  :return:\n', ""  '''\n""]","['make_scorer', 'GridSearchCV', 'grid.fit', 'EvolutionaryAlgorithmSearchCV', 'cv=StratifiedKFold', 'cv.fit']",6
utilmy/zarchive/datanalysis.py:sk_distribution_kernel_bestbandwidth,sk_distribution_kernel_bestbandwidth,function,8,17,17,176,10.35,0,0,['kde'],[None],[None],1403,"["" '''Find best Bandwidht for a  given kernel\n"", '  :param kde:\n', '  :return:\n', "" '''\n""]","['GridSearchCV', 'np.linspace', 'grid.fit']",3
utilmy/zarchive/datanalysis.py:sk_distribution_kernel_sample,sk_distribution_kernel_sample,function,15,31,29,211,6.81,1,0,"['kde', 'n']","[None, None]","['None', '1']",1414,"["" ''' \n"", '  kde = sm.nonparametric.KDEUnivariate(np.array(Y[Y_cluster==0],dtype=np.float64))\n', '  kde = sm.nonparametric.KDEMultivariate()  # ... you already did this\n', "" '''\n""]","['np.zeros', 'func', 'kde.cdf', 'range', 'brentq']",5
utilmy/zarchive/datanalysis.py:sk_correl_rank,sk_correl_rank,function,16,38,31,289,7.61,2,0,"['correl', '0]', '[0', '1]]']","[None, None, None, None]","['[[1', None, None, None]",1432,"[' """""" Correl Ranking:  Col i, Col j, Correl_i_j, Abs_Correl_i_j    """"""\n']","['np.shape', 'np.zeros', 'range', 'abs', 'util.sortcol']",5
utilmy/zarchive/datanalysis.py:sk_error_r2,sk_error_r2,function,8,19,16,175,9.21,0,1,"['Ypred', 'y_true', 'sample_weight', 'multioutput']","[None, None, None, None]","[None, None, 'None', 'None']",1447,[],"['r2_score', 'np.sign']",2
utilmy/zarchive/datanalysis.py:sk_error_rmse,sk_error_rmse,function,6,10,10,100,10.0,0,0,"['Ypred', 'Ytrue']","[None, None]","[None, None]",1454,[],"['np.sqrt', 'len', 'np.std']",3
utilmy/zarchive/datanalysis.py:sk_cluster_distance_pair,sk_cluster_distance_pair,function,5,16,14,195,12.19,0,2,"['Xmat', 'metric']","[None, None]","[None, ""'jaccard'""]",1458,"[""   '''\n"", ""    'euclidean, 'minkowski', 'cityblock', 'seuclidean', 'sqeuclidean, 'cosine, 'correlation, 'hamming, 'jaccard, 'chebyshev, 'canberra, 'braycurtis, 'mahalanobis', VI=None) 'yule, 'matching, 'dice, 'kulsinski, 'rogerstanimoto, 'russellrao, 'sokalmichener, 'sokalsneath,\n"", '\n', ""    'braycurtis': hdbscan.dist_metrics.BrayCurtisDistance,\n"", "" 'canberra': hdbscan.dist_metrics.CanberraDistance,\n"", "" 'chebyshev': hdbscan.dist_metrics.ChebyshevDistance,\n"", "" 'cityblock': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'dice': hdbscan.dist_metrics.DiceDistance,\n"", "" 'euclidean': hdbscan.dist_metrics.EuclideanDistance,\n"", "" 'hamming': hdbscan.dist_metrics.HammingDistance,\n"", "" 'haversine': hdbscan.dist_metrics.HaversineDistance,\n"", "" 'infinity': hdbscan.dist_metrics.ChebyshevDistance,\n"", "" 'jaccard': hdbscan.dist_metrics.JaccardDistance,\n"", "" 'kulsinski': hdbscan.dist_metrics.KulsinskiDistance,\n"", "" 'l1': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'l2': hdbscan.dist_metrics.EuclideanDistance,\n"", "" 'mahalanobis': hdbscan.dist_metrics.MahalanobisDistance,\n"", "" 'manhattan': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'matching': hdbscan.dist_metrics.MatchingDistance,\n"", "" 'minkowski': hdbscan.dist_metrics.MinkowskiDistance,\n"", "" 'p': hdbscan.dist_metrics.MinkowskiDistance,\n"", "" 'pyfunc': hdbscan.dist_metrics.PyFuncDistance,\n"", "" 'rogerstanimoto': hdbscan.dist_metrics.RogersTanimotoDistance,\n"", "" 'russellrao': hdbscan.dist_metrics.RussellRaoDistance,\n"", "" 'seuclidean': hdbscan.dist_metrics.SEuclideanDistance,\n"", "" 'sokalmichener': hdbscan.dist_metrics.SokalMichenerDistance,\n"", "" 'sokalsneath': hdbscan.dist_metrics.SokalSneathDistance,\n"", "" 'wminkowski': hdbscan.dist_metrics.WMinkowskiDistance}\n"", '   #Visualize discretization scheme\n', '\n', '   Xtrain_dist= sci.spatial.distance.squareform(sci.spatial.distance.pdist(Xtrain_d,\n', ""             metric='cityblock', p=2, w=None, V=None, VI=None))\n"", '\n', ""   Xtsne= da.plot_cluster_tsne(Xtrain_dist, metric='', perplexity=40, ncomponent=2, isprecompute=True)\n"", '\n', ""   '''\n""]",['fast.distance_jaccard_X'],1
utilmy/zarchive/datanalysis.py:sk_cluster,sk_cluster,function,52,173,122,1635,9.45,2,11,"['Xmat', 'metric']","[None, None]","[None, ""'jaccard'""]",1500,[],"['fast.distance_jaccard_X', 'sk_cluster', 'hdbscan.HDBSCAN', 'print', 'len', 'np.std', 'np.mean', 'Xmat.reshape', 'KMeans', 'kmeans.fit', 'range', 'plt.plot', 'plt.show', 'sk_cluster_algo_custom', 'algorithm']",15
utilmy/zarchive/datanalysis.py:sk_cluster_algo_custom,sk_cluster_algo_custom,function,9,17,15,174,10.24,0,1,"['Xmat', 'algorithm', 'args', 'kwds', 'returnval']","[None, None, None, None, None]","[None, None, None, None, '1']",1563,"[""    ''' Plot the cLuster using specific Algo\n"", '    distance_matrix = pairwise_distances(blobs)\n', ""    clusterer = hdbscan.HDBSCAN(metric='precomputed')\n"", '    clusterer.fit(distance_matrix)\n', '    clusterer.labels_\n', '\n', ""    {'braycurtis': hdbscan.dist_metrics.BrayCurtisDistance,\n"", "" 'canberra': hdbscan.dist_metrics.CanberraDistance,\n"", "" 'chebyshev': hdbscan.dist_metrics.ChebyshevDistance,\n"", "" 'cityblock': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'dice': hdbscan.dist_metrics.DiceDistance,\n"", "" 'euclidean': hdbscan.dist_metrics.EuclideanDistance,\n"", "" 'hamming': hdbscan.dist_metrics.HammingDistance,\n"", "" 'haversine': hdbscan.dist_metrics.HaversineDistance,\n"", "" 'infinity': hdbscan.dist_metrics.ChebyshevDistance,\n"", "" 'jaccard': hdbscan.dist_metrics.JaccardDistance,\n"", "" 'kulsinski': hdbscan.dist_metrics.KulsinskiDistance,\n"", "" 'l1': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'l2': hdbscan.dist_metrics.EuclideanDistance,\n"", "" 'mahalanobis': hdbscan.dist_metrics.MahalanobisDistance,\n"", "" 'manhattan': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'matching': hdbscan.dist_metrics.MatchingDistance,\n"", "" 'minkowski': hdbscan.dist_metrics.MinkowskiDistance,\n"", "" 'p': hdbscan.dist_metrics.MinkowskiDistance,\n"", "" 'pyfunc': hdbscan.dist_metrics.PyFuncDistance,\n"", "" 'rogerstanimoto': hdbscan.dist_metrics.RogersTanimotoDistance,\n"", "" 'russellrao': hdbscan.dist_metrics.RussellRaoDistance,\n"", "" 'seuclidean': hdbscan.dist_metrics.SEuclideanDistance,\n"", "" 'sokalmichener': hdbscan.dist_metrics.SokalMichenerDistance,\n"", "" 'sokalsneath': hdbscan.dist_metrics.SokalSneathDistance,\n"", "" 'wminkowski': hdbscan.dist_metrics.WMinkowskiDistance}\n"", '\n', ""    '''\n""]","['algorithm', 'print', 'len']",3
utilmy/zarchive/datanalysis.py:sk_optim_de,sk_optim_de,function,35,125,96,1182,9.46,1,4,"['obj_fun', 'bounds', 'maxiter', 'name1', 'solver1', 'isreset', 'popsize']","[None, None, None, None, None, None, None]","[None, None, '1', ""''"", 'None', '1', '15']",1636,"[""    ''' Optimization and Save Data into file'''\n""]","['print', 'load_obj', 'int', 'copy.deepcopy', 'range', 'next', 'util.save_obj', 'util.date_now', 'util.np_int_tostr', 'np.mod', 'np.abs']",11
utilmy/zarchive/datanalysis.py:sk_feature_importance,sk_feature_importance,function,8,19,19,230,12.11,1,1,"['clfrf', 'feature_name']","[None, None]","[None, None]",1739,[],"['np.argsort', 'range', 'len', 'print', 'str']",5
utilmy/zarchive/datanalysis.py:sk_showconfusion,sk_showconfusion,function,11,22,21,246,11.18,0,1,"['clfrf', 'X_train', 'Y_train', 'isprint']","[None, None, None, None]","[None, None, None, 'True']",1747,[],"['clfrf.predict', 'cm.astype', 'cm.sum', 'print']",4
utilmy/zarchive/datanalysis.py:sk_tree,sk_tree,function,13,33,32,447,13.55,0,1,"['Xtrain', 'Ytrain', 'nbtree', 'maxdepth', 'isprint1', 'njobs']","[None, None, None, None, None, None]","[None, None, None, None, '1', '1']",1756,[],"['clfrf.fit', 'clfrf.predict', 'cm.astype', 'cm.sum', 'print']",5
utilmy/zarchive/datanalysis.py:sk_gen_ensemble_weight,sk_gen_ensemble_weight,function,14,29,27,213,7.34,1,0,"['vv', 'acclevel', 'maxlevel']","[None, None, None]","[None, None, '0.88']",1768,[],"['min', 'np.empty', 'range', 'estww.append', 'np.log', 'np.array']",6
utilmy/zarchive/datanalysis.py:sk_votingpredict,sk_votingpredict,function,19,33,28,267,8.09,2,1,"['estimators', 'voting', 'ww', 'X_test']","[None, None, None, None]","[None, None, None, None]",1782,[],"['np.zeros', 'enumerate', 'clf.predict_proba', 'range', 'len']",5
utilmy/zarchive/datanalysis.py:sk_tree_get_ifthen,sk_tree_get_ifthen,function,26,91,64,780,8.57,2,3,"['tree', 'feature_names', 'target_names', 'spacer_base']","[None, None, None, None]","[None, None, None, '"" ""']",1797,"['    """"""Produce psuedo-code for decision tree.\n', '    tree -- scikit-leant DescisionTree.\n', '    feature_names -- list of feature names.\n', '    target_names -- list of target (output) names.\n', '    spacer_base -- used for spacing code (default: ""    "").\n', '    """"""\n']","['recurse', 'print', 'str', 'zip', 'int']",5
utilmy/zarchive/datanalysis.py:sk_model_template1,sk_model_template1,class,39,143,87,1197,8.37,0,3,[],[],[],1683,[],[],0
utilmy/zarchive/datanalysis.py:sk_stateRule,sk_stateRule,class,38,138,104,1121,8.12,0,5,[],[],[],1868,[],[],0
utilmy/zarchive/datanalysis.py:sk_model_template1:__init__,sk_model_template1:__init__,method,10,13,13,166,12.77,0,0,"['self', 'alpha', 'low_y_cut', 'high_y_cut', 'ww0']","[None, None, None, None, None]","[None, '0.5', '-0.09', '0.09', '0.95']",1684,[],['Ridge'],1
utilmy/zarchive/datanalysis.py:sk_model_template1:fit,sk_model_template1:fit,method,17,46,40,471,10.24,0,1,"['self', 'X', 'Y']","[None, None, None]","[None, None, 'None']",1690,[],"['len', 'print', 'np.median']",3
utilmy/zarchive/datanalysis.py:sk_model_template1:predict,sk_model_template1:predict,method,10,27,21,166,6.15,0,1,"['self', 'X', 'y', 'ymedian']","[None, None, None, None]","[None, None, 'None', 'None']",1708,[],['Y.clip'],1
utilmy/zarchive/datanalysis.py:sk_model_template1:score,sk_model_template1:score,method,14,32,27,216,6.75,0,1,"['self', 'X', 'Ytrue', 'ymedian']","[None, None, None, None]","[None, None, 'None', 'None']",1719,[],"['Y.clip', 'r2_score']",2
utilmy/zarchive/datanalysis.py:sk_stateRule:__init__,sk_stateRule:__init__,method,8,44,37,370,8.41,0,1,"['self', 'state', 'trigger', 'colname']","[None, None, None, None]","[None, None, None, '[]']",1871,[],"['np.empty', 'np.shape', 'len', 'range', 'util.np_torecarray']",5
utilmy/zarchive/datanalysis.py:sk_stateRule:addrule,sk_stateRule:addrule,method,20,47,42,383,8.15,0,2,"['self', 'rulefun', 'name', 'desc']","[None, None, None, None]","[None, None, ""''"", ""''""]",1883,[],"['util.findnone', 'util.find', 'print', 'util.np_addcolumn', 'rulefun', 'copy.deepcopy']",6
utilmy/zarchive/datanalysis.py:sk_stateRule:eval,sk_stateRule:eval,method,9,29,24,233,8.03,0,2,"['self', 'idrule', 't', 'ktrig']","[None, None, None, None]","[None, None, None, '0']",1901,[],"['isinstance', 'util.find', 'print']",3
utilmy/zarchive/datanalysis.py:sk_stateRule:help,sk_stateRule:help,method,0,0,0,0,0.0,0,0,[],[],[],1909,"[""    '''\n"", 's1= np.arange(5000).reshape((1000, 5))\n', 'trig1= np.ones((1,5))\n', ""state1= sk_stateRule(aa, trig1, ['drawdown','ma100d','ret10d','state_1','state_2'] )\n"", '\n', 'def fun1(s, tr,t):\n', '  return  s.drawdown[t] < tr.drawdown[0] and  s.drawdown[t] < tr.drawdown[0]\n', '\n', 'def fun2(s, tr,t):\n', ' return  s.drawdown[t] > tr.drawdown[0] and  s.drawdown[t] < tr.drawdown[0]\n', '\n', ""state1.addrule(fun1, 'rule6')\n"", ""state1.addrule(fun2, 'rule5')\n"", '\n', 'state1.eval(idrule=0,t=5)\n', '\n', 'state1.eval(idrule=1,t=5)\n', '\n', ""state1.eval(idrule='rule5',t=6)\n"", '\n', ""util.save_obj(state1, 'state1')\n"", '\n', 'np.shape(aa2)\n', '\n', ""aa2= util.np_torecarray(aa,  ['drawdown','a2','a3','a4','a5'])\n"", '\n', 'util.find(5.0, aa2[0])\n', '\n', 'recordarr = np.rec.array([(1,2.,7),(2,3.,5)],\n', ""                   dtype=[('col1', 'f8'),('col2', 'f8'), ('col3', 'f8')])\n"", 'recordarr.col3[0]\n', '\n', 'state1= stateRule(np.ones((100,10)), np.ones((1,10)))\n', '\n', 'col= aa2.a2\n', '\n', ""'''\n""]",[],0
utilmy/zarchive/coke_functions.py:date_diffsecond,date_diffsecond,function,4,8,7,71,8.88,0,0,"['str_t1', 'str_t0', ""fmt='YYYY-MM-DD HH""]","[None, None, '']","[None, None, ""'YYYY-MM-DD HH:mm:SS'""]",8,[],"['arrow.get', 'dd.total_seconds']",2
utilmy/zarchive/coke_functions.py:date_diffstart,date_diffstart,function,4,9,8,31,3.44,1,0,['t'],[None],[None],13,[],['dd.items'],1
utilmy/zarchive/coke_functions.py:date_diffend,date_diffend,function,4,9,8,31,3.44,1,0,['t'],[None],[None],14,[],['dd.items'],1
utilmy/zarchive/coke_functions.py:np_dict_tolist,np_dict_tolist,function,4,9,8,31,3.44,1,0,['dd'],[None],[None],17,[],['dd.items'],1
utilmy/zarchive/coke_functions.py:np_dict_tostr_val,np_dict_tostr_val,function,1,9,9,46,5.11,0,0,['dd'],[None],[None],20,[],"['str', 'dd.items']",2
utilmy/zarchive/coke_functions.py:np_dict_tostr_key,np_dict_tostr_key,function,1,8,8,46,5.75,0,0,['dd'],[None],[None],23,[],"['str', 'dd.items']",2
utilmy/zarchive/coke_functions.py:day,day,function,9,16,15,117,7.31,0,0,['s'],[None],[None],31,[],['arrow.get'],1
utilmy/zarchive/coke_functions.py:month,month,function,9,16,15,117,7.31,0,0,['s'],[None],[None],32,[],['arrow.get'],1
utilmy/zarchive/coke_functions.py:year,year,function,9,16,15,117,7.31,0,0,['s'],[None],[None],33,[],['arrow.get'],1
utilmy/zarchive/coke_functions.py:hour,hour,function,9,16,15,117,7.31,0,0,['s'],[None],[None],34,[],['arrow.get'],1
utilmy/zarchive/coke_functions.py:weekday,weekday,function,9,16,15,117,7.31,0,0,"['s', 'fmt', 'i0', 'i1']","[None, None, None, None]","[None, ""'YYYY-MM-DD'"", '0', '10']",38,[],['arrow.get'],1
utilmy/zarchive/coke_functions.py:season,season,function,4,15,13,47,3.13,0,1,['d'],[None],[None],48,[],['int'],1
utilmy/zarchive/coke_functions.py:daytime,daytime,function,7,42,23,123,2.93,0,1,['d'],[None],[None],53,[],['int'],1
utilmy/zarchive/coke_functions.py:pd_date_splitall,pd_date_splitall,function,11,22,17,255,11.59,0,0,"['df', 'coldate']","[None, None]","[None, ""'purchased_at'""]",62,[],['copy.deepcopy'],1
utilmy/zarchive/portfolio_withdate.py:data_jpsector,data_jpsector,function,3,4,4,47,11.75,0,0,[],[],[],30,[],['util.load_obj'],1
utilmy/zarchive/portfolio_withdate.py:date_earningquater,date_earningquater,function,9,103,42,604,5.86,0,5,['t1'],[None],[None],40,[],['datetime.datetime'],1
utilmy/zarchive/portfolio_withdate.py:date_is_3rdfriday,date_is_3rdfriday,function,4,14,13,67,4.79,0,0,['s'],[None],[None],59,[],"['datetime.strptime', 'd.weekday']",2
utilmy/zarchive/portfolio_withdate.py:date_option_expiry,date_option_expiry,function,10,50,33,354,7.08,0,2,['date'],[None],[None],64,[],['datetime.datetime'],1
utilmy/zarchive/portfolio_withdate.py:date_find_kday_fromintradaydate,date_find_kday_fromintradaydate,function,2,3,3,68,22.67,0,0,"['kintraday', 'intradaydate', 'dailydate']","[None, None, None]","[None, None, None]",81,[],['util.np_find'],1
utilmy/zarchive/portfolio_withdate.py:date_find_kintraday_fromdate,date_find_kintraday_fromdate,function,4,10,10,88,8.8,0,0,"['d1', 'intradaydate1', 'h1', 'm1']","[None, None, None, None]","[None, None, '9', '30']",85,[],"['datetime.datetime', 'util.np_find']",2
utilmy/zarchive/portfolio_withdate.py:date_find_intradateid,date_find_intradateid,function,8,13,13,96,7.38,1,0,"['datetimelist', 'stringdate']","[None, None]","[None, ""['20160420223000']""]",92,[],"['datestring_todatetime', 'util.np_find', 'str']",3
utilmy/zarchive/portfolio_withdate.py:datetime_convertzone1_tozone2,datetime_convertzone1_tozone2,function,16,29,25,343,11.83,1,1,"['tt', 'fromzone', 'tozone']","[None, None, None]","[None, ""'Japan'"", ""'US/Eastern'""]",103,[],"['pytz.timezone', 'type', 'tz.localize', 't2.append']",4
utilmy/zarchive/portfolio_withdate.py:date_extract_dailyopenclosetime,date_extract_dailyopenclosetime,function,23,51,35,431,8.45,2,3,"['spdateref1', 'market']","[None, None]","[None, ""'us'""]",120,[],"['enumerate', 'spdailyopendate.append', 'np.array', 'spdailyclosedate.append']",4
utilmy/zarchive/portfolio_withdate.py:datetime_tostring,datetime_tostring,function,6,34,22,269,7.91,1,3,['tt'],[None],[None],138,[],"['isinstance', 'tt.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zarchive/portfolio_withdate.py:datestring_todatetime,datestring_todatetime,function,6,15,13,125,8.33,1,1,"['datelist1', 'format1']","[None, None]","[None, ' ""%Y%m%d""']",149,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/portfolio_withdate.py:datetime_todate,datetime_todate,function,6,40,25,319,7.97,1,3,['tt'],[None],[None],158,[],"['isinstance', 'datetime.date', 'pd.to_datetime', 'date2.append']",4
utilmy/zarchive/portfolio_withdate.py:datetime_toint,datetime_toint,function,12,37,28,424,11.46,2,2,['datelist1'],[None],[None],169,[],"['isinstance', 'int', 'date2.append', 'datetime_tointhour', 'yy2.append', 'np.array']",6
utilmy/zarchive/portfolio_withdate.py:datetime_tointhour,datetime_tointhour,function,7,20,19,234,11.7,1,1,['datelist1'],[None],[None],177,[],"['isinstance', 'int', 'yy2.append', 'np.array']",4
utilmy/zarchive/portfolio_withdate.py:dateint_tostring,dateint_tostring,function,7,15,14,183,12.2,1,1,"['datelist1', 'format1']","[None, None]","[None, ""'%b-%y'""]",185,[],"['isinstance', 'dateint_todatetime', 'date2.append', 'np.array']",4
utilmy/zarchive/portfolio_withdate.py:dateint_todatetime,dateint_todatetime,function,6,14,12,126,9.0,1,1,['datelist1'],[None],[None],207,[],"['isint', 'parser.parse', 'date2.append']",3
utilmy/zarchive/portfolio_withdate.py:datenumpy_todatetime,datenumpy_todatetime,function,9,48,26,381,7.94,0,3,"['tt', 'islocaltime']","[None, None]","[None, 'True']",218,[],['type'],1
utilmy/zarchive/portfolio_withdate.py:datetime_tonumpypdate,datetime_tonumpypdate,function,2,2,2,22,11.0,0,0,"['t', 'islocaltime']","[None, None]","[None, 'True']",232,[],['np.datetime64'],1
utilmy/zarchive/portfolio_withdate.py:date_diffindays,date_diffindays,function,4,5,5,74,14.8,0,0,"['intdate1', 'intdate2']","[None, None]","[None, None]",237,[],['dateint_todatetime'],1
utilmy/zarchive/portfolio_withdate.py:date_finddateid,date_finddateid,function,5,75,20,492,6.56,0,10,"['date1', 'dateref']","[None, None]","[None, None]",242,[],['util.np_findfirst'],1
utilmy/zarchive/portfolio_withdate.py:datestring_toint,datestring_toint,function,6,16,14,106,6.62,1,1,['datelist1'],[None],[None],256,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/portfolio_withdate.py:date_as_float,date_as_float,function,12,32,26,311,9.72,0,2,['dt'],[None],[None],263,[],"['isint', 'dateint_todatetime', 'datetime.datetime', 'isleap', 'timedelta']",5
utilmy/zarchive/portfolio_withdate.py:date_todatetime,date_todatetime,function,1,9,9,58,6.44,0,0,['tlist'],[None],[None],277,[],[],0
utilmy/zarchive/portfolio_withdate.py:date_removetimezone,date_removetimezone,function,1,6,6,63,10.5,0,0,['datelist'],[None],[None],281,[],[],0
utilmy/zarchive/portfolio_withdate.py:datediff_inyear,datediff_inyear,function,2,3,3,53,17.67,0,0,"['startdate', 'enddate']","[None, None]","[None, None]",285,[],['date_as_float'],1
utilmy/zarchive/portfolio_withdate.py:date_generatedatetime,date_generatedatetime,function,18,27,27,267,9.89,0,1,"['start', 'nbday', 'end']","[None, None, None]","['""20100101""', '10', '""""']",289,[],"['datestring_todatetime', 'date_add_bdays', 'datetime.timedelta', 'list', 'np.array']",5
utilmy/zarchive/portfolio_withdate.py:date_add_bdays,date_add_bdays,function,12,37,30,369,9.97,1,2,"['from_date', 'add_days']","[None, None]","[None, None]",298,[],"['type', 'dateint_todatetime', 'datestring_todatetime', 'datetime.timedelta', 'current_date.weekday']",5
utilmy/zarchive/portfolio_withdate.py:date_getspecificdate,date_getspecificdate,function,21,203,58,1312,6.46,8,19,"['datelist', 'datetype1', 'outputype1', 'includelastdate', 'includefirstdate', '']","[None, None, None, None, None, None]","[None, '""yearend""', '""intdate""', 'True', 'False', None]",312,[],"['isint', 'dateint_todatetime', 'enumerate', 'vec2.append', 'np.mod', 'np.array']",6
utilmy/zarchive/portfolio_withdate.py:date_alignfromdateref,date_alignfromdateref,function,13,31,27,206,6.65,1,1,"['array1', 'dateref']","[None, None]","[None, None]",373,[],"['range', 'np.float', 'len']",3
utilmy/zarchive/portfolio_withdate.py:_date_align,_date_align,function,16,42,33,256,6.1,2,2,"['dateref', 'datei', 'tmax', 'closei']","[None, None, None, None]","[None, None, None, None]",398,[],"['np.zeros', 'range', 'len']",3
utilmy/zarchive/portfolio_withdate.py:date_align,date_align,function,13,31,27,206,6.65,1,1,"['array1', 'dateref']","[None, None]","[None, None]",412,"["" ''' #Aligne the price with the same dates date\tyear\tmonth\tday\td\topen\tclose\thigh\tlow\tvolume\taclose '''\n""]","['range', 'np.float', 'len']",3
utilmy/zarchive/portfolio_withdate.py:min_withposition,min_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],459,[],['min'],1
utilmy/zarchive/portfolio_withdate.py:max_withposition,max_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],463,[],['max'],1
utilmy/zarchive/portfolio_withdate.py:_reshape,_reshape,function,2,14,12,68,4.86,0,1,['x'],[None],[None],469,[],"['len', 'np.reshape', 'np.size']",3
utilmy/zarchive/portfolio_withdate.py:_notnone,_notnone,function,39,211,114,1769,8.38,1,11,['x'],[None],[None],473,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend']",23
utilmy/zarchive/portfolio_withdate.py:plot_price,plot_price,function,69,272,170,2746,10.1,2,11,"['asset', 'y2', 'y3', 'y4', 'y5', 'sym', 'savename1', 'tickperday', 'date1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', 'None', ""''"", '20', 'None', '(10', None]",477,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend', 'plot_priceintraday', 'np.loadtxt', 'np.unique', 'xdays.append', 'np.hstack', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick']",35
utilmy/zarchive/portfolio_withdate.py:plot_priceintraday,plot_priceintraday,function,32,59,57,947,16.05,1,0,['data'],[None],[None],556,[],"['np.loadtxt', 'np.unique', 'np.arange', 'xdays.append', 'np.hstack', 'plt.figure', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick', 'plt.show']",14
utilmy/zarchive/portfolio_withdate.py:plot_check,plot_check,function,8,19,19,257,13.53,0,0,"['close', 'tt0i', 'tt1i', 'dateref', 'sym', 'tickperday']","[None, None, None, None, None, None]","[None, '20140102', '20160815', '[]', '[]', '120']",598,[],"['util.find', 'getret_fromquotes', 'price_normalize100', 'plot_price']",4
utilmy/zarchive/portfolio_withdate.py:plot_pricedate,plot_pricedate,function,18,61,49,534,8.75,1,4,"['date1', 'sym1', 'asset1', 'sym2', 'bsk1', 'verticaldate', 'savename1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None]","[None, None, None, 'None', 'None', 'None', ""''"", '(10', None]",608,[],"['type', 'datestring_todatetime', 'np.row_stack', 'np.concatenate', 'array_todataframe', 'df_asset.plot', 'ax.axvline', 'np.arange', 'int', 'len', 'ax.savefig']",11
utilmy/zarchive/portfolio_withdate.py:generate_sepvertical,generate_sepvertical,function,9,13,12,149,11.46,0,1,"['asset1', 'tt', 'tmax', 'start', 'datebar']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",690,[],"['np.zeros', 'datestring_todatetime']",2
utilmy/zarchive/portfolio_withdate.py:save_asset_tofile,save_asset_tofile,function,31,129,87,978,7.58,5,9,"['file1', 'asset1', 'asset2', 'asset3', 'date1', 'title1']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",705,[],"['np.shape', 'len', 'np.reshape', 'print', 'np.zeros', 'range', 'int', 'np.row_stack', 'np.savetxt']",9
utilmy/zarchive/portfolio_withdate.py:load_asset_fromfile,load_asset_fromfile,function,1,4,4,51,12.75,0,0,['file1'],[None],[None],744,[],[],0
utilmy/zarchive/portfolio_withdate.py:array_todataframe,array_todataframe,function,11,41,27,259,6.32,0,2,"['price', 'symbols', 'date1']","[None, None, None]","[None, 'None', 'None']",748,[],"['np.shape', 'len', 'pd.DataFrame']",3
utilmy/zarchive/portfolio_withdate.py:pd_dataframe_toarray,pd_dataframe_toarray,function,9,10,10,107,10.7,0,0,['df'],[None],[None],759,[],[],0
utilmy/zarchive/portfolio_withdate.py:isfloat,isfloat,function,3,17,13,80,4.71,0,1,['value'],[None],[None],768,[],['float'],1
utilmy/zarchive/portfolio_withdate.py:isint,isint,function,32,105,61,1036,9.87,0,8,['x'],[None],[None],776,[],"['Xmat.copy', 'x.mean', 'x.std', 'np.corrcoef', 'MinCovDet', 'OAS', 'correl_inv.copy', 'np.sqrt']",8
utilmy/zarchive/portfolio_withdate.py:correlation_mat,correlation_mat,function,32,105,61,1036,9.87,0,8,"['Xmat', 'type1', 'type2']","[None, None, None]","[None, '""robust""', '""correl""']",783,[],"['Xmat.copy', 'x.mean', 'x.std', 'np.corrcoef', 'MinCovDet', 'OAS', 'correl_inv.copy', 'np.sqrt']",8
utilmy/zarchive/portfolio_withdate.py:correl_reducebytrigger,correl_reducebytrigger,function,9,22,18,128,5.82,2,1,"['correl2', 'trigger']","[None, None]","[None, None]",831,"[""  ''' Put Zero below trigger  '''\n""]","['correl2.copy', 'range', 'abs']",3
utilmy/zarchive/portfolio_withdate.py:sk_cov_fromcorrel,sk_cov_fromcorrel,function,8,14,13,115,8.21,0,0,"['correl', 'ret_close1']","[None, None]","[None, None]",843,[],"['volhisto_fromret', 'np.diag', 'np.dot']",3
utilmy/zarchive/portfolio_withdate.py:cointegration,cointegration,function,18,47,37,385,8.19,0,0,"['x', 'y']","[None, None]","[None, None]",849,"['  """""" Output :\n', '    coint_t :t-statistic of unit-root test on residuals\n', ""    pvalue :   MacKinnon's approximate p-value based on MacKinnon (1994)\n"", '    crit_value : Critical values for the test statistic at the 1 %, 5 %, and 10 %  Signifiance levels.\n', '\n', '    The Null hypothesis is that there is no cointegration, the alt hypothesis is that there is cointegrating relationship. \n', '    If the pvalue is small, below a critical size:  Reject Null Hypothesis -->  Cointegration\n', '           pvalue is high --> No Cointegration    \n', '    \n', '    Significance level, also denoted as alpha or , is the probability of rejecting  the null hypothesis when it is true.\n', '    Probability of incorrectly rejecting a true null hypothesis\n', '\n', '    P=0.05  , At least 23% (and typically close to 50%)\n', '    P=0.01  , At least 7% (and typically close to 15%\n', '\n', '    (-2.9607012342275936,\n', '    0.038730981052330332, 0,\n', '    249,\n', ""    {'1%': -3.4568881317725864,\n"", ""       '10%': -2.5729936189738876,\n"", ""     '5%': -2.8732185133016057},\n"", '      601.96849256295991)\n', '        It can be seen that the calculated test statistic of -2.96 is smaller than the 5% critical value of -2.87, \n', ""        which means that we can reject the null hypothesis that  there isn't a cointegrating relationship at the 5% level.\n"", '  """"""\n']",['np.zeros'],1
utilmy/zarchive/portfolio_withdate.py:causality_y1_y2,causality_y1_y2,function,10,14,13,153,10.93,0,0,"['price2', 'price1', 'maxlag']","[None, None, None]","[None, None, None]",887,[],[],0
utilmy/zarchive/portfolio_withdate.py:reg_slope,reg_slope,function,16,24,24,261,10.88,1,0,"['close', 'dateref', 'tlag', 'type1']","[None, None, None, None]","[None, None, None, ""'elasticv'""]",895,[],"['np.ones', 'xrange', 'regression', 'np.arange']",4
utilmy/zarchive/portfolio_withdate.py:rolling_cointegration,rolling_cointegration,function,13,29,26,280,9.66,0,0,"['x', 'y']","[None, None]","[None, None]",910,[],"['rolling_coint', 'pd.expanding_apply']",2
utilmy/zarchive/portfolio_withdate.py:regression,regression,function,13,67,32,589,8.79,0,5,"['yreturn', 'xreturn', 'type1']","[None, None, None]","[None, None, '""elasticv""']",929,"["" '''Y = X* reg.coef_  +  reg.intercept_, r2 '''\n""]","['len', 'yreturn.ravel', 'xreturn.reshape', 'reg.fit', 'F', 'reg.score']",6
utilmy/zarchive/portfolio_withdate.py:regression_fixedsymbolstock,regression_fixedsymbolstock,function,30,59,54,591,10.02,1,1,"['sym', 'ret_close2', 'tsstart', 'tsample', 'ret_spy', 'spyclose', 'regonly']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, 'True']",966,"["" ''' Price, Weight, R2, Error  '''\n""]","['len', 'range', 'np.reshape', 'np.column_stack', 'regression', 'getpriceret_fromregression', 'np.max']",7
utilmy/zarchive/portfolio_withdate.py:regression_getpricefromww,regression_getpricefromww,function,20,38,30,298,7.84,3,0,"['spyclose', 'ww01', 'regasset01', 'ret_close2', 'tstart', 'tlag']","[None, None, None, None, None, None]","[None, None, None, None, None, '1']",989,[],"['np.copy', 'np.zeros', 'range', 'len']",4
utilmy/zarchive/portfolio_withdate.py:regression_allstocks_vs_riskfactors,regression_allstocks_vs_riskfactors,function,65,137,113,1351,9.86,2,1,"['symstock', 'pricestock', 'symriskfac', 'priceriskfac', 'nlaglist']","[None, None, None, None, None]","[None, None, None, None, None]",1010,"["" '''Make Regression on all stocks ''' \n""]","['np.shape', 'print', 'np.arange', 'np.empty', 'len', 'enumerate', 'getlogret_fromquotes', 'range', 'np.reshape', 'regression_fixedsymbolstock', 'util.np_sortbycolumn', 'util.pd_array_todataframe', 'util.print_object']",13
utilmy/zarchive/portfolio_withdate.py:getdiff_fromquotes,getdiff_fromquotes,function,4,6,6,73,12.17,0,0,"['close', 'timelag']","[None, None]","[None, None]",1107,[],['np.shape'],1
utilmy/zarchive/portfolio_withdate.py:getret_fromquotes,getret_fromquotes,function,7,23,17,159,6.91,0,1,"['close', 'timelag']","[None, None]","[None, '1']",1111,[],"['np.shape', 'len']",2
utilmy/zarchive/portfolio_withdate.py:getlogret_fromquotes,getlogret_fromquotes,function,4,7,7,81,11.57,0,0,"['close', 'timelag']","[None, None]","[None, '1']",1122,[],"['np.shape', 'np.log']",2
utilmy/zarchive/portfolio_withdate.py:getprice_fromret,getprice_fromret,function,12,23,21,190,8.26,2,0,"['ret', 'normprice']","[None, None]","[None, '100']",1127,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/portfolio_withdate.py:price_normalize100,price_normalize100,function,16,43,36,292,6.79,1,1,"['ret', 'normprice']","[None, None]","[None, '100']",1139,[],"['np.shape', 'len', 'np.reshape', 'np.zeros', 'range', 'np.cumprod']",6
utilmy/zarchive/portfolio_withdate.py:price_normalize_1d,price_normalize_1d,function,8,22,19,147,6.68,0,0,"['ret', 'normprice', 'dtype1']","[None, None, None]","[None, '100', ' np.float32']",1159,[],"['np.empty', 'np.cumprod']",2
utilmy/zarchive/portfolio_withdate.py:norm_fast,norm_fast,function,7,8,8,85,10.62,0,0,"['y', 'ny']","[None, None]","[None, None]",1168,[],['ne.evaluate'],1
utilmy/zarchive/portfolio_withdate.py:correl_fast,correl_fast,function,8,13,13,116,8.92,0,0,"['xn', 'y', 'nx']","[None, None, None]","[None, None, None]",1176,[],['ne.evaluate'],1
utilmy/zarchive/portfolio_withdate.py:volhisto_fromret,volhisto_fromret,function,2,14,11,168,12.0,0,1,"['retbsk', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",1184,[],"['len', 'np.std']",2
utilmy/zarchive/portfolio_withdate.py:volhisto_fromprice,volhisto_fromprice,function,6,25,17,245,9.8,0,1,"['price', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",1191,[],"['len', 'np.shape', 'np.std']",3
utilmy/zarchive/portfolio_withdate.py:volhistorolling_fromprice,volhistorolling_fromprice,function,16,42,30,318,7.57,2,1,"['price', 'volrange']","[None, None]","[None, None]",1200,[],"['np.shape', 'len', 'np.zeros', 'xrange', 'volhisto_fromprice']",5
utilmy/zarchive/portfolio_withdate.py:rsk_calc_all_TA,rsk_calc_all_TA,function,51,223,105,1352,6.06,0,10,['df'],[None],"[""'panda_dataframe'""]",1219,"[""  '''Add All TA RMI, RSI To the '''\n""]","['ta.MA', 'ta.distance', 'ta.RET', 'ta.RMI', 'ta.STDDEV', 'ta.nbday_low', 'ta.nbday_high', 'ta.RSI', 'ta.qearning_dist', 'ta.optionexpiry_dist', 'linearreg2', 'np.array', 'np.sum', 'util.np_findlocalmin2', 'len', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",18
utilmy/zarchive/portfolio_withdate.py:ta_highbandtrend1,ta_highbandtrend1,function,42,170,75,910,5.35,0,11,"['close2', 'type1']","[None, None]","[None, '0']",1301,[],"['linearreg', 'np.array', 'np.sum', 'util.np_findlocalmax2', 'len', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",8
utilmy/zarchive/portfolio_withdate.py:pd_transform_asset,pd_transform_asset,function,11,35,27,409,11.69,0,1,"['q0', 'q1', 'type1']","[None, None, None]","[None, None, '""spread""']",1350,[],"['util.pd_date_intersection', 'util.pd_array_todataframe', 'util.pd_insertcol']",3
utilmy/zarchive/portfolio_withdate.py:calcbasket_table,calcbasket_table,function,25,59,53,477,8.08,1,2,"['wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps']","[None, None, None, None, None, None]","[None, None, '""table""', '""constant""', '1', ' 0.000']",1364,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.mod', 'np.sum']",6
utilmy/zarchive/portfolio_withdate.py:folio_createvolta_asset,folio_createvolta_asset,function,10,19,19,159,8.37,1,0,"['close', 'vol', 'volrange', 'lev']","[None, None, None, None]","[None, '0.12', '120', '1.0']",1391,[],"['np.zeros_like', 'np.shape', 'xrange', 'folio_volta']",4
utilmy/zarchive/portfolio_withdate.py:folio_lowcorrelation,folio_lowcorrelation,function,145,431,300,3894,9.03,8,7,"['sym01', 'nstock', 'periodlist', 'dateref', 'close1', 'kbenchmark', 'badlist', 'costbppa', 'showgraph']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, '0.02', 'True']",1399,[],"['ww_selection', 'np.corrcoef', 'np.nan_to_num', 'np.zeros', 'range', 'util.np_sortbycol', 'int', 'np.std', 'np.isnan', 'util.find', 'np.sum', 'stk_select.append', 'volhisto_fromret', 'sum', '1/len', 'getret_fromquotes', 'len', 'np.empty', 'np.ones', 'enumerate', 'calcbasket_table', 'price_normalize100', 'np.column_stack', 'np.zeros_like', 'np.array', 'np.concatenate', 'print', 'str', 'plot_price', 'sym01[int', 'volhisto_fromprice', 'xrange', 'folio_inverseetf']",33
utilmy/zarchive/portfolio_withdate.py:folio_leverageetf,folio_leverageetf,function,8,23,20,156,6.78,1,0,"['price', 'lev', 'costpa']","[None, None, None]","[None, '1.0', '0.0']",1537,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/portfolio_withdate.py:folio_inverseetf,folio_inverseetf,function,8,18,18,151,8.39,1,0,"['price', 'costpa']","[None, None]","[None, '0.0']",1546,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/portfolio_withdate.py:folio_longshort_unit,folio_longshort_unit,function,59,240,112,1594,6.64,2,7,"['long1', 'short1', 'ww', '-1]', 'costpa', 'tlag', 'istable', 'wwschedule']","[None, None, None, None, None, None, None, None]","[None, None, '[1', None, '0.0', '1', '1', '[]']",1555,[],"['len', 'np.zeros', 'xrange', 'np.mod', 'folio_longshort_unitfixed']",5
utilmy/zarchive/portfolio_withdate.py:folio_longshort_unitfixed,folio_longshort_unitfixed,function,43,111,73,719,6.48,1,3,"['long1', 'short1', 'nn', '-1]', 'costpa', 'tlag', 'istable']","[None, None, None, None, None, None, None]","[None, None, '[1', None, '0.0', '1', '1']",1596,[],"['len', 'np.zeros', 'xrange', 'np.mod']",4
utilmy/zarchive/portfolio_withdate.py:folio_longshort_pct,folio_longshort_pct,function,8,21,20,189,9.0,1,0,"['long1', 'short1', 'ww', '-1]', 'costpa']","[None, None, None, None, None]","[None, None, '[1', None, '0.0']",1634,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/portfolio_withdate.py:folio_histogram,folio_histogram,function,13,75,41,914,12.19,0,0,['close'],[None],[None],1642,[],"['getret_fromquotes', 'plt.subplots', 'np.ones_like', 'ax2.hist', 'ax2.set_title', 'ax2.set_xlabel', 'ax2.set_ylabel', 'ax2.axvline', 'plt.show', 'print']",10
utilmy/zarchive/portfolio_withdate.py:folio_voltarget,folio_voltarget,function,2,6,6,61,10.17,0,0,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1670,[],['folio_volta'],1
utilmy/zarchive/portfolio_withdate.py:folio_volta,folio_volta,function,54,197,112,1663,8.44,2,8,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1674,[],"['folio_volta', 'len', 'np.shape', 'np.zeros', 'range', 'np.std', 'util.findhigher', 'max', 'min', 'folio_volta2', 'np.abs']",11
utilmy/zarchive/portfolio_withdate.py:folio_volta2,folio_volta2,function,29,61,54,514,8.43,1,2,"['bsk', 'riskind', 'par', 'targetvol', 'volrange', 'cap', 'floor', 'costbp']","[None, None, None, None, None, None, None, None]","[None, None, None, '0.11', ' 90', '1.5', '0.0', '0.0005']",1713,[],"['np.shape', 'np.zeros', 'range', 'np.std', 'max', 'min', 'np.abs']",7
utilmy/zarchive/portfolio_withdate.py:folio_fixedunitprice,folio_fixedunitprice,function,13,36,33,221,6.14,2,0,"['price', 'fixedww', 'costpa']","[None, None, None]","[None, None, '0.0']",1738,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/portfolio_withdate.py:folio_fixedweightprice,folio_fixedweightprice,function,13,36,33,225,6.25,2,0,"['price', 'fixedww', 'costpa']","[None, None, None]","[None, None, '0.0']",1752,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/portfolio_withdate.py:folio_fixedweightret,folio_fixedweightret,function,13,31,28,193,6.23,2,0,"['ret', 'fixedww']","[None, None]","[None, None]",1764,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/portfolio_withdate.py:folio_cost_turnover,folio_cost_turnover,function,26,88,71,825,9.38,1,1,"['wwall', 'bsk', 'dateref', 'costbp']","[None, None, None, None]","[None, None, None, None]",1777,[],"['len', 'np.zeros', 'xrange', 'np.sum', 'plot_price', 'date1=dateint_tostring']",6
utilmy/zarchive/portfolio_withdate.py:folio_riskpa,folio_riskpa,function,20,52,40,422,8.12,2,0,"['ret', 'targetvol', 'volrange', 'cap']","[None, None, None, None]","[None, '0.1', '90', '1.0']",1807,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.sum', 'np.std', 'min', 'list']",8
utilmy/zarchive/portfolio_withdate.py:folio_perfreport_schedule,folio_perfreport_schedule,function,14,20,19,189,9.45,1,0,"['sym', 'dateref', 'close', 'wwind', 't0', 'scheduleperiod']","[None, None, None, None, None, None]","[None, None, None, None, None, '""1monthend""']",2121,[],"['np.shape', 'generate_schedule', 'np.zeros', 'enumerate']",4
utilmy/zarchive/portfolio_withdate.py:folio_concenfactor2,folio_concenfactor2,function,7,12,12,103,8.58,0,0,"['ww', 'masset']","[None, None]","[None, '12']",2421,[],"['getweight', 'np.sum', 'np.mean']",3
utilmy/zarchive/portfolio_withdate.py:calcbasket_objext,calcbasket_objext,function,50,203,96,1094,5.39,1,7,"['RETURN', 'TMAX', 'riskind_i', 'wwmat', 'wwasset0', 'ww0', 'nbrange', 'criteria']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",2439,[],"['xrange', 'np.sum', 'math.sqrt']",3
utilmy/zarchive/portfolio_withdate.py:calc_ranktable,calc_ranktable,function,20,44,41,351,7.98,1,0,"['close2', 'symjp1', 'nlag', 'refindex', 'funeval', 'funargs']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",3849,[],"['np.zeros', 'getlogret_fromquotes', 'enumerate', 'funeval', 'util.np_sortbycolumn']",5
utilmy/zarchive/portfolio_withdate.py:similarity_correl,similarity_correl,function,7,10,9,88,8.8,0,0,"['ret_close2', 'funargs']","[None, None]","[None, None]",3869,[],['correlation_mat'],1
utilmy/zarchive/portfolio_withdate.py:np_similarity,np_similarity,function,6,17,12,138,8.12,0,3,"['x', 'y', 'wwerr', 'type1']","[None, None, None, None]","[None, None, '[]', '0']",3924,[],['np_distance_l1'],1
utilmy/zarchive/portfolio_withdate.py:np_distance_l1,np_distance_l1,function,2,3,3,31,10.33,0,0,"['x', 'y', 'wwerr']","[None, None, None]","[None, None, None]",3930,[],"['np.sum', 'np.abs']",2
utilmy/zarchive/portfolio_withdate.py:np_similarity,np_similarity,function,6,17,12,138,8.12,0,3,"['x', 'y', 'wwerr', 'type1']","[None, None, None, None]","[None, None, '[]', '0']",3924,[],['np_distance_l1'],1
utilmy/zarchive/portfolio_withdate.py:np_distance_l1,np_distance_l1,function,2,3,3,31,10.33,0,0,"['x', 'y', 'wwerr']","[None, None, None]","[None, None, None]",3930,[],"['np.sum', 'np.abs']",2
utilmy/zarchive/portfolio_withdate.py:get,get,function,11,44,26,436,9.91,0,1,"['close', 'timelag']","[None, None]","[None, None]",4293,"['    """"""Return dataframe of requested dataset from Quandl.\n', '    :param dataset: str or list, depending on single dataset usage or multiset usage\n', '            Dataset codes are available on the Quandl website\n', '    :param str api_key: Downloads are limited to 50 unless api_key is specified\n', '    :param str start_date, end_date: Optional datefilers, otherwise entire dataset is returned\n', '    :param str collapse: Options are daily, weekly, monthly, quarterly, annual\n', '    :param str transform: options are diff, rdiff, cumul, and normalize\n', '    :param int rows: Number of rows which will be returned\n', '    :param str order: options are asc, desc. Default: `asc`\n', '    :param str returns: specify what format you wish your dataset returned as,\n', '        either `numpy` for a numpy ndarray or `pandas`. Default: `pandas`\n', '    :returns: :class:`pandas.DataFrame` or :class:`numpy.ndarray`\n', '    Any other `kwargs` passed to `get` are sent as field/value params to Quandl\n', '    with no interference.\n', '\n', ""'''\n"", '\n', '\n', '\n', '#--------------------Import data from Yahoo --------------------------------------------------\n', 'def imp_yahoo_getquotes(symbols, start=""20150101"", end=""20160101"", type1=1):\n', ' d1 = datetime.datetime(int(start[0:4]), int(start[4:6]), int(start[6:8]))\n', ' d2 = datetime.datetime(int(end[0:4]), int(end[4:6]), int(end[6:8]))\n', '# symbols = np.array(symbol1)\n', ' quotes= []; errorlist=[]; correctlist=[]\n', ' for i, symbol in enumerate(symbols) :\n', '   # if np.mod(i,400) ==0 and i != 0 :\n', ""     # print('Waiting 30s'); time.sleep(30)\n"", '   try:   \n', '     array1= quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n', '     df= pd.DataFrame.from_records(array1)\n', '     quotes.append(df)\n', '     correctlist.append(symbol)\n', '   except :\n', '       errorlist.append(symbol)\n', ""       print('Err: '+symbol)\n"", ' #quotes = [quotes_historical_yahoo(symbol, d1, d2, asobject=True)  for symbol in symbols]\n', ' #print(errorlist) \n', ' if type1==1 :  return quotes, correctlist\n', ' else: return quotes, correctlist, errorlist  \n', '\n', '              \n', '#Find error in Ticker import\n', 'def imp_yahoo_geterrorticker(symbols, start=""20150101"", end=""20160101""):\n', ' d1 = datetime.datetime(int(start[0:4]), int(start[4:6]), int(start[6:8]))\n', ' d2 = datetime.datetime(int(end[0:4]), int(end[4:6]), int(end[6:8]))\n', ' errorlist= []\n', ' for symbol in symbols :\n', '   try :  \n', '     quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n', '   except :\n', '     errorlist.append(symbol)\n', '     print(symbol)       \n', ' return errorlist\n', ' \n', ""'''\n"", ""symbols0= ['KHC', 'PYPL', 'HPE', 'BXLT', 'SYF', 'WLTW', 'CFG', 'BF-B', 'WRK']\n"", '\n', '#Show list of ticker in error\n', 'import_errorticker(symbols0,""20110101"",""20150601""  )\n', ""'''\n"", '\n', '\n', 'def imp_yahoo_financials_url(ticker_symbol, statement=""is"", quarterly=False):\n', '    if statement == ""is"" or statement == ""bs"" or statement == ""cf"":\n', '        url = ""https://finance.yahoo.com/q/"" + statement + ""?s="" + ticker_symbol\n', '        if not quarterly:\n', '            url += ""&annual""; return BeautifulSoup(requests.get(url).text, ""html.parser"")\n', '    return sys.exit(""Invalid financial statement code \'"" + statement + ""\' passed."")\n', '\n', 'def imp_yahoo_periodic_figure(soup, yahoo_figure):\n', '    values = []; pattern = re.compile(yahoo_figure)\n', '\n', '    title = soup.find(""strong"", text=pattern)    # works for the figures printed in bold\n', '    if title:  row = title.parent.parent\n', '    else:\n', '        title = soup.find(""td"", text=pattern)    # works for any other available figure\n', '        if title:  row = title.parent\n', '        else:      sys.exit(""Invalid figure \'"" + yahoo_figure + ""\' passed."")\n', '\n', '    cells = row.find_all(""td"")[1:]    # exclude the <td> with figure name\n', '    for cell in cells:\n', '        if cell.text.strip() != yahoo_figure:    # needed because some figures are indented\n', '            str_value = cell.text.strip().replace("","", """").replace(""("", ""-"").replace("")"", """")\n', '            if str_value == ""-"": str_value = 0\n', '            value = int(str_value) * 1000\n', '            values.append(value)\n', '\n', '    return values\n', '    \n', '# print(imp_yahoo_periodic(imp_yahoo_financials_url(""AAPL"", ""is""), ""Income Tax Expense""))\n', '\n', ' \n', '\n', '#--------------------Import Quotes Google  ------------------------------------------\n', 'import requests.packages.urllib3\n', 'requests.packages.urllib3.disable_warnings()\n', 'import urllib, os\n', '\n', ""dirstockcsv= 'E:\\_data\\stock\\csv'\n"", '\n', 'def imp_googleIntradayQuote(symbol, freqsec=300, nday=5):\n', ""    ''' Intraday quotes from google. Specify interval seconds and number of days\n"", '    http://www.google.com/finance/getprices?q=7261&i=300&p=3d&f=d,o,h,l,c,v\n', ""    '''\n"", '    url =""http://www.google.com/finance/getprices?q={0}"".format(symbol)\n', '    url +=""&i={0}&p={1}d&f=d,o,h,l,c,v"".format(freqsec, nday)\n', '    try :      \n', '       resp= requests.post(url)\n', '       csv= resp.text\n', '       csv= csv.splitlines()\n', '    except :\n', ""       print('Err Intaday '+symbol)\n"", '\n', '    if len(csv) < 9 :\n', ""      print 'Using Daily',\n"", '\n', ""      url='http://www.google.com/finance/historical?q='+symbol+'&i='+str(freqsec)+'&p=3d&f=d,o,h,l,c,v&output=csv'\n"", '      csv = urllib.urlopen(url).readlines()\n', '      csv.reverse()\n', '      q=[]\n', '      for bar in xrange(0,len(csv)-1):\n', ""        ds,open_,high,low,close,volume = csv[bar].rstrip().split(',')\n"", '        if isfloat(open_) and isfloat(high) :\n', '          open_,high,low,close = [float(x) for x in [open_,high,low,close]]\n', ""          dt = datetime.datetime.strptime(ds,'%d-%b-%y')\n"", '          q.append([dt,open_,high,low,close,volume])\n', ""      df= util.pd_array_todataframe(q, ['date','open','high','low','close','volume'])\n"", '      return df\n', '\n', '\n', '    # print(csv)\n', '    qq=[]\n', '    for bar in xrange(7,len(csv)):\n', ""      if csv[bar].count(',')!=5: continue\n"", ""      offset,close,high,low,open_,volume = csv[bar].split(',')\n"", '      \n', ""      if offset[0]=='a':\n"", '        day = float(offset[1:])\n', '        offset = 0\n', '      else:\n', '        offset = float(offset)\n', '      open_, high, low, close = [float(x) for x in [open_,high,low,close]]\n', '      dt = datetime.datetime.fromtimestamp(day + (freqsec * offset))\n', '      #print (dt,open_,high,low,close,volume)\n', '      qq.append([dt,open_,high,low,close,volume])\n', ""    df= util.pd_array_todataframe(qq, ['date','open','high','low','close','volume'])\n"", '    return df\n', '\n', ""def imp_googleIntradayQuoteSave(symbols=['NKE'], freqsec=300, nday=2000, dircsv='', dbname=''):\n"", "" ''' Save Under various format: csv / db / output in Dataframe '''\n"", ' if isinstance(symbols, str) : symbols= [symbols]\n', ' symout,qlist, sym_error= [], [], []\n', ' for sym in symbols:\n', '   try :\n', '    q = imp_googleIntradayQuote(sym, freqsec, nday)  #interval, timeframe\n', '    print sym,\n', ""    sym= sym.replace(':', '_')\n"", '\n', ""    if dircsv != '' :\n"", '      if not os.path.isdir(dircsv) :  os.makedirs(dircsv)\n', ""      start1= util.datetime_toint(util.datenumpy_todatetime(q['date'].values[0]))\n"", ""      file1= dircsv+ '\\\\' + sym + '_' + str(start1) + '_' + str(freqsec) + '_' + str(nday) + '.csv'\n"", '\n', ""      q= util.pd_addcol(q, 'symbol'); q['symbol']= sym\n"", '      q.to_csv(file1, index=False)\n', '\n', ""    elif dbname != '' :\n"", '      import sqlalchemy as sql\n', '      dbcon= sql.create_engine(dbname)\n', '      if interv_sec== 300 : # 5mins\n', ""        q.to_sql('q5min', dbcon)\n"", '\n', '    else :\n', '      qlist.append(q);   symout.append(sym)\n', '   except:\n', ""      print('Error:'+sym, )\n"", '      sym_error.append(sym)\n', '\n', ' return qlist, symout, sym_error\n', '\n', ""def imp_googleQuote(symbol, start_date='20160101', end_date= '20160101') : # datetime.date.today().isoformat()):\n"", ""    ''' Daily quotes from google. Date format='yyyymmdd' '''\n"", '    symbol = symbol.upper()\n', '    start = datetime.date(int(start_date[0:4]),int(start_date[4:6]),int(start_date[6:8]))\n', '    end =   datetime.date(int(end_date[0:4]),int(end_date[4:6]),int(end_date[6:8]))\n', '    url =""http://www.google.com/finance/historical?q={0}"".format(self.symbol)\n', '    url +=""&startdate={0}&enddate={1}&output=csv"".format( start.strftime(\'%b %d, %Y\'),end.strftime(\'%b %d, %Y\'))\n', '    csv = urllib.urlopen(url).readlines()\n', '    csv.reverse()\n', '    # print(url)\n', '    q=[]\n', '    for bar in xrange(0,len(csv)-1):\n', ""      ds,open_,high,low,close,volume = csv[bar].rstrip().split(',')\n"", '      if isfloat(open_) and isfloat(high) :\n', '        open_,high,low,close = [float(x) for x in [open_,high,low,close]]\n', ""        dt = datetime.datetime.strptime(ds,'%d-%b-%y')\n"", '        q.append([dt,open_,high,low,close,volume])\n', '    return q\n', '\n', 'def imp_googleQuoteSave(symbols, date1, date2, dircsv):\n', ' if isinstance(symbols, str) : symbols= [symbols]\n', ' for sym in symbols:\n', '   try :\n', '      q = imp_googleQuote(sym, date1, date2)   #interval, timeframe\n', ""   except: print('Error:'+symbol)\n"", '\n', ""   sym= sym.replace(':', '_')\n"", '\n', ""   if dircsv != '' :\n"", '     if not os.path.isdir(dircsv) :  os.makedirs(dircsv)\n', ""     start1= util.datetime_toint(util.datenumpy_todatetime(q['date'].values[0]))\n"", ""     file1= dircsv+ '\\\\'+ name1+'_'+ start1 +'_'+ str(date2) +'.csv'\n"", '     q.to_csv(file1)\n', '\n', ""   if dbname != '' :\n"", '     import sqlalchemy as sql\n', '     dbcon= sql.create_engine(dbname)\n', '     if interv_sec== 300 : # 5mins\n', ""       q.to_sql('q5min', dbcon)\n"", '\n', '\n', '\n', ""def imp_csv_dbupdate(indir='E:/_data/stock/intraday/intraday_google_usetf2.h5',\n"", ""                     outdir='E:/_data/stock/intraday/q5min/us/etf/', filelist=[], intype='csv',\n"", ""                     refcols=['date', 'open', 'high', 'low', 'close', 'volume', 'symbol']) :\n"", '\n', '  for file1 in filelist :\n', ""    sym=  file1[:file1.find('_')]  #Clean Up name\n"", '    print sym,\n', ""    outfile= outdir+'/'+sym+'.csv'\n"", '    if os.path.isfile(outfile) :\n', ""      df0= imp_csv_toext(file1=outfile, fromzone='Japan', tozone='UTC', header=0, cols=refcols)\n"", '    else :\n', '      df0= None\n', '\n', ""    if intype== 'hdfs':\n"", '      df1= imp_hdfs_getquote(indir, sym)\n', '      if isinstance(df1, float) : df1= None\n', '\n', ""    if intype=='csv' :\n"", ""      infile= indir+'/'+ file1   #csv\n"", ""      df1= imp_csv_toext(file1=infile, header=0, cols=refcols, fromzone='Japan', tozone='UTC')\n"", '\n', '    if df1 is not None :\n', '       if df0 is not None  : df1= imp_pd_merge(df1[refcols], df0[refcols])\n', '       # return outfile, df1\n', '       df1[refcols].to_csv(outfile, index=False)\n', '\n', '\n', '#####################################################################################\n', 'def imp_numpy_close_fromdb(dbname=\'/aaserialize/store/yahoo.db\', table1=\'\', symlist=[], t0=20010101, t1=20010101, priceid=""close"", batchsize= 400, maxasset=2600, tmax2=2000) :\n', ' print(""Get Numpy Matrix Close from DB SQL"")\n', ' import data.hist_data_storage as yhh\n', ' dstore= yhh.dailyDataStore(dbname)\n', ' close2= np.zeros((maxasset,tmax2),dtype=np.float16)\n', '\n', "" dateref0= dstore.get_histo(['SPY'], table1='daily_us_etf', start1=str(t0), end1=str(t1),  split_df=0).date.values\n"", ' \n', ' k=0; symfull=[];   i=0\n', ' nsym= len(symlist); nbatch= int(nsym / batchsize)+1\n', ' for j in xrange(0, nbatch) : \n', '      qlist, sym= dstore.get_histo(symlist[i:min(nsym,i+batchsize)], table1=table1, start1=str(t0), end1=str(t1),  split_df=1)\n', '\n', '      #print(""batch: ""+str(j), len(qlist), len(qlist[0].index) )\n', '      #for k,q in enumerate(qlist) :print sym[k], len(q.index)\n', '      i= i + batchsize\n', '      symfull= np.concatenate((symfull,sym))\n', '      close,dateref= date_align(qlist, dateref= dateref0, type1=priceid)\n', '      print(""batch: ""+str(j), len(sym),dateref[0],dateref[-1] )\n', '      qlist=None; sym=None\n', '      masset,tmax= np.shape(close)\n', '      if tmax2 < tmax: print(""Error tmax2 < tmax, wrong dates""); break\n', '      close2[k:k+masset,0:tmax] = close\n', '      k=k+ masset\n', '\n', ' maxasset= k\n', ' tmaxx= util.find(0.0, close2[0,:])\n', ' for k in xrange(0,maxasset):\n', '   if close2[k,tmaxx-1] == 0.0 : close2[k,tmaxx-1]=   close2[k,tmaxx-2]\n', ' \n', ' return close2[0:maxasset,0:tmaxx], symfull, dateref\n', '\n', 'def imp_numpy_close_fromhdfs(dbfile, symlist=[], t0=20010101, t1=20010101, priceid=""close"", maxasset=2600, tmax2=2000) :\n', ' print(""Get Numpy Matrix Close from Pandas"")\n', ' close2= np.zeros((maxasset,tmax2),dtype=np.float16)\n', ' store = pd.HDFStore(dbfile)\n', ' \n', "" df= store.select('SPY')\n"", ' df= df[(df.date >= t0) & (df.date <= t1 )]\n', ' dateref0= df.date.values\n', ' \n', ' k=0; symfull=[]; qlist=[]; sym=[]; tmax= 2000\n', ' for j, symbol in enumerate(store.keys()):\n', '  symbol=  symbol[1:]\n', '\n', '  if len(symlist)==0  or  util.find(symbol,symlist) > -1 : \n', '   #print symbol, util.find(symbol,symlist) \n', '   df= store.select(symbol) \n', '   if t0 !=t1 :     df= df[(df.date >= t0) & (df.date <= t1 )]\n', '   \n', '   qlist.append(df);   sym.append(symbol)\n', '   if np.mod(j+1, 401) == 0 :\n', '      print(""batch: ""+str(j))\n', '      close,dateref= date_align(qlist, dateref= dateref0, type1=priceid)   \n', '      print len(sym),dateref[0],dateref[-1] \n', '      symfull= symfull+sym\n', '      qlist=[]; sym=[]\n', ' \n', '      masset,tmax= np.shape(close)\n', '      if tmax2 < tmax: print(""Error tmax2 < tmax, wrong dates""); break\n', '      close2[(k):(k+masset),0:tmax] = close\n', '      k=k+ masset   \n', '     \n', ' if len(qlist) > 0:\n', '  # return qlist, qlist[0] ,  qlist[1] \n', '   close,dateref= date_align(qlist,type1=priceid)   \n', '   print len(sym),dateref[0],dateref[-1] \n', '   symfull= symfull+sym\n', '   masset,tmax= np.shape(close);\n', '   close2[(k):(k+masset),0:tmax] = close\n', ' \n', ' maxasset= k+masset\n', ' tmaxx= util.find(0.0, close2[0,:])\n', ' for k in xrange(0,maxasset):\n', '   if close2[k,tmaxx-1] == 0.0 : close2[k,tmaxx-1]=   close2[k,tmaxx-2]\n', ' \n', ' return close2[0:maxasset,0:tmaxx], symfull, dateref\n', '\n', '\n', ""def imp_sql_getquotes(stocklist01, dbname='sqlite:///aaserialize/store/yahoo.db', start1=20150101, end1=20160616, table1='daily_us_stock'):\n"", '    import data.hist_data_storage as yhh\n', '    dstore = yhh.dailyDataStore(dbname)\n', '    qlist, sym= dstore.get_histo(stocklist01, table1=table1,  start1=start1, end1=end1, split_df=1)\n', '    return qlist, sym\n', '\n', ""def imp_txt_getquotes(stocklist01, filedir='E:/_data/stock/daily/20160610/jp', startdate=20150101, endate=20160616):\n"", ' liststockfile= util.listallfile(filedir, ""*.txt"", dirlevel=5)\n', ' #print liststockfile\n', ' if len(np.shape(liststockfile)) > 1 :\n', '  liststockname=[ x.split(""."")[0] for x in liststockfile[:,0] ]\n', '#  for k in range(0, np.shape(liststockfile)[0]):  liststockname.append( (liststockfile[k,0])[0:4])   \n', '   \n', ' else: \n', '  stk0= liststockfile[0]; #stk2= liststockfile[2]\n', '#  liststockname=[ x[0:4] for x in stk0 ]\n', '  liststockname=[ x for x in stk0 ]\n', '\n', '#  liststockname=[]\n', '#  for k in range(0, len(stk0)):  liststockname.append( (stk0[k])[0:4])\n', ' \n', ' quotes=[]\n', ' print liststockname\n', ' for sym in stocklist01:\n', '   kstock= util.np_findfirst(sym, liststockname)\n', '   if kstock == -1 :\n', ""     print('Not Found '+sym)\n"", '   else :\n', '     df= pd.read_csv(liststockfile[kstock][2])\n', ""     df.columns = ['date', 'open', 'high','low','close', 'volume', 'openint']  #Change Column Name\n"", '     df2= df[(df.date > startdate  )& ( df.date   < endate )] # Filter by date\n', '     quotes.append(df2)\n', '  \n', ' return quotes \n', '\n', '\n', 'def imp_csv_getname(name1, date1, inter, tframe):\n', "" file1= dirstockcsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(inter) +'_'+ str(tframe) +'.csv'\n"", ' return file1\n', '\n', '\n', 'from dateutil import tz\n', ""def imp_csv_toext(file1='SPY.csv', outputfile='.h5', fromzone='Japan', tozone='UTC', header=None,\n"", ""                  cols=['date', 'time','open','high','low','close','volume', 'symbol'], coldate=[0]):\n"", ""  ''' cols: column name,  coldate: position of date column   '''\n"", '  from dateutil import parser\n', '\n', '  if os.path.getsize(file1)> 500 :\n', ""   df = pd.read_csv(file1,sep=',',header=header)  # date_parser=dateparse)  # parse_dates={'date': [] },\n"", '   df.date= [ parser.parse(x) for x in  df.date]\n', '\n', ""   #if util.find('symbol', df.columns.values) < 0 :\n"", ""     #df= util.pd_addcol(df, 'symbol')\n"", ""     #df['symbol']= sym\n"", '\n', ""   type1= outputfile[outputfile.find('.'):]\n"", '\n', ""   if outputfile=='':\n"", '     # df.columns = [  x.lower() for x in df.columns.values ]\n', ""     # df.columns = ['date', 'symbol','open','high','low','close','volume']\n"", '     df.drop(df.columns[dropcol], axis=1, inplace=True)\n', ""     df= util.pd_addcol(df, 'symbol');  df['symbol']= dfname\n"", ""     df.columns = ['date', 'open','high','low','close','volume', 'symbol']\n"", '     return df[cols]\n', '\n', ""   if type1=='csv' :\n"", ""     df[cols].to_csv(outputfile , index=False)  # , compression='gzip' )\n"", '\n', ""   if type1== 'h5' :\n"", ""      # dateparse= lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '      dateparse= lambda x: (parse(x).replace(tzinfo=from_zone).astimezone(tozone))\n', '      # dateparse= lambda x: parse(x, tzinfos=from_zone).astimezone(to_zone)\n', '      # df.date= [pd.to_datetime((str(x)[:-6])) for x in  df.date]\n', '      # df.date= [x.to_datetime() for x in  df.date]\n', '\n', '      df.drop(df.columns[[0]], axis=1, inplace=True)\n', '      print filenameh5\n', ""      if util.find('symbol', df.columns.values) < 0 :\n"", ""         df= util.pd_addcol(df, 'symbol'); df['symbol']= dfname\n"", '      store = pd.HDFStore(outputfile); store.append(dfname, df);  store.close()\n', '\n', '\n', '\n', ""def imp_hdfs_db_updatefromcsv(dircsv, filepd=r'E:\\_data\\stock\\intraday_google.h5', fromtimezone='Japan', tozone='UTC'):\n"", ' lfile= util.os_file_listall(dircsv, pattern=""*.csv"", dirlevel=0)\n', ' for x in lfile:\n', '   name1= (x[0]).split(""_"")[0] #get Ticker name, splitting by _\n', ""   if util.str_isfloat(name1):  name1= 'jp'+name1  #Japanese Stocks case\n"", '   file1 = x[2];  print(name1)\n', '   if os.path.getsize(file1)> 500:\n', ""      filepd2= filepd + '/' +name1 +'.csv'\n"", '      imp_csv_toext(file1, filepd2, name1, fromtimezone, tozone)\n', '\n', ""def imp_hdfs_db_dumpinfo(dbfile='E:\\_data\\stock\\intraday_google.h5'):\n"", '  store = pd.HDFStore(dbfile)\n', '  extract=[]; errsym=[]\n', '  for symbol in store.keys():\n', '     try:\n', '       df= pd.read_hdf(dbfile, symbol)\n', ""       t0= df['date'].values[0]\n"", ""       t1= df['date'].values[-1]\n"", '\n', '       extract.append([symbol[1:], df.shape[1],   df.shape[0], t0, t1 ])\n', '\n', '     except: errsym.append(symbol)\n', '  return np.array(extract), errsym\n', '\n', 'def imp_hdfs_mergedb(filepdfrom, filepdto) :\n', '  store0 = pd.HDFStore(filepdfrom)\n', '  store1 = pd.HDFStore(filepdto)\n', '\n', '  for symbol in store0.keys():\n', '    qq= imp_hdfs_getquote(filepdfrom, symbol)\n', '    store1.append(symbol, qq)\n', '\n', '    qq= imp_hdfs_getquote(filepdto, symbol)\n', ""    qq= qq.drop_duplicates(subset='date', take_last=True)\n"", ""    qq= qq.sort('date', ascending=1)\n"", '    print(symbol)\n', '  store0.close(); store1.close()\n', '\n', ""def imp_hdfs_storecopy(hdfs1='F:/usstock8.h5', hdfs2='F:/usstock8.h5'):\n"", ' store = pd.HDFStore(hdfs1);  store2 = pd.HDFStore(hdfs2)\n', ' for symbol in store.keys() :\n', '   df= imp_hdfs_getquote(hdfs1, symbol)\n', '   symbol= symbol[1:].replace(""-"",""_"")\n', '   store2.append(symbol, df, data_columns=True)\n', ' store.close(); store2.close()\n', '\n', 'def imp_hdfs_getquote(filenameh5, dfname=""data""):\n', '  try: return  pd.read_hdf(filenameh5, dfname)\n', '  except: return -1.0;\n', '\n', ""def imp_hdfs_getListquote(symbols, close1='close', start='12/18/2015 00:00:00+00:00',\n"", ""                          end='3/1/2016 00:00:00+00:00', freq='0d0h10min', filepd= 'E:\\_data\\stock\\intraday_google.h5',\n"", ""                          tozone='Japan', fillna=True, interpo=True):\n"", '\n', ' datefilter= pd.date_range(start=start, end=end, freq= freq,tz=tozone).values\n', '\n', ' errorsym=[]; quotes=[]; correctsym=[]\n', ' for symbol in symbols:  # Issue Not same size\n', '   qq= imp_hdfs_getquote(filepd, symbol)\n', '\n', '   if type(qq)==np.float :  # Error qq=-1\n', '        errorsym.append(symbol)\n', '   else:\n', ""       qq.columns = ['date', 'symbol','open','high','low','close','volume']\n"", ""       qq= qq[qq['date'].isin(datefilter)]   #Only date in the range\n"", ""       #qq= qq.drop_duplicates(cols='date', take_last=True)\n"", ""       #  qq= qq.sort('date', asc=True)\n"", '\n', '       #print qq.date.values[0]\n', ""       if fillna : qq= qq.fillna(method='backfill')\n"", '       if interpo : qq= qq.interpolate()\n', '       quotes.append(qq)\n', '       correctsym.append(symbol)\n', '\n', ' datefilter= datenumpy_todatetime(datefilter)\n', ' return quotes,datefilter, correctsym,   errorsym\n', '\n', '\n', ""def imp_hdfs_removeDuplicate(filepd='E:\\_data\\stock\\intraday_google.h5') :\n"", '  #-------Clean by removing duplicate-------------------------------------------------\n', '  store = pd.HDFStore(filepd)\n', '  for symbol in store.keys():\n', '    #qq= imp_hdfs_getquote(filepd, symbol);\n', '    qq= store.select(symbol[1:])\n', ""    qq= qq.drop_duplicates(subset='date', take_last=True)\n"", ""    qq= qq.sort('date', ascending=1)\n"", '    qq.index = list(np.arange(0,len(qq.index)))\n', '\n', '    store.remove(symbol); store.append(symbol, qq);\n', '  store.close()\n', '\n', '\n', '\n', ""def imp_pd_tohdfs(sym, qqlist, filenameh5, fromzone='Japan', tozone='UTC') :\n"", ""  ''' df list to HDFS '''\n"", '  store = pd.HDFStore(filenameh5);\n', '  for k, df2 in enumerate(qqlist) :\n', '    df= copy.deepcopy(df2)\n', ""    df['date']= datetime_toint(df.date.values)\n"", '\n', ""    symbol= sym[k].replace('-','_')\n"", '    qq= imp_hdfs_getquote(filenameh5, symbol)\n', '    # print symbol, type(qq)\n', '    if type(qq) ==  float or  type(qq) ==  int  :\n', '      store.append(symbol, df)  # , data_columns=True\n', '      # print(symbol + str(df.date.values[-1]))\n', '    else :\n', '      qq= pd.concat([qq, df], ignore_index=True)\n', ""      qq= qq.drop_duplicates(subset='date', keep='last')\n"", ""      qq= qq.sort('date', ascending=1)\n"", '      store.append(symbol, qq)  #Too much space , data_columns=True\n', '      print(symbol + str(qq.date.values[-1]))\n', '\n', '  store.close()\n', '\n', 'def imp_pd_merge(df1, df2) :\n', '  df= pd.concat([df1, df2], axis=0)\n', ""  df= df.drop_duplicates(subset='date', keep='last')\n"", ""  df= df.sort_values(by=['date'], ascending=1)\n"", '  df.index = list(np.arange(0,len(df.index)))\n', '  return df\n', '\n', 'def imp_pd_checkquote(quotes) :\n', ""   for c in quotes: print np.shape(c), c['date'].values[0], c['date'].values[-1]\n"", '\n', 'def imp_pd_getclose(df, datefilter=None):\n', ' if datefilter is  not None :\n', ""    df= df[df['date'].isin(datefilter)]   #Only date in the range\n"", "" df= df.sort('date')\n"", "" close= df.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", ' \n', ' close= np.array(close.values[1:,1:]).astype(np.float)            #Only Price\n', ' close= close[(np.abs(sci.stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close\n', '\n', 'def imp_pd_cashyield(q, duration=10):\n', ' qbond= copy.deepcopy(q)\n', "" for id1 in ['close','open','high','low','aclose'] :\n"", '   try:\n', '     vv= q[id1].values\n', '     pbond=  100 * np.cumprod( (1 + np.ones(len(vv)) * 0.005/365.0 ))\n', '     qbond[id1]= pbond\n', '   except: pass\n', ' return qbond\n', '\n', 'def imp_pd_yield_tobond(q, duration=10):\n', ' qbond= copy.deepcopy(q)\n', "" for id1 in ['close','open','high','low','aclose'] :\n"", '   try:\n', '     vv= q[id1].values\n', '     pbond=  1/ np.power(1+ vv*0.01,duration) *100\n', '     qbond[id1]= pbond\n', ""#     print 'ok'\n"", '   except: pass\n', ' return qbond\n', '\n', 'def imp_pd_errordate(quotes, dateref):\n', "" ''' Show Symbol in Error when importing '''\n"", ' for i, stock in enumerate(quotes) :\n', '   print(i, symbols1[i],  datetime_tostring(stock[0][0]))\n', ' print(""\\n\\n"")\n', '\n', ' for i, stock in enumerate(quotes) :\n', '   date1= datetime_toint(stock[0][0])\n', '   if date1 != dateref : print(i, symbols1[i],  str(date1))\n', '\n', 'def imp_pd_fxtoprice(q):\n', ' dfprice= copy.deepcopy(q)\n', "" for kid in ['close','open','high','low'] :\n"", '   qret= getret_fromquotes(q[kid].values,1)\n', '   qprice= price_normalize100(qret)\n', '   dfprice[kid]= qprice.T\n', ' return dfprice\n', '\n', 'def imp_pd_fxinversetoprice(q):\n', ' dfprice= copy.deepcopy(q)\n', "" for kid in ['close','open','high','low'] :\n"", '   qret= getret_fromquotes(1.0 / q[kid].values,1)\n', '   qprice= price_normalize100(qret)\n', '   dfprice[kid]= qprice.T\n', ' return dfprice\n', '\n', ""def imp_pd_filterbydate(df, dtref=None, start='2016-06-06 00:00:00', end='2016-06-14 00:00:00', freq='0d0h05min', timezone='Japan'):\n"", "" ''' df: DateSeries or TimeSeries of Quotes   '''\n"", '\n', ' if type(df) in {pd.core.frame.DataFrame} :  #Data frame version\n', '   if type(dtref)== str:\n', '      dtref= pd.date_range(start=start, end=end, freq= freq,tz=timezone).values\n', '\n', ""   return df[df['date'].isin(dtref)]\n"", ' else :\n', '   if type(dtref)== str:  #Date version\n', '      dtref= pd.date_range(start=start, end=end, freq= freq,tz=timezone).values\n', '\n', '   return df[df.isin(dtref)].values\n', '\n', 'def imp_pd_cleanquote(q):\n', ' col= q.columns.values\n', "" if isinstance(q['date'].values[0] , str) :\n"", ""   q['date']= datetime_todate(datestring_todatetime(q['date'].values))\n"", '\n', ' for kid in col:\n', ""   if kid not in ['date', 'day','month','year'] :\n"", ""      q[kid]= pd.to_numeric(q[kid], errors='coerce').values  #Put NA on string\n"", '\n', "" q= q.fillna(method='pad')\n"", ' return q\n', '\n', 'def imp_pd_divide(q1, q2, funapply):\n', "" close, dateref= date_align([q1,q2], type1='close')\n"", ' m,tt= np.shape(close)\n', '\n', ' q3= util.pd_createdf( np.zeros((tt,len(q1.columns))) ,q1.columns, dateref)\n', "" q3['date']= dateref\n"", "" q3['close']= (close[0,:] / close[1,:])\n"", '\n', ' for kid in q1.columns :\n', ""   if kid not in ['close', 'volume','date','day','month', 'year'] :\n"", '     close, _= date_align([q1,q2], type1=kid)\n', '     q3[kid]= (close[0,:] / close[1,:])\n', ' return q3\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ""def imp_quote_csvdir_tofile(dircsv='', outdir1='E:/_data/stock/', fromtimezone='Japan', tozone='UTC'):\n"", ' lfile= util.os_file_listall(dircsv, pattern=""*.csv"", dirlevel=0)\n', ' for x in lfile:\n', '   name1= (x[0]).split(""_"")[0] #get Ticker name, splitting by _\n', ""   if util.str_isfloat(name1):  name1= 'jp'+name1  #Japanese Stocks case\n"", '   file1 = x[2];  print(name1)\n', '   if os.path.getsize(file1)> 500:\n', ""      filepd2= outdir + '/' +name1 +'.csv'\n"", '      imp_csv_toext(file1, filepd2,  header=True, fromzone=fromtimezone, tozone=tozone)\n', ""'''\n"", '\n', '\n', ""'''\n"", ""close1='Close' ; start='12/18/2015 00:00:00';\n"", ""end='3/1/2016 00:00:00'; freq='1d0h00min';\n"", ""filepd= 'E:\\_data\\stock\\intraday_google.h5'\n"", '\n', 'datefilter= pd.date_range(start=start, end=end,   freq= freq).values\n', '\n', 'symbols, names = np.array(list(symbol_dict.items())).T\n', 'i=0; vv=[]\n', 'for symbol in symbols:  # Issue Not same size\n', '   qq= quote_frompanda(filepd, symbol); i+=1\n', '   if isfloat(qq) :  vv.append(symbol)\n', '   else:\n', ""     if i==1: close = qq[['Datetime']]  #new dataframe    \n"", '     tmp= pd.DataFrame(data={symbol: qq[close1]});\n', '     if not tmp.isnull().values.all():\n', ""      close = close.join(tmp, how='outer', rsuffix='_1')\n"", ""      close= close.drop_duplicates(cols='Datetime', take_last=True)\n"", '\n', ""close= close[close['Datetime'].isin(datefilter)]   #Only date in the range\n"", ""close= close.sort('Datetime')\n"", ""close= close.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", 'close= np.array(close.values[1:,1:]).astype(np.float)  #Only Price\n', 'close= close[(np.abs(stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close, vv\n', ""'''\n"", '\n', ""'''\n"", 'from datetime import datetime\n', 'from dateutil import tz\n', '\n', ""from_zone = tz.gettz('Japan'); to_zone = tz.gettz('UTC')\n"", '\n', ""date1 = datetime.strptime('2011-01-21 09:00:00', '%Y-%m-%d %H:%M:%S')\n"", 'date1 = date1.replace(tzinfo=from_zone)\n', 'dateutc = date1.astimezone(to_zone)\n', 'date1, dateutc\n', '\n', ""Suppose you have a column 'datetime' with your string, then:\n"", ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '\n', ""df = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n"", ' combine multiple columns into a single datetime column,\n', ""this merges a 'date' and a 'time' column into a single 'datetime' column:\n"", '\n', ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '\n', ""df = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)\n"", ""'''\n"", '\n', '\n', '#date1 = ""20160302""; inter= 60; tframe=80; name1= ""spy""\n', '#googleIntradayQuoteSave(name1, inter, tframe)\n', '\n', '\n', '\n', 'def ta_mar(close,t,m):  return  100*( np.mean(close[(t-m):(t+1)], axis=0) / close[t] )\n', 'def ta_ma(close,t,m):  return   np.mean(close[(t-m):(t+1)], axis=0)\n', '\n', '\n', '############################################################################\n', '#---------------------Calculate the Statistics         --------------------  \n', 'def calc_statestock(close2, dateref, symfull):\n', ' def sort(x,col,asc): return   util.sortcol(x,col,asc)\n', ' def perf(close,t0,t1):  return  100*( close[:,t1] / close[:,t0] -1)\n', ' def and2(tuple1):  return np.logical_and.reduce(tuple1)\n', ' def mar(close,t,m):  return  100*( np.mean(close[:,(t-m):(t+1)], axis=1) / close[:,t] )\n', ' def ma(close,t,m):  return   np.mean(close[:,(t-m):(t+1)], axis=1)\n', '\n', '\n', ' # a= ma(close2, tmax, 5) / ma(close2, tmax, 50)  \n', '\n', '\n', ' def dd(t) :\n', '  x= util.find(t,dateref)  # 607\n', ""  if x==-1 : print 'Error date not found in dateref '+str(t) ; return np.nan\n"", '  else :  return x\n', '\n', ' def gap(close,t0,t1,lag):   \n', '  ret= getret_fromquotes(close[:,t0:t1],lag)\n', '  rmin= 100*np.amin(ret,axis=1)\n', '  return rmin\n', ' \n', ' #--------------------Size----------------------------------------- \n', ' masset= len(symfull)\n', ' ttmax= util.find(0,close2[0,:])\n', ' if ttmax==-1 : ttmax= np.shape(close2)[1]\n', ' close2= close2[0:masset,0:ttmax]\n', ' m, tmax= masset, ttmax-1\n', ' \n', ' #correl\n', ' #close3= np.array(close2[:,0:ttmax],dtype=np.float16)\n', ' #close2ret= getret_fromquotes(close3)\n', ' #close2ret= np.nan_to_num(close2ret)\n', ' #close2ret= util.np_cleanmatrix(close2ret);  del close3\n', '\n', '##################################################################################\n', '#1: 1d,2:5d,3:10d,4:15d,5:20d,11:50d,12:60d,13:90d,15:1y\n', '#16: YTD,17:From Feb,21: 2015Ret,45: Price,50,51: Min,52,53: Max\n', ' print(""Calcul the Stats "" + str(dateref[-1]))\n', ' stat= np.zeros((2600,200),dtype=np.float16)\n', '\n', ' k=0\n', ' stat[k:(k+m),0]= np.arange(k,k+m)\n', ' stat[k:(k+m),1]=  perf(close2,tmax-1,tmax)  \n', ' stat[k:(k+m),2]=  perf(close2,tmax-5,tmax)  \n', ' stat[k:(k+m),3]=  perf(close2,tmax-10,tmax)  \n', ' stat[k:(k+m),4]=  perf(close2,tmax-15,tmax)  \n', ' stat[k:(k+m),5]=  perf(close2,tmax-20,tmax)  \n', '# stat[k:(k+m),6]=  perf(close2,tmax-25,tmax)  \n', ' stat[k:(k+m),7]=  perf(close2,tmax-30,tmax)  \n', '# stat[k:(k+m),8]=  perf(close2,tmax-35,tmax)  \n', ' stat[k:(k+m),9]=  perf(close2,tmax-40,tmax)  \n', '# stat[k:(k+m),10]=  perf(close2,tmax-45,tmax)  \n', ' stat[k:(k+m),11]=  perf(close2,tmax-50,tmax)  \n', ' stat[k:(k+m),12]=  perf(close2,tmax-60,tmax)  \n', ' stat[k:(k+m),13]=  perf(close2,tmax-90,tmax)  \n', ' stat[k:(k+m),14]=  perf(close2,tmax-120,tmax)  \n', ' stat[k:(k+m),15]=  perf(close2,tmax-250,tmax)  \n', '\n', '\n', '# stat[k:(k+m),16]=  perf(close2, dd(20160105),tmax)   #1Jan 2016\n', '# stat[k:(k+m),17]=  perf(close2, dd(20160222),tmax)   # 22 Feb 2016\n', '# stat[k:(k+m),18]=  perf(close2,dd(20160105),dd(20160222))   # Crash DrawDown\n', ' # stat[k:(k+m),19]=  perf(close2,624,tmax)   #  24 June 2016\n', ' # stat[k:(k+m),20]=  perf(close2,623,625)   #  Crash 24 June 2016\n', ' # stat[k:(k+m),21]=  perf(close2,dd(20151230)-250,dd(20151230))    #2015 Return\n', '\n', ' stat[k:(k+m),22]= gap(close2,tmax-80,tmax,1) #Max 1 days Gap\n', ' stat[k:(k+m),23]= gap(close2,tmax-80,tmax,5) #Max 5 days Gap\n', ' stat[k:(k+m),24]= perf(close2,tmax-2,tmax)  \n', ' stat[k:(k+m),25]= perf(close2,tmax-3,tmax)  \n', ' stat[k:(k+m),26]= perf(close2,tmax-4,tmax)  \n', '\n', ' stat[k:(k+m),27]= perf(close2,tmax-2,tmax-1)   \n', ' stat[k:(k+m),28]= perf(close2,tmax-3,tmax-2)   \n', ' stat[k:(k+m),29]= perf(close2,tmax-4,tmax-3)   \n', ' stat[k:(k+m),30]= perf(close2,tmax-5,tmax-4)\n', ' \n', ' stat[k:(k+m),120]= mar(close2, tmax, 20) \n', ' stat[k:(k+m),121]= mar(close2, tmax, 50) \n', ' stat[k:(k+m),122]= 100*ma(close2, tmax, 5) / ma(close2, tmax, 50)  #OverSold /OverBought\n', ' stat[k:(k+m),123]= mar(close2, tmax, 100)  \n', ' stat[k:(k+m),124]= mar(close2, tmax, 10) \n', '# stat[k:(k+m),35]= 0 \n', '# stat[k:(k+m),36]= 0 \n', '# stat[k:(k+m),37]= 0 \n', '# stat[k:(k+m),38]= 0 \n', '# stat[k:(k+m),39]= 0 \n', '\n', ' # stat[k:(k+m),30]= volume / AvgVol3M\n', ' # stat[k:(k+m),30]= Nbday_to_Earnings\n', ' #Volume/3M AvgVolume\n', ' # stat[k:(k+m),26]=  volume(volume2,tmax-120,tmax)  \n', '\n', ' #==============================================================================\n', ' #-------------- Data Fundamental    -------------------------------------------\n', "" df= util.sql_query(sqlr='SELECT ticker,shortratio,sector1_id, sector2_id, marketcap   FROM stockfundamental ', dburl='/aaserialize/store/finviz.db')\n"", ' npdf= df.values; del df\n', ' npdf0= npdf[:,0]  \n', ' for i in range(0,m):  \n', '   try :\n', '     kid= util.find(symfull[i], npdf0)\n', '     if kid != -1 :\n', '      stat[i,150]=  float(npdf[kid, 1])   # short ratio\n', '      stat[i,151]=  float(npdf[kid, 2])   # sector 1      \n', '      stat[i,152]=  float(npdf[kid, 3])   # sector 2     \n', '      stat[i,153]=  float(npdf[kid, 4] / 100000000.0)   # market Cap    \n', '   except : pass\n', ' del npdf, npdf0\n', '\n', '\n', '#==============================================================================\n', ' ################ Volatility\n', ' stat[k:(k+m),40]= volhisto_fromprice(close2,tmax,20,axis=1)* 100\n', ' stat[k:(k+m),41]= volhisto_fromprice(close2,tmax,60,axis=1)* 100\n', ' stat[k:(k+m),42]= volhisto_fromprice(close2,tmax,252,axis=1)* 100\n', '# stat[k:(k+m),43]= stat[k:(k+m),12] / stat[k:(k+m),41] #Sharpe 3Month\n', '# stat[k:(k+m),44]= stat[k:(k+m),15] / stat[k:(k+m),42] #Sharpe 1Year\n', '\n', ' stat[k:(k+m),45]= close2[:,tmax] #Last Close\n', '\n', '# stat[k:(k+m),46]= 0 \n', '# stat[k:(k+m),47]= 0 \n', '# stat[k:(k+m),48]= 0 \n', '# stat[k:(k+m),49]= 0 \n', '\n', '\n', ' ###############  Technical Indicator\n', ' #Min of last 6 months, Max of last 6months\n', ' for i in range(0,m):\n', '  pp0= stat[i,45]\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-120):tmax])\n', '  stat[i,50]= kmin+(tmax-120);  stat[i,51]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-120):tmax])\n', '  stat[i,52]= kmax+(tmax-120);  stat[i,53]= 100*pp0 / pmax\n', '\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-10):tmax])\n', '  stat[i,66]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-10):tmax])\n', '  stat[i,67]= 100*pp0 / pmax\n', '\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-5):tmax])\n', '  stat[i,68]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-5):tmax])\n', '  stat[i,69]= 100*pp0 / pmax\n', '\n', ' del kmax,pmax,i,k  \n', '\n', '\n', ' #Regression from Min Price Time\n', ' for i in range(0,m):\n', '  t0= int(stat[i,50])\n', '  try :\n', '    res= regression(close2[i,t0:tmax ]  ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,54]=  res[2]  #R2\n', '    stat[i,55]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', ' #Regression from Max Price Time\n', ' for i in range(0,m):  \n', '  t0= stat[i,52]\n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,56]=  res[2]  #R2\n', '    stat[i,57]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', ' #Regression from 5 days ago\n', ' t0= tmax-5\n', ' for i in range(0,m): \n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,58]=  res[2]  #R2\n', '    stat[i,59]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', '#Regression from 10 days ago\n', ' t0= tmax-10\n', ' for i in range(0,m):  \n', '   try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,60]=  res[2]  #R2\n', '    stat[i,61]=  res[0][0]  #Slope\n', '   except : pass\n', '\n', "" '''\n"", '#Regression from 1 month\n', ' t0= tmax-20   \n', ' for i in range(0,m):\n', '   try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,62]=  res[2]  #R2\n', '    stat[i,63]=  res[0][0]  #Slope\n', '   except : pass\n', "" '''\n"", '\n', "" '''\n"", '#Regression from 2 months ago\n', ' t0= tmax-40\n', ' for i in range(0,m):  \n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,64]=  res[2]  #R2\n', '    stat[i,65]=  res[0][0]  #Slope\n', '  except : pass\n', "" '''\n"", '\n', '#Nb of Positive days last 5d\n', ' for i in range(0,m):  stat[i,66]=  np_countretsign(close2[i,(tmax-5):tmax])  \n', '\n', '#Nb of Positive days last 10d\n', ' for i in range(0,m):  stat[i,67]=  np_countretsign(close2[i,(tmax-10):tmax])  \n', '\n', '#Nb of Positive days last 20d\n', '# for i in range(0,m):  stat[i,68]=  np_countretsign(close2[i,(tmax-20):tmax])  \n', '\n', '\n', '#Regression from Max Values,200days\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmax[:,1] ,vmax[:,0],type1=""linear"")\n', '    stat[i,70]=  res[2]  #R2\n', '    stat[i,71]=  res[0][0]  #Slope\n', '  except : pass\n', '# del vmax\n', '\n', '\n', '#Regression from Min Values,200days \n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmin= util.np_findlocalmin2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmin[:,1] ,vmin[:,0],type1=""linear"")\n', '    stat[i,72]=  res[2]  #R2\n', '    stat[i,73]=  res[0][0]  #Slope\n', '  except : pass\n', '# del vmin\n', '\n', '\n', '#Regression from Max Values,100days\n', ' t0= tmax-20*5\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmax[:,1] ,vmax[:,0],type1=""linear"")\n', '    stat[i,74]=  res[2]  #R2\n', '    stat[i,75]=  res[0][0]  #Slope\n', '  except : pass\n', ' del vmax\n', '\n', '\n', '#Regression from Min Values,100days \n', ' t0= tmax-20*5\n', ' for i in range(0,m):  \n', '  try :\n', '    vmin= util.np_findlocalmin2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmin[:,1] ,vmin[:,0],type1=""linear"")\n', '    stat[i,76]=  res[2]  #R2\n', '    stat[i,77]=  res[0][0]  #Slope\n', '  except : pass\n', ' del vmin\n', '\n', '\n', '\n', '# Upside Trend with Higher Lows detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '   vmax= util.np_findlocalmin2(close2[i,t0:tmax],4)\n', '   vmax= util.sort(vmax,0,asc=0)\n', '   stat[i,80]=  -1\n', '   if len(vmax) > 1 :\n', '    ss=0\n', '    for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] > vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '    if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,80]=  ss  \n', '        stat[i,81]=  vmax[0,0] + t0\n', '  except: pass\n', '\n', '\n', '# Upside Trend with Higher High detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax],4)\n', '    vmax= util.sort(vmax,0,asc=0)\n', '    stat[i,82]=  -1\n', '    if len(vmax) > 1 :\n', '      ss=0\n', '      for k in range(0,len(vmax)-1):\n', '        if vmax[k,1] > vmax[k+1,1] : ss+=1   #Higher high\n', '        else : break;\n', '      if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,82]=  ss  \n', '        stat[i,83]=  vmax[0,0] + t0\n', '  except: pass\n', '\n', '\n', '# Downside Trend with lower Lows detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  vmax= util.np_findlocalmin2(close2[i,t0:tmax],4)\n', '  vmax= util.sort(vmax,0,asc=0)\n', '  stat[i,84]=  -1\n', '  if len(vmax) > 1 :\n', '   ss=0\n', '   for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] < vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '   if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,84]=  ss  \n', '        stat[i,85]=  vmax[0,0] + t0\n', '\n', '\n', '# Downside Trend  with lower highs detection\n', ' t0= tmax-20*6\n', ' for i in range(0,m):  \n', '  vmax= util.np_findlocalmax2(close2[i,t0:tmax],4)\n', '  vmax= util.sort(vmax,0,asc=0)\n', '  stat[i,80]=  -1\n', '  if len(vmax) > 1 :\n', '   ss=0\n', '   for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] < vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '   if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,86]=  ss    #Nb  of lower high consecutive\n', '        stat[i,87]=  vmax[0,0] + t0\n', '\n', "" print('Higher/Lower Band Support')\n"", '#-----higher band Support / Lower Band Support 120days-------------------------------\n', ' t0= tmax-120\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,90]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,91]= 100.0* stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,92]= 100.0* (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '    # pmin=  (res2.x[0] * (np.arange(t0,tmax)-t0) + res2.x[1]) \n', '    # breachmin= np.sum(np.sign(close2[i, t0:tmax] - pmin*1.03))\n', '  except: pass\n', '\n', '\n', '#-----higher band Support / Lower Band Support 200days-------------------------------\n', ' t0= tmax-200\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,93]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,94]= 100.0 * stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,95]= 100.0 * (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '  except: pass\n', '\n', '\n', '#-----higher band Support / Lower Band Support 300days-------------------------------------\n', ' t0= tmax-300\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,96]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,97]= 100.0* stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,98]= 100.0* (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '  except: pass\n', '\n', '\n', ""  '''\n"", '  # -----higher band Support / Lower Band Support 60days-------------------------------\n', '  t0 = tmax - 60\n', '  for i in range(0, m):\n', '      try:\n', '          res1 = ta_highbandtrend1(close2[i, t0:tmax])\n', '          pmax = (res1.x[0] * (tmax - t0) + res1.x[1])\n', '          stat[i, 99] = 100.0 * stat[i, 45] / pmax  # Higher Band\n', '\n', '          res2 = ta_lowbandtrend1(close2[i, t0:tmax])\n', '          pmin = (res2.x[0] * (tmax - t0) + res2.x[1])\n', '          stat[i, 100] = 100.0 * stat[i, 45] / pmin  # LowerBand\n', '\n', '          stat[i, 101] = 100.0 * (pmax - stat[i, 45]) / (pmax - pmin)  # Price inside Range\n', '\n', '          # pmin=  (res2.x[0] * (np.arange(t0,tmax)-t0) + res2.x[1])\n', '          # breachmin= np.sum(np.sign(close2[i, t0:tmax] - pmin*1.03))\n', '      except:\n', '          pass\n', ""  '''\n"", '\n', '  #-----Technical Indicator------------------------------------------------\n', '# for i in range(0,m): \n', '#  try :\n', '#   res2=0\n', '   # df= pd.read_hdf(dbfile, symfull[i])\n', '   # stat[i,110]= ta.RMI(close2[i,:])\n', '   # stat[i,111]= 100*close2[i,:] / ta.MA(close2[i,:],20)\n', '   # stat[i,112]= 100*close2[i,:] / ta.MA(close2[i,:],50)   \n', '#  except: pass\n', '\n', '\n', ' return np.array(stat, dtype=np.float16) \n', '\n', '\n', ""'''  \n"", 'i=1964  \n', 'stat= s1\n', 'stat[i, 91]  \n', '\n', 'tt= np.arange(t0, tmax)\n', 'plot_price(close2[i,t0:tmax], (res2.x[0] * (tt-t0) + res2.x[1]) )\n', ""'''\n"", '\n', '  \n', '#------------------- Stock Monitoring Tools ------------------------------------------------\n', ""def monitor_addrecommend(string1, dbname='stock_recommend') :\n"", ' ss= string1.replace(""("","" "").replace(\')\',\' \').replace(\':\',\' \')\n', "" ss= ss.replace('\\t', ' ').replace('\\n', ' ')\n"", ' sl1= ss.split("" "")\n', ' print sl1\n', ' aux= [datestring_toint(util.date_now())]\n', ' symfull= copy.deepcopy(a_us_all)\n', ' for x in sl1:\n', '   if len(x) < 5 :\n', '    if util.find(x,symfull) > 0 :   aux.append(x)\n', '\n', ' if len(aux) >  1 :\n', '  stock_recommend= util.load_obj(dbname)\n', '  stock_recommend.append(aux)\n', '  util.save_obj(stock_recommend,dbname)\n', '  print aux\n', ' return stock_recommend\n', '\n', '\n', '\n', '#################### Finviz  ###############################################################\n', 'from bs4 import BeautifulSoup\n', 'import csv,datetime, requests\n', 'def imp_finviz():\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print ""Finviz Overview Start""\n', ' url = ""http://www.finviz.com/screener.ashx?v=111&f=geo_usa""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', "" firstcount = soup.find_all('option')\n"", ' lastnum = len(firstcount) - 1\n', "" lastpagenum = firstcount[lastnum].attrs['value']\n"", ' currentpage = int(lastpagenum)\n', '\n', ' alldata = []\n', ' templist = []\n', ' # Overview = 111, Valuation = 121, Financial = 161, Ownership = 131, Performance = 141\n', ' #pagesarray = [111,121,161,131,141]\n', ' titleslist = soup.find_all(\'td\',{""class"" : ""table-top""})\n', ' titleslisttickerid = soup.find_all(\'td\',{""class"" : ""table-top-s""})\n', ' titleticker = titleslisttickerid[0].text\n', ' titlesarray = []\n', ' for title in titleslist:\n', '    titlesarray.append(title.text)\n', '\n', ' titlesarray.insert(1,titleticker)\n', ' i = 0\n', '\n', ' while(currentpage > 0):\n', '    i += 1\n', '    print str(i) + "" page(s) done""\n', '    secondurl = ""http://www.finviz.com/screener.ashx?v="" + str(111) + ""&f=geo_usa"" + ""&r="" + str(currentpage)\n', '    secondresponse = requests.get(secondurl)\n', '    secondhtml = secondresponse.content\n', '    secondsoup = BeautifulSoup(secondhtml)\n', '    stockdata = secondsoup.find_all(\'a\', {""class"" : ""screener-link""})\n', '    stockticker = secondsoup.find_all(\'a\', {""class"" : ""screener-link-primary""})\n', '    datalength = len(stockdata)\n', '    tickerdatalength = len(stockticker)\n', '\n', '    while(datalength > 0):\n', '        templist = [stockdata[datalength - 10].text,stockticker[tickerdatalength-1].text,stockdata[datalength - 9].text,stockdata[datalength - 8].text,stockdata[datalength - 7].text,stockdata[datalength - 6].text,stockdata[datalength - 5].text,stockdata[datalength - 4].text,stockdata[datalength - 3].text,stockdata[datalength - 2].text,stockdata[datalength - 1].text,]\n', '        alldata.append(templist)\n', '        templist = []\n', '        datalength -= 10\n', '        tickerdatalength -= 1\n', '    currentpage -= 20\n', '\n', "" with open('stockoverview.csv', 'wb') as csvfile:\n"", ""    overview = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=titlesarray)\n"", '    overview.writeheader()\n', '\n', '    for stock in alldata:\n', '        overview.writerow({titlesarray[0] : stock[0], titlesarray[1] : stock[1],titlesarray[2] : stock[2],titlesarray[3] : stock[3],titlesarray[4] : stock[4], titlesarray[5] : stock[5], titlesarray[6] : stock[6], titlesarray[7] : stock[7] , titlesarray[8] : stock[8], titlesarray[9] : stock[9], titlesarray[10] : stock[10] })\n', '\n', ' print ""Finviz Overview Completed""\n', ' return overview\n', ' \n', '\n', 'def imp_finviz_news():\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print ""Finviz Performance Start""\n', ' url = ""http://www.finviz.com/quote.ashx?t=intc""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', ' titleslist = soup.find_all(\'a\',{""class"" : ""tab-link-news""})\n', ' return titleslist\n', '\n', '\n', 'def imp_finviz_financials():\n', ' import datetime\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print datetime.datetime.now()\n', ' print ""Finviz Financial Start""\n', ' url = ""http://www.finviz.com/screener.ashx?v=161&f=geo_usa""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', "" firstcount = soup.find_all('option')\n"", ' lastnum = len(firstcount) - 1\n', "" lastpagenum = firstcount[lastnum].attrs['value']\n"", ' currentpage = int(lastpagenum)\n', '\n', ' alldata = []\n', ' templist = []\n', ' # Overview = 111, Valuation = 121, Financial = 161, Ownership = 131, Performance = 141\n', ' #pagesarray = [111,121,161,131,141]\n', ' titleslist = soup.find_all(\'td\',{""class"" : ""table-top""})\n', ' titleslisttickerid = soup.find_all(\'td\',{""class"" : ""table-top-s""})\n', ' titleticker = titleslisttickerid[0].text\n', ' titlesarray = []\n', ' for title in titleslist:\n', '    titlesarray.append(title.text)\n', '\n', ' titlesarray.insert(1,titleticker)\n', ' i = 0\n', '\n', ' while(currentpage > 0):\n', '    i += 1\n', '    print str(i) + "" page(s) done""\n', '    secondurl = ""http://www.finviz.com/screener.ashx?v="" + str(161) + ""&f=geo_usa"" + ""&r="" + str(currentpage)\n', '    secondresponse = requests.get(secondurl)\n', '    secondhtml = secondresponse.content\n', '    secondsoup = BeautifulSoup(secondhtml)\n', '    stockdata = secondsoup.find_all(\'a\', {""class"" : ""screener-link""})\n', '    stockticker = secondsoup.find_all(\'a\', {""class"" : ""screener-link-primary""})\n', '    datalength = len(stockdata)\n', '    tickerdatalength = len(stockticker)\n', '\n', '    while(datalength > 0):\n', '        templist = [stockdata[datalength - 17].text,stockticker[tickerdatalength-1].text,stockdata[datalength - 16].text,stockdata[datalength - 15].text,stockdata[datalength - 14].text,stockdata[datalength - 13].text,stockdata[datalength - 12].text,stockdata[datalength - 11].text,stockdata[datalength - 10].text,stockdata[datalength - 9].text,stockdata[datalength - 8].text,stockdata[datalength - 7].text,stockdata[datalength - 6].text,stockdata[datalength - 5].text,stockdata[datalength - 4].text,stockdata[datalength - 3].text,stockdata[datalength - 2].text,stockdata[datalength - 1].text,]\n', '        alldata.append(templist)\n', '        templist = []\n', '        datalength -= 17\n', '        tickerdatalength -= 1\n', '    currentpage -= 20\n', '\n', "" with open('stockfinancial.csv', 'wb') as csvfile:\n"", ""    financial = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=titlesarray)\n"", '    financial.writeheader()\n', '\n', '    for stock in alldata:\n', '        financial.writerow({titlesarray[0] : stock[0], titlesarray[1] : stock[1],titlesarray[2] : stock[2],titlesarray[3] : stock[3],titlesarray[4] : stock[4], titlesarray[5] : stock[5], titlesarray[6] : stock[6], titlesarray[7] : stock[7] , titlesarray[8] : stock[8], titlesarray[9] : stock[9], titlesarray[10] : stock[10],titlesarray[11] : stock[11],titlesarray[12] : stock[12],titlesarray[13] : stock[13],titlesarray[14] : stock[14],titlesarray[15] : stock[15],titlesarray[16] : stock[16],titlesarray[17] : stock[17] })\n', '\n', ' print datetime.datetime.now()\n', ' print ""Finviz Financial Completed""\n', '\n', '\n', '\n', 'def get_price2book( symbol ):\n', '   from bs4 import BeautifulSoup as bs\n', '   import urllib as u\n', '   try:\n', ""    \turl = r'http://finviz.com/quote.ashx?t={}'.format(symbol.lower())\n"", '        html = u.request.urlopen(url).read()\n', ""        soup = bs(html, 'lxml')\n"", '        # Change the text below to get a diff metric\n', ""        pb =  soup.find(text = r'P/B')\n"", ""        pb_ = pb.find_next(class_='snapshot-td2').text\n"", ""        print( '{} price to book = {}'.format(symbol, pb_) )\n"", '        return pb_\n', '   except Exception as e:\n', '        print(e)\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ' try:\n', '   q = imp_googleQuote(name1, date1, date2)  #interval, timeframe\n', ""   name1= name1.replace(':','_')\n"", ""   file1= dircsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(date2) +'.csv'\n"", '   q.write_csv(file1)\n', ' except: print\n', ""'''\n"", '\n', '\n', ""'''\n"", 'def imp_googleQuoteList(symbols, freqsec=300, nday=1, intraday1=True) :\n', ' qlist=[]; sym=[]\n', ' if intraday1:\n', '  for symbol in symbols:\n', '     #try :\n', '        q = imp_googleIntradayQuote(symbol, freqsec, nday)  #interval, timeframe\n', '        qlist.append(q)\n', '        sym.append(symbol)\n', ""     #except: print('Error:'+symbol)\n"", '\n', ' else :\n', '  for symbol in symbols:\n', '     try :\n', '       q = imp_googleQuote(name1, freqsec, num_days)  #interval, timeframe\n', '     except: print(symbol)\n', '\n', ' return qlist, sym\n', ""'''\n"", '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''        \n"", ""stock_list = ['XOM','AMZN','AAPL','SWKS']\n"", 'p2b_series = pd.Series( index=stock_list )\n', '\n', 'for sym in stock_list:\tp2b_series[sym] = get_price2book(sym)\n', ""'''\n"", '\n', '\n', '\n', ""'''\n"", ""def imp_csv_toext(file1, filenameh5, dfname='sym1', fromzone='Japan', tozone='UTC'):\n"", "" # dateparse= lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", ' from dateutil.parser import parse\n', ' if os.path.getsize(file1)> 500 :\n', '  from_zone = tz.gettz(fromzone); tozone = tz.gettz(tozone)\n', ""  # dateparse= lambda x: (pd.datetime.strptime(x,'%Y-%m-%d %H:%M:%S').replace(tzinfo=from_zone).astimezone(tozone))\n"", '\n', ""  # dateparse= lambda x: (pd.datetime.strptime(x,'%m-%d-%Y %H:%M:%S').replace(tzinfo=from_zone).astimezone(tozone))\n"", '\n', '  dateparse= lambda x: (parse(x).replace(tzinfo=from_zone).astimezone(tozone))\n', '\n', '  # dateparse= lambda x: parse(x, tzinfos=from_zone).astimezone(to_zone)\n', '  print file1\n', ""  #df = pd.read_csv(file1,sep=',',header=None, parse_dates={'date': [1]}, date_parser=dateparse)\n"", '\n', ""  df = pd.read_csv(file1,sep=',',header=0)\n"", '  #df.date= [pd.to_datetime((str(x)[:-6])) for x in  df.date]\n', '  #df.date= [x.to_datetime() for x in  df.date]\n', '\n', '\n', '  df.drop(df.columns[[0]], axis=1, inplace=True)\n', ""  df= util.pd_addcol(df, 'symbol')\n"", ""  df['symbol']= dfname\n"", '\n', '  print filenameh5\n', ""  df.columns = ['date', 'open','high','low','close','volume', 'symbol' ]\n"", ""  df.to_csv(filenameh5 , index=False)  # , compression='gzip' )\n"", '\n', '  # store = pd.HDFStore(filenameh5);\n', '  # store.append(dfname, df);  store.close()\n', ""'''\n"", '\n', ""'''\n"", ""def imp_csv_toext(file1, filenameh5, dropcol=[], type1='csv', hasheader=True, dfname='sym1', fromzone='Japan', tozone='UTC'):\n"", ' if os.path.getsize(file1)> 500 :\n', '   header= 0 if hasheader else None\n', ""   df = pd.read_csv(file1, sep=',',header=0)\n"", '\n', '   df.drop(df.columns[dropcol], axis=1, inplace=True)\n', ""   df= util.pd_addcol(df, 'symbol');  df['symbol']= dfname\n"", ""   df.columns = ['date', 'open','high','low','close','volume', 'symbol' ]\n"", '\n', ""   if type1=='csv' :\n"", ""     df.to_csv(filenameh5 , index=False)  # , compression='gzip' )\n"", ""'''\n"", '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ""     if i==1: close = qq[['Datetime']]  #new dataframe\n"", '     tmp= pd.DataFrame(data={symbol: qq[close1]});\n', ""     close = close.join(tmp, how='outer', rsuffix='_1')\n"", ""     close= close.drop_duplicates(cols='Datetime', take_last=True)\n"", '\n', "" close= close[close['Datetime'].isin(datefilter)]   #Only date in the range\n"", "" close= close.sort('Datetime')\n"", "" close= close.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", '\n', ' close= np.array(close.values[1:,1:]).astype(np.float)  #Only Price\n', ' close= close[(np.abs(sci.stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close, vv\n', ""'''\n"", '\n', '\n', '\n', ""'''\n"", 'class Quote(object):\n', ""  DATE_FMT = '%Y-%m-%d';  TIME_FMT = '%H:%M:%S'\n"", '\n', '  def __init__(self):\n', ""    self.symbol = '';    self.exchn=''\n"", '    self.date,self.time,self.open_,self.high,self.low,self.close,self.volume = ([] for _ in range(7))\n', '\n', '  def append(self,dt,open_,high,low,close,volume):\n', '    self.date.append(dt.date())\n', '    self.time.append(dt.time())\n', '    self.open_.append(float(open_))\n', '    self.high.append(float(high))\n', '    self.low.append(float(low))\n', '    self.close.append(float(close))\n', '    self.volume.append(int(volume))\n', '\n', '  def to_csv(self):\n', '    return \'\'.join([""{0},{1},{2},{3:.2f},{4:.2f},{5:.2f},{6:.2f},{7}\\n"".format(self.symbol,\n', ""              self.date[bar].strftime('%Y-%m-%d'),self.time[bar].strftime('%H:%M:%S'),\n"", '              self.open_[bar],self.high[bar],self.low[bar],self.close[bar],self.volume[bar])\n', '              for bar in xrange(len(self.close))])\n', '\n', '  def write_csv(self,filename):\n', ""    with open(filename,'w') as f:\n"", '      txt= self.to_csv()\n', '      f.write(txt)\n', '\n', '  def read_csv(self,filename):\n', ""    self.symbol = ''\n"", '    self.date,self.time,self.open_,self.high,self.low,self.close,self.volume = ([] for _ in range(7))\n', ""    for line in open(filename,'r'):\n"", ""      symbol,ds,ts,open_,high,low,close,volume = line.rstrip().split(',')\n"", '      self.symbol = symbol\n', ""      dt = datetime.datetime.strptime(ds+' '+ts,self.DATE_FMT+' '+self.TIME_FMT)\n"", '      self.append(dt,open_,high,low,close,volume)\n', '    return True\n', '\n', '  def __repr__(self):\n', '    return self.to_csv()\n', ""'''\n"", '\n', ' \n', '\n', ""'''\n"", ""def imp_googleIntradayQuoteSave2(name1, date1, inter, tframe, dircsv='', dbname=''):\n"", ' q = imp_googleIntradayQuote(name1, inter, tframe)  #interval, timeframe\n', "" name1= name1.replace(':','_')\n"", '\n', "" if dircsv != '' :\n"", ""    file1= dircsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(inter) +'_'+ str(tframe) +'.csv'\n"", '    q.to_csv(file1)\n', '\n', "" if dbname != '' :\n"", '    import sqlalchemy as sql\n', '    dbcon= sql.create_engine(dbname)\n', '    if inter== 300 : # 5mins\n', ""      q.to_sql('q5min', dbcon)\n"", '\n', '\n', ""def imp_googleQuoteList_save(symbols, date1, date2, inter=23400, tframe=2000, dircsv='', intraday1=True) :\n"", ' if intraday1:\n', '  for symbol in symbols:\n', '     try :\n', '       imp_googleIntradayQuoteSave(symbol, date1, inter, tframe, dircsv)\n', ""     except: print('Error:'+symbol)\n"", '\n', ' else :\n', '  for symbol in symbols:\n', '     try :\n', '       imp_googleQuoteSave(symbol, date1, date2, dircsv)\n', '     except: print(symbol)\n', ""'''\n"", '\n']","['np.shape', 'getret_fromquotes', 'len', 'getlogret_fromquotes', 'np.log', 'getprice_fromret']",6
utilmy/zarchive/portfolio_withdate.py:index,index,class,103,300,221,3009,10.03,1,3,[],[],[],1835,[],[],0
utilmy/zarchive/portfolio_withdate.py:folioCalc,folioCalc,class,125,509,332,4736,9.3,2,17,[],[],[],2134,[],[],0
utilmy/zarchive/portfolio_withdate.py:folioRiskIndicator,folioRiskIndicator,class,89,306,215,3180,10.39,1,9,[],[],[],2309,[],[],0
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF,folioOptimizationF,class,280,1291,671,10994,8.52,6,39,[],[],[],2487,[],[],0
utilmy/zarchive/portfolio_withdate.py:searchSimilarity,searchSimilarity,class,173,497,328,4550,9.15,8,17,[],[],[],3934,[],[],0
utilmy/zarchive/portfolio_withdate.py:index:__init__,index:__init__,method,11,12,12,116,9.67,0,0,"['self', 'id1', 'sym', 'ww', 'tstart']","[None, None, None, None, None]","[None, None, None, None, None]",1836,[],['date_generatedatetime'],1
utilmy/zarchive/portfolio_withdate.py:index:close,index:close,method,0,1,1,4,4.0,0,0,['self'],[None],[None],1841,[],[],0
utilmy/zarchive/portfolio_withdate.py:index:updatehisto,index:updatehisto,method,2,12,11,62,5.17,0,0,['self'],[None],[None],1844,[],['print'],1
utilmy/zarchive/portfolio_withdate.py:index:help,index:help,method,2,12,11,62,5.17,0,0,['self'],[None],[None],1848,[],['print'],1
utilmy/zarchive/portfolio_withdate.py:index:__init__,index:__init__,method,11,12,12,116,9.67,0,0,"['self', 'id1', 'sym', 'ww', 'tstart']","[None, None, None, None, None]","[None, None, None, None, None]",1869,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]",['date_generatedatetime'],1
utilmy/zarchive/portfolio_withdate.py:index:calc_baskettable_pct,index:calc_baskettable_pct,method,37,95,77,782,8.23,1,2,"['self', 'type1', 'showdetail']","[None, None, None]","[None, '""table""', '0']",1913,"[""      '''   Calc Basket Values from Input data '''\n""]","['np.zeros', 'np.sum', 'self._udpate_wwindpct', 'range', 'np.mod', 'self._wwpct_rebal']",6
utilmy/zarchive/portfolio_withdate.py:index:_wwpct_rebal,index:_wwpct_rebal,method,3,4,3,41,10.25,0,0,"['self', 'wwpct_actual', 't', 'trebal']","[None, None, None, None]","[None, None, None, None]",1940,[],[],0
utilmy/zarchive/portfolio_withdate.py:index:_udpate_wwindpct,index:_udpate_wwindpct,method,11,25,17,224,8.96,0,0,"['self', 't', 'bskt', 'hedgecost', 'wwpct_actual', 'wwpct_th']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",1950,[],[],0
utilmy/zarchive/portfolio_withdate.py:index:calc_baskettable_unit,index:calc_baskettable_unit,method,0,0,0,0,0.0,0,0,[],[],[],1959,"[""    '''   t           --->  t+1                 ---->   t+2\n"", '            wwactual:    B[t]    -->   B[t+1]   using  w[t+1], B[t] /   S[t]\n', '            wwnew   :    B[t+1]  -->   B[t+2]   using  w[t+2], B[t+1] /   S[t+1]\n', ""    '''\n""]",[],0
utilmy/zarchive/portfolio_withdate.py:folioCalc:__init__,folioCalc:__init__,method,21,39,37,356,9.13,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2135,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio_withdate.py:folioCalc:set_symclose,folioCalc:set_symclose,method,17,34,33,312,9.18,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2148,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio_withdate.py:folioCalc:setcriteria,folioCalc:setcriteria,method,41,76,71,1046,13.76,0,0,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2157,"[""        ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '            wwtype:     constant / regime /regimExtra\n', '            initperiod: NbOfDays_initial_Risk_Monitoring\n', '            riskid:     spprice/spperf/multi\n', ""        '''\n""]","['np.array', 'len']",2
utilmy/zarchive/portfolio_withdate.py:folioCalc:_weightcalc_generic,folioCalc:_weightcalc_generic,method,5,35,23,396,11.31,0,4,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2207,[],"['self._weightcalc_constant', 'self._weightcalc_regime', 'print']",3
utilmy/zarchive/portfolio_withdate.py:folioCalc:_weightcalc_regime,folioCalc:_weightcalc_regime,method,10,37,32,272,7.35,0,1,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2218,[],"['self._regimecalc', 'np.reshape', 'self._mapping_risk_ww']",3
utilmy/zarchive/portfolio_withdate.py:folioCalc:_regimecalc,folioCalc:_regimecalc,method,3,7,6,83,11.86,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2228,[],['self._mapping_calc_risk'],1
utilmy/zarchive/portfolio_withdate.py:folioCalc:_weightcalc_constant,folioCalc:_weightcalc_constant,method,3,4,4,28,7.0,0,0,"['self', 'ww2', 't']","[None, None, None]","[None, None, None]",2237,[],['np.sum'],1
utilmy/zarchive/portfolio_withdate.py:folioCalc:getweight,folioCalc:getweight,method,5,12,9,126,10.5,0,0,['self'],[None],[None],2241,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/portfolio_withdate.py:folioCalc:calc_baskettable,folioCalc:calc_baskettable,method,33,111,83,811,7.31,1,4,"['self', 'wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps', 'showdetail']","[None, None, None, None, None, None, None, None]","[None, None, None, '""table""', '""constant""', '1', '0.000', '0']",2248,[],"['np.shape', 'np.zeros', 'range', 'np.mod', 'self._weightcalc_generic', 'np.sum', 'print']",7
utilmy/zarchive/portfolio_withdate.py:folioCalc:plot,folioCalc:plot,method,10,29,26,295,10.17,0,2,"['self', 'wwvec', 'show1', 'tickperday']","[None, None, None, None]","[None, 'None', '1', '60']",2276,[],"['self.calc_baskettable', 'plot_price', 'print', 'np.array']",4
utilmy/zarchive/portfolio_withdate.py:folioCalc:multiperiod_ww,folioCalc:multiperiod_ww,method,15,36,29,240,6.67,1,1,"['self', 't']","[None, None]","[None, None]",2287,[],['xrange'],1
utilmy/zarchive/portfolio_withdate.py:folioCalc:help,folioCalc:help,method,1,2,2,13,6.5,0,0,['self'],[None],[None],2300,[],['print'],1
utilmy/zarchive/portfolio_withdate.py:folioRiskIndicator:__init__,folioRiskIndicator:__init__,method,23,40,38,356,8.9,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1869,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]","['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio_withdate.py:folioRiskIndicator:set_symclose,folioRiskIndicator:set_symclose,method,18,32,32,295,9.22,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2324,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio_withdate.py:folioRiskIndicator:setcriteria,folioRiskIndicator:setcriteria,method,34,81,70,1006,12.42,0,0,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2334,"[""    ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '        wwtype:     constant / regime /regimExtra\n', '        initperiod: NbOfDays_initial_Risk_Monitoring\n', '        riskid:     spprice/spperf/multi\n', ""    '''    \n""]","['np.array', 'len', 'np.zeros']",3
utilmy/zarchive/portfolio_withdate.py:folioRiskIndicator:calcrisk,folioRiskIndicator:calcrisk,method,13,18,17,220,12.22,1,0,"['self', 'wwvec', 'initval']","[None, None, None]","[None, '[]', '1']",2382,[],"['xrange', 'self._weightcalc_generic', 'np.column_stack']",3
utilmy/zarchive/portfolio_withdate.py:folioRiskIndicator:_weightcalc_generic,folioRiskIndicator:_weightcalc_generic,method,4,34,24,327,9.62,0,3,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2394,[],"['self._weightcalc_regime', 'print']",2
utilmy/zarchive/portfolio_withdate.py:folioRiskIndicator:_weightcalc_regime,folioRiskIndicator:_weightcalc_regime,method,3,11,11,68,6.18,0,1,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2403,[],['self._regimecalc'],1
utilmy/zarchive/portfolio_withdate.py:folioRiskIndicator:_regimecalc,folioRiskIndicator:_regimecalc,method,7,18,17,216,12.0,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2408,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/portfolio_withdate.py:folioRiskIndicator:calc_optimal_weight,folioRiskIndicator:calc_optimal_weight,method,7,18,17,216,12.0,0,0,"['self', 'maxiter', 'name1', 'isreset', 'popsize']","[None, None, None, None, None]","[None, '1', ""''"", '1', '15']",2414,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:__init__,folioOptimizationF:__init__,method,23,39,37,340,8.72,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1869,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]","['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:set_symclose,folioOptimizationF:set_symclose,method,18,32,32,295,9.22,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2324,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:setcriteria,folioOptimizationF:setcriteria,method,44,128,103,1669,13.04,0,2,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2334,"[""    ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '        wwtype:     constant / regime /regimExtra\n', '        initperiod: NbOfDays_initial_Risk_Monitoring\n', '        riskid:     spprice/spperf/multi\n', ""    '''    \n""]","['np.array', 'len', 'print', 'list']",4
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:calcbasket_obj2,folioOptimizationF:calcbasket_obj2,method,23,76,63,756,9.95,0,1,"['self', 'wwvec']","[None, None]","[None, None]",2572,[],"['np.reshape', 'np.array', 'np.sum', 'round', 'calcbasket_objext', 'float']",6
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:calcbasket_obj,folioOptimizationF:calcbasket_obj,method,55,157,118,1494,9.52,1,4,"['self', 'wwvec']","[None, None]","[None, None]",2598,[],"['np.reshape', 'np.array', 'np.sum', 'round', 'calcbasket_objext', 'float', 'calcbasket_obj', 'concenfactor=folio_concenfactor2', 'np.zeros', 'xrange', 'np.mod', 'self._weightcalc_generic', 'self._loss_obj', 'self._objective_criteria']",14
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:_loss_obj,folioOptimizationF:_loss_obj,method,4,6,6,68,11.33,0,0,"['self', 'ww2', 'wwpenalty']","[None, None, None]","[None, None, None]",2623,[],['np.sum'],1
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:_weightcalc_generic,folioOptimizationF:_weightcalc_generic,method,5,40,26,394,9.85,0,4,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2394,[],"['self._weightcalc_constant', 'self._weightcalc_regime', 'print']",3
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:_weightcalc_regime,folioOptimizationF:_weightcalc_regime,method,8,32,28,239,7.47,0,0,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2403,"['""""""\n', 'if bsk > maxprice :       maxprice= bsk\n', 'if maxprice-bsk > maxdd : maxdd=    maxprice-bsk\n', '\n', '""""""\n']","['self._regimecalc', 'np.reshape', 'self._mapping_risk_ww']",3
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:_regimecalc,folioOptimizationF:_regimecalc,method,4,7,7,83,11.86,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2408,"['""""""\n', 'if bsk > maxprice :       maxprice= bsk\n', 'if maxprice-bsk > maxdd : maxdd=    maxprice-bsk\n', '\n', '""""""\n']",['self._mapping_calc_risk'],1
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:_weightcalc_constant,folioOptimizationF:_weightcalc_constant,method,3,4,4,28,7.0,0,0,"['self', 'ww2', 't']","[None, None, None]","[None, None, None]",2662,[],['np.sum'],1
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:calc_optimal_weight,folioOptimizationF:calc_optimal_weight,method,7,18,17,216,12.0,0,0,"['self', 'maxiter', 'name1', 'isreset', 'popsize']","[None, None, None, None, None]","[None, '1', ""''"", '1', '15']",2414,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:getweight,folioOptimizationF:getweight,method,6,11,9,126,11.45,0,0,['self'],[None],[None],2671,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:calc_baskettable,folioOptimizationF:calc_baskettable,method,36,86,72,827,9.62,1,4,"['self', 'wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps', 'showdetail']","[None, None, None, None, None, None, None, None]","[None, None, None, '""table""', '""constant""', '1', ' 0.000', '0']",2679,[],"['np.shape', 'np.zeros', 'range', 'np.mod', 'self._weightcalc_generic', 'np.sum', 'print']",7
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:plot,folioOptimizationF:plot,method,11,31,30,294,9.48,0,2,"['self', 'wwvec', 'show1', 'tickperday']","[None, None, None, None]","[None, 'None', '1', '60']",2708,[],"['self.calc_baskettable', 'plot_price', 'print', 'np.array']",4
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:_objective_criteria,folioOptimizationF:_objective_criteria,method,31,147,78,1105,7.52,2,7,"['self', 'bsk']","[None, None]","[None, None]",2720,[],"['np.std', 'np.sum', 'range', 'np.arange']",4
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:multiperiod_ww,folioOptimizationF:multiperiod_ww,method,18,29,27,238,8.21,1,1,"['self', 't']","[None, None]","[None, None]",2767,[],['xrange'],1
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:help,folioOptimizationF:help,method,50,152,103,1472,9.68,1,6,['self'],[None],[None],2779,[],"['print', 'util.load_obj', 'int', 'copy.deepcopy', 'xrange', 'next', 'util.save_obj', 'np.mod', 'np.abs', 'np.reshape', 'np.sum']",11
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:mapping_risk_ww,folioOptimizationF:mapping_risk_ww,method,12,56,29,245,4.38,0,1,"['self', 'risk', 'wwmat', 'ww2']","[None, None, None, None]","[None, None, None, 'self.wwasset0']",2838,[],"['np.sum', 'np.array']",2
utilmy/zarchive/portfolio_withdate.py:folioOptimizationF:_mapping_calc_risk,folioOptimizationF:_mapping_calc_risk,method,27,201,76,831,4.13,0,3,"['self', 'ss', 'tr', 't', 'risk0']","[None, None, None, None, None]","[None, None, None, None, None]",2850,[],[],0
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:__init__,searchSimilarity:__init__,method,15,18,17,193,10.72,0,0,"['self', ""filejpstock=r'E"", 'sym01', 'symname', 'startdate', 'enddate', 'pricetype']","[None, '', None, None, None, None, None]","[None, ""r'E:/_data/stock/daily/20160616/jp'"", ""['7203']"", ""['Toyota']"", ' 20150101', '20160601', '""close""']",3935,[],[],0
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:load_quotes_fromdb,searchSimilarity:load_quotes_fromdb,method,15,35,33,355,10.14,0,1,"['self', 'picklefile']","[None, None]","[None, ""''""]",3940,[],"['util.load_obj', 'imp_txt_getquotes', 'date_align', 'util.a_cleanmemory']",4
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:__generate_return__,searchSimilarity:__generate_return__,method,4,4,4,64,16.0,0,0,"['self', 'nlag']","[None, None]","[None, None]",3951,[],['getret_fromquotes'],1
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:__overweight__,searchSimilarity:__overweight__,method,17,62,41,412,6.65,2,4,"['self', 'px']","[None, None]","[None, None]",3955,[],"['len', 'np.ones', 'util.np_findlocalmax', 'type', 'np.abs', 'wwerr[max', 'util.np_findlocalmin']",7
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:set_searchcriteria,searchSimilarity:set_searchcriteria,method,34,79,63,793,10.04,0,5,"['self', 'name1', 'date1', 'date2', 'nlag', 'searchperiodstart', 'typesearch', '']","[None, None, None, None, None, None, None, None]","[None, ""'7203'"", '20160301', '20160601', '1', '20120101', '""pattern2""', None]",3975,[],"['getret_fromquotes', 'date_finddateid', 'max', 'util.np_findfirst', 'print', 'price_normalize_1d', 'len', 'self.__overweight__']",8
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:launch_search,searchSimilarity:launch_search,method,53,107,81,908,8.49,3,3,['self'],[None],[None],4004,[],"['len', 'np.zeros', 'range', 'price_normalize_1d', 'np_similarity', 'util.np_sortbycolumn', 'enumerate']",7
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:show_comparison_graph,searchSimilarity:show_comparison_graph,method,33,72,63,682,9.47,1,3,"['self', 'maxresult', 'show_only_different_time', 'fromid', 'fromend', 'filenameout']","[None, None, None, None, None, None]","[None, '20', 'True', '0', ' 0', ""''""]",4038,[],"['np.shape', 'range', 'int', 'print', 'price_normalize_1d', 'str', 'plot_price']",7
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:staticmethod,searchSimilarity:staticmethod,method,0,1,1,4,4.0,0,0,"['self', 'x']","[None, None]","[None, None]",4067,[],[],0
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:get_rankresult,searchSimilarity:get_rankresult,method,24,63,53,496,7.87,2,1,"['self', 'filetosave']","[None, None]","[None, ""''""]",4071,[],"['len', 'np.savetxt', 'np.shape', 'np.array', 'np.empty', 'range', 'int', 'str']",8
utilmy/zarchive/portfolio_withdate.py:searchSimilarity:export_results,searchSimilarity:export_results,method,0,0,0,0,0.0,0,0,[],[],[],4093,"[""'''\n"", '#####################################################################################\n', '#---------Run over all the indexes, with time lag -----------------------------------\n', 'def np_similarity(x,y, wwerr=[], type1=0):\n', '  if type1==2 : return np_distance_l1(x,y, wwerr)\n', '  if type1==1: return sci.spatial.distance.correlation(x, y)  \n', '  if type1==0 : return 1-np.corrcoef(x,y)[0,1]\n', '\n', 'def np_distance_l1(x,y, wwerr) :\n', ' return np.sum(wwerr * np.abs(x-y))  \n', '  \n', '#---------------------  Universe Load         ---------------------------------------\n', ""filejpstock= r'E:/_data/stock/daily/20160616/jp'\n"", 'sym01= nk400list\n', '\n', 'quotes= imp_txt_getquotes(sym01, filejpstock, startdate=20090101, endate=20160616)\n', 'open1, dateref= date_align(quotes, type1=""close"")    #Get the data and align dates\n', 'del quotes; util.a_cleanmemory()\n', '\n', ""#util.save_obj(open1, 'close_nk400_2009_2016')\n"", ""#util.save_obj(dateref, 'dateref_nk400_2009_2016')\n"", '\n', ""open1= util.load_obj('close_nk400_2009_2016')\n"", ""dateref= util.load_obj( 'dateref_nk400_2009_2016')\n"", '\n', 'nlag=1\n', 'ret_open1 =  getdailyret_fromquotes(open1,nlag)\n', 'del open1\n', '\n', '#--------------------  Search Asset Input     ---------------------------------------\n', ""name1= '7203' \n"", 'date1= 20160328\n', 'date2= 20160531\n', 'twindow= util.np_findfirst(date2, dateref) - util.np_findfirst(date1, dateref) \n', '\n', '\n', 'tstartx= util.np_findfirst(date1, dateref)          # Date Index\n', 'stockx= util.np_findfirst(name1, sym01)\n', 'retx= ret_open1[stockx, tstartx:(tstartx+twindow-1)]    # Recent Period\n', 'px= price_normalize_1d(retx)\n', 'nx= len(px)\n', '\n', '\n', '#----OverWeight for Local Min, Local Max         -----------------------------------\n', 'wwerr= np.ones(nx, dtype=np.float16)\n', 'vvmax= util.np_findlocalmax(px); xprev=0\n', 'for x in vvmax:\n', '  if x[0] !=0 and np.abs(xprev-x[0]) > 6: \n', '    wwerr[max(0,x[0]-1):min(nx,(x[0]+1))]= 1.5;    xprev= x[0] \n', '\n', 'vvmin= util.np_findlocalmin(px); xprev=0\n', 'for x in vvmin:\n', '  if x[0] !=0 and np.abs(xprev-x[0]) > 6: \n', '    wwerr[max(0,x[0]-1):min(nx,(x[0]+1))]= 1.5;    xprev= x[0] \n', '\n', '\n', '#--------------------  Search the Pattern       -------------------------------------\n', 'tmaxx= len(dateref)-1\n', 'similar_list= np.zeros((nstock*(tmaxx-twindow)+1, 3), np.float16)\n', 'i=0\n', 'for iy in range(0, nstock) :\n', ' for t in range(0, tmaxx-twindow):\n', '   if iy == stockx and t > tstartx : \n', '     pass\n', '   else :\n', '    rety= ret_open1[iy, t:(t+twindow-1)]\n', '    py=   price_normalize_1d(rety)\n', '    ss= np_similarity(px, py, wwerr=wwerr, type1=2)\n', '    i+=1\n', '    similar_list[i,0]= iy\n', '    similar_list[i,1]= t\n', '    similar_list[i,2]= ss\n', '\n', '\n', ' #-------------------Clean the table -------------------------------------------------\n', 'rank= util.np_sortbycolumn(similar_list,2, asc=True)\n', 'for i,x in enumerate(rank) :\n', '  if rank[i,2]==0.0 : rank[i,2]= 1000         # Empty cell\n', '  if rank[i,1]> tstartx-3 : rank[i,2]= 100    # Start Element\n', 'rank= util.np_sortbycolumn(rank,2, asc=True)  \n', '\n', '\n', ' #----- Show Similar Stocks ---------------------------------------------------------\n', 'show_only_different_time= True\n', '\n', 'for i in range(0,10) :\n', ' tstarti= rank[i,1]\n', ' stocki= int(rank[i,0])\n', ' \n', ' if show_only_different_time and tstarti < tstartx-1  :\n', '   py= price_normalize100(ret_open1[stocki, tstarti:(tstarti+twindow-1)]) \n', '\n', '   namei= sym01[stocki]\n', '   namefulli= nk400name[util.np_findfirst(namei, nk400list)]\n', '\n', '   tit= namei+ "" "" +namefulli+ "" "" + str(dateref[tstarti])          \n', ""   plot_price(px-100, py-100, label=('Similar '+tit, 'Time','% Var') )\n"", '\n', '\n', 'kix= util.np_findfirst(name1, statsjp[:,0, ilag])\n', 'stocklist01[390]\n', '\n', '\n', 'stocklist01[157] , nk400name[157] ,  dateref1adj[86]     \n', 'plot_price(price_normalize100(ret_open1[157, 86:86+twindow-1])-100,px-100   )\n', ""'''    \n""]",[],0
utilmy/zarchive/util_sql.py:sql_create_dbengine,sql_create_dbengine,function,4,45,26,358,7.96,0,1,"['type1', 'dbname', 'login', 'password', 'url', 'port']","[None, None, None, None, None, None]","[""''"", ""''"", ""''"", ""''"", ""'localhost'"", '5432']",43,"[""   ''' Return SQL Alchemy Connector\n"", '\n', '# psycopg2\n', ""engine = create_engine('postgresql+psycopg2://scott:tiger@localhost/mydatabase')\n"", '\n', '# MySQL-connector-python  Official one\n', ""engine = create_engine('mysql+mysqlconnector://scott:tiger@localhost/foo')\n"", 'conda install -c anaconda mysql-connector-python=2.0.4\n', ""engine = create_engine('postgresql://%s:%s@localhost:5432/%s' %(myusername, mypassword, mydatabase))\n"", '\n', ""engine = create_engine('sqlite:///  folder/foo.db')\n"", '\n', ""   '''\n""]","['sql.create_engine', 'str']",2
utilmy/zarchive/util_sql.py:sql_query,sql_query,function,6,22,18,155,7.05,0,2,"['sqlr', 'dbengine', 'output', ""dburl='sqlite""]","[None, None, None, '']","[""'SELECT ticker,shortratio,sector1_id, FROM stockfundamental'"", 'None', ""'df'"", ""'sqlite:///aaserialize/store/finviz.db'""]",69,"["" '''\n"", "" :param sqlr:       'SELECT ticker,shortratio,sector1_id, FROM stockfundamental'\n"", ' :param output:     df   /   file1.csv\n', "" :param dburl:      'sqlite:///aaserialize/store/finviz.db'\n"", "" :param dbengine:   dbengine = sql.create_engine('postgresql+psycopg2://postgres:postgres@localhost/coke')\n"", ' :return:\n', "" '''\n""]","['sql.create_engine', 'pd.read_sql_query', 'output.find', 'df.to_csv']",4
utilmy/zarchive/util_sql.py:sql_get_dbschema,sql_get_dbschema,function,16,37,32,318,8.59,2,2,"[""dburl='sqlite"", 'dbengine', 'isprint']","['', None, None]","[""'sqlite:///aapackage/store/yahoo.db'"", 'None', '0']",83,[],"['sql.create_engine', 'sql.inspect', 'inspector.get_table_names', 'inspector.get_columns', 'l1.append', 'print', 'np.array']",7
utilmy/zarchive/util_sql.py:sql_delete_table,sql_delete_table,function,1,8,8,89,11.12,0,0,"['name', 'dbengine']","[None, None]","[None, None]",96,[],[],0
utilmy/zarchive/util_sql.py:sql_insert_excel,sql_insert_excel,function,50,131,97,992,7.57,5,2,"['file1', 'dbengine', 'dbtype']","[None, None, None]","[""'.xls'"", 'None', ""''""]",103,"["" ''' http://flask-excel.readthedocs.io/en/latest/\n"", ' https://pythonhosted.org/pyexcel/tutorial_data_conversion.html#import-excel-sheet-into-a-database-table\n', ' from sqlalchemy import create_engine\n', 'from sqlalchemy.ext.declarative import declarative_base\n', 'from sqlalchemy import Column , Integer, String, Float, Date\n', 'from sqlalchemy.orm import sessionmaker\n', 'engine = create_engine(""sqlite:///birth.db"")\n', 'Base = declarative_base()\n', 'Session = sessionmaker(bind=engine)\n', '\n', 'class BirthRegister(Base):\n', ""...     __tablename__='birth'\n"", '...     id=Column(Integer, primary_key=True)\n', '...     name=Column(String)\n', '...     weight=Column(Float)\n', '...     birth=Column(Date)\n', '\n', 'Base.metadata.create_all(engine)\n', '\n', 'https://www.digitalocean.com/community/tutorials/how-to-use-celery-with-rabbitmq-to-queue-tasks-on-an-ubuntu-vps\n', 'import os\n', 'import pyexcel\n', 'import datetime\n', '\n', 'from sqlalchemy import create_engine\n', 'from sqlalchemy.ext.declarative import declarative_base\n', 'from sqlalchemy import Column, Integer, String, Float, Date\n', 'from sqlalchemy.orm import sessionmaker\n', '\n', '\n', 'engine = create_engine(""sqlite:///birth.db"")\n', 'Base = declarative_base()\n', 'Session = sessionmaker(bind=engine)\n', '\n', '\n', '# here is the destination table\n', 'class BirthRegister(Base):\n', ""    __tablename__ = 'birth'\n"", '    id = Column(Integer, primary_key=True)\n', '    name = Column(String)\n', '    weight = Column(Float)\n', '    birth = Column(Date)\n', '\n', '\n', 'Base.metadata.create_all(engine)\n', '\n', '\n', '# create fixture\n', 'data = [\n', '    [""name"", ""weight"", ""birth""],\n', '    [""Adam"", 3.4, datetime.date(2015, 2, 3)],\n', '    [""Smith"", 4.2, datetime.date(2014, 11, 12)]\n', ']\n', 'pyexcel.save_as(array=data,\n', '                dest_file_name=""birth.xls"")\n', '\n', '# import the xls file\n', 'session = Session()  # obtain a sql session\n', 'pyexcel.save_as(file_name=""birth.xls"",\n', '                name_columns_by_row=0,\n', '                dest_session=session,\n', '                dest_table=BirthRegister)\n', '\n', '# verify results\n', 'sheet = pyexcel.get_sheet(session=session, table=BirthRegister)\n', 'print(sheet)\n', '\n', 'session.close()\n', ""os.unlink('birth.db')\n"", 'os.unlink(""birth.xls"")\n', '\n', ' This code uses the openpyxl package for playing around with excel using Python code\n', ' to convert complete excel workbook (all sheets) to an SQLite database\n', ' The code assumes that the first row of every sheet is the column name\n', ' Every sheet is stored in a separate table\n', ' The sheet name is assigned as the table name for every sheet\n', "" '''\n""]","['slugify', 'text.strip', 're.sub', 'load_workbook', 'wb.get_sheet_names', 'str', 'columns.append', 'dbengine.execute', 'enumerate', 'tuprow.append', 'tup.append', 'VALUES', 'dbengine.executemany', 'dbengine.commit', 'dbengine.close']",15
utilmy/zarchive/util_sql.py:sql_insert_df,sql_insert_df,function,22,59,51,481,8.15,1,2,"['df', 'dbtable', 'dbengine', 'col_drop', 'verbose']","[None, None, None, None, None]","[None, None, None, ""['id']"", '1']",226,[],"['df.drop', 'df.to_dict', 'print', 'sql.Table', 'Session', 'dbengine.execute', 'session.commit', 'session.close']",8
utilmy/zarchive/util_sql.py:sql_insert_csv,sql_insert_csv,function,35,164,130,1385,8.45,3,3,"['csvfile', 'dbtable', 'dbengine', 'col_drop']","[None, None, None, None]","[None, None, None, '[]']",256,[],"['datetime.now', 'pd.read_csv', 'df.rename', 'c.replace', 'df.drop', 'df.to_dict', 'sql.Table', 'Session', 'dbengine.execute', 'session.commit', 'session.close', 'print', 'sql_insert_csv2', 'os.listdir', 'i.endswith', 'i.startswith', 'df.to_sql']",17
utilmy/zarchive/util_sql.py:sql_insert_csv2,sql_insert_csv2,function,11,63,57,490,7.78,1,2,"['csvfile', 'dbtable', 'columns', 'dbengine', 'nrows']","[None, None, None, None, None]","[""''"", ""''"", '[]', 'None', ' 10000']",328,"['    """"""\n', '    Upload data to a temporary table first using PANDAs to identify optimal data-types for columns\n', '    PANDAS is not speed-efficient as it uses INSERT commands rather than COPY e.g. it took COPY 16mins average\n', '    to get a 15GB CSV into the database (door-to-door) whereas pandas.to_sql took 50mins\n', '    """"""\n']","['os.listdir', 'i.endswith', 'i.startswith', 'print', 'pd.read_csv', 'df.to_sql']",6
utilmy/zarchive/util_sql.py:sql_postgres_create_table,sql_postgres_create_table,function,27,124,96,1032,8.32,1,3,"['mytable', 'database', 'username', 'password']","[None, None, None, None]","[""''"", ""''"", ""''"", ""''""]",361,"['    """""" Create table copying the structure of the temp table created using pandas  Timer to benchmark """"""\n']","['psycopg2.connect', 'con.cursor', 'print', 'sys.exit', 'cur.execute', 'time.time', 'os.listdir', 'i.endswith', 'i.startswith', 'open', 'cur.copy_expert', 'con.commit', 'con.close']",13
utilmy/zarchive/util_sql.py:sql_postgres_query_to_csv,sql_postgres_query_to_csv,function,12,42,41,364,8.67,0,1,"['sqlr', 'csv_out']","[None, None]","[""'SELECT ticker,shortratio,sector1_id, FROM stockfundamental'"", ""''""]",432,"['    """""" Submit query to created PostgreSQL database and output results to a CSV  """"""\n']","['psycopg2.connect', 'con.cursor', 'print', 'open', 'cur.copy_expert', 'con.close']",6
utilmy/zarchive/util_sql.py:sql_postgres_pivot,sql_postgres_pivot,function,0,1,1,4,4.0,0,0,[],[],[],447,"[""   '''\n"", 'Enabling the Crosstab Function\n', 'As we previously mentioned, the crosstab function is part of a PostgreSQL extension called tablefunc. To call the crosstab function,\n', 'you must first enable the tablefunc extension by executing the following SQL command:\n', 'CREATE extension tablefunc;\n', '\n', 'SELECT *\n', ""FROM crosstab( 'select student, subject, evaluation_result from evaluations order by 1,2')\n"", 'AS final_result(Student TEXT, Geography NUMERIC,History NUMERIC,Language NUMERIC,Maths NUMERIC,Music NUMERIC);\n', '\n', '\n', '##### Correct Even iF there are missing values :\n', 'http://www.vertabelo.com/blog/technical-articles/creating-pivot-tables-in-postgresql-using-the-crosstab-function\n', '\n', 'SELECT *\n', ""FROM crosstab( 'select student, subject, evaluation_result from evaluations\n"", ""                where extract (month from evaluation_day) = 7 order by 1,2',\n"", ""                'select name from subject order by 1')\n"", '     AS final_result(Student TEXT, Geography NUMERIC,History NUMERIC,Language NUMERIC,Maths NUMERIC,Music NUMERIC);\n', '\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/util_sql.py:sql_mysql_insert_excel,sql_mysql_insert_excel,function,30,73,69,712,9.75,1,0,[],[],[],473,[],"['xlrd.open_workbook', 'list.sheet_by_index', 'database.cursor', 'range', 'sheet.cell', 'cursor.execute', 'cursor.close', 'database.commit', 'database.close', 'print', 'str', 'rows=str']",12
utilmy/zarchive/util_sql.py:sql_pivotable,sql_pivotable,function,1,3,3,10,3.33,0,0,"['dbcon', 'ss']","[None, None]","[None, ""'select  '""]",506,"[""  '''\n"", '\n', ' 1) get the category\n', '\n', ' 2) Build the Pivot From category\n', '  SELECT *\n', ""  FROM crosstab( 'select student, subject, evaluation_result from evaluations\n"", ""                where extract (month from evaluation_day) = 7 order by 1,2',\n"", ""                'select name from subject order by 1')\n"", '   AS final_result(Student TEXT, Geography NUMERIC,History NUMERIC,Language NUMERIC,Maths NUMERIC,Music NUMERIC);\n', '\n', '  https://www.amazon.com/PostgreSQL-High-Performance-Gregory-Smith/dp/184951030X/ref=as_li_ss_tl?s=books&ie=UTF8&qid=1458352081&sr=1-6&keywords=postgres&linkCode=sl1&tag=postgres-bottom-20&linkId=c981783121cbd5542dc2b44a2297df57\n', '\n', '\n', 'http://blog.brakmic.com/data-science-for-losers-part-2/\n', '\n', 'Here we instruct Pandas to merge two tables by using certain primary keys from both when combining their rows into a new table. The parameter how instructs Pandas to use the inner-join which means it will only combine such rows which belong to both of the tables. Therefore well not receive any NaN-rows. But in some cases this could be desirable. Then use the alternative options like left, right or outer.\n', '\n', 'Pivots with Tables from SQLAlchemy\n', '\n', 'And of course its possible to generate the same pivot tables with data that came from SQLAlchemy.\n', 'Theyre nothing else but DataFrames all the way down. OK, not absolutely all the way down,\n', 'because there are also Series and NumPy arrays etc.,\n', 'but this is a little bit too much of knowledge for Losers like us. Maybe in some later articles.\n', '\n', '\n', ' :return:\n', ""  '''\n""]",[],0
utilmy/zarchive/utilgeo.py:df_to_geojson,df_to_geojson,function,10,24,21,329,13.71,2,0,"['df', 'col_properties', 'lat', 'lon']","[None, None, None, None]","[None, None, ""'latitude'"", ""'longitude'""]",24,[],['df.iterrows'],1
utilmy/zarchive/_HELP.py:os_compileVSsolution,os_compileVSsolution,function,6,32,21,239,7.47,0,4,"['dir1', 'flags1', 'type1', 'compilerdir']","[None, None, None, None]","[None, '""""', '""devenv""', '""""']",744,[],['os.system'],1
utilmy/zarchive/_HELP.py:os_VS_build,os_VS_build,function,16,62,54,614,9.9,1,2,"['self', 'lib_to_build']","[None, None]","[None, None]",773,[],"['print', 'subprocess.Popen', 'process.poll', 'process.communicate']",4
utilmy/zarchive/_HELP.py:set_rc_version,set_rc_version,function,12,37,34,502,13.57,0,0,"['rcfile', 'target_version']","[None, None]","[None, None]",810,[],"['open', 'f.seek', 'f.write', 'f.truncate']",4
utilmy/zarchive/_HELP.py:os_VS_start,os_VS_start,function,18,56,52,881,15.73,0,2,"['self', 'version']","[None, None]","[None, None]",847,[],"['os.getenv', 'Exception', 'default.append', 'libs.append']",4
utilmy/zarchive/_HELP.py:fun_cython,fun_cython,function,5,10,9,34,3.4,1,0,['a'],[None],[None],890,[],['xrange'],1
utilmy/zarchive/_HELP.py:fun_python,fun_python,function,5,10,9,34,3.4,1,0,['a'],[None],[None],897,[],['xrange'],1
utilmy/zarchive/_HELP.py:LookupTable,LookupTable,class,4,7,7,94,13.43,0,0,[],[],[],207,[],[],0
utilmy/zarchive/geospatial.py:coordinates_box,coordinates_box,class,1,2,2,16,8.0,0,0,[],[],[],33,[],[],0
utilmy/zarchive/util_web.py:web_restapi_toresp,web_restapi_toresp,function,12,19,18,168,8.84,0,1,['apiurl1'],[None],[None],59,[],"['requests.get', 'ApiError']",2
utilmy/zarchive/util_web.py:web_getrawhtml,web_getrawhtml,function,13,20,20,176,8.8,0,1,['url1'],[None],[None],66,[],"['requests.get', 'ApiError']",2
utilmy/zarchive/util_web.py:web_importio_todataframe,web_importio_todataframe,function,41,77,59,641,8.32,4,4,"['apiurl1', 'isurl']","[None, None]","[None, '1']",74,[],"['requests.get', 'ApiError', 'resp.json', 'list', 'colname.append', 'str', 'np.array', 'colmax=len', 'np.empty', 'pd_createdf', 'np.arange', 'len']",12
utilmy/zarchive/util_web.py:web_getjson_fromurl,web_getjson_fromurl,function,10,11,10,138,12.55,0,0,['url'],[None],[None],101,[],"['urllib3.connection_from_url', 'http.urlopen', 'print', 'json.loads']",4
utilmy/zarchive/util_web.py:web_gettext_fromurl,web_gettext_fromurl,function,9,18,17,203,11.28,0,0,"['url', 'htmltag']","[None, None]","[None, ""'p'""]",116,[],"['urllib3.connection_from_url', 'http.urlopen', 'BeautifulSoup', 'soup.find_all']",4
utilmy/zarchive/util_web.py:web_gettext_fromhtml,web_gettext_fromhtml,function,8,20,19,176,8.8,0,0,"['file1', 'htmltag']","[None, None]","[None, ""'p'""]",124,[],"['open', 'BeautifulSoup', 'soup.find_all']",3
utilmy/zarchive/util_web.py:web_getlink_fromurl,web_getlink_fromurl,function,14,20,20,248,12.4,1,0,['url'],[None],[None],183,[],"['urllib3.connection_from_url', 'http.urlopen', 'BeautifulSoup', 'soup.prettify', 'soup.findAll', 'links.append', 'set']",7
utilmy/zarchive/util_web.py:web_send_email,web_send_email,function,33,126,77,1266,10.05,0,0,"['FROM', 'recipient', 'subject', 'body', 'login1', 'pss1', 'server1', 'port1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, '""mizenjapan@gmail.com""', '""sophieelise237""', '""smtp.gmail.com""', '465']",195,"['    \'\'\'  # send_email(""Kevin"", ""brookm291@gmail.com"", ""JapaneseText:"" , ""txt"") \'\'\'\n']","['MIMEMultipart', 'msg.set_charset', 'MIMEText', 'msg.attach', 'smtplib.SMTP_SSL', 'server_ssl.ehlo', 'login', 'server_ssl.login', 'server_ssl.sendmail', 'msg.as_string', 'server_ssl.close', 'print', 'web_send_email_tls', 'smtplib.SMTP', 'mailserver.ehlo', 'mailserver.starttls', 'mailserver.login', 'mailserver.sendmail', 'mailserver.quit']",19
utilmy/zarchive/util_web.py:web_send_email_tls,web_send_email_tls,function,25,56,51,568,10.14,0,0,"['FROM', 'recipient', 'subject', 'body', 'login1', 'pss1', 'server1', 'port1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, '""mizenjapan@gmail.com""', '""sophieelise237""', '""smtp.gmail.com""', '465']",220,[],"['MIMEMultipart', 'msg.set_charset', 'MIMEText', 'msg.attach', 'smtplib.SMTP', 'mailserver.ehlo', 'mailserver.starttls', 'mailserver.login', 'mailserver.sendmail', 'msg.as_string', 'mailserver.quit', 'print']",12
utilmy/zarchive/util_web.py:web_sendurl,web_sendurl,function,3,10,10,95,9.5,0,0,['url1'],[None],[None],256,[],"['web_gettext_fromurl', 'send_email']",2
utilmy/zarchive/allmodule.py:pprint,pprint,function,10,32,23,378,11.81,0,1,"['table1', 'tablefmt']","[None, None]","[None, '""simple""']",34,"[' """"""To print nice column headers, supply the second argument (`headers`):\n', '      - `headers` can be an explicit list of column headers\n', '      - if `headers=""firstrow""`, then the first row of data is used\n', '      - if `headers=""keys""`, then dictionary keys or column indices are used\n', '    print(tabulate([[1, 2.34], [-56, ""8.999""], [""2"", ""10001""]]))\n', '  """"""\n']","['isinstance', 'tabulate', 'pprint2', 'pprint.PrettyPrinter', 'pp.pprint']",5
utilmy/zarchive/allmodule.py:pprint2,pprint2,function,5,5,5,59,11.8,0,0,['x'],[None],[None],51,"[""  '''import pprint\n"", ""     stuff = ['spam', 'eggs', 'lumberjack', 'knights', 'ni']\n"", 'stuff.insert(0, stuff[:])\n', 'pp = pprint.PrettyPrinter(indent=4)\n', 'pp.pprint(ALLDB)\n', ""  '''\n""]","['pprint.PrettyPrinter', 'pp.pprint']",2
utilmy/zarchive/allmodule.py:str_convert_beforeprint,str_convert_beforeprint,function,2,2,2,23,11.5,0,0,['x'],[None],[None],64,"[""  ''' #Before writing/output to printer put in utf-8 '''\n""]",['x.encode'],1
utilmy/zarchive/allmodule.py:str_to_utf8,str_to_utf8,function,2,2,2,23,11.5,0,0,['x'],[None],[None],69,"[""  ''' #Before writing/output to printer put in utf-8 '''\n""]",['x.encode'],1
utilmy/zarchive/allmodule.py:str_to_unicode,str_to_unicode,function,4,16,12,98,6.12,0,2,"['x', 'encoding']","[None, None]","[None, ""'utf-8'""]",74,"[""  ''' #After Loading Text from CSV ---> Immediately in unicode '''\n""]","['isinstance', 'unicode']",2
utilmy/zarchive/multithread.py:multithread_run,multithread_run,function,45,139,103,1182,8.5,8,3,"['fun_async', 'input_list', 'n_pool', 'start_delay', 'verbose', '**kw']","[None, 'list', None, None, None, None]","[None, None, '5', '0.1', 'True', None]",8,"['    """"""  input is as list of tuples  [(x1,x2,x3), (y1,y2,y3) ]\n', '    def fun_async(xlist):\n', '      for x in xlist :\n', '            hdfs.upload(x[0], x[1])\n', '    """"""\n']","['range', 'enumerate', 'tuple', 'len', 'print', 'time.sleep', 'log', 'job_list.append', 'pool.apply_async', 'res_list.append', 'pool.terminate', 'pool.join', 'multithread_run_list', 'ThreadWithResult', '__init__', 'function', 'target', 'super', 'kwargs.values', 'list_of_threads.append', 'thread.start', 'zip', 'kwargs.keys', 'thread.join', 'results.append']",25
utilmy/zarchive/multithread.py:multithread_run_list,multithread_run_list,function,19,48,40,530,11.04,3,0,['**kwargs'],[None],[None],48,"['    """""" Creating n number of threads:  1 thread per function,    starting them and waiting for their subsequent completion\n', '    os_multithread(function1=(test_print, (""some text"",)),\n', '                          function2=(test_print, (""bbbbb"",)),\n', '                          function3=(test_print, (""ccccc"",)))\n', '    """"""\n']","['ThreadWithResult', '__init__', 'function', 'target', 'super', 'kwargs.values', 'list_of_threads.append', 'thread.start', 'zip', 'kwargs.keys', 'thread.join', 'results.append']",12
utilmy/zarchive/util_min.py:os_wait_cpu,os_wait_cpu,function,12,31,27,286,9.23,1,0,"['priority', 'cpu_min']","[None, None]","['300', '50']",23,[],"['psutil.cpu_percent', 'print', 'arrow.utcnow', 'sleep']",4
utilmy/zarchive/util_min.py:isexist,isexist,function,3,11,10,58,5.27,0,0,['a'],[None],[None],48,[],['print'],1
utilmy/zarchive/util_min.py:isfloat,isfloat,function,5,15,13,67,4.47,0,1,['x'],[None],[None],54,[],['float'],1
utilmy/zarchive/util_min.py:isint,isint,function,6,14,13,80,5.71,0,1,['x'],[None],[None],61,[],['txt.find'],1
utilmy/zarchive/util_min.py:a_isanaconda,a_isanaconda,function,6,14,13,80,5.71,0,1,[],[],[],63,[],['txt.find'],1
utilmy/zarchive/util_min.py:os_zip_checkintegrity,os_zip_checkintegrity,function,9,33,31,185,5.61,0,1,['filezip1'],[None],[None],72,[],"['zipfile.ZipFile', 'zip_file.testzip', 'print']",3
utilmy/zarchive/util_min.py:os_zipfile,os_zipfile,function,23,43,37,397,9.23,2,2,"['folderin', 'folderzipname', 'iscompress']","[None, None, None]","[None, None, 'True']",83,[],"['zipfile.ZipFile', 'os.walk', 'zf.write', 'zf.close', 'os_zip_checkintegrity', 'print']",6
utilmy/zarchive/util_min.py:os_zipfolder,os_zipfolder,function,15,35,29,346,9.89,0,3,"['dir_tozip', 'zipname', 'dir_prefix', 'iscompress']","[None, None, None, None]","[""'/zdisks3/output'"", ""'/zdisk3/output.zip'"", 'None', 'True']",99,"["" '''\n"", "" shutil.make_archive('/zdisks3/results/output', 'zip',\n"", ""                     root_dir=/zdisks3/results/',\n"", ""                     base_dir='output')\n"", '\n', "" os_zipfolder('zdisk/test/aapackage', 'zdisk/test/aapackage.zip', 'zdisk/test')'''\n""]","['dir_tozip.split', 'shutil.make_archive', 'os_zip_checkintegrity', 'print']",4
utilmy/zarchive/util_min.py:os_zipextractall,os_zipextractall,function,23,59,48,538,9.12,1,3,"['filezip_or_dir', 'tofolderextract', 'isprint']","[None, None, None]","['""folder1/*.zip""', ""'zdisk/test'"", '1']",148,"[""   '''os_zipextractall( 'aapackage.zip','zdisk/test/'      )  '''\n""]","['filezip_or_dir.find', 'os_file_listall', 'os_file_getname', 'zipfile.ZipFile', 'zip_ref.extractall', 'zip_ref.close', 'print']",7
utilmy/zarchive/util_min.py:os_folder_copy,os_folder_copy,function,17,36,35,345,9.58,0,1,"['src', 'dst', 'symlinks', 'pattern1', 'fun_file_toignore']","[None, None, None, None, None]","[None, None, 'False', '""*.py""', 'None']",178,"[""   '''\n"", '       callable(src, names) -> ignored_names\n', ""       'src' parameter, which is the directory being visited by copytree(), and\n"", ""       'names' which is the list of `src` contents, as returned by os.listdir():\n"", '\n', '    Since copytree() is called recursively, the callable will be called once for each directory that is copied.\n', '    It returns a  list of names relative to the `src` directory that should not be copied.\n', ""   '''\n""]","['fun_file_toignore', 'fnmatch.filter', 'shutil.copytree', 'shutil.copy']",4
utilmy/zarchive/util_min.py:os_folder_create,os_folder_create,function,4,7,7,85,12.14,0,1,['directory'],[None],[None],200,[],"['os.getcwd', 'os.makedirs', 'os.chdir']",3
utilmy/zarchive/util_min.py:os_folder_robocopy,os_folder_robocopy,function,3,15,15,166,11.07,0,1,"['from_folder', 'to_folder', ""my_log='H""]","[None, None, '']","[""''"", ""''"", ""'H:/robocopy_log.txt'""]",205,"['    """"""\n', '    Copy files to working directory\n', '    robocopy <Source> <Destination> [<File>[ ...]] [<Options>]\n', '    We want to copy the files to a fast SSD drive\n', '    """"""\n']",['print'],1
utilmy/zarchive/util_min.py:os_file_replacestring1,os_file_replacestring1,function,11,25,24,238,9.52,1,0,"['findStr', 'repStr', 'filePath']","[None, None, None]","[None, None, None]",216,[],"['fileinput.FileInput', 'line.replace', 'file1.close', 'print']",4
utilmy/zarchive/util_min.py:os_file_replacestring2,os_file_replacestring2,function,6,14,13,161,11.5,1,0,"['findstr', 'replacestr', 'some_dir', 'pattern', 'dirlevel']","[None, None, None, None, None]","[None, None, None, '""*.*""', '1']",227,"['  \'\'\' #fil_replacestring_files(""logo.png"", ""logonew.png"", r""D:/__Alpaca__details/aiportfolio"",    pattern=""*.html"", dirlevel=5  )\n', ""  '''\n""]","['os_file_listall', 'os_file_replacestring1']",2
utilmy/zarchive/util_min.py:os_file_getname,os_file_getname,function,8,9,8,75,8.33,0,0,['path'],[None],[None],234,[],"['ntpath.split', 'ntpath.basename']",2
utilmy/zarchive/util_min.py:os_file_getpath,os_file_getpath,function,7,7,7,52,7.43,0,0,['path'],[None],[None],239,[],['ntpath.split'],1
utilmy/zarchive/util_min.py:os_file_gettext,os_file_gettext,function,4,7,7,56,8.0,0,0,['file1'],[None],[None],244,[],"['open', 'f.read']",2
utilmy/zarchive/util_min.py:os_file_listall,os_file_listall,function,28,79,46,902,11.42,4,3,"['dir1', 'pattern', 'dirlevel', 'onlyfolder']","[None, None, None, None]","[None, '""*.*""', '1', '0']",250,"[""  '''\n"", '   # DIRCWD=r""D:\\_devs\\Python01\\project""\n', '   # aa= listallfile(DIRCWD, ""*.*"", 2)\n', '   # aa[0][30];   aa[2][30]\n', '\n', '   :param dir1:\n', '   :param pattern:\n', '   :param dirlevel:\n', '   :param onlyfolder:\n', '   :return:\n', ""  '''\n""]","['dir1.rstrip', 'dir1.count', 'os.walk', 'root.count', 'matches.append', 'fnmatch.filter', 'np.array']",7
utilmy/zarchive/util_min.py:os_file_rename,os_file_rename,function,32,56,48,642,11.46,2,1,"['some_dir', 'pattern', 'pattern2', 'dirlevel']","[None, None, None, None]","[None, '""*.*""', '""""', '1']",287,[],"['some_dir.rstrip', 'some_dir.count', 'os.walk', 'root.count', 'matches.append', 'fnmatch.filter', 're.sub', 'os.rename', 'np.array']",9
utilmy/zarchive/util_min.py:os_print_tofile,os_print_tofile,function,1,2,2,7,3.5,0,0,"['vv', 'file1', 'mode1']","[None, None, None]","[None, None, ""'a'""]",309,"[""    '''\n"", '    Here is a list of the different modes of opening a file:\n', 'r\n', 'Opens a file for reading only. The file pointer is placed at the beginning of the file. This is the default mode.\n', '\n', 'rb\n', '\n', 'Opens a file for reading only in binary format. The file pointer is placed at the beginning of the file. This is the default mode.\n', 'r+\n', '\n', 'Opens a file for both reading and writing. The file pointer will be at the beginning of the file.\n', 'rb+\n', '\n', 'Opens a file for both reading and writing in binary format. The file pointer will be at the beginning of the file.\n', 'w\n', '\n', 'Opens a file for writing only. Overwrites the file if the file exists. If the file does not exist, creates a new file for writing.\n', 'wb\n', '\n', 'Opens a file for writing only in binary format. Overwrites the file if the file exists. If the file does not exist, creates a new file for writing.\n', 'w+\n', '\n', 'Opens a file for both writing and reading. Overwrites the existing file if the file exists. If the file does not exist, creates a new file for reading and writing.\n', 'wb+\n', '\n', 'Opens a file for both writing and reading in binary format. Overwrites the existing file if the file exists. If the file does not exist, creates a new file for reading and writing.\n', 'a\n', '\n', 'Opens a file for appending. The file pointer is at the end of the file if the file exists. That is, the file is in the append mode. If the file does not exist, it creates a new file for writing.\n', 'ab\n', '\n', 'Opens a file for appending in binary format. The file pointer is at the end of the file if the file exists. That is, the file is in the append mode. If the file does not exist, it creates a new file for writing.\n', 'a+\n', '\n', 'Opens a file for both appending and reading. The file pointer is at the end of the file if the file exists. The file opens in the append mode. If the file does not exist, it creates a new file for reading and writing.\n', 'ab+\n', '\n', 'Opens a file for both appending and reading in binary format. The file pointer is at the end of the file if the file exists. The file opens in the append mode. If the file does not exist, it creates a new file for reading and writing.\n', 'To open a text file, use:\n', 'fh = open(""hello.txt"", ""r"")\n', '\n', 'To read a text file, use:\n', 'print fh.read()\n', '\n', 'To read one line at a time, use:\n', 'print fh.readline()\n', '\n', 'To read a list of lines use:\n', 'print fh.readlines()\n', '\n', 'To write to a file, use:\n', 'fh = open(""hello.txt"", ""w"")\n', 'lines_of_text = [""a line of text"", ""another line of text"", ""a third line""]\n', 'fh.writelines(lines_of_text)\n', 'fh.close()\n', '\n', 'To append to file, use:\n', 'fh = open(""Hello.txt"", ""a"")\n', 'fh.close()\n', '\n', ""    '''\n""]",[],0
utilmy/zarchive/util_min.py:a_get_pythonversion,a_get_pythonversion,function,1,2,2,7,3.5,0,0,[],[],[],374,[],[],0
utilmy/zarchive/util_min.py:os_path_norm,os_path_norm,function,8,20,18,174,8.7,0,1,['pth'],[None],[None],377,"['    \'\'\' #r""D:\\_devs\\Python01\\project\\03-Connect_Java_CPP_Excel\\PyBindGen\\examples"" \'\'\'\n']","['pth.find', 'b.lstrip']",2
utilmy/zarchive/util_min.py:os_path_change,os_path_change,function,4,4,4,34,8.5,0,0,['path1'],[None],[None],389,[],"['open', 'fh.read']",2
utilmy/zarchive/util_min.py:os_path_current,os_path_current,function,4,4,4,34,8.5,0,0,[],[],[],391,[],"['open', 'fh.read']",2
utilmy/zarchive/util_min.py:os_file_exist,os_file_exist,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],393,[],"['open', 'fh.read']",2
utilmy/zarchive/util_min.py:os_file_size,os_file_size,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],395,[],"['open', 'fh.read']",2
utilmy/zarchive/util_min.py:os_file_read,os_file_read,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],397,[],"['open', 'fh.read']",2
utilmy/zarchive/util_min.py:os_file_mergeall,os_file_mergeall,function,10,20,20,185,9.25,1,0,"['nfile', 'dir1', 'pattern1', 'deepness']","[None, None, None, None]","[None, None, None, '2']",401,[],"['os_file_listall', 'open', 'os_file_gettext', 'nfile1.write', 'nfile1.close']",5
utilmy/zarchive/util_min.py:os_path_append,os_path_append,function,4,19,11,121,6.37,0,3,"['p1', 'p2', 'p3', 'p4']","[None, None, None, None]","[None, 'None', 'None', 'None']",412,[],[],0
utilmy/zarchive/util_min.py:os_split_dir_file,os_split_dir_file,function,6,15,15,112,7.47,0,1,['dirfile'],[None],[None],421,[],"['dirfile.split', 'len']",2
utilmy/zarchive/util_min.py:py_importfromfile,py_importfromfile,function,12,21,19,251,11.95,0,1,"['modulename', 'dir1']","[None, None]","[None, None]",430,[],"['a_get_pythonversion', 'SourceFileLoader', 'foo.MyClass', 'imp.load_source']",4
utilmy/zarchive/util_min.py:py_memorysize,py_memorysize,function,16,56,38,320,5.71,0,4,"['o', 'ids', 'hint', 'set(']","[None, None, None, None]","[None, None, '"" deep_getsizeof(df_pd', None]",442,"['    """""" deep_getsizeof(df_pd, set())\n', '    Find the memory footprint of a Python object\n', '    The sys.getsizeof function does a shallow size of only. It counts each\n', '    object inside a container as pointer only regardless of how big it\n', '    """"""\n']","['id', 'getsizeof', 'ids.add', 'isinstance', 'sum', 'd', 'o.iteritems']",7
utilmy/zarchive/util_min.py:save,save,function,2,4,4,64,16.0,0,0,"['obj', 'folder', 'isabsolutpath']","[None, None, None]","[None, ""'/folder1/keyname'"", '0']",470,[],['py_save_obj'],1
utilmy/zarchive/util_min.py:load,load,function,2,3,3,60,20.0,0,0,"['folder', 'isabsolutpath']","[None, None]","[""'/folder1/keyname'"", '0']",473,[],['py_load_obj'],1
utilmy/zarchive/util_min.py:save_test,save_test,function,6,11,11,124,11.27,0,0,"['folder', 'isabsolutpath']","[None, None]","[""'/folder1/keyname'"", '0']",476,[],"['py_load_obj', 'print', 'str', 'gc.collect']",4
utilmy/zarchive/util_min.py:py_save_obj,py_save_obj,function,17,36,35,308,8.56,0,1,"['obj', 'folder', 'isabsolutpath']","[None, None, None]","[None, ""'/folder1/keyname'"", '0']",482,[],"['folder.find', 'z_key_splitinto_dir_name', 'os_folder_create', 'open', 'pickle.dump']",5
utilmy/zarchive/util_min.py:py_load_obj,py_load_obj,function,16,30,28,265,8.83,0,1,"['folder', 'isabsolutpath', 'encoding1']","[None, None, None]","[""'/folder1/keyname'"", '0', ""'utf-8'""]",495,"[""    '''def load_obj(name, encoding1='utf-8' ):\n"", ""         with open('D:/_devs/Python01/aaserialize/' + name + '.pkl', 'rb') as f:\n"", '            return pickle.load(f, encoding=encoding1)\n', ""    '''\n""]","['folder.find', 'z_key_splitinto_dir_name', 'os_folder_create', 'open', 'pickle.load']",5
utilmy/zarchive/util_min.py:z_key_splitinto_dir_name,z_key_splitinto_dir_name,function,6,15,15,112,7.47,0,1,['keyname'],[None],[None],511,[],"['keyname.split', 'len']",2
utilmy/zarchive/rstatpy.py:stl,stl,function,32,165,104,1260,7.64,0,11,"['data', 'ns', 'np', 'nt', 'nl', 'isdeg', 'itdeg', 'ildeg', 'nsjump', 'ntjump', 'nljump', 'ni', 'no', 'fulloutput']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', '0', '1', '1', 'None', 'None', 'None', '2', '0', 'False']",9,"['    """"""\n', '    Seasonal-Trend decomposition procedure based on LOESS\n', '    data : pandas.Series\n', '    ns : int\n', '        Length of the seasonal smoother.\n', '        The value of  ns should be an odd integer greater than or equal to 3.\n', '        A value ns>6 is recommended. As ns  increases  the  values  of  the\n', '        seasonal component at a given point in the seasonal cycle (e.g., January\n', '        values of a monthly series with  a  yearly cycle) become smoother.\n', '    np : int\n', '        Period of the seasonal component.\n', '        For example, if  the  time series is monthly with a yearly cycle, then\n', '        np=12.\n', '        If no value is given, then the period will be determined from the\n', '        ``data`` timeseries.\n', '    nt : int\n', '        Length of the trend smoother.\n', '        The  value  of  nt should be an odd integer greater than or equal to 3.\n', '        A value of nt between 1.5*np and 2*np is  recommended. As nt increases,\n', '        the values of the trend component become  smoother.\n', '        If nt is None, it is estimated as the smallest odd integer greater\n', '        or equal to ``(1.5*np)/[1-(1.5/ns)]``\n', '    nl : int\n', '        Length of the low-pass filter.\n', '        The value of nl should  be an odd integer greater than or equal to 3.\n', '        The smallest odd integer greater than or equal to np is used by default.\n', '    isdeg : int\n', '        Degree of locally-fitted polynomial in seasonal smoothing.\n', '        The value is 0 or 1.\n', '    itdeg : int\n', '        Degree of locally-fitted polynomial in trend smoothing.\n', '        The value is 0 or 1.\n', '    ildeg : int\n', '        Degree of locally-fitted polynomial in low-pass smoothing.\n', '        The value is 0 or 1.\n', '    nsjump : int\n', '        Skipping value for seasonal smoothing.\n', '        The seasonal smoother skips ahead nsjump points and then linearly\n', '        interpolates in between.  The value  of nsjump should be a positive\n', '        integer; if nsjump=1, a seasonal smooth is calculated at all n points.\n', '        To make the procedure run faster, a reasonable choice for nsjump is\n', '        10%-20% of ns. By default, nsjump= 0.1*ns.\n', '    ntjump : int\n', '        Skipping value for trend smoothing. If None, ntjump= 0.1*nt\n', '    nljump : int\n', '        Skipping value for low-pass smoothing. If None, nljump= 0.1*nl\n', '    ni :int\n', '        Number of loops for updating the seasonal and trend  components.\n', '        The value of ni should be a positive integer.\n', '        See the next argument for advice on the  choice of ni.\n', '        If ni is None, ni is set to 2 for robust fitting, to 5 otherwise.\n', '    no : int\n', '        Number of iterations of robust fitting. The value of no should\n', '        be a nonnegative integer. If the data are well behaved without\n', '        outliers, then robustness iterations are not needed. In this case\n', '        set no=0, and set ni=2 to 5 depending on how much security\n', '        you want that  the seasonal-trend looping converges.\n', '        If outliers are present then no=3 is a very secure value unless\n', '        the outliers are radical, in which case no=5 or even 10 might\n', '        be better.  If no>0 then set ni to 1 or 2.\n', '        If None, then no is set to 15 for robust fitting, to 0 otherwise.\n', '    fulloutput : bool\n', '        If True, a dictionary holding the full output of the original R routine\n', '        will be returned.\n', '    returns\n', '    data : pandas.DataFrame\n', '        The seasonal, trend, and remainder components\n', '    """"""\n']","['data.copy', '_data.dropna', 'pandas.DateRange', 'pandas.Series', 'ceil', 'robjects.IntVector', 'ts_', 'stl_', 'asarray', 'pandas.DataFrame']",10
utilmy/zarchive/linux.py:load_session,load_session,function,4,7,7,86,12.29,0,0,['name'],[None],"[""'test_20160815'""]",100,[],['dill.load_session'],1
utilmy/zarchive/linux.py:save_session,save_session,function,6,9,9,107,11.89,0,0,['name'],[None],"[""''""]",106,[],"['date_now', 'dill.dump_session']",2
utilmy/zarchive/linux.py:isfloat,isfloat,function,3,17,13,80,4.71,0,1,['value'],[None],[None],121,[],['float'],1
utilmy/zarchive/linux.py:isint,isint,function,6,15,14,80,5.33,0,1,['x'],[None],[None],129,[],['txt.find'],1
utilmy/zarchive/linux.py:aa_isanaconda,aa_isanaconda,function,6,15,14,80,5.33,0,1,[],[],[],132,[],['txt.find'],1
utilmy/zarchive/linux.py:aa_cleanmemory,aa_cleanmemory,function,3,3,3,21,7.0,0,0,[],[],[],147,[],['gc.collect'],1
utilmy/zarchive/linux.py:aa_getmodule_doc,aa_getmodule_doc,function,8,13,13,110,8.46,0,0,"['module1', 'fileout']","[None, None]","[None, ""''""]",150,[],"['ca.getmodule_doc', 'np.isnan', 'z.nonzero']",3
utilmy/zarchive/linux.py:np_interpolate_nan,np_interpolate_nan,function,7,9,9,78,8.67,0,0,['y'],[None],[None],1176,[],"['__np_nan_helper', 'np.interp', 'x']",3
utilmy/zarchive/linux.py:and1,and1,function,8,55,22,356,6.47,0,6,"['x', 'y', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']","[None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None', 'None', 'None']",1182,[],[],0
utilmy/zarchive/linux.py:sortcol,sortcol,function,6,8,8,77,9.62,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",1192,"[' """""" df.sort([\'A\', \'B\'], ascending=[1, 0])  """"""\n']","['pd.DataFrame', 'df.sort_values']",2
utilmy/zarchive/linux.py:sort,sort,function,7,20,12,181,9.05,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",1198,[],"['pd.DataFrame', 'df.sort_values', 'sort']",3
utilmy/zarchive/linux.py:np_ma,np_ma,function,2,3,3,45,15.0,0,0,"['vv', 'n']","[None, None]","[None, None]",1205,"[""  '''Moving average '''\n""]","['np.convolve', 'np.ones']",2
utilmy/zarchive/linux.py:np_cleanmatrix,np_cleanmatrix,function,11,22,20,124,5.64,2,1,['m'],[None],[None],1211,[],"['np.nan_to_num', 'np.shape', 'range', 'abs']",4
utilmy/zarchive/linux.py:np_sortbycolumn,np_sortbycolumn,function,6,8,8,77,9.62,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",1222,[],"['pd.DataFrame', 'df.sort_values']",2
utilmy/zarchive/linux.py:np_sortbycol,np_sortbycol,function,8,30,20,257,8.57,0,1,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",1227,[],"['pd.DataFrame', 'df.sort_values', 'np_sortbycol', 'len', 'np.reshape']",5
utilmy/zarchive/linux.py:np_findfirst,np_findfirst,function,5,11,10,55,5.0,1,1,"['item', 'vec']","[None, None]","[None, None]",1238,"['    """"""return the index of the first occurence of item in vec""""""\n']",['xrange'],1
utilmy/zarchive/linux.py:np_find,np_find,function,6,24,13,133,5.54,2,2,"['item', 'vec']","[None, None]","[None, None]",1244,[],"['xrange', 'np_find']",2
utilmy/zarchive/linux.py:find,find,function,12,42,30,219,5.21,1,3,"['item', 'vec']","[None, None]","[None, None]",1250,"['    """"""return the index of the first occurence of item in vec""""""\n']","['xrange', 'findx', 'type', 'vec.index', 'len', 'finds']",6
utilmy/zarchive/linux.py:findx,findx,function,6,27,18,119,4.41,0,2,"['item', 'vec']","[None, None]","[None, None]",1257,"['    """"""return the index of the first occurence of item in vec""""""\n']","['type', 'vec.index', 'len']",3
utilmy/zarchive/linux.py:finds,finds,function,12,34,23,156,4.59,2,3,"['itemlist', 'vec']","[None, None]","[None, None]",1268,"['  """"""return the index of the first occurence of item in vec""""""\n']","['xrange', 'idlist.append']",2
utilmy/zarchive/linux.py:findhigher,findhigher,function,5,13,12,51,3.92,1,1,"['x', 'vec']","[None, None]","[None, None]",1280,"['    """"""return the index of the first occurence of item in vec""""""\n']",['xrange'],1
utilmy/zarchive/linux.py:findlower,findlower,function,5,12,11,51,4.25,1,1,"['x', 'vec']","[None, None]","[None, None]",1287,"['    """"""return the index of the first occurence of item in vec""""""\n']",['xrange'],1
utilmy/zarchive/linux.py:np_find_minpos,np_find_minpos,function,4,7,5,95,13.57,0,0,['values'],[None],[None],1297,[],['min'],1
utilmy/zarchive/linux.py:np_find_maxpos,np_find_maxpos,function,17,45,34,266,5.91,1,3,['values'],[None],[None],1301,[],"['max', 'np_find_maxpos_2nd', 'float', 'enumerate']",4
utilmy/zarchive/linux.py:np_find_maxpos_2nd,np_find_maxpos_2nd,function,13,37,28,138,3.73,1,3,['numbers'],[None],[None],1306,[],"['float', 'enumerate']",2
utilmy/zarchive/linux.py:np_findlocalmax2,np_findlocalmax2,function,45,116,77,690,5.95,2,5,"['v', 'trig']","[None, None]","[None, None]",1320,[],"['n=len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'range', 'len', 'findhigher', 'np.abs']",8
utilmy/zarchive/linux.py:np_findlocalmin2,np_findlocalmin2,function,46,119,78,703,5.91,2,6,"['v', 'trig']","[None, None]","[None, None]",1355,[],"['n=len', 'np.zeros', 'np_find_minpos', 'enumerate', 'range', 'len', 'findlower', 'np.abs']",8
utilmy/zarchive/linux.py:np_findlocalmax,np_findlocalmax,function,45,116,77,690,5.95,2,5,"['v', 'trig']","[None, None]","[None, None]",1392,[],"['n=len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'range', 'len', 'findhigher', 'np.abs']",8
utilmy/zarchive/linux.py:np_findlocalmin,np_findlocalmin,function,46,119,78,703,5.91,2,6,"['v', 'trig']","[None, None]","[None, None]",1407,[],"['n=len', 'np.zeros', 'np_find_minpos', 'enumerate', 'range', 'len', 'findlower', 'np.abs']",8
utilmy/zarchive/linux.py:np_stack,np_stack,function,12,53,25,323,6.09,0,7,"['v1', 'v2', 'v3', 'v4', 'v5']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",1428,[],"['np.shape', 'np.row_stack', 'np.column_stack']",3
utilmy/zarchive/linux.py:np_uniquerows,np_uniquerows,function,6,8,8,148,18.5,0,0,['a'],[None],[None],1443,[],"['np.ascontiguousarray', 'np.unique', 'unique_a.view']",3
utilmy/zarchive/linux.py:np_remove_zeros,np_remove_zeros,function,2,4,4,35,8.75,0,0,"['vv', 'axis1']","[None, None]","[None, '1']",1448,[],[],0
utilmy/zarchive/linux.py:np_sort,np_sort,function,8,30,20,257,8.57,0,1,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",1451,"['    """"""Produce psuedo-code for decision tree.\n', '    tree -- scikit-leant DescisionTree.\n', '    feature_names -- list of feature names.\n', '    target_names -- list of target (output) names.\n', '    spacer_base -- used for spacing code (default: ""    "").\n', '    """"""\n']","['pd.DataFrame', 'df.sort_values', 'np_sortbycol', 'len', 'np.reshape']",5
utilmy/zarchive/linux.py:np_memory_array_adress,np_memory_array_adress,function,2,2,2,38,19.0,0,0,['x'],[None],[None],1454,[],[],0
utilmy/zarchive/linux.py:sk_featureimportance,sk_featureimportance,function,9,21,21,228,10.86,1,1,"['clfrf', 'feature_name']","[None, None]","[None, None]",1463,[],"['np.argsort', 'range', 'len', 'str']",4
utilmy/zarchive/linux.py:sk_showconfusion,sk_showconfusion,function,11,22,21,244,11.09,0,1,"['clfrf', 'X_train', 'Y_train', 'isprint']","[None, None, None, None]","[None, None, None, 'True']",1471,[],"['clfrf.predict', 'cm.astype', 'cm.sum', 'print']",4
utilmy/zarchive/linux.py:sk_tree,sk_tree,function,13,34,32,430,12.65,0,1,"['Xtrain', 'Ytrain', 'nbtree', 'maxdepth', 'print1']","[None, None, None, None, None]","[None, None, None, None, None]",1479,[],"['clfrf.fit', 'clfrf.predict', 'cm.astype', 'cm.sum', 'print']",5
utilmy/zarchive/linux.py:sk_gen_ensemble_weight,sk_gen_ensemble_weight,function,14,30,28,213,7.1,1,0,"['vv', 'acclevel', 'maxlevel']","[None, None, None]","[None, None, '0.88']",1492,[],"['min', 'np.empty', 'range', 'estww.append', 'np.log', 'np.array']",6
utilmy/zarchive/linux.py:sk_votingpredict,sk_votingpredict,function,19,33,28,267,8.09,2,1,"['estimators', 'voting', 'ww', 'X_test']","[None, None, None, None]","[None, None, None, None]",1506,[],"['np.zeros', 'enumerate', 'clf.predict_proba', 'range', 'len']",5
utilmy/zarchive/linux.py:sk_tree_get_ifthen,sk_tree_get_ifthen,function,26,89,62,768,8.63,2,3,"['tree', 'feature_names', 'target_names', 'spacer_base']","[None, None, None, None]","[None, None, None, '"" ""']",1524,"['    """"""Produce psuedo-code for decision tree.\n', '    tree -- scikit-leant DescisionTree.\n', '    feature_names -- list of feature names.\n', '    target_names -- list of target (output) names.\n', '    spacer_base -- used for spacing code (default: ""    "").\n', '    """"""\n']","['recurse', 'print', 'str', 'zip', 'int']",5
utilmy/zarchive/linux.py:pd_array_todataframe,pd_array_todataframe,function,13,45,29,277,6.16,0,2,"['price', 'symbols', 'date1', 'dotranspose']","[None, None, None, None]","[None, 'None', 'None', 'False']",1562,[],"['np.shape', 'len', 'pd.DataFrame']",3
utilmy/zarchive/linux.py:pd_date_intersection,pd_date_intersection,function,7,18,15,157,8.72,1,0,['qlist'],[None],[None],1574,[],"['set', 'set.intersection', 'list', 'sorted']",4
utilmy/zarchive/linux.py:pd_resetindex,pd_resetindex,function,3,4,4,50,12.5,0,0,['df'],[None],[None],1585,[],['list'],1
utilmy/zarchive/linux.py:pd_create_colmap_nametoid,pd_create_colmap_nametoid,function,9,12,12,77,6.42,1,0,['df'],[None],[None],1591,"[""  ''' 'close' ---> 5    '''\n""]",['enumerate'],1
utilmy/zarchive/linux.py:pd_dataframe_toarray,pd_dataframe_toarray,function,9,10,10,107,10.7,0,0,['df'],[None],[None],1599,[],[],0
utilmy/zarchive/linux.py:pd_changeencoding,pd_changeencoding,function,6,8,8,89,11.12,1,0,"['data', 'cols']","[None, None]","[None, None]",1606,[],[],0
utilmy/zarchive/linux.py:pd_createdf,pd_createdf,function,2,4,4,53,13.25,0,0,"['val1', 'col1', 'idx1']","[None, None, None]","[None, 'None', 'None']",1613,[],['pd.DataFrame'],1
utilmy/zarchive/linux.py:pd_insertcolumn,pd_insertcolumn,function,12,29,27,179,6.17,1,1,"['df', 'colname', 'vec']","[None, None, None]","[None, None, None]",1618,"["" ''' Vec and Colname must be aligned '''\n""]","['len', 'np.shape', 'range', 'df.insert']",4
utilmy/zarchive/linux.py:pd_insertrows,pd_insertrows,function,17,31,30,237,7.65,1,1,"['df', 'rowval', 'index1']","[None, None, None]","[None, None, 'None']",1632,[],"['np.shape', 'np.arange', 'pd.DataFrame', 'range', 'df.append']",5
utilmy/zarchive/linux.py:pd_replacevalues,pd_replacevalues,function,11,14,14,101,7.21,1,0,"['df', 'matrix']","[None, None]","[None, None]",1645,"["" ''' Matrix replaces df.values  '''\n""]",['np.shape'],1
utilmy/zarchive/linux.py:pd_storeadddf,pd_storeadddf,function,6,15,15,113,7.53,0,1,"['df', 'dfname', ""dbfile='F""]","[None, None, '']","[None, None, ""'F:\\temp_pandas.h5'""]",1655,[],"['pd.HDFStore', 'find', 'store.keys', 'store.append', 'store.close']",5
utilmy/zarchive/linux.py:pd_storedumpinfo,pd_storedumpinfo,function,13,29,24,298,10.28,1,0,"[""dbfile='E""]",[''],"[""'E:\\_data\\stock\\intraday_google.h5'""]",1663,[],"['pd.HDFStore', 'store.keys', 'pd.read_hdf', 'extract.append', 'datetime_tostring', 'errsym.append', 'np.array']",7
utilmy/zarchive/linux.py:pd_remove_row,pd_remove_row,function,2,2,2,29,14.5,0,0,"['df', 'row_list_index', '45]']","[None, None, None]","[None, '[23', None]",1677,[],['df.drop'],1
utilmy/zarchive/linux.py:pd_extract_col_idx_val,pd_extract_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],1680,[],[],0
utilmy/zarchive/linux.py:pd_split_col_idx_val,pd_split_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],1684,[],[],0
utilmy/zarchive/linux.py:pd_addcolumn,pd_addcolumn,function,10,23,19,190,8.26,1,1,"['df1', 'name1']","[None, None]","[None, ""'new'""]",1688,[],"['len', 'type', 'pd.Series']",3
utilmy/zarchive/linux.py:pd_removecolumn,pd_removecolumn,function,2,3,3,23,7.67,0,0,"['df1', 'name1']","[None, None]","[None, None]",1697,[],['df1.drop'],1
utilmy/zarchive/linux.py:pd_save_vectopanda,pd_save_vectopanda,function,9,22,19,182,8.27,1,0,"['vv', 'filenameh5']","[None, None]","[None, None]",1701,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/linux.py:pd_load_panda2vec,pd_load_panda2vec,function,9,22,19,182,8.27,1,0,"['filenameh5', 'store_id']","[None, None]","[None, ""'data'""]",1706,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/linux.py:pd_csv_topanda,pd_csv_topanda,function,9,22,19,182,8.27,1,0,"['filein1', 'filename', 'tablen']","[None, None, None]","[None, None, ""'data'""]",1711,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/linux.py:pd_getpanda_tonumpy,pd_getpanda_tonumpy,function,6,10,10,92,9.2,0,0,"['filename', 'nsize', 'tablen']","[None, None, None]","[None, None, ""'data'""]",1723,[],['pd.read_hdf'],1
utilmy/zarchive/linux.py:pd_getrandom_tonumpy,pd_getrandom_tonumpy,function,6,9,9,101,11.22,0,0,"['filename', 'nbdim', 'nbsample', 'tablen']","[None, None, None, None]","[None, None, None, ""'data'""]",1728,[],['pd.read_hdf'],1
utilmy/zarchive/linux.py:sk_cluster_kmeans,sk_cluster_kmeans,function,21,38,37,375,9.87,1,1,"['x', 'nbcluster', 'isplot']","[None, None, None]","[None, '5', 'True']",1861,[],"['np.std', 'kmeans.fit', 'range', 'plt.plot', 'plt.show']",5
utilmy/zarchive/linux.py:datetime_tostring,datetime_tostring,function,8,26,21,244,9.38,1,2,['datelist1'],[None],[None],1885,[],"['isinstance', 'datelist1.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zarchive/linux.py:date_remove_bdays,date_remove_bdays,function,16,41,28,351,8.56,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",1896,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/linux.py:date_add_bdays,date_add_bdays,function,16,40,29,350,8.75,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",1914,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/linux.py:datestring_todatetime,datestring_todatetime,function,6,15,13,125,8.33,1,1,"['datelist1', 'format1']","[None, None]","[None, ' ""%Y%m%d""']",1929,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/linux.py:datetime_toint,datetime_toint,function,6,15,13,155,10.33,1,1,['datelist1'],[None],[None],1938,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/linux.py:dateint_todatetime,dateint_todatetime,function,6,15,13,135,9.0,1,1,['datelist1'],[None],[None],1946,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/linux.py:date_diffindays,date_diffindays,function,4,5,5,74,14.8,0,0,"['intdate1', 'intdate2']","[None, None]","[None, None]",1955,[],['dateint_todatetime'],1
utilmy/zarchive/linux.py:date_finddateid,date_finddateid,function,5,75,20,492,6.56,0,10,"['date1', 'dateref']","[None, None]","[None, None]",1960,[],['util.np_findfirst'],1
utilmy/zarchive/linux.py:datestring_toint,datestring_toint,function,6,16,14,106,6.62,1,1,['datelist1'],[None],[None],1974,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/linux.py:date_now,date_now,function,10,18,16,130,7.22,0,1,['i'],[None],['0'],1981,[],"['datetime.now', 'date_add_bdays', 'date_remove_bdays', 'str']",4
utilmy/zarchive/linux.py:date_as_float,date_as_float,function,11,28,23,273,9.75,0,1,['dt'],[None],[None],1989,[],"['datetime.datetime', 'isleap', 'timedelta']",3
utilmy/zarchive/linux.py:datediff_inyear,datediff_inyear,function,2,3,3,53,17.67,0,0,"['startdate', 'enddate']","[None, None]","[None, None]",2001,[],['date_as_float'],1
utilmy/zarchive/linux.py:date_generatedatetime,date_generatedatetime,function,18,27,27,267,9.89,0,1,"['start', 'nbday', 'end']","[None, None, None]","['""20100101""', '10', '""""']",2005,[],"['datestring_todatetime', 'date_add_bdays', 'datetime.timedelta', 'list', 'np.array']",5
utilmy/zarchive/linux.py:datetime_tostring,datetime_tostring,function,8,26,21,244,9.38,1,2,['datelist1'],[None],[None],1885,[],"['isinstance', 'datelist1.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zarchive/linux.py:datestring_todatetime,datestring_todatetime,function,6,15,13,125,8.33,1,1,"['datelist1', 'format1']","[None, None]","[None, ' ""%Y%m%d""']",1929,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/linux.py:datestring_toint,datestring_toint,function,6,16,14,106,6.62,1,1,['datelist1'],[None],[None],1974,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/linux.py:numexpr_vect_calc,numexpr_vect_calc,function,13,28,22,214,7.64,0,0,"['filename', 'expr', 'i0', 'imax', ""fileout='E""]","[None, None, None, None, '']","[None, None, '0', '1000', ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",2048,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/linux.py:numexpr_topanda,numexpr_topanda,function,10,28,22,248,8.86,0,0,"['filename', 'expr', 'i0', 'imax', ""fileout='E""]","[None, None, None, None, '']","[None, None, '0', '1000', ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",2060,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/linux.py:textvect_topanda,textvect_topanda,function,2,8,8,70,8.75,0,0,"['vv', 'fileout']","[None, None]","[None, '""""']",2070,[],"['pd.DataFrame', 'pd.HDFStore', 'st.append']",3
utilmy/zarchive/linux.py:comoment,comoment,function,4,6,6,61,10.17,0,0,"['xx', 'yy', 'nsample', 'kx', 'ky']","[None, None, None, None, None]","[None, None, None, None, None]",2093,[],['ne.evaluate'],1
utilmy/zarchive/linux.py:acf,acf,function,12,34,31,244,7.18,0,0,['data'],[None],[None],2101,[],"['len', 'np.mean', 'np.sum', 'float', 'r', 'np.arange', 'np.asarray']",7
utilmy/zarchive/linux.py:plotsave,plotsave,function,7,16,16,199,12.44,0,0,"['xx', 'yy', 'title1']","[None, None, None]","[None, None, '""""']",2124,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",6
utilmy/zarchive/linux.py:plotshow,plotshow,function,4,9,9,101,11.22,0,0,"['xx', 'yy', 'title1']","[None, None, None]","[None, None, '""""']",2135,[],"['plt.scatter', 'plt.autoscale', 'plt.title', 'plt.show']",4
utilmy/zarchive/linux.py:compileVSsolution,compileVSsolution,function,5,33,19,239,7.24,0,4,"['dir1', 'flags1', 'type1', 'compilerdir']","[None, None, None, None]","[None, '""""', '""devenv""', '""""']",2568,[],['os.system'],1
utilmy/zarchive/linux.py:VS_start,VS_start,function,18,56,52,881,15.73,0,2,"['self', 'version']","[None, None]","[None, None]",2597,[],"['os.getenv', 'Exception', 'default.append', 'libs.append']",4
utilmy/zarchive/linux.py:VS_build,VS_build,function,16,62,54,614,9.9,1,2,"['self', 'lib_to_build']","[None, None]","[None, None]",2632,[],"['print', 'subprocess.Popen', 'process.poll', 'process.communicate']",4
utilmy/zarchive/linux.py:set_rc_version,set_rc_version,function,12,43,39,500,11.63,0,0,"['rcfile', 'target_version']","[None, None]","[None, None]",2666,[],"['open', 'f.read', 're.compile', 're.sub', 'f.seek', 'f.write', 'f.truncate']",7
utilmy/zarchive/kagglegym.py:r_score,r_score,function,4,15,12,146,9.73,0,1,"['y_true', 'y_pred', 'sample_weight', 'multioutput']","[None, None, None, None]","[None, None, 'None', 'None']",77,[],['r2_score'],1
utilmy/zarchive/kagglegym.py:make,make,function,2,2,2,19,9.5,0,0,[],[],[],175,[],['Environment'],1
utilmy/zarchive/kagglegym.py:Observation,Observation,class,7,11,11,99,9.0,0,0,[],[],[],86,[],[],0
utilmy/zarchive/kagglegym.py:Environment,Environment,class,49,148,101,1719,11.61,0,1,[],[],[],93,[],[],0
utilmy/zarchive/kagglegym.py:Observation:__init__,Observation:__init__,method,6,6,6,58,9.67,0,0,"['self', 'train', 'target', 'features']","[None, None, None, None]","[None, None, None, None]",87,[],[],0
utilmy/zarchive/kagglegym.py:Environment:__init__,Environment:__init__,method,21,37,35,452,12.22,0,0,['self'],[None],[None],94,[],"['pd.HDFStore', 'hfdata.get', 'len', 'int']",4
utilmy/zarchive/kagglegym.py:Environment:reset,Environment:reset,method,16,36,34,403,11.19,0,0,['self'],[None],[None],115,[],"['int', 'Observation']",2
utilmy/zarchive/kagglegym.py:Environment:step,Environment:step,method,29,64,52,765,11.95,0,1,"['self', 'target']","[None, None]","[None, None]",135,[],"['r_score', 'Observation']",2
utilmy/zarchive/kagglegym.py:Environment:__str__,Environment:__str__,method,1,2,2,21,10.5,0,0,['self'],[None],[None],171,[],[],0
utilmy/zarchive/portfolio.py:data_jpsector,data_jpsector,function,3,4,4,47,11.75,0,0,[],[],[],37,[],['util.load_obj'],1
utilmy/zarchive/portfolio.py:date_earningquater,date_earningquater,function,9,103,42,604,5.86,0,5,['t1'],[None],[None],47,[],['datetime.datetime'],1
utilmy/zarchive/portfolio.py:date_is_3rdfriday,date_is_3rdfriday,function,4,14,13,67,4.79,0,0,['s'],[None],[None],66,[],"['datetime.strptime', 'd.weekday']",2
utilmy/zarchive/portfolio.py:date_option_expiry,date_option_expiry,function,10,50,33,354,7.08,0,2,['date'],[None],[None],71,[],['datetime.datetime'],1
utilmy/zarchive/portfolio.py:date_find_kday_fromintradaydate,date_find_kday_fromintradaydate,function,2,3,3,68,22.67,0,0,"['kintraday', 'intradaydate', 'dailydate']","[None, None, None]","[None, None, None]",88,[],['util.np_find'],1
utilmy/zarchive/portfolio.py:date_find_kintraday_fromdate,date_find_kintraday_fromdate,function,4,10,10,88,8.8,0,0,"['d1', 'intradaydate1', 'h1', 'm1']","[None, None, None, None]","[None, None, '9', '30']",92,[],"['datetime.datetime', 'util.np_find']",2
utilmy/zarchive/portfolio.py:date_find_intradateid,date_find_intradateid,function,8,13,13,96,7.38,1,0,"['datetimelist', 'stringdate']","[None, None]","[None, ""['20160420223000']""]",99,[],"['datestring_todatetime', 'util.np_find', 'str']",3
utilmy/zarchive/portfolio.py:datetime_convertzone1_tozone2,datetime_convertzone1_tozone2,function,16,29,25,343,11.83,1,1,"['tt', 'fromzone', 'tozone']","[None, None, None]","[None, ""'Japan'"", ""'US/Eastern'""]",110,[],"['pytz.timezone', 'type', 'tz.localize', 't2.append']",4
utilmy/zarchive/portfolio.py:date_extract_dailyopenclosetime,date_extract_dailyopenclosetime,function,23,51,35,431,8.45,2,3,"['spdateref1', 'market']","[None, None]","[None, ""'us'""]",127,[],"['enumerate', 'spdailyopendate.append', 'np.array', 'spdailyclosedate.append']",4
utilmy/zarchive/portfolio.py:date_finddateid,date_finddateid,function,5,75,20,492,6.56,0,10,"['date1', 'dateref']","[None, None]","[None, None]",147,[],['util.np_findfirst'],1
utilmy/zarchive/portfolio.py:date_alignfromdateref,date_alignfromdateref,function,13,31,27,206,6.65,1,1,"['array1', 'dateref']","[None, None]","[None, None]",162,[],"['range', 'np.float', 'len']",3
utilmy/zarchive/portfolio.py:_date_align,_date_align,function,16,42,33,256,6.1,2,2,"['dateref', 'datei', 'tmax', 'closei']","[None, None, None, None]","[None, None, None, None]",187,[],"['np.zeros', 'range', 'len']",3
utilmy/zarchive/portfolio.py:date_align,date_align,function,13,31,27,206,6.65,1,1,"['array1', 'dateref']","[None, None]","[None, None]",201,"["" ''' #Aligne the price with the same dates date\tyear\tmonth\tday\td\topen\tclose\thigh\tlow\tvolume\taclose '''\n""]","['range', 'np.float', 'len']",3
utilmy/zarchive/portfolio.py:min_withposition,min_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],249,[],['min'],1
utilmy/zarchive/portfolio.py:max_withposition,max_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],253,[],['max'],1
utilmy/zarchive/portfolio.py:_reshape,_reshape,function,2,14,12,68,4.86,0,1,['x'],[None],[None],259,[],"['len', 'np.reshape', 'np.size']",3
utilmy/zarchive/portfolio.py:_notnone,_notnone,function,39,211,114,1769,8.38,1,11,['x'],[None],[None],263,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend']",23
utilmy/zarchive/portfolio.py:plot_price,plot_price,function,69,272,170,2746,10.1,2,11,"['asset', 'y2', 'y3', 'y4', 'y5', 'sym', 'savename1', 'tickperday', 'date1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', 'None', ""''"", '20', 'None', '(10', None]",267,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend', 'plot_priceintraday', 'np.loadtxt', 'np.unique', 'xdays.append', 'np.hstack', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick']",35
utilmy/zarchive/portfolio.py:plot_priceintraday,plot_priceintraday,function,32,59,57,947,16.05,1,0,['data'],[None],[None],346,[],"['np.loadtxt', 'np.unique', 'np.arange', 'xdays.append', 'np.hstack', 'plt.figure', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick', 'plt.show']",14
utilmy/zarchive/portfolio.py:plot_check,plot_check,function,8,19,19,257,13.53,0,0,"['close', 'tt0i', 'tt1i', 'dateref', 'sym', 'tickperday']","[None, None, None, None, None, None]","[None, '20140102', '20160815', '[]', '[]', '120']",388,[],"['util.find', 'getret_fromquotes', 'price_normalize100', 'plot_price']",4
utilmy/zarchive/portfolio.py:plot_pricedate,plot_pricedate,function,18,61,49,534,8.75,1,4,"['date1', 'sym1', 'asset1', 'sym2', 'bsk1', 'verticaldate', 'savename1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None]","[None, None, None, 'None', 'None', 'None', ""''"", '(10', None]",398,[],"['type', 'datestring_todatetime', 'np.row_stack', 'np.concatenate', 'array_todataframe', 'df_asset.plot', 'ax.axvline', 'np.arange', 'int', 'len', 'ax.savefig']",11
utilmy/zarchive/portfolio.py:generate_sepvertical,generate_sepvertical,function,9,13,12,149,11.46,0,1,"['asset1', 'tt', 'tmax', 'start', 'datebar']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",480,[],"['np.zeros', 'datestring_todatetime']",2
utilmy/zarchive/portfolio.py:save_asset_tofile,save_asset_tofile,function,31,129,87,978,7.58,5,9,"['file1', 'asset1', 'asset2', 'asset3', 'date1', 'title1']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",495,[],"['np.shape', 'len', 'np.reshape', 'print', 'np.zeros', 'range', 'int', 'np.row_stack', 'np.savetxt']",9
utilmy/zarchive/portfolio.py:load_asset_fromfile,load_asset_fromfile,function,1,4,4,51,12.75,0,0,['file1'],[None],[None],534,[],[],0
utilmy/zarchive/portfolio.py:array_todataframe,array_todataframe,function,11,41,27,259,6.32,0,2,"['price', 'symbols', 'date1']","[None, None, None]","[None, 'None', 'None']",538,[],"['np.shape', 'len', 'pd.DataFrame']",3
utilmy/zarchive/portfolio.py:pd_dataframe_toarray,pd_dataframe_toarray,function,9,10,10,107,10.7,0,0,['df'],[None],[None],549,[],[],0
utilmy/zarchive/portfolio.py:isfloat,isfloat,function,3,17,13,80,4.71,0,1,['value'],[None],[None],558,[],['float'],1
utilmy/zarchive/portfolio.py:isint,isint,function,32,105,61,1036,9.87,0,8,['x'],[None],[None],566,[],"['Xmat.copy', 'x.mean', 'x.std', 'np.corrcoef', 'MinCovDet', 'OAS', 'correl_inv.copy', 'np.sqrt']",8
utilmy/zarchive/portfolio.py:correlation_mat,correlation_mat,function,32,105,61,1036,9.87,0,8,"['Xmat', 'type1', 'type2']","[None, None, None]","[None, '""robust""', '""correl""']",573,[],"['Xmat.copy', 'x.mean', 'x.std', 'np.corrcoef', 'MinCovDet', 'OAS', 'correl_inv.copy', 'np.sqrt']",8
utilmy/zarchive/portfolio.py:correl_reducebytrigger,correl_reducebytrigger,function,9,22,18,128,5.82,2,1,"['correl2', 'trigger']","[None, None]","[None, None]",621,"[""  ''' Put Zero below trigger  '''\n""]","['correl2.copy', 'range', 'abs']",3
utilmy/zarchive/portfolio.py:sk_cov_fromcorrel,sk_cov_fromcorrel,function,8,14,13,115,8.21,0,0,"['correl', 'ret_close1']","[None, None]","[None, None]",633,[],"['volhisto_fromret', 'np.diag', 'np.dot']",3
utilmy/zarchive/portfolio.py:cointegration,cointegration,function,18,47,37,385,8.19,0,0,"['x', 'y']","[None, None]","[None, None]",639,"['  """""" Output :\n', '    coint_t :t-statistic of unit-root test on residuals\n', ""    pvalue :   MacKinnon's approximate p-value based on MacKinnon (1994)\n"", '    crit_value : Critical values for the test statistic at the 1 %, 5 %, and 10 %  Signifiance levels.\n', '\n', '    The Null hypothesis is that there is no cointegration, the alt hypothesis is that there is cointegrating relationship. \n', '    If the pvalue is small, below a critical size:  Reject Null Hypothesis -->  Cointegration\n', '           pvalue is high --> No Cointegration    \n', '    \n', '    Significance level, also denoted as alpha or , is the probability of rejecting  the null hypothesis when it is true.\n', '    Probability of incorrectly rejecting a true null hypothesis\n', '\n', '    P=0.05  , At least 23% (and typically close to 50%)\n', '    P=0.01  , At least 7% (and typically close to 15%\n', '\n', '    (-2.9607012342275936,\n', '    0.038730981052330332, 0,\n', '    249,\n', ""    {'1%': -3.4568881317725864,\n"", ""       '10%': -2.5729936189738876,\n"", ""     '5%': -2.8732185133016057},\n"", '      601.96849256295991)\n', '        It can be seen that the calculated test statistic of -2.96 is smaller than the 5% critical value of -2.87, \n', ""        which means that we can reject the null hypothesis that  there isn't a cointegrating relationship at the 5% level.\n"", '  """"""\n']",['np.zeros'],1
utilmy/zarchive/portfolio.py:causality_y1_y2,causality_y1_y2,function,10,14,13,153,10.93,0,0,"['price2', 'price1', 'maxlag']","[None, None, None]","[None, None, None]",677,[],[],0
utilmy/zarchive/portfolio.py:reg_slope,reg_slope,function,16,24,24,261,10.88,1,0,"['close', 'dateref', 'tlag', 'type1']","[None, None, None, None]","[None, None, None, ""'elasticv'""]",685,[],"['np.ones', 'xrange', 'regression', 'np.arange']",4
utilmy/zarchive/portfolio.py:rolling_cointegration,rolling_cointegration,function,13,29,26,280,9.66,0,0,"['x', 'y']","[None, None]","[None, None]",700,[],"['rolling_coint', 'pd.expanding_apply']",2
utilmy/zarchive/portfolio.py:regression,regression,function,13,67,32,589,8.79,0,5,"['yreturn', 'xreturn', 'type1']","[None, None, None]","[None, None, '""elasticv""']",719,"["" '''Y = X* reg.coef_  +  reg.intercept_, r2 '''\n""]","['len', 'yreturn.ravel', 'xreturn.reshape', 'reg.fit', 'F', 'reg.score']",6
utilmy/zarchive/portfolio.py:regression_fixedsymbolstock,regression_fixedsymbolstock,function,30,59,54,591,10.02,1,1,"['sym', 'ret_close2', 'tsstart', 'tsample', 'ret_spy', 'spyclose', 'regonly']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, 'True']",756,"["" ''' Price, Weight, R2, Error  '''\n""]","['len', 'range', 'np.reshape', 'np.column_stack', 'regression', 'getpriceret_fromregression', 'np.max']",7
utilmy/zarchive/portfolio.py:regression_getpricefromww,regression_getpricefromww,function,20,38,30,298,7.84,3,0,"['spyclose', 'ww01', 'regasset01', 'ret_close2', 'tstart', 'tlag']","[None, None, None, None, None, None]","[None, None, None, None, None, '1']",779,[],"['np.copy', 'np.zeros', 'range', 'len']",4
utilmy/zarchive/portfolio.py:regression_allstocks_vs_riskfactors,regression_allstocks_vs_riskfactors,function,65,137,113,1351,9.86,2,1,"['symstock', 'pricestock', 'symriskfac', 'priceriskfac', 'nlaglist']","[None, None, None, None, None]","[None, None, None, None, None]",800,"["" '''Make Regression on all stocks ''' \n""]","['np.shape', 'print', 'np.arange', 'np.empty', 'len', 'enumerate', 'getlogret_fromquotes', 'range', 'np.reshape', 'regression_fixedsymbolstock', 'util.np_sortbycolumn', 'util.pd_array_todataframe', 'util.print_object']",13
utilmy/zarchive/portfolio.py:getdiff_fromquotes,getdiff_fromquotes,function,4,6,6,73,12.17,0,0,"['close', 'timelag']","[None, None]","[None, None]",897,[],['np.shape'],1
utilmy/zarchive/portfolio.py:getret_fromquotes,getret_fromquotes,function,7,23,17,159,6.91,0,1,"['close', 'timelag']","[None, None]","[None, '1']",901,[],"['np.shape', 'len']",2
utilmy/zarchive/portfolio.py:getlogret_fromquotes,getlogret_fromquotes,function,4,7,7,81,11.57,0,0,"['close', 'timelag']","[None, None]","[None, '1']",912,[],"['np.shape', 'np.log']",2
utilmy/zarchive/portfolio.py:getprice_fromret,getprice_fromret,function,12,23,21,190,8.26,2,0,"['ret', 'normprice']","[None, None]","[None, '100']",917,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/portfolio.py:price_normalize100,price_normalize100,function,16,43,36,292,6.79,1,1,"['ret', 'normprice']","[None, None]","[None, '100']",929,[],"['np.shape', 'len', 'np.reshape', 'np.zeros', 'range', 'np.cumprod']",6
utilmy/zarchive/portfolio.py:price_normalize_1d,price_normalize_1d,function,8,22,19,147,6.68,0,0,"['ret', 'normprice', 'dtype1']","[None, None, None]","[None, '100', ' np.float32']",949,[],"['np.empty', 'np.cumprod']",2
utilmy/zarchive/portfolio.py:norm_fast,norm_fast,function,7,8,8,85,10.62,0,0,"['y', 'ny']","[None, None]","[None, None]",958,[],['ne.evaluate'],1
utilmy/zarchive/portfolio.py:correl_fast,correl_fast,function,8,13,13,116,8.92,0,0,"['xn', 'y', 'nx']","[None, None, None]","[None, None, None]",966,[],['ne.evaluate'],1
utilmy/zarchive/portfolio.py:volhisto_fromret,volhisto_fromret,function,2,14,11,168,12.0,0,1,"['retbsk', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",974,[],"['len', 'np.std']",2
utilmy/zarchive/portfolio.py:volhisto_fromprice,volhisto_fromprice,function,6,25,17,245,9.8,0,1,"['price', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",981,[],"['len', 'np.shape', 'np.std']",3
utilmy/zarchive/portfolio.py:volhistorolling_fromprice,volhistorolling_fromprice,function,16,42,30,318,7.57,2,1,"['price', 'volrange']","[None, None]","[None, None]",990,[],"['np.shape', 'len', 'np.zeros', 'xrange', 'volhisto_fromprice']",5
utilmy/zarchive/portfolio.py:rsk_calc_all_TA,rsk_calc_all_TA,function,51,223,105,1352,6.06,0,10,['df'],[None],"[""'panda_dataframe'""]",1009,"[""  '''Add All TA RMI, RSI To the '''\n""]","['ta.MA', 'ta.distance', 'ta.RET', 'ta.RMI', 'ta.STDDEV', 'ta.nbday_low', 'ta.nbday_high', 'ta.RSI', 'ta.qearning_dist', 'ta.optionexpiry_dist', 'linearreg2', 'np.array', 'np.sum', 'util.np_findlocalmin2', 'len', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",18
utilmy/zarchive/portfolio.py:ta_highbandtrend1,ta_highbandtrend1,function,42,170,75,910,5.35,0,11,"['close2', 'type1']","[None, None]","[None, '0']",1091,[],"['linearreg', 'np.array', 'np.sum', 'util.np_findlocalmax2', 'len', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",8
utilmy/zarchive/portfolio.py:pd_transform_asset,pd_transform_asset,function,11,35,27,409,11.69,0,1,"['q0', 'q1', 'type1']","[None, None, None]","[None, None, '""spread""']",1140,[],"['util.pd_date_intersection', 'util.pd_array_todataframe', 'util.pd_insertcol']",3
utilmy/zarchive/portfolio.py:calcbasket_table,calcbasket_table,function,25,59,53,477,8.08,1,2,"['wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps']","[None, None, None, None, None, None]","[None, None, '""table""', '""constant""', '1', ' 0.000']",1154,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.mod', 'np.sum']",6
utilmy/zarchive/portfolio.py:folio_createvolta_asset,folio_createvolta_asset,function,10,19,19,159,8.37,1,0,"['close', 'vol', 'volrange', 'lev']","[None, None, None, None]","[None, '0.12', '120', '1.0']",1181,[],"['np.zeros_like', 'np.shape', 'xrange', 'folio_volta']",4
utilmy/zarchive/portfolio.py:folio_lowcorrelation,folio_lowcorrelation,function,145,431,300,3894,9.03,8,7,"['sym01', 'nstock', 'periodlist', 'dateref', 'close1', 'kbenchmark', 'badlist', 'costbppa', 'showgraph']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, '0.02', 'True']",1189,[],"['ww_selection', 'np.corrcoef', 'np.nan_to_num', 'np.zeros', 'range', 'util.np_sortbycol', 'int', 'np.std', 'np.isnan', 'util.find', 'np.sum', 'stk_select.append', 'volhisto_fromret', 'sum', '1/len', 'getret_fromquotes', 'len', 'np.empty', 'np.ones', 'enumerate', 'calcbasket_table', 'price_normalize100', 'np.column_stack', 'np.zeros_like', 'np.array', 'np.concatenate', 'print', 'str', 'plot_price', 'sym01[int', 'volhisto_fromprice', 'xrange', 'folio_inverseetf']",33
utilmy/zarchive/portfolio.py:folio_leverageetf,folio_leverageetf,function,8,23,20,156,6.78,1,0,"['price', 'lev', 'costpa']","[None, None, None]","[None, '1.0', '0.0']",1327,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/portfolio.py:folio_inverseetf,folio_inverseetf,function,8,18,18,151,8.39,1,0,"['price', 'costpa']","[None, None]","[None, '0.0']",1336,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/portfolio.py:folio_longshort_unit,folio_longshort_unit,function,59,240,112,1594,6.64,2,7,"['long1', 'short1', 'ww', '-1]', 'costpa', 'tlag', 'istable', 'wwschedule']","[None, None, None, None, None, None, None, None]","[None, None, '[1', None, '0.0', '1', '1', '[]']",1345,[],"['len', 'np.zeros', 'xrange', 'np.mod', 'folio_longshort_unitfixed']",5
utilmy/zarchive/portfolio.py:folio_longshort_unitfixed,folio_longshort_unitfixed,function,43,111,73,719,6.48,1,3,"['long1', 'short1', 'nn', '-1]', 'costpa', 'tlag', 'istable']","[None, None, None, None, None, None, None]","[None, None, '[1', None, '0.0', '1', '1']",1386,[],"['len', 'np.zeros', 'xrange', 'np.mod']",4
utilmy/zarchive/portfolio.py:folio_longshort_pct,folio_longshort_pct,function,8,21,20,189,9.0,1,0,"['long1', 'short1', 'ww', '-1]', 'costpa']","[None, None, None, None, None]","[None, None, '[1', None, '0.0']",1424,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/portfolio.py:folio_histogram,folio_histogram,function,13,75,41,914,12.19,0,0,['close'],[None],[None],1432,[],"['getret_fromquotes', 'plt.subplots', 'np.ones_like', 'ax2.hist', 'ax2.set_title', 'ax2.set_xlabel', 'ax2.set_ylabel', 'ax2.axvline', 'plt.show', 'print']",10
utilmy/zarchive/portfolio.py:folio_voltarget,folio_voltarget,function,2,6,6,61,10.17,0,0,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1460,[],['folio_volta'],1
utilmy/zarchive/portfolio.py:folio_volta,folio_volta,function,54,197,112,1663,8.44,2,8,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1464,[],"['folio_volta', 'len', 'np.shape', 'np.zeros', 'range', 'np.std', 'util.findhigher', 'max', 'min', 'folio_volta2', 'np.abs']",11
utilmy/zarchive/portfolio.py:folio_volta2,folio_volta2,function,29,61,54,514,8.43,1,2,"['bsk', 'riskind', 'par', 'targetvol', 'volrange', 'cap', 'floor', 'costbp']","[None, None, None, None, None, None, None, None]","[None, None, None, '0.11', ' 90', '1.5', '0.0', '0.0005']",1503,[],"['np.shape', 'np.zeros', 'range', 'np.std', 'max', 'min', 'np.abs']",7
utilmy/zarchive/portfolio.py:folio_fixedunitprice,folio_fixedunitprice,function,13,36,33,221,6.14,2,0,"['price', 'fixedww', 'costpa']","[None, None, None]","[None, None, '0.0']",1528,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/portfolio.py:folio_fixedweightprice,folio_fixedweightprice,function,13,36,33,225,6.25,2,0,"['price', 'fixedww', 'costpa']","[None, None, None]","[None, None, '0.0']",1542,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/portfolio.py:folio_fixedweightret,folio_fixedweightret,function,13,31,28,193,6.23,2,0,"['ret', 'fixedww']","[None, None]","[None, None]",1554,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/portfolio.py:folio_cost_turnover,folio_cost_turnover,function,26,88,71,825,9.38,1,1,"['wwall', 'bsk', 'dateref', 'costbp']","[None, None, None, None]","[None, None, None, None]",1567,[],"['len', 'np.zeros', 'xrange', 'np.sum', 'plot_price', 'date1=dateint_tostring']",6
utilmy/zarchive/portfolio.py:folio_riskpa,folio_riskpa,function,20,52,40,422,8.12,2,0,"['ret', 'targetvol', 'volrange', 'cap']","[None, None, None, None]","[None, '0.1', '90', '1.0']",1597,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.sum', 'np.std', 'min', 'list']",8
utilmy/zarchive/portfolio.py:folio_perfreport_schedule,folio_perfreport_schedule,function,14,20,19,189,9.45,1,0,"['sym', 'dateref', 'close', 'wwind', 't0', 'scheduleperiod']","[None, None, None, None, None, None]","[None, None, None, None, None, '""1monthend""']",1911,[],"['np.shape', 'generate_schedule', 'np.zeros', 'enumerate']",4
utilmy/zarchive/portfolio.py:folio_concenfactor2,folio_concenfactor2,function,7,12,12,103,8.58,0,0,"['ww', 'masset']","[None, None]","[None, '12']",2211,[],"['getweight', 'np.sum', 'np.mean']",3
utilmy/zarchive/portfolio.py:calcbasket_objext,calcbasket_objext,function,50,203,96,1094,5.39,1,7,"['RETURN', 'TMAX', 'riskind_i', 'wwmat', 'wwasset0', 'ww0', 'nbrange', 'criteria']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",2229,[],"['xrange', 'np.sum', 'math.sqrt']",3
utilmy/zarchive/portfolio.py:calc_ranktable,calc_ranktable,function,20,44,41,351,7.98,1,0,"['close2', 'symjp1', 'nlag', 'refindex', 'funeval', 'funargs']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",3639,[],"['np.zeros', 'getlogret_fromquotes', 'enumerate', 'funeval', 'util.np_sortbycolumn']",5
utilmy/zarchive/portfolio.py:similarity_correl,similarity_correl,function,7,10,9,88,8.8,0,0,"['ret_close2', 'funargs']","[None, None]","[None, None]",3659,[],['correlation_mat'],1
utilmy/zarchive/portfolio.py:np_similarity,np_similarity,function,6,17,12,138,8.12,0,3,"['x', 'y', 'wwerr', 'type1']","[None, None, None, None]","[None, None, '[]', '0']",3714,[],['np_distance_l1'],1
utilmy/zarchive/portfolio.py:np_distance_l1,np_distance_l1,function,2,3,3,31,10.33,0,0,"['x', 'y', 'wwerr']","[None, None, None]","[None, None, None]",3720,[],"['np.sum', 'np.abs']",2
utilmy/zarchive/portfolio.py:np_similarity,np_similarity,function,6,17,12,138,8.12,0,3,"['x', 'y', 'wwerr', 'type1']","[None, None, None, None]","[None, None, '[]', '0']",3714,[],['np_distance_l1'],1
utilmy/zarchive/portfolio.py:np_distance_l1,np_distance_l1,function,2,3,3,31,10.33,0,0,"['x', 'y', 'wwerr']","[None, None, None]","[None, None, None]",3720,[],"['np.sum', 'np.abs']",2
utilmy/zarchive/portfolio.py:get,get,function,11,44,26,436,9.91,0,1,"['close', 'timelag']","[None, None]","[None, None]",4085,"['    """"""Return dataframe of requested dataset from Quandl.\n', '    :param dataset: str or list, depending on single dataset usage or multiset usage\n', '            Dataset codes are available on the Quandl website\n', '    :param str api_key: Downloads are limited to 50 unless api_key is specified\n', '    :param str start_date, end_date: Optional datefilers, otherwise entire dataset is returned\n', '    :param str collapse: Options are daily, weekly, monthly, quarterly, annual\n', '    :param str transform: options are diff, rdiff, cumul, and normalize\n', '    :param int rows: Number of rows which will be returned\n', '    :param str order: options are asc, desc. Default: `asc`\n', '    :param str returns: specify what format you wish your dataset returned as,\n', '        either `numpy` for a numpy ndarray or `pandas`. Default: `pandas`\n', '    :returns: :class:`pandas.DataFrame` or :class:`numpy.ndarray`\n', '    Any other `kwargs` passed to `get` are sent as field/value params to Quandl\n', '    with no interference.\n', '\n', ""'''\n"", '\n', '\n', '\n', '#--------------------Import data from Yahoo --------------------------------------------------\n', 'def imp_yahoo_getquotes(symbols, start=""20150101"", end=""20160101"", type1=1):\n', ' d1 = datetime.datetime(int(start[0:4]), int(start[4:6]), int(start[6:8]))\n', ' d2 = datetime.datetime(int(end[0:4]), int(end[4:6]), int(end[6:8]))\n', '# symbols = np.array(symbol1)\n', ' quotes= []; errorlist=[]; correctlist=[]\n', ' for i, symbol in enumerate(symbols) :\n', '   # if np.mod(i,400) ==0 and i != 0 :\n', ""     # print('Waiting 30s'); time.sleep(30)\n"", '   try:   \n', '     array1= quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n', '     df= pd.DataFrame.from_records(array1)\n', '     quotes.append(df)\n', '     correctlist.append(symbol)\n', '   except :\n', '       errorlist.append(symbol)\n', ""       print('Err: '+symbol)\n"", ' #quotes = [quotes_historical_yahoo(symbol, d1, d2, asobject=True)  for symbol in symbols]\n', ' #print(errorlist) \n', ' if type1==1 :  return quotes, correctlist\n', ' else: return quotes, correctlist, errorlist  \n', '\n', '              \n', '#Find error in Ticker import\n', 'def imp_yahoo_geterrorticker(symbols, start=""20150101"", end=""20160101""):\n', ' d1 = datetime.datetime(int(start[0:4]), int(start[4:6]), int(start[6:8]))\n', ' d2 = datetime.datetime(int(end[0:4]), int(end[4:6]), int(end[6:8]))\n', ' errorlist= []\n', ' for symbol in symbols :\n', '   try :  \n', '     quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n', '   except :\n', '     errorlist.append(symbol)\n', '     print(symbol)       \n', ' return errorlist\n', ' \n', ""'''\n"", ""symbols0= ['KHC', 'PYPL', 'HPE', 'BXLT', 'SYF', 'WLTW', 'CFG', 'BF-B', 'WRK']\n"", '\n', '#Show list of ticker in error\n', 'import_errorticker(symbols0,""20110101"",""20150601""  )\n', ""'''\n"", '\n', '\n', 'def imp_yahoo_financials_url(ticker_symbol, statement=""is"", quarterly=False):\n', '    if statement == ""is"" or statement == ""bs"" or statement == ""cf"":\n', '        url = ""https://finance.yahoo.com/q/"" + statement + ""?s="" + ticker_symbol\n', '        if not quarterly:\n', '            url += ""&annual""; return BeautifulSoup(requests.get(url).text, ""html.parser"")\n', '    return sys.exit(""Invalid financial statement code \'"" + statement + ""\' passed."")\n', '\n', 'def imp_yahoo_periodic_figure(soup, yahoo_figure):\n', '    values = []; pattern = re.compile(yahoo_figure)\n', '\n', '    title = soup.find(""strong"", text=pattern)    # works for the figures printed in bold\n', '    if title:  row = title.parent.parent\n', '    else:\n', '        title = soup.find(""td"", text=pattern)    # works for any other available figure\n', '        if title:  row = title.parent\n', '        else:      sys.exit(""Invalid figure \'"" + yahoo_figure + ""\' passed."")\n', '\n', '    cells = row.find_all(""td"")[1:]    # exclude the <td> with figure name\n', '    for cell in cells:\n', '        if cell.text.strip() != yahoo_figure:    # needed because some figures are indented\n', '            str_value = cell.text.strip().replace("","", """").replace(""("", ""-"").replace("")"", """")\n', '            if str_value == ""-"": str_value = 0\n', '            value = int(str_value) * 1000\n', '            values.append(value)\n', '\n', '    return values\n', '    \n', '# print(imp_yahoo_periodic(imp_yahoo_financials_url(""AAPL"", ""is""), ""Income Tax Expense""))\n', '\n', ' \n', '\n', '#--------------------Import Quotes Google  ------------------------------------------\n', 'import requests.packages.urllib3\n', 'requests.packages.urllib3.disable_warnings()\n', 'import urllib, os\n', '\n', ""dirstockcsv= 'E:\\_data\\stock\\csv'\n"", '\n', 'def imp_googleIntradayQuote(symbol, freqsec=300, nday=5):\n', ""    ''' Intraday quotes from google. Specify interval seconds and number of days\n"", '    http://www.google.com/finance/getprices?q=7261&i=300&p=3d&f=d,o,h,l,c,v\n', ""    '''\n"", '    url =""http://www.google.com/finance/getprices?q={0}"".format(symbol)\n', '    url +=""&i={0}&p={1}d&f=d,o,h,l,c,v"".format(freqsec, nday)\n', '    try :      \n', '       resp= requests.post(url)\n', '       csv= resp.text\n', '       csv= csv.splitlines()\n', '    except :\n', ""       print('Err Intaday '+symbol)\n"", '\n', '    if len(csv) < 9 :\n', ""      print 'Using Daily',\n"", '\n', ""      url='http://www.google.com/finance/historical?q='+symbol+'&i='+str(freqsec)+'&p=3d&f=d,o,h,l,c,v&output=csv'\n"", '      csv = urllib.urlopen(url).readlines()\n', '      csv.reverse()\n', '      q=[]\n', '      for bar in xrange(0,len(csv)-1):\n', ""        ds,open_,high,low,close,volume = csv[bar].rstrip().split(',')\n"", '        if isfloat(open_) and isfloat(high) :\n', '          open_,high,low,close = [float(x) for x in [open_,high,low,close]]\n', ""          dt = datetime.datetime.strptime(ds,'%d-%b-%y')\n"", '          q.append([dt,open_,high,low,close,volume])\n', ""      df= util.pd_array_todataframe(q, ['date','open','high','low','close','volume'])\n"", '      return df\n', '\n', '\n', '    # print(csv)\n', '    qq=[]\n', '    for bar in xrange(7,len(csv)):\n', ""      if csv[bar].count(',')!=5: continue\n"", ""      offset,close,high,low,open_,volume = csv[bar].split(',')\n"", '      \n', ""      if offset[0]=='a':\n"", '        day = float(offset[1:])\n', '        offset = 0\n', '      else:\n', '        offset = float(offset)\n', '      open_, high, low, close = [float(x) for x in [open_,high,low,close]]\n', '      dt = datetime.datetime.fromtimestamp(day + (freqsec * offset))\n', '      #print (dt,open_,high,low,close,volume)\n', '      qq.append([dt,open_,high,low,close,volume])\n', ""    df= util.pd_array_todataframe(qq, ['date','open','high','low','close','volume'])\n"", '    return df\n', '\n', ""def imp_googleIntradayQuoteSave(symbols=['NKE'], freqsec=300, nday=2000, dircsv='', dbname=''):\n"", "" ''' Save Under various format: csv / db / output in Dataframe '''\n"", ' if isinstance(symbols, str) : symbols= [symbols]\n', ' symout,qlist, sym_error= [], [], []\n', ' for sym in symbols:\n', '   try :\n', '    q = imp_googleIntradayQuote(sym, freqsec, nday)  #interval, timeframe\n', '    print sym,\n', ""    sym= sym.replace(':', '_')\n"", '\n', ""    if dircsv != '' :\n"", '      if not os.path.isdir(dircsv) :  os.makedirs(dircsv)\n', ""      start1= util.datetime_toint(util.datenumpy_todatetime(q['date'].values[0]))\n"", ""      file1= dircsv+ '\\\\' + sym + '_' + str(start1) + '_' + str(freqsec) + '_' + str(nday) + '.csv'\n"", '\n', ""      q= util.pd_addcol(q, 'symbol'); q['symbol']= sym\n"", '      q.to_csv(file1, index=False)\n', '\n', ""    elif dbname != '' :\n"", '      import sqlalchemy as sql\n', '      dbcon= sql.create_engine(dbname)\n', '      if interv_sec== 300 : # 5mins\n', ""        q.to_sql('q5min', dbcon)\n"", '\n', '    else :\n', '      qlist.append(q);   symout.append(sym)\n', '   except:\n', ""      print('Error:'+sym, )\n"", '      sym_error.append(sym)\n', '\n', ' return qlist, symout, sym_error\n', '\n', ""def imp_googleQuote(symbol, start_date='20160101', end_date= '20160101') : # datetime.date.today().isoformat()):\n"", ""    ''' Daily quotes from google. Date format='yyyymmdd' '''\n"", '    symbol = symbol.upper()\n', '    start = datetime.date(int(start_date[0:4]),int(start_date[4:6]),int(start_date[6:8]))\n', '    end =   datetime.date(int(end_date[0:4]),int(end_date[4:6]),int(end_date[6:8]))\n', '    url =""http://www.google.com/finance/historical?q={0}"".format(symbol)\n', '    url +=""&startdate={0}&enddate={1}&output=csv"".format( start.strftime(\'%b %d, %Y\'),end.strftime(\'%b %d, %Y\'))\n', '    csv = urllib.urlopen(url).readlines()\n', '    csv.reverse()\n', '    # print(url)\n', '    q=[]\n', '    for bar in xrange(0,len(csv)-1):\n', ""      ds,open_,high,low,close,volume = csv[bar].rstrip().split(',')\n"", '      if isfloat(open_) and isfloat(high) :\n', '        open_,high,low,close = [float(x) for x in [open_, high, low, close]  ]\n', ""        dt = datetime.datetime.strptime(ds,'%d-%b-%y')\n"", '        q.append([dt,open_,high,low,close,volume])\n', '    return  pd.DataFrame(np.array(q), columns= [""date"",""open"",""high"",""low"",""Close"",""volume""])\n', '\n', '\n', 'def imp_googleQuoteSave(symbols, date1, date2, dircsv):\n', ' if isinstance(symbols, str) : symbols= [symbols]\n', ' for sym in symbols:\n', '   try :\n', '      q = imp_googleQuote(sym, date1, date2)   #interval, timeframe\n', ""   except: print('Error:'+symbol)\n"", '\n', ""   sym= sym.replace(':', '_')\n"", '\n', ""   if dircsv != '' :\n"", '     if not os.path.isdir(dircsv) :  os.makedirs(dircsv)\n', ""     start1= util.datetime_toint(util.datenumpy_todatetime(q['date'].values[0]))\n"", ""     file1= dircsv+ '\\\\'+ name1+'_'+ start1 +'_'+ str(date2) +'.csv'\n"", '     q.to_csv(file1)\n', '\n', ""   if dbname != '' :\n"", '     import sqlalchemy as sql\n', '     dbcon= sql.create_engine(dbname)\n', '     if interv_sec== 300 : # 5mins\n', ""       q.to_sql('q5min', dbcon)\n"", '\n', '\n', '\n', ""def imp_csv_dbupdate(indir='E:/_data/stock/intraday/intraday_google_usetf2.h5',\n"", ""                     outdir='E:/_data/stock/intraday/q5min/us/etf/', filelist=[], intype='csv',\n"", ""                     refcols=['date', 'open', 'high', 'low', 'close', 'volume', 'symbol']) :\n"", '\n', '  for file1 in filelist :\n', ""    sym=  file1[:file1.find('_')]  #Clean Up name\n"", '    print sym,\n', ""    outfile= outdir+'/'+sym+'.csv'\n"", '    if os.path.isfile(outfile) :\n', ""      df0= imp_csv_toext(file1=outfile, fromzone='Japan', tozone='UTC', header=0, cols=refcols)\n"", '    else :\n', '      df0= None\n', '\n', ""    if intype== 'hdfs':\n"", '      df1= imp_hdfs_getquote(indir, sym)\n', '      if isinstance(df1, float) : df1= None\n', '\n', ""    if intype=='csv' :\n"", ""      infile= indir+'/'+ file1   #csv\n"", ""      df1= imp_csv_toext(file1=infile, header=0, cols=refcols, fromzone='Japan', tozone='UTC')\n"", '\n', '    if df1 is not None :\n', '       if df0 is not None  : df1= imp_pd_merge(df1[refcols], df0[refcols])\n', '       # return outfile, df1\n', '       df1[refcols].to_csv(outfile, index=False)\n', '\n', '\n', '#####################################################################################\n', 'def imp_numpy_close_fromdb(dbname=\'/aaserialize/store/yahoo.db\', table1=\'\', symlist=[], t0=20010101, t1=20010101, priceid=""close"", batchsize= 400, maxasset=2600, tmax2=2000) :\n', ' print(""Get Numpy Matrix Close from DB SQL"")\n', ' import data.hist_data_storage as yhh\n', ' dstore= yhh.dailyDataStore(dbname)\n', ' close2= np.zeros((maxasset,tmax2),dtype=np.float16)\n', '\n', "" dateref0= dstore.get_histo(['SPY'], table1='daily_us_etf', start1=str(t0), end1=str(t1),  split_df=0).date.values\n"", ' \n', ' k=0; symfull=[];   i=0\n', ' nsym= len(symlist); nbatch= int(nsym / batchsize)+1\n', ' for j in xrange(0, nbatch) : \n', '      qlist, sym= dstore.get_histo(symlist[i:min(nsym,i+batchsize)], table1=table1, start1=str(t0), end1=str(t1),  split_df=1)\n', '\n', '      #print(""batch: ""+str(j), len(qlist), len(qlist[0].index) )\n', '      #for k,q in enumerate(qlist) :print sym[k], len(q.index)\n', '      i= i + batchsize\n', '      symfull= np.concatenate((symfull,sym))\n', '      close,dateref= date_align(qlist, dateref= dateref0, type1=priceid)\n', '      print(""batch: ""+str(j), len(sym),dateref[0],dateref[-1] )\n', '      qlist=None; sym=None\n', '      masset,tmax= np.shape(close)\n', '      if tmax2 < tmax: print(""Error tmax2 < tmax, wrong dates""); break\n', '      close2[k:k+masset,0:tmax] = close\n', '      k=k+ masset\n', '\n', ' maxasset= k\n', ' tmaxx= util.find(0.0, close2[0,:])\n', ' for k in xrange(0,maxasset):\n', '   if close2[k,tmaxx-1] == 0.0 : close2[k,tmaxx-1]=   close2[k,tmaxx-2]\n', ' \n', ' return close2[0:maxasset,0:tmaxx], symfull, dateref\n', '\n', 'def imp_numpy_close_fromhdfs(dbfile, symlist=[], t0=20010101, t1=20010101, priceid=""close"", maxasset=2600, tmax2=2000) :\n', ' print(""Get Numpy Matrix Close from Pandas"")\n', ' close2= np.zeros((maxasset,tmax2),dtype=np.float16)\n', ' store = pd.HDFStore(dbfile)\n', ' \n', "" df= store.select('SPY')\n"", ' df= df[(df.date >= t0) & (df.date <= t1 )]\n', ' dateref0= df.date.values\n', ' \n', ' k=0; symfull=[]; qlist=[]; sym=[]; tmax= 2000\n', ' for j, symbol in enumerate(store.keys()):\n', '  symbol=  symbol[1:]\n', '\n', '  if len(symlist)==0  or  util.find(symbol,symlist) > -1 : \n', '   #print symbol, util.find(symbol,symlist) \n', '   df= store.select(symbol) \n', '   if t0 !=t1 :     df= df[(df.date >= t0) & (df.date <= t1 )]\n', '   \n', '   qlist.append(df);   sym.append(symbol)\n', '   if np.mod(j+1, 401) == 0 :\n', '      print(""batch: ""+str(j))\n', '      close,dateref= date_align(qlist, dateref= dateref0, type1=priceid)   \n', '      print len(sym),dateref[0],dateref[-1] \n', '      symfull= symfull+sym\n', '      qlist=[]; sym=[]\n', ' \n', '      masset,tmax= np.shape(close)\n', '      if tmax2 < tmax: print(""Error tmax2 < tmax, wrong dates""); break\n', '      close2[(k):(k+masset),0:tmax] = close\n', '      k=k+ masset   \n', '     \n', ' if len(qlist) > 0:\n', '  # return qlist, qlist[0] ,  qlist[1] \n', '   close,dateref= date_align(qlist,type1=priceid)   \n', '   print len(sym),dateref[0],dateref[-1] \n', '   symfull= symfull+sym\n', '   masset,tmax= np.shape(close);\n', '   close2[(k):(k+masset),0:tmax] = close\n', ' \n', ' maxasset= k+masset\n', ' tmaxx= util.find(0.0, close2[0,:])\n', ' for k in xrange(0,maxasset):\n', '   if close2[k,tmaxx-1] == 0.0 : close2[k,tmaxx-1]=   close2[k,tmaxx-2]\n', ' \n', ' return close2[0:maxasset,0:tmaxx], symfull, dateref\n', '\n', '\n', ""def imp_sql_getquotes(stocklist01, dbname='sqlite:///aaserialize/store/yahoo.db', start1=20150101, end1=20160616, table1='daily_us_stock'):\n"", '    import data.hist_data_storage as yhh\n', '    dstore = yhh.dailyDataStore(dbname)\n', '    qlist, sym= dstore.get_histo(stocklist01, table1=table1,  start1=start1, end1=end1, split_df=1)\n', '    return qlist, sym\n', '\n', ""def imp_txt_getquotes(stocklist01, filedir='E:/_data/stock/daily/20160610/jp', startdate=20150101, endate=20160616):\n"", ' liststockfile= util.listallfile(filedir, ""*.txt"", dirlevel=5)\n', ' #print liststockfile\n', ' if len(np.shape(liststockfile)) > 1 :\n', '  liststockname=[ x.split(""."")[0] for x in liststockfile[:,0] ]\n', '#  for k in range(0, np.shape(liststockfile)[0]):  liststockname.append( (liststockfile[k,0])[0:4])   \n', '   \n', ' else: \n', '  stk0= liststockfile[0]; #stk2= liststockfile[2]\n', '#  liststockname=[ x[0:4] for x in stk0 ]\n', '  liststockname=[ x for x in stk0 ]\n', '\n', '#  liststockname=[]\n', '#  for k in range(0, len(stk0)):  liststockname.append( (stk0[k])[0:4])\n', ' \n', ' quotes=[]\n', ' print liststockname\n', ' for sym in stocklist01:\n', '   kstock= util.np_findfirst(sym, liststockname)\n', '   if kstock == -1 :\n', ""     print('Not Found '+sym)\n"", '   else :\n', '     df= pd.read_csv(liststockfile[kstock][2])\n', ""     df.columns = ['date', 'open', 'high','low','close', 'volume', 'openint']  #Change Column Name\n"", '     df2= df[(df.date > startdate  )& ( df.date   < endate )] # Filter by date\n', '     quotes.append(df2)\n', '  \n', ' return quotes \n', '\n', '\n', 'def imp_csv_getname(name1, date1, inter, tframe):\n', "" file1= dirstockcsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(inter) +'_'+ str(tframe) +'.csv'\n"", ' return file1\n', '\n', '\n', ""def imp_csv_toext(file1='SPY.csv', outputfile='.h5', fromzone='Japan', tozone='UTC', header=None,\n"", ""                  cols=['date', 'time','open','high','low','close','volume', 'symbol'], coldate=[0]):\n"", ""  ''' cols: column name,  coldate: position of date column   '''\n"", '  from dateutil import parser\n', '\n', '  if os.path.getsize(file1)> 500 :\n', ""   df = pd.read_csv(file1,sep=',',header=header)  # date_parser=dateparse)  # parse_dates={'date': [] },\n"", '   df.date= [ parser.parse(x) for x in  df.date]\n', '\n', ""   #if util.find('symbol', df.columns.values) < 0 :\n"", ""     #df= util.pd_addcol(df, 'symbol')\n"", ""     #df['symbol']= sym\n"", '\n', ""   type1= outputfile[outputfile.find('.'):]\n"", '\n', ""   if outputfile=='':\n"", '     # df.columns = [  x.lower() for x in df.columns.values ]\n', ""     # df.columns = ['date', 'symbol','open','high','low','close','volume']\n"", '     df.drop(df.columns[dropcol], axis=1, inplace=True)\n', ""     df= util.pd_addcol(df, 'symbol');  df['symbol']= dfname\n"", ""     df.columns = ['date', 'open','high','low','close','volume', 'symbol']\n"", '     return df[cols]\n', '\n', ""   if type1=='csv' :\n"", ""     df[cols].to_csv(outputfile , index=False)  # , compression='gzip' )\n"", '\n', ""   if type1== 'h5' :\n"", ""      # dateparse= lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '      dateparse= lambda x: (parse(x).replace(tzinfo=from_zone).astimezone(tozone))\n', '      # dateparse= lambda x: parse(x, tzinfos=from_zone).astimezone(to_zone)\n', '      # df.date= [pd.to_datetime((str(x)[:-6])) for x in  df.date]\n', '      # df.date= [x.to_datetime() for x in  df.date]\n', '\n', '      df.drop(df.columns[[0]], axis=1, inplace=True)\n', '      print filenameh5\n', ""      if util.find('symbol', df.columns.values) < 0 :\n"", ""         df= util.pd_addcol(df, 'symbol'); df['symbol']= dfname\n"", '      store = pd.HDFStore(outputfile); store.append(dfname, df);  store.close()\n', '\n', '\n', '\n', ""def imp_hdfs_db_updatefromcsv(dircsv, filepd=r'E:\\_data\\stock\\intraday_google.h5', fromtimezone='Japan', tozone='UTC'):\n"", ' lfile= util.os_file_listall(dircsv, pattern=""*.csv"", dirlevel=0)\n', ' for x in lfile:\n', '   name1= (x[0]).split(""_"")[0] #get Ticker name, splitting by _\n', ""   if util.str_isfloat(name1):  name1= 'jp'+name1  #Japanese Stocks case\n"", '   file1 = x[2];  print(name1)\n', '   if os.path.getsize(file1)> 500:\n', ""      filepd2= filepd + '/' +name1 +'.csv'\n"", '      imp_csv_toext(file1, filepd2, name1, fromtimezone, tozone)\n', '\n', ""def imp_hdfs_db_dumpinfo(dbfile='E:\\_data\\stock\\intraday_google.h5'):\n"", '  store = pd.HDFStore(dbfile)\n', '  extract=[]; errsym=[]\n', '  for symbol in store.keys():\n', '     try:\n', '       df= pd.read_hdf(dbfile, symbol)\n', ""       t0= df['date'].values[0]\n"", ""       t1= df['date'].values[-1]\n"", '\n', '       extract.append([symbol[1:], df.shape[1],   df.shape[0], t0, t1 ])\n', '\n', '     except: errsym.append(symbol)\n', '  return np.array(extract), errsym\n', '\n', 'def imp_hdfs_mergedb(filepdfrom, filepdto) :\n', '  store0 = pd.HDFStore(filepdfrom)\n', '  store1 = pd.HDFStore(filepdto)\n', '\n', '  for symbol in store0.keys():\n', '    qq= imp_hdfs_getquote(filepdfrom, symbol)\n', '    store1.append(symbol, qq)\n', '\n', '    qq= imp_hdfs_getquote(filepdto, symbol)\n', ""    qq= qq.drop_duplicates(subset='date', take_last=True)\n"", ""    qq= qq.sort('date', ascending=1)\n"", '    print(symbol)\n', '  store0.close(); store1.close()\n', '\n', ""def imp_hdfs_storecopy(hdfs1='F:/usstock8.h5', hdfs2='F:/usstock8.h5'):\n"", ' store = pd.HDFStore(hdfs1);  store2 = pd.HDFStore(hdfs2)\n', ' for symbol in store.keys() :\n', '   df= imp_hdfs_getquote(hdfs1, symbol)\n', '   symbol= symbol[1:].replace(""-"",""_"")\n', '   store2.append(symbol, df, data_columns=True)\n', ' store.close(); store2.close()\n', '\n', 'def imp_hdfs_getquote(filenameh5, dfname=""data""):\n', '  try: return  pd.read_hdf(filenameh5, dfname)\n', '  except: return -1.0;\n', '\n', ""def imp_hdfs_getListquote(symbols, close1='close', start='12/18/2015 00:00:00+00:00',\n"", ""                          end='3/1/2016 00:00:00+00:00', freq='0d0h10min', filepd= 'E:\\_data\\stock\\intraday_google.h5',\n"", ""                          tozone='Japan', fillna=True, interpo=True):\n"", '\n', ' datefilter= pd.date_range(start=start, end=end, freq= freq,tz=tozone).values\n', '\n', ' errorsym=[]; quotes=[]; correctsym=[]\n', ' for symbol in symbols:  # Issue Not same size\n', '   qq= imp_hdfs_getquote(filepd, symbol)\n', '\n', '   if type(qq)==np.float :  # Error qq=-1\n', '        errorsym.append(symbol)\n', '   else:\n', ""       qq.columns = ['date', 'symbol','open','high','low','close','volume']\n"", ""       qq= qq[qq['date'].isin(datefilter)]   #Only date in the range\n"", ""       #qq= qq.drop_duplicates(cols='date', take_last=True)\n"", ""       #  qq= qq.sort('date', asc=True)\n"", '\n', '       #print qq.date.values[0]\n', ""       if fillna : qq= qq.fillna(method='backfill')\n"", '       if interpo : qq= qq.interpolate()\n', '       quotes.append(qq)\n', '       correctsym.append(symbol)\n', '\n', ' datefilter= datenumpy_todatetime(datefilter)\n', ' return quotes,datefilter, correctsym,   errorsym\n', '\n', '\n', ""def imp_hdfs_removeDuplicate(filepd='E:\\_data\\stock\\intraday_google.h5') :\n"", '  #-------Clean by removing duplicate-------------------------------------------------\n', '  store = pd.HDFStore(filepd)\n', '  for symbol in store.keys():\n', '    #qq= imp_hdfs_getquote(filepd, symbol);\n', '    qq= store.select(symbol[1:])\n', ""    qq= qq.drop_duplicates(subset='date', take_last=True)\n"", ""    qq= qq.sort('date', ascending=1)\n"", '    qq.index = list(np.arange(0,len(qq.index)))\n', '\n', '    store.remove(symbol); store.append(symbol, qq);\n', '  store.close()\n', '\n', '\n', '\n', ""def imp_pd_tohdfs(sym, qqlist, filenameh5, fromzone='Japan', tozone='UTC') :\n"", ""  ''' df list to HDFS '''\n"", '  store = pd.HDFStore(filenameh5);\n', '  for k, df2 in enumerate(qqlist) :\n', '    df= copy.deepcopy(df2)\n', ""    df['date']= datetime_toint(df.date.values)\n"", '\n', ""    symbol= sym[k].replace('-','_')\n"", '    qq= imp_hdfs_getquote(filenameh5, symbol)\n', '    # print symbol, type(qq)\n', '    if type(qq) ==  float or  type(qq) ==  int  :\n', '      store.append(symbol, df)  # , data_columns=True\n', '      # print(symbol + str(df.date.values[-1]))\n', '    else :\n', '      qq= pd.concat([qq, df], ignore_index=True)\n', ""      qq= qq.drop_duplicates(subset='date', keep='last')\n"", ""      qq= qq.sort('date', ascending=1)\n"", '      store.append(symbol, qq)  #Too much space , data_columns=True\n', '      print(symbol + str(qq.date.values[-1]))\n', '\n', '  store.close()\n', '\n', 'def imp_pd_merge(df1, df2) :\n', '  df= pd.concat([df1, df2], axis=0)\n', ""  df= df.drop_duplicates(subset='date', keep='last')\n"", ""  df= df.sort_values(by=['date'], ascending=1)\n"", '  df.index = list(np.arange(0,len(df.index)))\n', '  return df\n', '\n', 'def imp_pd_checkquote(quotes) :\n', ""   for c in quotes: print np.shape(c), c['date'].values[0], c['date'].values[-1]\n"", '\n', 'def imp_pd_getclose(df, datefilter=None):\n', ' if datefilter is  not None :\n', ""    df= df[df['date'].isin(datefilter)]   #Only date in the range\n"", "" df= df.sort('date')\n"", "" close= df.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", ' \n', ' close= np.array(close.values[1:,1:]).astype(np.float)            #Only Price\n', ' close= close[(np.abs(sci.stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close\n', '\n', 'def imp_pd_cashyield(q, duration=10):\n', ' qbond= copy.deepcopy(q)\n', "" for id1 in ['close','open','high','low','aclose'] :\n"", '   try:\n', '     vv= q[id1].values\n', '     pbond=  100 * np.cumprod( (1 + np.ones(len(vv)) * 0.005/365.0 ))\n', '     qbond[id1]= pbond\n', '   except: pass\n', ' return qbond\n', '\n', 'def imp_pd_yield_tobond(q, duration=10):\n', ' qbond= copy.deepcopy(q)\n', "" for id1 in ['close','open','high','low','aclose'] :\n"", '   try:\n', '     vv= q[id1].values\n', '     pbond=  1/ np.power(1+ vv*0.01,duration) *100\n', '     qbond[id1]= pbond\n', ""#     print 'ok'\n"", '   except: pass\n', ' return qbond\n', '\n', 'def imp_pd_errordate(quotes, dateref):\n', "" ''' Show Symbol in Error when importing '''\n"", ' for i, stock in enumerate(quotes) :\n', '   print(i, symbols1[i],  datetime_tostring(stock[0][0]))\n', ' print(""\\n\\n"")\n', '\n', ' for i, stock in enumerate(quotes) :\n', '   date1= datetime_toint(stock[0][0])\n', '   if date1 != dateref : print(i, symbols1[i],  str(date1))\n', '\n', 'def imp_pd_fxtoprice(q):\n', ' dfprice= copy.deepcopy(q)\n', "" for kid in ['close','open','high','low'] :\n"", '   qret= getret_fromquotes(q[kid].values,1)\n', '   qprice= price_normalize100(qret)\n', '   dfprice[kid]= qprice.T\n', ' return dfprice\n', '\n', 'def imp_pd_fxinversetoprice(q):\n', ' dfprice= copy.deepcopy(q)\n', "" for kid in ['close','open','high','low'] :\n"", '   qret= getret_fromquotes(1.0 / q[kid].values,1)\n', '   qprice= price_normalize100(qret)\n', '   dfprice[kid]= qprice.T\n', ' return dfprice\n', '\n', ""def imp_pd_filterbydate(df, dtref=None, start='2016-06-06 00:00:00', end='2016-06-14 00:00:00', freq='0d0h05min', timezone='Japan'):\n"", "" ''' df: DateSeries or TimeSeries of Quotes   '''\n"", '\n', ' if type(df) in {pd.core.frame.DataFrame} :  #Data frame version\n', '   if type(dtref)== str:\n', '      dtref= pd.date_range(start=start, end=end, freq= freq,tz=timezone).values\n', '\n', ""   return df[df['date'].isin(dtref)]\n"", ' else :\n', '   if type(dtref)== str:  #Date version\n', '      dtref= pd.date_range(start=start, end=end, freq= freq,tz=timezone).values\n', '\n', '   return df[df.isin(dtref)].values\n', '\n', 'def imp_pd_cleanquote(q):\n', ' col= q.columns.values\n', "" if isinstance(q['date'].values[0] , str) :\n"", ""   q['date']= datetime_todate(datestring_todatetime(q['date'].values))\n"", '\n', ' for kid in col:\n', ""   if kid not in ['date', 'day','month','year'] :\n"", ""      q[kid]= pd.to_numeric(q[kid], errors='coerce').values  #Put NA on string\n"", '\n', "" q= q.fillna(method='pad')\n"", ' return q\n', '\n', 'def imp_pd_divide(q1, q2, funapply):\n', "" close, dateref= date_align([q1,q2], type1='close')\n"", ' m,tt= np.shape(close)\n', '\n', ' q3= util.pd_createdf( np.zeros((tt,len(q1.columns))) ,q1.columns, dateref)\n', "" q3['date']= dateref\n"", "" q3['close']= (close[0,:] / close[1,:])\n"", '\n', ' for kid in q1.columns :\n', ""   if kid not in ['close', 'volume','date','day','month', 'year'] :\n"", '     close, _= date_align([q1,q2], type1=kid)\n', '     q3[kid]= (close[0,:] / close[1,:])\n', ' return q3\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ""def imp_quote_csvdir_tofile(dircsv='', outdir1='E:/_data/stock/', fromtimezone='Japan', tozone='UTC'):\n"", ' lfile= util.os_file_listall(dircsv, pattern=""*.csv"", dirlevel=0)\n', ' for x in lfile:\n', '   name1= (x[0]).split(""_"")[0] #get Ticker name, splitting by _\n', ""   if util.str_isfloat(name1):  name1= 'jp'+name1  #Japanese Stocks case\n"", '   file1 = x[2];  print(name1)\n', '   if os.path.getsize(file1)> 500:\n', ""      filepd2= outdir + '/' +name1 +'.csv'\n"", '      imp_csv_toext(file1, filepd2,  header=True, fromzone=fromtimezone, tozone=tozone)\n', ""'''\n"", '\n', '\n', ""'''\n"", ""close1='Close' ; start='12/18/2015 00:00:00';\n"", ""end='3/1/2016 00:00:00'; freq='1d0h00min';\n"", ""filepd= 'E:\\_data\\stock\\intraday_google.h5'\n"", '\n', 'datefilter= pd.date_range(start=start, end=end,   freq= freq).values\n', '\n', 'symbols, names = np.array(list(symbol_dict.items())).T\n', 'i=0; vv=[]\n', 'for symbol in symbols:  # Issue Not same size\n', '   qq= quote_frompanda(filepd, symbol); i+=1\n', '   if isfloat(qq) :  vv.append(symbol)\n', '   else:\n', ""     if i==1: close = qq[['Datetime']]  #new dataframe    \n"", '     tmp= pd.DataFrame(data={symbol: qq[close1]});\n', '     if not tmp.isnull().values.all():\n', ""      close = close.join(tmp, how='outer', rsuffix='_1')\n"", ""      close= close.drop_duplicates(cols='Datetime', take_last=True)\n"", '\n', ""close= close[close['Datetime'].isin(datefilter)]   #Only date in the range\n"", ""close= close.sort('Datetime')\n"", ""close= close.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", 'close= np.array(close.values[1:,1:]).astype(np.float)  #Only Price\n', 'close= close[(np.abs(stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close, vv\n', ""'''\n"", '\n', ""'''\n"", 'from datetime import datetime\n', 'from dateutil import tz\n', '\n', ""from_zone = tz.gettz('Japan'); to_zone = tz.gettz('UTC')\n"", '\n', ""date1 = datetime.strptime('2011-01-21 09:00:00', '%Y-%m-%d %H:%M:%S')\n"", 'date1 = date1.replace(tzinfo=from_zone)\n', 'dateutc = date1.astimezone(to_zone)\n', 'date1, dateutc\n', '\n', ""Suppose you have a column 'datetime' with your string, then:\n"", ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '\n', ""df = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n"", ' combine multiple columns into a single datetime column,\n', ""this merges a 'date' and a 'time' column into a single 'datetime' column:\n"", '\n', ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '\n', ""df = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)\n"", ""'''\n"", '\n', '\n', '#date1 = ""20160302""; inter= 60; tframe=80; name1= ""spy""\n', '#googleIntradayQuoteSave(name1, inter, tframe)\n', '\n', '\n', '\n', 'def ta_mar(close,t,m):  return  100*( np.mean(close[(t-m):(t+1)], axis=0) / close[t] )\n', 'def ta_ma(close,t,m):  return   np.mean(close[(t-m):(t+1)], axis=0)\n', '\n', '\n', '############################################################################\n', '#---------------------Calculate the Statistics         --------------------  \n', 'def calc_statestock(close2, dateref, symfull):\n', ' def sort(x,col,asc): return   util.sortcol(x,col,asc)\n', ' def perf(close,t0,t1):  return  100*( close[:,t1] / close[:,t0] -1)\n', ' def and2(tuple1):  return np.logical_and.reduce(tuple1)\n', ' def mar(close,t,m):  return  100*( np.mean(close[:,(t-m):(t+1)], axis=1) / close[:,t] )\n', ' def ma(close,t,m):  return   np.mean(close[:,(t-m):(t+1)], axis=1)\n', '\n', '\n', ' # a= ma(close2, tmax, 5) / ma(close2, tmax, 50)  \n', '\n', '\n', ' def dd(t) :\n', '  x= util.find(t,dateref)  # 607\n', ""  if x==-1 : print 'Error date not found in dateref '+str(t) ; return np.nan\n"", '  else :  return x\n', '\n', ' def gap(close,t0,t1,lag):   \n', '  ret= getret_fromquotes(close[:,t0:t1],lag)\n', '  rmin= 100*np.amin(ret,axis=1)\n', '  return rmin\n', ' \n', ' #--------------------Size----------------------------------------- \n', ' masset= len(symfull)\n', ' ttmax= util.find(0,close2[0,:])\n', ' if ttmax==-1 : ttmax= np.shape(close2)[1]\n', ' close2= close2[0:masset,0:ttmax]\n', ' m, tmax= masset, ttmax-1\n', ' \n', ' #correl\n', ' #close3= np.array(close2[:,0:ttmax],dtype=np.float16)\n', ' #close2ret= getret_fromquotes(close3)\n', ' #close2ret= np.nan_to_num(close2ret)\n', ' #close2ret= util.np_cleanmatrix(close2ret);  del close3\n', '\n', '##################################################################################\n', '#1: 1d,2:5d,3:10d,4:15d,5:20d,11:50d,12:60d,13:90d,15:1y\n', '#16: YTD,17:From Feb,21: 2015Ret,45: Price,50,51: Min,52,53: Max\n', ' print(""Calcul the Stats "" + str(dateref[-1]))\n', ' stat= np.zeros((2600,200),dtype=np.float16)\n', '\n', ' k=0\n', ' stat[k:(k+m),0]= np.arange(k,k+m)\n', ' stat[k:(k+m),1]=  perf(close2,tmax-1,tmax)  \n', ' stat[k:(k+m),2]=  perf(close2,tmax-5,tmax)  \n', ' stat[k:(k+m),3]=  perf(close2,tmax-10,tmax)  \n', ' stat[k:(k+m),4]=  perf(close2,tmax-15,tmax)  \n', ' stat[k:(k+m),5]=  perf(close2,tmax-20,tmax)  \n', '# stat[k:(k+m),6]=  perf(close2,tmax-25,tmax)  \n', ' stat[k:(k+m),7]=  perf(close2,tmax-30,tmax)  \n', '# stat[k:(k+m),8]=  perf(close2,tmax-35,tmax)  \n', ' stat[k:(k+m),9]=  perf(close2,tmax-40,tmax)  \n', '# stat[k:(k+m),10]=  perf(close2,tmax-45,tmax)  \n', ' stat[k:(k+m),11]=  perf(close2,tmax-50,tmax)  \n', ' stat[k:(k+m),12]=  perf(close2,tmax-60,tmax)  \n', ' stat[k:(k+m),13]=  perf(close2,tmax-90,tmax)  \n', ' stat[k:(k+m),14]=  perf(close2,tmax-120,tmax)  \n', ' stat[k:(k+m),15]=  perf(close2,tmax-250,tmax)  \n', '\n', '\n', '# stat[k:(k+m),16]=  perf(close2, dd(20160105),tmax)   #1Jan 2016\n', '# stat[k:(k+m),17]=  perf(close2, dd(20160222),tmax)   # 22 Feb 2016\n', '# stat[k:(k+m),18]=  perf(close2,dd(20160105),dd(20160222))   # Crash DrawDown\n', ' # stat[k:(k+m),19]=  perf(close2,624,tmax)   #  24 June 2016\n', ' # stat[k:(k+m),20]=  perf(close2,623,625)   #  Crash 24 June 2016\n', ' # stat[k:(k+m),21]=  perf(close2,dd(20151230)-250,dd(20151230))    #2015 Return\n', '\n', ' stat[k:(k+m),22]= gap(close2,tmax-80,tmax,1) #Max 1 days Gap\n', ' stat[k:(k+m),23]= gap(close2,tmax-80,tmax,5) #Max 5 days Gap\n', ' stat[k:(k+m),24]= perf(close2,tmax-2,tmax)  \n', ' stat[k:(k+m),25]= perf(close2,tmax-3,tmax)  \n', ' stat[k:(k+m),26]= perf(close2,tmax-4,tmax)  \n', '\n', ' stat[k:(k+m),27]= perf(close2,tmax-2,tmax-1)   \n', ' stat[k:(k+m),28]= perf(close2,tmax-3,tmax-2)   \n', ' stat[k:(k+m),29]= perf(close2,tmax-4,tmax-3)   \n', ' stat[k:(k+m),30]= perf(close2,tmax-5,tmax-4)\n', ' \n', ' stat[k:(k+m),120]= mar(close2, tmax, 20) \n', ' stat[k:(k+m),121]= mar(close2, tmax, 50) \n', ' stat[k:(k+m),122]= 100*ma(close2, tmax, 5) / ma(close2, tmax, 50)  #OverSold /OverBought\n', ' stat[k:(k+m),123]= mar(close2, tmax, 100)  \n', ' stat[k:(k+m),124]= mar(close2, tmax, 10) \n', '# stat[k:(k+m),35]= 0 \n', '# stat[k:(k+m),36]= 0 \n', '# stat[k:(k+m),37]= 0 \n', '# stat[k:(k+m),38]= 0 \n', '# stat[k:(k+m),39]= 0 \n', '\n', ' # stat[k:(k+m),30]= volume / AvgVol3M\n', ' # stat[k:(k+m),30]= Nbday_to_Earnings\n', ' #Volume/3M AvgVolume\n', ' # stat[k:(k+m),26]=  volume(volume2,tmax-120,tmax)  \n', '\n', ' #==============================================================================\n', ' #-------------- Data Fundamental    -------------------------------------------\n', "" df= util.sql_query(sqlr='SELECT ticker,shortratio,sector1_id, sector2_id, marketcap   FROM stockfundamental ', dburl='/aaserialize/store/finviz.db')\n"", ' npdf= df.values; del df\n', ' npdf0= npdf[:,0]  \n', ' for i in range(0,m):  \n', '   try :\n', '     kid= util.find(symfull[i], npdf0)\n', '     if kid != -1 :\n', '      stat[i,150]=  float(npdf[kid, 1])   # short ratio\n', '      stat[i,151]=  float(npdf[kid, 2])   # sector 1      \n', '      stat[i,152]=  float(npdf[kid, 3])   # sector 2     \n', '      stat[i,153]=  float(npdf[kid, 4] / 100000000.0)   # market Cap    \n', '   except : pass\n', ' del npdf, npdf0\n', '\n', '\n', '#==============================================================================\n', ' ################ Volatility\n', ' stat[k:(k+m),40]= volhisto_fromprice(close2,tmax,20,axis=1)* 100\n', ' stat[k:(k+m),41]= volhisto_fromprice(close2,tmax,60,axis=1)* 100\n', ' stat[k:(k+m),42]= volhisto_fromprice(close2,tmax,252,axis=1)* 100\n', '# stat[k:(k+m),43]= stat[k:(k+m),12] / stat[k:(k+m),41] #Sharpe 3Month\n', '# stat[k:(k+m),44]= stat[k:(k+m),15] / stat[k:(k+m),42] #Sharpe 1Year\n', '\n', ' stat[k:(k+m),45]= close2[:,tmax] #Last Close\n', '\n', '# stat[k:(k+m),46]= 0 \n', '# stat[k:(k+m),47]= 0 \n', '# stat[k:(k+m),48]= 0 \n', '# stat[k:(k+m),49]= 0 \n', '\n', '\n', ' ###############  Technical Indicator\n', ' #Min of last 6 months, Max of last 6months\n', ' for i in range(0,m):\n', '  pp0= stat[i,45]\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-120):tmax])\n', '  stat[i,50]= kmin+(tmax-120);  stat[i,51]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-120):tmax])\n', '  stat[i,52]= kmax+(tmax-120);  stat[i,53]= 100*pp0 / pmax\n', '\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-10):tmax])\n', '  stat[i,66]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-10):tmax])\n', '  stat[i,67]= 100*pp0 / pmax\n', '\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-5):tmax])\n', '  stat[i,68]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-5):tmax])\n', '  stat[i,69]= 100*pp0 / pmax\n', '\n', ' del kmax,pmax,i,k  \n', '\n', '\n', ' #Regression from Min Price Time\n', ' for i in range(0,m):\n', '  t0= int(stat[i,50])\n', '  try :\n', '    res= regression(close2[i,t0:tmax ]  ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,54]=  res[2]  #R2\n', '    stat[i,55]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', ' #Regression from Max Price Time\n', ' for i in range(0,m):  \n', '  t0= stat[i,52]\n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,56]=  res[2]  #R2\n', '    stat[i,57]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', ' #Regression from 5 days ago\n', ' t0= tmax-5\n', ' for i in range(0,m): \n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,58]=  res[2]  #R2\n', '    stat[i,59]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', '#Regression from 10 days ago\n', ' t0= tmax-10\n', ' for i in range(0,m):  \n', '   try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,60]=  res[2]  #R2\n', '    stat[i,61]=  res[0][0]  #Slope\n', '   except : pass\n', '\n', "" '''\n"", '#Regression from 1 month\n', ' t0= tmax-20   \n', ' for i in range(0,m):\n', '   try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,62]=  res[2]  #R2\n', '    stat[i,63]=  res[0][0]  #Slope\n', '   except : pass\n', "" '''\n"", '\n', "" '''\n"", '#Regression from 2 months ago\n', ' t0= tmax-40\n', ' for i in range(0,m):  \n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,64]=  res[2]  #R2\n', '    stat[i,65]=  res[0][0]  #Slope\n', '  except : pass\n', "" '''\n"", '\n', '#Nb of Positive days last 5d\n', ' for i in range(0,m):  stat[i,66]=  np_countretsign(close2[i,(tmax-5):tmax])  \n', '\n', '#Nb of Positive days last 10d\n', ' for i in range(0,m):  stat[i,67]=  np_countretsign(close2[i,(tmax-10):tmax])  \n', '\n', '#Nb of Positive days last 20d\n', '# for i in range(0,m):  stat[i,68]=  np_countretsign(close2[i,(tmax-20):tmax])  \n', '\n', '\n', '#Regression from Max Values,200days\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmax[:,1] ,vmax[:,0],type1=""linear"")\n', '    stat[i,70]=  res[2]  #R2\n', '    stat[i,71]=  res[0][0]  #Slope\n', '  except : pass\n', '# del vmax\n', '\n', '\n', '#Regression from Min Values,200days \n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmin= util.np_findlocalmin2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmin[:,1] ,vmin[:,0],type1=""linear"")\n', '    stat[i,72]=  res[2]  #R2\n', '    stat[i,73]=  res[0][0]  #Slope\n', '  except : pass\n', '# del vmin\n', '\n', '\n', '#Regression from Max Values,100days\n', ' t0= tmax-20*5\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmax[:,1] ,vmax[:,0],type1=""linear"")\n', '    stat[i,74]=  res[2]  #R2\n', '    stat[i,75]=  res[0][0]  #Slope\n', '  except : pass\n', ' del vmax\n', '\n', '\n', '#Regression from Min Values,100days \n', ' t0= tmax-20*5\n', ' for i in range(0,m):  \n', '  try :\n', '    vmin= util.np_findlocalmin2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmin[:,1] ,vmin[:,0],type1=""linear"")\n', '    stat[i,76]=  res[2]  #R2\n', '    stat[i,77]=  res[0][0]  #Slope\n', '  except : pass\n', ' del vmin\n', '\n', '\n', '\n', '# Upside Trend with Higher Lows detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '   vmax= util.np_findlocalmin2(close2[i,t0:tmax],4)\n', '   vmax= util.sort(vmax,0,asc=0)\n', '   stat[i,80]=  -1\n', '   if len(vmax) > 1 :\n', '    ss=0\n', '    for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] > vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '    if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,80]=  ss  \n', '        stat[i,81]=  vmax[0,0] + t0\n', '  except: pass\n', '\n', '\n', '# Upside Trend with Higher High detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax],4)\n', '    vmax= util.sort(vmax,0,asc=0)\n', '    stat[i,82]=  -1\n', '    if len(vmax) > 1 :\n', '      ss=0\n', '      for k in range(0,len(vmax)-1):\n', '        if vmax[k,1] > vmax[k+1,1] : ss+=1   #Higher high\n', '        else : break;\n', '      if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,82]=  ss  \n', '        stat[i,83]=  vmax[0,0] + t0\n', '  except: pass\n', '\n', '\n', '# Downside Trend with lower Lows detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  vmax= util.np_findlocalmin2(close2[i,t0:tmax],4)\n', '  vmax= util.sort(vmax,0,asc=0)\n', '  stat[i,84]=  -1\n', '  if len(vmax) > 1 :\n', '   ss=0\n', '   for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] < vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '   if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,84]=  ss  \n', '        stat[i,85]=  vmax[0,0] + t0\n', '\n', '\n', '# Downside Trend  with lower highs detection\n', ' t0= tmax-20*6\n', ' for i in range(0,m):  \n', '  vmax= util.np_findlocalmax2(close2[i,t0:tmax],4)\n', '  vmax= util.sort(vmax,0,asc=0)\n', '  stat[i,80]=  -1\n', '  if len(vmax) > 1 :\n', '   ss=0\n', '   for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] < vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '   if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,86]=  ss    #Nb  of lower high consecutive\n', '        stat[i,87]=  vmax[0,0] + t0\n', '\n', "" print('Higher/Lower Band Support')\n"", '#-----higher band Support / Lower Band Support 120days-------------------------------\n', ' t0= tmax-120\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,90]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,91]= 100.0* stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,92]= 100.0* (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '    # pmin=  (res2.x[0] * (np.arange(t0,tmax)-t0) + res2.x[1]) \n', '    # breachmin= np.sum(np.sign(close2[i, t0:tmax] - pmin*1.03))\n', '  except: pass\n', '\n', '\n', '#-----higher band Support / Lower Band Support 200days-------------------------------\n', ' t0= tmax-200\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,93]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,94]= 100.0 * stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,95]= 100.0 * (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '  except: pass\n', '\n', '\n', '#-----higher band Support / Lower Band Support 300days-------------------------------------\n', ' t0= tmax-300\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,96]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,97]= 100.0* stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,98]= 100.0* (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '  except: pass\n', '\n', '\n', ""  '''\n"", '  # -----higher band Support / Lower Band Support 60days-------------------------------\n', '  t0 = tmax - 60\n', '  for i in range(0, m):\n', '      try:\n', '          res1 = ta_highbandtrend1(close2[i, t0:tmax])\n', '          pmax = (res1.x[0] * (tmax - t0) + res1.x[1])\n', '          stat[i, 99] = 100.0 * stat[i, 45] / pmax  # Higher Band\n', '\n', '          res2 = ta_lowbandtrend1(close2[i, t0:tmax])\n', '          pmin = (res2.x[0] * (tmax - t0) + res2.x[1])\n', '          stat[i, 100] = 100.0 * stat[i, 45] / pmin  # LowerBand\n', '\n', '          stat[i, 101] = 100.0 * (pmax - stat[i, 45]) / (pmax - pmin)  # Price inside Range\n', '\n', '          # pmin=  (res2.x[0] * (np.arange(t0,tmax)-t0) + res2.x[1])\n', '          # breachmin= np.sum(np.sign(close2[i, t0:tmax] - pmin*1.03))\n', '      except:\n', '          pass\n', ""  '''\n"", '\n', '  #-----Technical Indicator------------------------------------------------\n', '# for i in range(0,m): \n', '#  try :\n', '#   res2=0\n', '   # df= pd.read_hdf(dbfile, symfull[i])\n', '   # stat[i,110]= ta.RMI(close2[i,:])\n', '   # stat[i,111]= 100*close2[i,:] / ta.MA(close2[i,:],20)\n', '   # stat[i,112]= 100*close2[i,:] / ta.MA(close2[i,:],50)   \n', '#  except: pass\n', '\n', '\n', ' return np.array(stat, dtype=np.float16) \n', '\n', '\n', ""'''  \n"", 'i=1964  \n', 'stat= s1\n', 'stat[i, 91]  \n', '\n', 'tt= np.arange(t0, tmax)\n', 'plot_price(close2[i,t0:tmax], (res2.x[0] * (tt-t0) + res2.x[1]) )\n', ""'''\n"", '\n', '  \n', '#------------------- Stock Monitoring Tools ------------------------------------------------\n', ""def monitor_addrecommend(string1, dbname='stock_recommend') :\n"", ' ss= string1.replace(""("","" "").replace(\')\',\' \').replace(\':\',\' \')\n', "" ss= ss.replace('\\t', ' ').replace('\\n', ' ')\n"", ' sl1= ss.split("" "")\n', ' print sl1\n', ' aux= [datestring_toint(util.date_now())]\n', ' symfull= copy.deepcopy(a_us_all)\n', ' for x in sl1:\n', '   if len(x) < 5 :\n', '    if util.find(x,symfull) > 0 :   aux.append(x)\n', '\n', ' if len(aux) >  1 :\n', '  stock_recommend= util.load_obj(dbname)\n', '  stock_recommend.append(aux)\n', '  util.save_obj(stock_recommend,dbname)\n', '  print aux\n', ' return stock_recommend\n', '\n', '\n', '\n', '#################### Finviz  ###############################################################\n', 'from bs4 import BeautifulSoup\n', 'import csv,datetime, requests\n', 'def imp_finviz():\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print ""Finviz Overview Start""\n', ' url = ""http://www.finviz.com/screener.ashx?v=111&f=geo_usa""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', "" firstcount = soup.find_all('option')\n"", ' lastnum = len(firstcount) - 1\n', "" lastpagenum = firstcount[lastnum].attrs['value']\n"", ' currentpage = int(lastpagenum)\n', '\n', ' alldata = []\n', ' templist = []\n', ' # Overview = 111, Valuation = 121, Financial = 161, Ownership = 131, Performance = 141\n', ' #pagesarray = [111,121,161,131,141]\n', ' titleslist = soup.find_all(\'td\',{""class"" : ""table-top""})\n', ' titleslisttickerid = soup.find_all(\'td\',{""class"" : ""table-top-s""})\n', ' titleticker = titleslisttickerid[0].text\n', ' titlesarray = []\n', ' for title in titleslist:\n', '    titlesarray.append(title.text)\n', '\n', ' titlesarray.insert(1,titleticker)\n', ' i = 0\n', '\n', ' while(currentpage > 0):\n', '    i += 1\n', '    print str(i) + "" page(s) done""\n', '    secondurl = ""http://www.finviz.com/screener.ashx?v="" + str(111) + ""&f=geo_usa"" + ""&r="" + str(currentpage)\n', '    secondresponse = requests.get(secondurl)\n', '    secondhtml = secondresponse.content\n', '    secondsoup = BeautifulSoup(secondhtml)\n', '    stockdata = secondsoup.find_all(\'a\', {""class"" : ""screener-link""})\n', '    stockticker = secondsoup.find_all(\'a\', {""class"" : ""screener-link-primary""})\n', '    datalength = len(stockdata)\n', '    tickerdatalength = len(stockticker)\n', '\n', '    while(datalength > 0):\n', '        templist = [stockdata[datalength - 10].text,stockticker[tickerdatalength-1].text,stockdata[datalength - 9].text,stockdata[datalength - 8].text,stockdata[datalength - 7].text,stockdata[datalength - 6].text,stockdata[datalength - 5].text,stockdata[datalength - 4].text,stockdata[datalength - 3].text,stockdata[datalength - 2].text,stockdata[datalength - 1].text,]\n', '        alldata.append(templist)\n', '        templist = []\n', '        datalength -= 10\n', '        tickerdatalength -= 1\n', '    currentpage -= 20\n', '\n', "" with open('stockoverview.csv', 'wb') as csvfile:\n"", ""    overview = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=titlesarray)\n"", '    overview.writeheader()\n', '\n', '    for stock in alldata:\n', '        overview.writerow({titlesarray[0] : stock[0], titlesarray[1] : stock[1],titlesarray[2] : stock[2],titlesarray[3] : stock[3],titlesarray[4] : stock[4], titlesarray[5] : stock[5], titlesarray[6] : stock[6], titlesarray[7] : stock[7] , titlesarray[8] : stock[8], titlesarray[9] : stock[9], titlesarray[10] : stock[10] })\n', '\n', ' print ""Finviz Overview Completed""\n', ' return overview\n', ' \n', '\n', 'def imp_finviz_news():\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print ""Finviz Performance Start""\n', ' url = ""http://www.finviz.com/quote.ashx?t=intc""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', ' titleslist = soup.find_all(\'a\',{""class"" : ""tab-link-news""})\n', ' return titleslist\n', '\n', '\n', 'def imp_finviz_financials():\n', ' import datetime\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print datetime.datetime.now()\n', ' print ""Finviz Financial Start""\n', ' url = ""http://www.finviz.com/screener.ashx?v=161&f=geo_usa""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', "" firstcount = soup.find_all('option')\n"", ' lastnum = len(firstcount) - 1\n', "" lastpagenum = firstcount[lastnum].attrs['value']\n"", ' currentpage = int(lastpagenum)\n', '\n', ' alldata = []\n', ' templist = []\n', ' # Overview = 111, Valuation = 121, Financial = 161, Ownership = 131, Performance = 141\n', ' #pagesarray = [111,121,161,131,141]\n', ' titleslist = soup.find_all(\'td\',{""class"" : ""table-top""})\n', ' titleslisttickerid = soup.find_all(\'td\',{""class"" : ""table-top-s""})\n', ' titleticker = titleslisttickerid[0].text\n', ' titlesarray = []\n', ' for title in titleslist:\n', '    titlesarray.append(title.text)\n', '\n', ' titlesarray.insert(1,titleticker)\n', ' i = 0\n', '\n', ' while(currentpage > 0):\n', '    i += 1\n', '    print str(i) + "" page(s) done""\n', '    secondurl = ""http://www.finviz.com/screener.ashx?v="" + str(161) + ""&f=geo_usa"" + ""&r="" + str(currentpage)\n', '    secondresponse = requests.get(secondurl)\n', '    secondhtml = secondresponse.content\n', '    secondsoup = BeautifulSoup(secondhtml)\n', '    stockdata = secondsoup.find_all(\'a\', {""class"" : ""screener-link""})\n', '    stockticker = secondsoup.find_all(\'a\', {""class"" : ""screener-link-primary""})\n', '    datalength = len(stockdata)\n', '    tickerdatalength = len(stockticker)\n', '\n', '    while(datalength > 0):\n', '        templist = [stockdata[datalength - 17].text,stockticker[tickerdatalength-1].text,stockdata[datalength - 16].text,stockdata[datalength - 15].text,stockdata[datalength - 14].text,stockdata[datalength - 13].text,stockdata[datalength - 12].text,stockdata[datalength - 11].text,stockdata[datalength - 10].text,stockdata[datalength - 9].text,stockdata[datalength - 8].text,stockdata[datalength - 7].text,stockdata[datalength - 6].text,stockdata[datalength - 5].text,stockdata[datalength - 4].text,stockdata[datalength - 3].text,stockdata[datalength - 2].text,stockdata[datalength - 1].text,]\n', '        alldata.append(templist)\n', '        templist = []\n', '        datalength -= 17\n', '        tickerdatalength -= 1\n', '    currentpage -= 20\n', '\n', "" with open('stockfinancial.csv', 'wb') as csvfile:\n"", ""    financial = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=titlesarray)\n"", '    financial.writeheader()\n', '\n', '    for stock in alldata:\n', '        financial.writerow({titlesarray[0] : stock[0], titlesarray[1] : stock[1],titlesarray[2] : stock[2],titlesarray[3] : stock[3],titlesarray[4] : stock[4], titlesarray[5] : stock[5], titlesarray[6] : stock[6], titlesarray[7] : stock[7] , titlesarray[8] : stock[8], titlesarray[9] : stock[9], titlesarray[10] : stock[10],titlesarray[11] : stock[11],titlesarray[12] : stock[12],titlesarray[13] : stock[13],titlesarray[14] : stock[14],titlesarray[15] : stock[15],titlesarray[16] : stock[16],titlesarray[17] : stock[17] })\n', '\n', ' print datetime.datetime.now()\n', ' print ""Finviz Financial Completed""\n', '\n', '\n', '\n', 'def get_price2book( symbol ):\n', '   from bs4 import BeautifulSoup as bs\n', '   import urllib as u\n', '   try:\n', ""    \turl = r'http://finviz.com/quote.ashx?t={}'.format(symbol.lower())\n"", '        html = u.request.urlopen(url).read()\n', ""        soup = bs(html, 'lxml')\n"", '        # Change the text below to get a diff metric\n', ""        pb =  soup.find(text = r'P/B')\n"", ""        pb_ = pb.find_next(class_='snapshot-td2').text\n"", ""        print( '{} price to book = {}'.format(symbol, pb_) )\n"", '        return pb_\n', '   except Exception as e:\n', '        print(e)\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ' try:\n', '   q = imp_googleQuote(name1, date1, date2)  #interval, timeframe\n', ""   name1= name1.replace(':','_')\n"", ""   file1= dircsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(date2) +'.csv'\n"", '   q.write_csv(file1)\n', ' except: print\n', ""'''\n"", '\n', '\n', ""'''\n"", 'def imp_googleQuoteList(symbols, freqsec=300, nday=1, intraday1=True) :\n', ' qlist=[]; sym=[]\n', ' if intraday1:\n', '  for symbol in symbols:\n', '     #try :\n', '        q = imp_googleIntradayQuote(symbol, freqsec, nday)  #interval, timeframe\n', '        qlist.append(q)\n', '        sym.append(symbol)\n', ""     #except: print('Error:'+symbol)\n"", '\n', ' else :\n', '  for symbol in symbols:\n', '     try :\n', '       q = imp_googleQuote(name1, freqsec, num_days)  #interval, timeframe\n', '     except: print(symbol)\n', '\n', ' return qlist, sym\n', ""'''\n"", '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''        \n"", ""stock_list = ['XOM','AMZN','AAPL','SWKS']\n"", 'p2b_series = pd.Series( index=stock_list )\n', '\n', 'for sym in stock_list:\tp2b_series[sym] = get_price2book(sym)\n', ""'''\n"", '\n', '\n', '\n', ""'''\n"", ""def imp_csv_toext(file1, filenameh5, dfname='sym1', fromzone='Japan', tozone='UTC'):\n"", "" # dateparse= lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", ' from dateutil.parser import parse\n', ' if os.path.getsize(file1)> 500 :\n', '  from_zone = tz.gettz(fromzone); tozone = tz.gettz(tozone)\n', ""  # dateparse= lambda x: (pd.datetime.strptime(x,'%Y-%m-%d %H:%M:%S').replace(tzinfo=from_zone).astimezone(tozone))\n"", '\n', ""  # dateparse= lambda x: (pd.datetime.strptime(x,'%m-%d-%Y %H:%M:%S').replace(tzinfo=from_zone).astimezone(tozone))\n"", '\n', '  dateparse= lambda x: (parse(x).replace(tzinfo=from_zone).astimezone(tozone))\n', '\n', '  # dateparse= lambda x: parse(x, tzinfos=from_zone).astimezone(to_zone)\n', '  print file1\n', ""  #df = pd.read_csv(file1,sep=',',header=None, parse_dates={'date': [1]}, date_parser=dateparse)\n"", '\n', ""  df = pd.read_csv(file1,sep=',',header=0)\n"", '  #df.date= [pd.to_datetime((str(x)[:-6])) for x in  df.date]\n', '  #df.date= [x.to_datetime() for x in  df.date]\n', '\n', '\n', '  df.drop(df.columns[[0]], axis=1, inplace=True)\n', ""  df= util.pd_addcol(df, 'symbol')\n"", ""  df['symbol']= dfname\n"", '\n', '  print filenameh5\n', ""  df.columns = ['date', 'open','high','low','close','volume', 'symbol' ]\n"", ""  df.to_csv(filenameh5 , index=False)  # , compression='gzip' )\n"", '\n', '  # store = pd.HDFStore(filenameh5);\n', '  # store.append(dfname, df);  store.close()\n', ""'''\n"", '\n', ""'''\n"", ""def imp_csv_toext(file1, filenameh5, dropcol=[], type1='csv', hasheader=True, dfname='sym1', fromzone='Japan', tozone='UTC'):\n"", ' if os.path.getsize(file1)> 500 :\n', '   header= 0 if hasheader else None\n', ""   df = pd.read_csv(file1, sep=',',header=0)\n"", '\n', '   df.drop(df.columns[dropcol], axis=1, inplace=True)\n', ""   df= util.pd_addcol(df, 'symbol');  df['symbol']= dfname\n"", ""   df.columns = ['date', 'open','high','low','close','volume', 'symbol' ]\n"", '\n', ""   if type1=='csv' :\n"", ""     df.to_csv(filenameh5 , index=False)  # , compression='gzip' )\n"", ""'''\n"", '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ""     if i==1: close = qq[['Datetime']]  #new dataframe\n"", '     tmp= pd.DataFrame(data={symbol: qq[close1]});\n', ""     close = close.join(tmp, how='outer', rsuffix='_1')\n"", ""     close= close.drop_duplicates(cols='Datetime', take_last=True)\n"", '\n', "" close= close[close['Datetime'].isin(datefilter)]   #Only date in the range\n"", "" close= close.sort('Datetime')\n"", "" close= close.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", '\n', ' close= np.array(close.values[1:,1:]).astype(np.float)  #Only Price\n', ' close= close[(np.abs(sci.stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close, vv\n', ""'''\n"", '\n', '\n', '\n', ""'''\n"", 'class Quote(object):\n', ""  DATE_FMT = '%Y-%m-%d';  TIME_FMT = '%H:%M:%S'\n"", '\n', '  def __init__(self):\n', ""    self.symbol = '';    self.exchn=''\n"", '    self.date,self.time,self.open_,self.high,self.low,self.close,self.volume = ([] for _ in range(7))\n', '\n', '  def append(self,dt,open_,high,low,close,volume):\n', '    self.date.append(dt.date())\n', '    self.time.append(dt.time())\n', '    self.open_.append(float(open_))\n', '    self.high.append(float(high))\n', '    self.low.append(float(low))\n', '    self.close.append(float(close))\n', '    self.volume.append(int(volume))\n', '\n', '  def to_csv(self):\n', '    return \'\'.join([""{0},{1},{2},{3:.2f},{4:.2f},{5:.2f},{6:.2f},{7}\\n"".format(self.symbol,\n', ""              self.date[bar].strftime('%Y-%m-%d'),self.time[bar].strftime('%H:%M:%S'),\n"", '              self.open_[bar],self.high[bar],self.low[bar],self.close[bar],self.volume[bar])\n', '              for bar in xrange(len(self.close))])\n', '\n', '  def write_csv(self,filename):\n', ""    with open(filename,'w') as f:\n"", '      txt= self.to_csv()\n', '      f.write(txt)\n', '\n', '  def read_csv(self,filename):\n', ""    self.symbol = ''\n"", '    self.date,self.time,self.open_,self.high,self.low,self.close,self.volume = ([] for _ in range(7))\n', ""    for line in open(filename,'r'):\n"", ""      symbol,ds,ts,open_,high,low,close,volume = line.rstrip().split(',')\n"", '      self.symbol = symbol\n', ""      dt = datetime.datetime.strptime(ds+' '+ts,self.DATE_FMT+' '+self.TIME_FMT)\n"", '      self.append(dt,open_,high,low,close,volume)\n', '    return True\n', '\n', '  def __repr__(self):\n', '    return self.to_csv()\n', ""'''\n"", '\n', ' \n', '\n', ""'''\n"", ""def imp_googleIntradayQuoteSave2(name1, date1, inter, tframe, dircsv='', dbname=''):\n"", ' q = imp_googleIntradayQuote(name1, inter, tframe)  #interval, timeframe\n', "" name1= name1.replace(':','_')\n"", '\n', "" if dircsv != '' :\n"", ""    file1= dircsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(inter) +'_'+ str(tframe) +'.csv'\n"", '    q.to_csv(file1)\n', '\n', "" if dbname != '' :\n"", '    import sqlalchemy as sql\n', '    dbcon= sql.create_engine(dbname)\n', '    if inter== 300 : # 5mins\n', ""      q.to_sql('q5min', dbcon)\n"", '\n', '\n', ""def imp_googleQuoteList_save(symbols, date1, date2, inter=23400, tframe=2000, dircsv='', intraday1=True) :\n"", ' if intraday1:\n', '  for symbol in symbols:\n', '     try :\n', '       imp_googleIntradayQuoteSave(symbol, date1, inter, tframe, dircsv)\n', ""     except: print('Error:'+symbol)\n"", '\n', ' else :\n', '  for symbol in symbols:\n', '     try :\n', '       imp_googleQuoteSave(symbol, date1, date2, dircsv)\n', '     except: print(symbol)\n', ""'''\n"", '\n']","['np.shape', 'getret_fromquotes', 'len', 'getlogret_fromquotes', 'np.log', 'getprice_fromret']",6
utilmy/zarchive/portfolio.py:index,index,class,103,300,221,3009,10.03,1,3,[],[],[],1625,[],[],0
utilmy/zarchive/portfolio.py:folioCalc,folioCalc,class,125,509,332,4736,9.3,2,17,[],[],[],1924,[],[],0
utilmy/zarchive/portfolio.py:folioRiskIndicator,folioRiskIndicator,class,89,306,215,3180,10.39,1,9,[],[],[],2099,[],[],0
utilmy/zarchive/portfolio.py:folioOptimizationF,folioOptimizationF,class,280,1291,671,10994,8.52,6,39,[],[],[],2277,[],[],0
utilmy/zarchive/portfolio.py:searchSimilarity,searchSimilarity,class,173,497,328,4550,9.15,8,17,[],[],[],3724,[],[],0
utilmy/zarchive/portfolio.py:index:__init__,index:__init__,method,11,12,12,116,9.67,0,0,"['self', 'id1', 'sym', 'ww', 'tstart']","[None, None, None, None, None]","[None, None, None, None, None]",1626,[],['date_generatedatetime'],1
utilmy/zarchive/portfolio.py:index:close,index:close,method,0,1,1,4,4.0,0,0,['self'],[None],[None],1631,[],[],0
utilmy/zarchive/portfolio.py:index:updatehisto,index:updatehisto,method,2,12,11,62,5.17,0,0,['self'],[None],[None],1634,[],['print'],1
utilmy/zarchive/portfolio.py:index:help,index:help,method,2,12,11,62,5.17,0,0,['self'],[None],[None],1638,[],['print'],1
utilmy/zarchive/portfolio.py:index:__init__,index:__init__,method,11,12,12,116,9.67,0,0,"['self', 'id1', 'sym', 'ww', 'tstart']","[None, None, None, None, None]","[None, None, None, None, None]",1659,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]",['date_generatedatetime'],1
utilmy/zarchive/portfolio.py:index:calc_baskettable_pct,index:calc_baskettable_pct,method,37,95,77,782,8.23,1,2,"['self', 'type1', 'showdetail']","[None, None, None]","[None, '""table""', '0']",1703,"[""      '''   Calc Basket Values from Input data '''\n""]","['np.zeros', 'np.sum', 'self._udpate_wwindpct', 'range', 'np.mod', 'self._wwpct_rebal']",6
utilmy/zarchive/portfolio.py:index:_wwpct_rebal,index:_wwpct_rebal,method,3,4,3,41,10.25,0,0,"['self', 'wwpct_actual', 't', 'trebal']","[None, None, None, None]","[None, None, None, None]",1730,[],[],0
utilmy/zarchive/portfolio.py:index:_udpate_wwindpct,index:_udpate_wwindpct,method,11,25,17,224,8.96,0,0,"['self', 't', 'bskt', 'hedgecost', 'wwpct_actual', 'wwpct_th']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",1740,[],[],0
utilmy/zarchive/portfolio.py:index:calc_baskettable_unit,index:calc_baskettable_unit,method,0,0,0,0,0.0,0,0,[],[],[],1749,"[""    '''   t           --->  t+1                 ---->   t+2\n"", '            wwactual:    B[t]    -->   B[t+1]   using  w[t+1], B[t] /   S[t]\n', '            wwnew   :    B[t+1]  -->   B[t+2]   using  w[t+2], B[t+1] /   S[t+1]\n', ""    '''\n""]",[],0
utilmy/zarchive/portfolio.py:folioCalc:__init__,folioCalc:__init__,method,21,39,37,356,9.13,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1925,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio.py:folioCalc:set_symclose,folioCalc:set_symclose,method,17,34,33,312,9.18,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1938,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio.py:folioCalc:setcriteria,folioCalc:setcriteria,method,41,76,71,1046,13.76,0,0,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",1947,"[""        ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '            wwtype:     constant / regime /regimExtra\n', '            initperiod: NbOfDays_initial_Risk_Monitoring\n', '            riskid:     spprice/spperf/multi\n', ""        '''\n""]","['np.array', 'len']",2
utilmy/zarchive/portfolio.py:folioCalc:_weightcalc_generic,folioCalc:_weightcalc_generic,method,5,35,23,396,11.31,0,4,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",1997,[],"['self._weightcalc_constant', 'self._weightcalc_regime', 'print']",3
utilmy/zarchive/portfolio.py:folioCalc:_weightcalc_regime,folioCalc:_weightcalc_regime,method,10,37,32,272,7.35,0,1,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2008,[],"['self._regimecalc', 'np.reshape', 'self._mapping_risk_ww']",3
utilmy/zarchive/portfolio.py:folioCalc:_regimecalc,folioCalc:_regimecalc,method,3,7,6,83,11.86,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2018,[],['self._mapping_calc_risk'],1
utilmy/zarchive/portfolio.py:folioCalc:_weightcalc_constant,folioCalc:_weightcalc_constant,method,3,4,4,28,7.0,0,0,"['self', 'ww2', 't']","[None, None, None]","[None, None, None]",2027,[],['np.sum'],1
utilmy/zarchive/portfolio.py:folioCalc:getweight,folioCalc:getweight,method,5,12,9,126,10.5,0,0,['self'],[None],[None],2031,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/portfolio.py:folioCalc:calc_baskettable,folioCalc:calc_baskettable,method,33,111,83,811,7.31,1,4,"['self', 'wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps', 'showdetail']","[None, None, None, None, None, None, None, None]","[None, None, None, '""table""', '""constant""', '1', '0.000', '0']",2038,[],"['np.shape', 'np.zeros', 'range', 'np.mod', 'self._weightcalc_generic', 'np.sum', 'print']",7
utilmy/zarchive/portfolio.py:folioCalc:plot,folioCalc:plot,method,10,29,26,295,10.17,0,2,"['self', 'wwvec', 'show1', 'tickperday']","[None, None, None, None]","[None, 'None', '1', '60']",2066,[],"['self.calc_baskettable', 'plot_price', 'print', 'np.array']",4
utilmy/zarchive/portfolio.py:folioCalc:multiperiod_ww,folioCalc:multiperiod_ww,method,15,36,29,240,6.67,1,1,"['self', 't']","[None, None]","[None, None]",2077,[],['xrange'],1
utilmy/zarchive/portfolio.py:folioCalc:help,folioCalc:help,method,1,2,2,13,6.5,0,0,['self'],[None],[None],2090,[],['print'],1
utilmy/zarchive/portfolio.py:folioRiskIndicator:__init__,folioRiskIndicator:__init__,method,23,40,38,356,8.9,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1659,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]","['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio.py:folioRiskIndicator:set_symclose,folioRiskIndicator:set_symclose,method,18,32,32,295,9.22,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2114,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio.py:folioRiskIndicator:setcriteria,folioRiskIndicator:setcriteria,method,34,81,70,1006,12.42,0,0,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2124,"[""    ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '        wwtype:     constant / regime /regimExtra\n', '        initperiod: NbOfDays_initial_Risk_Monitoring\n', '        riskid:     spprice/spperf/multi\n', ""    '''    \n""]","['np.array', 'len', 'np.zeros']",3
utilmy/zarchive/portfolio.py:folioRiskIndicator:calcrisk,folioRiskIndicator:calcrisk,method,13,18,17,220,12.22,1,0,"['self', 'wwvec', 'initval']","[None, None, None]","[None, '[]', '1']",2172,[],"['xrange', 'self._weightcalc_generic', 'np.column_stack']",3
utilmy/zarchive/portfolio.py:folioRiskIndicator:_weightcalc_generic,folioRiskIndicator:_weightcalc_generic,method,4,34,24,327,9.62,0,3,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2184,[],"['self._weightcalc_regime', 'print']",2
utilmy/zarchive/portfolio.py:folioRiskIndicator:_weightcalc_regime,folioRiskIndicator:_weightcalc_regime,method,3,11,11,68,6.18,0,1,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2193,[],['self._regimecalc'],1
utilmy/zarchive/portfolio.py:folioRiskIndicator:_regimecalc,folioRiskIndicator:_regimecalc,method,7,18,17,216,12.0,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2198,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/portfolio.py:folioRiskIndicator:calc_optimal_weight,folioRiskIndicator:calc_optimal_weight,method,7,18,17,216,12.0,0,0,"['self', 'maxiter', 'name1', 'isreset', 'popsize']","[None, None, None, None, None]","[None, '1', ""''"", '1', '15']",2204,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/portfolio.py:folioOptimizationF:__init__,folioOptimizationF:__init__,method,23,39,37,340,8.72,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1659,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]","['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio.py:folioOptimizationF:set_symclose,folioOptimizationF:set_symclose,method,18,32,32,295,9.22,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2114,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/portfolio.py:folioOptimizationF:setcriteria,folioOptimizationF:setcriteria,method,44,128,103,1669,13.04,0,2,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2124,"[""    ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '        wwtype:     constant / regime /regimExtra\n', '        initperiod: NbOfDays_initial_Risk_Monitoring\n', '        riskid:     spprice/spperf/multi\n', ""    '''    \n""]","['np.array', 'len', 'print', 'list']",4
utilmy/zarchive/portfolio.py:folioOptimizationF:calcbasket_obj2,folioOptimizationF:calcbasket_obj2,method,23,76,63,756,9.95,0,1,"['self', 'wwvec']","[None, None]","[None, None]",2362,[],"['np.reshape', 'np.array', 'np.sum', 'round', 'calcbasket_objext', 'float']",6
utilmy/zarchive/portfolio.py:folioOptimizationF:calcbasket_obj,folioOptimizationF:calcbasket_obj,method,55,157,118,1494,9.52,1,4,"['self', 'wwvec']","[None, None]","[None, None]",2388,[],"['np.reshape', 'np.array', 'np.sum', 'round', 'calcbasket_objext', 'float', 'calcbasket_obj', 'concenfactor=folio_concenfactor2', 'np.zeros', 'xrange', 'np.mod', 'self._weightcalc_generic', 'self._loss_obj', 'self._objective_criteria']",14
utilmy/zarchive/portfolio.py:folioOptimizationF:_loss_obj,folioOptimizationF:_loss_obj,method,4,6,6,68,11.33,0,0,"['self', 'ww2', 'wwpenalty']","[None, None, None]","[None, None, None]",2413,[],['np.sum'],1
utilmy/zarchive/portfolio.py:folioOptimizationF:_weightcalc_generic,folioOptimizationF:_weightcalc_generic,method,5,40,26,394,9.85,0,4,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2184,[],"['self._weightcalc_constant', 'self._weightcalc_regime', 'print']",3
utilmy/zarchive/portfolio.py:folioOptimizationF:_weightcalc_regime,folioOptimizationF:_weightcalc_regime,method,8,32,28,239,7.47,0,0,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2193,"['""""""\n', 'if bsk > maxprice :       maxprice= bsk\n', 'if maxprice-bsk > maxdd : maxdd=    maxprice-bsk\n', '\n', '""""""\n']","['self._regimecalc', 'np.reshape', 'self._mapping_risk_ww']",3
utilmy/zarchive/portfolio.py:folioOptimizationF:_regimecalc,folioOptimizationF:_regimecalc,method,4,7,7,83,11.86,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2198,"['""""""\n', 'if bsk > maxprice :       maxprice= bsk\n', 'if maxprice-bsk > maxdd : maxdd=    maxprice-bsk\n', '\n', '""""""\n']",['self._mapping_calc_risk'],1
utilmy/zarchive/portfolio.py:folioOptimizationF:_weightcalc_constant,folioOptimizationF:_weightcalc_constant,method,3,4,4,28,7.0,0,0,"['self', 'ww2', 't']","[None, None, None]","[None, None, None]",2452,[],['np.sum'],1
utilmy/zarchive/portfolio.py:folioOptimizationF:calc_optimal_weight,folioOptimizationF:calc_optimal_weight,method,7,18,17,216,12.0,0,0,"['self', 'maxiter', 'name1', 'isreset', 'popsize']","[None, None, None, None, None]","[None, '1', ""''"", '1', '15']",2204,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/portfolio.py:folioOptimizationF:getweight,folioOptimizationF:getweight,method,6,11,9,126,11.45,0,0,['self'],[None],[None],2461,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/portfolio.py:folioOptimizationF:calc_baskettable,folioOptimizationF:calc_baskettable,method,36,86,72,827,9.62,1,4,"['self', 'wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps', 'showdetail']","[None, None, None, None, None, None, None, None]","[None, None, None, '""table""', '""constant""', '1', ' 0.000', '0']",2469,[],"['np.shape', 'np.zeros', 'range', 'np.mod', 'self._weightcalc_generic', 'np.sum', 'print']",7
utilmy/zarchive/portfolio.py:folioOptimizationF:plot,folioOptimizationF:plot,method,11,31,30,294,9.48,0,2,"['self', 'wwvec', 'show1', 'tickperday']","[None, None, None, None]","[None, 'None', '1', '60']",2498,[],"['self.calc_baskettable', 'plot_price', 'print', 'np.array']",4
utilmy/zarchive/portfolio.py:folioOptimizationF:_objective_criteria,folioOptimizationF:_objective_criteria,method,31,147,78,1105,7.52,2,7,"['self', 'bsk']","[None, None]","[None, None]",2510,[],"['np.std', 'np.sum', 'range', 'np.arange']",4
utilmy/zarchive/portfolio.py:folioOptimizationF:multiperiod_ww,folioOptimizationF:multiperiod_ww,method,18,29,27,238,8.21,1,1,"['self', 't']","[None, None]","[None, None]",2557,[],['xrange'],1
utilmy/zarchive/portfolio.py:folioOptimizationF:help,folioOptimizationF:help,method,50,152,103,1472,9.68,1,6,['self'],[None],[None],2569,[],"['print', 'util.load_obj', 'int', 'copy.deepcopy', 'xrange', 'next', 'util.save_obj', 'np.mod', 'np.abs', 'np.reshape', 'np.sum']",11
utilmy/zarchive/portfolio.py:folioOptimizationF:mapping_risk_ww,folioOptimizationF:mapping_risk_ww,method,12,56,29,245,4.38,0,1,"['self', 'risk', 'wwmat', 'ww2']","[None, None, None, None]","[None, None, None, 'self.wwasset0']",2628,[],"['np.sum', 'np.array']",2
utilmy/zarchive/portfolio.py:folioOptimizationF:_mapping_calc_risk,folioOptimizationF:_mapping_calc_risk,method,27,201,76,831,4.13,0,3,"['self', 'ss', 'tr', 't', 'risk0']","[None, None, None, None, None]","[None, None, None, None, None]",2640,[],[],0
utilmy/zarchive/portfolio.py:searchSimilarity:__init__,searchSimilarity:__init__,method,15,18,17,193,10.72,0,0,"['self', ""filejpstock=r'E"", 'sym01', 'symname', 'startdate', 'enddate', 'pricetype']","[None, '', None, None, None, None, None]","[None, ""r'E:/_data/stock/daily/20160616/jp'"", ""['7203']"", ""['Toyota']"", ' 20150101', '20160601', '""close""']",3725,[],[],0
utilmy/zarchive/portfolio.py:searchSimilarity:load_quotes_fromdb,searchSimilarity:load_quotes_fromdb,method,15,35,33,355,10.14,0,1,"['self', 'picklefile']","[None, None]","[None, ""''""]",3730,[],"['util.load_obj', 'imp_txt_getquotes', 'date_align', 'util.a_cleanmemory']",4
utilmy/zarchive/portfolio.py:searchSimilarity:__generate_return__,searchSimilarity:__generate_return__,method,4,4,4,64,16.0,0,0,"['self', 'nlag']","[None, None]","[None, None]",3741,[],['getret_fromquotes'],1
utilmy/zarchive/portfolio.py:searchSimilarity:__overweight__,searchSimilarity:__overweight__,method,17,62,41,412,6.65,2,4,"['self', 'px']","[None, None]","[None, None]",3745,[],"['len', 'np.ones', 'util.np_findlocalmax', 'type', 'np.abs', 'wwerr[max', 'util.np_findlocalmin']",7
utilmy/zarchive/portfolio.py:searchSimilarity:set_searchcriteria,searchSimilarity:set_searchcriteria,method,34,79,63,793,10.04,0,5,"['self', 'name1', 'date1', 'date2', 'nlag', 'searchperiodstart', 'typesearch', '']","[None, None, None, None, None, None, None, None]","[None, ""'7203'"", '20160301', '20160601', '1', '20120101', '""pattern2""', None]",3765,[],"['getret_fromquotes', 'date_finddateid', 'max', 'util.np_findfirst', 'print', 'price_normalize_1d', 'len', 'self.__overweight__']",8
utilmy/zarchive/portfolio.py:searchSimilarity:launch_search,searchSimilarity:launch_search,method,53,107,81,908,8.49,3,3,['self'],[None],[None],3794,[],"['len', 'np.zeros', 'range', 'price_normalize_1d', 'np_similarity', 'util.np_sortbycolumn', 'enumerate']",7
utilmy/zarchive/portfolio.py:searchSimilarity:show_comparison_graph,searchSimilarity:show_comparison_graph,method,33,72,63,682,9.47,1,3,"['self', 'maxresult', 'show_only_different_time', 'fromid', 'fromend', 'filenameout']","[None, None, None, None, None, None]","[None, '20', 'True', '0', ' 0', ""''""]",3828,[],"['np.shape', 'range', 'int', 'print', 'price_normalize_1d', 'str', 'plot_price']",7
utilmy/zarchive/portfolio.py:searchSimilarity:staticmethod,searchSimilarity:staticmethod,method,0,1,1,4,4.0,0,0,"['self', 'x']","[None, None]","[None, None]",3857,[],[],0
utilmy/zarchive/portfolio.py:searchSimilarity:get_rankresult,searchSimilarity:get_rankresult,method,24,63,53,496,7.87,2,1,"['self', 'filetosave']","[None, None]","[None, ""''""]",3861,[],"['len', 'np.savetxt', 'np.shape', 'np.array', 'np.empty', 'range', 'int', 'str']",8
utilmy/zarchive/portfolio.py:searchSimilarity:export_results,searchSimilarity:export_results,method,0,0,0,0,0.0,0,0,[],[],[],3883,"[""'''\n"", '#####################################################################################\n', '#---------Run over all the indexes, with time lag -----------------------------------\n', 'def np_similarity(x,y, wwerr=[], type1=0):\n', '  if type1==2 : return np_distance_l1(x,y, wwerr)\n', '  if type1==1: return sci.spatial.distance.correlation(x, y)  \n', '  if type1==0 : return 1-np.corrcoef(x,y)[0,1]\n', '\n', 'def np_distance_l1(x,y, wwerr) :\n', ' return np.sum(wwerr * np.abs(x-y))  \n', '  \n', '#---------------------  Universe Load         ---------------------------------------\n', ""filejpstock= r'E:/_data/stock/daily/20160616/jp'\n"", 'sym01= nk400list\n', '\n', 'quotes= imp_txt_getquotes(sym01, filejpstock, startdate=20090101, endate=20160616)\n', 'open1, dateref= date_align(quotes, type1=""close"")    #Get the data and align dates\n', 'del quotes; util.a_cleanmemory()\n', '\n', ""#util.save_obj(open1, 'close_nk400_2009_2016')\n"", ""#util.save_obj(dateref, 'dateref_nk400_2009_2016')\n"", '\n', ""open1= util.load_obj('close_nk400_2009_2016')\n"", ""dateref= util.load_obj( 'dateref_nk400_2009_2016')\n"", '\n', 'nlag=1\n', 'ret_open1 =  getdailyret_fromquotes(open1,nlag)\n', 'del open1\n', '\n', '#--------------------  Search Asset Input     ---------------------------------------\n', ""name1= '7203' \n"", 'date1= 20160328\n', 'date2= 20160531\n', 'twindow= util.np_findfirst(date2, dateref) - util.np_findfirst(date1, dateref) \n', '\n', '\n', 'tstartx= util.np_findfirst(date1, dateref)          # Date Index\n', 'stockx= util.np_findfirst(name1, sym01)\n', 'retx= ret_open1[stockx, tstartx:(tstartx+twindow-1)]    # Recent Period\n', 'px= price_normalize_1d(retx)\n', 'nx= len(px)\n', '\n', '\n', '#----OverWeight for Local Min, Local Max         -----------------------------------\n', 'wwerr= np.ones(nx, dtype=np.float16)\n', 'vvmax= util.np_findlocalmax(px); xprev=0\n', 'for x in vvmax:\n', '  if x[0] !=0 and np.abs(xprev-x[0]) > 6: \n', '    wwerr[max(0,x[0]-1):min(nx,(x[0]+1))]= 1.5;    xprev= x[0] \n', '\n', 'vvmin= util.np_findlocalmin(px); xprev=0\n', 'for x in vvmin:\n', '  if x[0] !=0 and np.abs(xprev-x[0]) > 6: \n', '    wwerr[max(0,x[0]-1):min(nx,(x[0]+1))]= 1.5;    xprev= x[0] \n', '\n', '\n', '#--------------------  Search the Pattern       -------------------------------------\n', 'tmaxx= len(dateref)-1\n', 'similar_list= np.zeros((nstock*(tmaxx-twindow)+1, 3), np.float16)\n', 'i=0\n', 'for iy in range(0, nstock) :\n', ' for t in range(0, tmaxx-twindow):\n', '   if iy == stockx and t > tstartx : \n', '     pass\n', '   else :\n', '    rety= ret_open1[iy, t:(t+twindow-1)]\n', '    py=   price_normalize_1d(rety)\n', '    ss= np_similarity(px, py, wwerr=wwerr, type1=2)\n', '    i+=1\n', '    similar_list[i,0]= iy\n', '    similar_list[i,1]= t\n', '    similar_list[i,2]= ss\n', '\n', '\n', ' #-------------------Clean the table -------------------------------------------------\n', 'rank= util.np_sortbycolumn(similar_list,2, asc=True)\n', 'for i,x in enumerate(rank) :\n', '  if rank[i,2]==0.0 : rank[i,2]= 1000         # Empty cell\n', '  if rank[i,1]> tstartx-3 : rank[i,2]= 100    # Start Element\n', 'rank= util.np_sortbycolumn(rank,2, asc=True)  \n', '\n', '\n', ' #----- Show Similar Stocks ---------------------------------------------------------\n', 'show_only_different_time= True\n', '\n', 'for i in range(0,10) :\n', ' tstarti= rank[i,1]\n', ' stocki= int(rank[i,0])\n', ' \n', ' if show_only_different_time and tstarti < tstartx-1  :\n', '   py= price_normalize100(ret_open1[stocki, tstarti:(tstarti+twindow-1)]) \n', '\n', '   namei= sym01[stocki]\n', '   namefulli= nk400name[util.np_findfirst(namei, nk400list)]\n', '\n', '   tit= namei+ "" "" +namefulli+ "" "" + str(dateref[tstarti])          \n', ""   plot_price(px-100, py-100, label=('Similar '+tit, 'Time','% Var') )\n"", '\n', '\n', 'kix= util.np_findfirst(name1, statsjp[:,0, ilag])\n', 'stocklist01[390]\n', '\n', '\n', 'stocklist01[157] , nk400name[157] ,  dateref1adj[86]     \n', 'plot_price(price_normalize100(ret_open1[157, 86:86+twindow-1])-100,px-100   )\n', ""'''    \n""]",[],0
utilmy/zarchive/fast.py:day,day,function,9,15,14,125,8.33,0,0,['s'],[None],[None],130,[],['arrow.get'],1
utilmy/zarchive/fast.py:month,month,function,9,15,14,125,8.33,0,0,['s'],[None],[None],131,[],['arrow.get'],1
utilmy/zarchive/fast.py:year,year,function,9,15,14,125,8.33,0,0,['s'],[None],[None],132,[],['arrow.get'],1
utilmy/zarchive/fast.py:hour,hour,function,9,15,14,125,8.33,0,0,['s'],[None],[None],133,[],['arrow.get'],1
utilmy/zarchive/fast.py:weekday,weekday,function,9,15,14,125,8.33,0,0,['s'],[None],[None],139,[],['arrow.get'],1
utilmy/zarchive/fast.py:season,season,function,4,15,13,47,3.13,0,1,['d'],[None],[None],148,[],['int'],1
utilmy/zarchive/fast.py:daytime,daytime,function,5,40,23,123,3.08,0,1,['d'],[None],[None],154,[],['int'],1
utilmy/zarchive/fast.py:fastStrptime,fastStrptime,function,8,46,35,288,6.26,0,2,"['val', 'format']","[None, None]","[None, None]",164,[],"['len', 'int', 'datetime.datetime']",3
utilmy/zarchive/fast.py:drawdown_calc_fast,drawdown_calc_fast,function,36,80,63,618,7.72,1,3,['price'],[None],[None],190,[],"['len', 'np.zeros', 'range']",3
utilmy/zarchive/fast.py:std,std,function,6,12,12,61,5.08,0,0,['x'],[None],[None],225,"['    """"""Std Deviation 1D array""""""\n']","['x.sum', 'sqrt']",2
utilmy/zarchive/fast.py:mean,mean,function,2,3,3,24,8.0,0,0,['x'],[None],[None],236,"['    """"""Mean  """"""\n']",['x.sum'],1
utilmy/zarchive/fast.py:_compute_overlaps,_compute_overlaps,function,13,27,20,108,4.0,1,0,"['u', 'v']","[None, None]","[None, None]",249,[],['xrange'],1
utilmy/zarchive/fast.py:distance_jaccard2,distance_jaccard2,function,5,11,11,55,5.0,0,0,"['u', 'v']","[None, None]","[None, None]",260,[],"['_compute_overlaps', 'float']",2
utilmy/zarchive/fast.py:distance_jaccard,distance_jaccard,function,5,11,11,55,5.0,0,0,"['u', 'v']","[None, None]","[None, None]",266,[],"['_compute_overlaps', 'float']",2
utilmy/zarchive/fast.py:distance_jaccard_X,distance_jaccard_X,function,13,21,19,163,7.76,2,0,['X'],[None],[None],277,[],"['np.zeros', 'xrange', 'distance_jaccard']",3
utilmy/zarchive/fast.py:cosine,cosine,function,11,43,26,262,6.09,0,2,"['u', 'v']","[None, None]","[None, None]",292,[],['np.sqrt'],1
utilmy/zarchive/fast.py:rmse,rmse,function,1,7,6,31,4.43,0,0,"['y', 'yhat']","[None, None]","[None, None]",321,"['    """""" Calculate and return Root Mean Squared Error (RMSE)\n', '    Returns: float: Root Mean Squared Error\n', '    """"""\n']",[],0
utilmy/zarchive/fast.py:cross,cross,function,17,31,25,211,6.81,0,0,"['vec1', 'vec2']","[None, None]","[None, None]",329,"['    """""" Calculate the dot product of two 3d vectors. """"""\n']","['double', 'np.zeros']",2
utilmy/zarchive/fast.py:norm,norm,function,4,8,8,61,7.62,1,0,['vec'],[None],[None],341,"['    """""" Calculate the norm of a 3d vector. """"""\n']","['sqrt', 'range']",2
utilmy/zarchive/fast.py:log_exp_sum2,log_exp_sum2,function,0,1,1,5,5.0,0,0,"['a', 'b']","[None, None]","[None, None]",242,[],[],0
utilmy/zarchive/util.py:session_load_function,session_load_function,function,6,9,9,88,9.78,0,0,['name'],[None],"[""'test_20160815'""]",183,[],"['dill.load_session', 'print']",2
utilmy/zarchive/util.py:session_save_function,session_save_function,function,9,12,12,129,10.75,0,0,['name'],[None],"[""'test'""]",192,[],"['date_now', 'dill.dump_session', 'print']",3
utilmy/zarchive/util.py:py_save_obj_dill,py_save_obj_dill,function,27,58,53,549,9.47,0,1,"['obj1', 'keyname', 'otherfolder']","[None, None, None]","[None, ""''"", '0']",201,[],"['z_key_splitinto_dir_name', 'os_folder_create', 'open', 'dill.dumps', 'print']",5
utilmy/zarchive/util.py:session_spyder_showall,session_spyder_showall,function,7,12,12,122,10.17,1,0,[],[],[],220,[],"['os_file_listall', 'print']",2
utilmy/zarchive/util.py:session_guispyder_save,session_guispyder_save,function,4,5,5,83,16.6,0,0,['filename'],[None],[None],226,[],['save_session'],1
utilmy/zarchive/util.py:session_guispyder_load,session_guispyder_load,function,4,5,5,76,15.2,0,0,['filename'],[None],[None],230,[],['load_session'],1
utilmy/zarchive/util.py:session_load,session_load,function,6,9,9,88,9.78,0,0,['name'],[None],"[""'test_20160815'""]",252,"[""  ''' .spydata file,  dict1: already provided Dict,  towhere= main, function, dict '''\n""]","['dill.load_session', 'print']",2
utilmy/zarchive/util.py:session_save,session_save,function,9,12,12,129,10.75,0,0,['name'],[None],"[""'test'""]",278,"[""    '''Need to pass globals() Cannot Get Save data to .spydata file\n"", '\n', '   BIG issue with Import, Impor FULL MODULE ----> BIG ISSUE\n', '     BIG ISSUE with DICT, USE LIST INSTEAD\n', '        If you try to put this code in a module and import the function then you will have to pass globals() to the function explicitly as the globals() in the function is not the IPython global namespace. However, you can put the above code inside your ~/.ipython/profile_PROFILE/startup/startup.ipy file and it will work as expected.\n', '       PROFILE is the name of the profile that you plan to start IPython with.\n', ""    '''\n""]","['date_now', 'dill.dump_session', 'print']",3
utilmy/zarchive/util.py:aa_unicode_ascii_utf8_issue,aa_unicode_ascii_utf8_issue,function,0,0,0,0,0.0,0,0,[],[],[],356,"[""   '''Take All csv in a folder and provide Table, Column Schema, type\n"", '\n', ' METHOD FOR Unicode / ASCII issue\n', '1. Decode early\n', ""Decode to <type 'unicode'> ASAP\n"", ""df['PREF_NAME']=       df['PREF_NAME'].apply(to_unicode)\n"", '\n', '2. Unicode everywhere\n', '\n', '\n', '3. Encode late\n', "">>> f = open('/tmp/ivan_out.txt','w')\n"", "">>> f.write(ivan_uni.encode('utf-8'))\n"", '\n', 'Important methods\n', ""s.decode(encoding)  <type 'str'> to <type 'unicode'>\n"", ""u.encode(encoding)  <type 'unicode'> to <type 'str'>\n"", '\n', 'http://farmdev.com/talks/unicode/\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/util.py:isexist,isexist,function,3,9,8,46,5.11,0,0,['a'],[None],[None],380,[],[],0
utilmy/zarchive/util.py:isfloat,isfloat,function,5,16,14,67,4.19,0,1,['x'],[None],[None],386,[],['float'],1
utilmy/zarchive/util.py:isint,isint,function,6,15,14,80,5.33,0,1,['x'],[None],[None],393,[],['txt.find'],1
utilmy/zarchive/util.py:a_isanaconda,a_isanaconda,function,6,15,14,80,5.33,0,1,[],[],[],395,[],['txt.find'],1
utilmy/zarchive/util.py:a_run_ipython,a_run_ipython,function,1,1,1,33,33.0,0,0,['cmd1'],[None],[None],405,"["" ''' Execute Ipython Command in python code\n"", '     run -i :  run including current interprete variable\n', "" '''\n""]",['IPython.get_ipython'],1
utilmy/zarchive/util.py:a_autoreload,a_autoreload,function,1,4,4,64,16.0,0,0,[],[],[],411,[],['a_run_ipython'],1
utilmy/zarchive/util.py:a_start_log,a_start_log,function,1,14,12,98,7.0,0,0,"['id1', 'folder']","[None, None]","[""''"", ""'aaserialize/log/'""]",414,[],"['a_run_ipython', 'str', 'a_get_platform', 'date_now']",4
utilmy/zarchive/util.py:a_cleanmemory,a_cleanmemory,function,3,3,3,21,7.0,0,0,[],[],[],417,[],['gc.collect'],1
utilmy/zarchive/util.py:a_module_codesample,a_module_codesample,function,6,9,9,130,14.44,0,0,['module_str'],[None],"[""'pandas'""]",420,[],"['os_file_read', 'os_gui_popup_show']",2
utilmy/zarchive/util.py:a_module_doc,a_module_doc,function,6,9,9,123,13.67,0,0,['module_str'],[None],"[""'pandas'""]",426,[],"['os_file_read', 'os_gui_popup_show']",2
utilmy/zarchive/util.py:a_module_generatedoc,a_module_generatedoc,function,8,16,16,180,11.25,0,1,"['module_str', 'fileout']","[None, None]","['""pandas""', ""''""]",432,"['  \'\'\' #  getmodule_doc(""jedi"", r""D:\\_devs\\Python01\\aapackage\\doc.txt"")\'\'\'\n']","['os.makedirs', 'ca.getmodule_doc']",2
utilmy/zarchive/util.py:a_info_conda_jupyter,a_info_conda_jupyter,function,61,133,110,1071,8.05,1,3,[],[],[],440,[],"['printProgress', 'shutil.make_archive', 'os_zipfolder', 'dir_tozip.split', 'os_zip_checkintegrity', 'print']",6
utilmy/zarchive/util.py:os_zipextractall,os_zipextractall,function,23,59,48,538,9.12,1,3,"['filezip_or_dir', 'tofolderextract', 'isprint']","[None, None, None]","['""folder1/*.zip""', ""'zdisk/test'"", '1']",711,"[""   '''os_zipextractall( 'aapackage.zip','zdisk/test/'      )  '''\n""]","['filezip_or_dir.find', 'os_file_listall', 'os_file_getname', 'zipfile.ZipFile', 'zip_ref.extractall', 'zip_ref.close', 'print']",7
utilmy/zarchive/util.py:os_folder_copy,os_folder_copy,function,17,36,35,345,9.58,0,1,"['src', 'dst', 'symlinks', 'pattern1', 'fun_file_toignore']","[None, None, None, None, None]","[None, None, 'False', '""*.py""', 'None']",741,"[""   '''\n"", '       callable(src, names) -> ignored_names\n', ""       'src' parameter, which is the directory being visited by copytree(), and\n"", ""       'names' which is the list of `src` contents, as returned by os.listdir():\n"", '\n', '    Since copytree() is called recursively, the callable will be called once for each directory that is copied.\n', '    It returns a  list of names relative to the `src` directory that should not be copied.\n', '   :param fun_ignore:\n', ""   '''\n""]","['fun_file_toignore', 'fnmatch.filter', 'shutil.copytree', 'shutil.copy']",4
utilmy/zarchive/util.py:os_folder_create,os_folder_create,function,4,7,7,85,12.14,0,1,['directory'],[None],[None],764,[],"['os.getcwd', 'os.makedirs', 'os.chdir']",3
utilmy/zarchive/util.py:os_folder_robocopy,os_folder_robocopy,function,3,15,15,163,10.87,0,1,"['from_folder', 'to_folder', ""my_log='H""]","[None, None, '']","[""''"", ""''"", ""'H:/robocopy_log.txt'""]",769,"['    """"""\n', '    Copy files to working directory\n', '    robocopy <Source> <Destination> [<File>[ ...]] [<Options>]\n', '    We want to copy the files to a fast SSD drive\n', '    """"""\n']","['subprocess.call', 'print']",2
utilmy/zarchive/util.py:os_file_replace,os_file_replace,function,26,69,57,697,10.1,2,0,"['source_file_path', 'pattern', 'substring']","[None, None, None]","[None, None, None]",784,[],"['mkstemp', 'open', 'target_file.write', 'remove', 'move', 'os_file_replacestring1', 'fileinput.FileInput', 'line.replace', 'file1.close', 'print', 'os_file_replacestring2']",11
utilmy/zarchive/util.py:os_file_replacestring1,os_file_replacestring1,function,11,26,25,240,9.23,1,0,"['findStr', 'repStr', 'filePath']","[None, None, None]","[None, None, None]",798,[],"['fileinput.FileInput', 'line.replace', 'file1.close', 'print']",4
utilmy/zarchive/util.py:os_file_replacestring2,os_file_replacestring2,function,6,14,13,161,11.5,1,0,"['findstr', 'replacestr', 'some_dir', 'pattern', 'dirlevel']","[None, None, None, None, None]","[None, None, None, '""*.*""', '1']",809,"['  \'\'\' #fil_replacestring_files(""logo.png"", ""logonew.png"", r""D:/__Alpaca__details/aiportfolio"",    pattern=""*.html"", dirlevel=5  )\n', ""  '''\n""]","['os_file_listall', 'os_file_replacestring1']",2
utilmy/zarchive/util.py:os_file_getname,os_file_getname,function,8,9,8,75,8.33,0,0,['path'],[None],[None],817,[],"['ntpath.split', 'ntpath.basename']",2
utilmy/zarchive/util.py:os_file_getpath,os_file_getpath,function,7,7,7,52,7.43,0,0,['path'],[None],[None],822,[],['ntpath.split'],1
utilmy/zarchive/util.py:os_file_gettext,os_file_gettext,function,4,8,8,56,7.0,0,0,['file1'],[None],[None],827,[],"['open', 'f.read']",2
utilmy/zarchive/util.py:os_file_listall,os_file_listall,function,28,82,46,904,11.02,4,3,"['dir1', 'pattern', 'dirlevel', 'onlyfolder']","[None, None, None, None]","[None, '""*.*""', '1', '0']",833,"[""  '''\n"", '   # DIRCWD=r""D:\\_devs\\Python01\\project""\n', '   # aa= listallfile(DIRCWD, ""*.*"", 2)\n', '   # aa[0][30];   aa[2][30]\n', '\n', '   :param dir1:\n', '   :param pattern:\n', '   :param dirlevel:\n', '   :param onlyfolder:\n', '   :return:\n', ""  '''\n""]","['dir1.rstrip', 'dir1.count', 'os.walk', 'root.count', 'matches.append', 'fnmatch.filter', 'np.array']",7
utilmy/zarchive/util.py:os_file_rename,os_file_rename,function,32,60,48,643,10.72,2,1,"['some_dir', 'pattern', 'pattern2', 'dirlevel']","[None, None, None, None]","[None, '""*.*""', '""""', '1']",870,[],"['some_dir.rstrip', 'some_dir.count', 'os.walk', 'root.count', 'matches.append', 'fnmatch.filter', 're.sub', 'os.rename', 'np.array']",9
utilmy/zarchive/util.py:os_gui_popup_show,os_gui_popup_show,function,25,38,37,361,9.5,0,0,['txt'],[None],[None],891,[],"['os_gui_popup_show2', 'Tk', 'Scrollbar', 'Text', 'S.pack', 'T.pack', 'S.config', 'T.config', 'T.insert', 'root.attributes', 'mainloop']",11
utilmy/zarchive/util.py:os_print_tofile,os_print_tofile,function,8,21,19,174,8.29,0,1,"['vv', 'file1', 'mode1']","[None, None, None]","[None, None, ""'a'""]",908,"[""    '''\n"", '    Here is a list of the different modes of opening a file:\n', 'r\n', 'Opens a file for reading only. The file pointer is placed at the beginning of the file. This is the default mode.\n', '\n', 'rb\n', '\n', 'Opens a file for reading only in binary format. The file pointer is placed at the beginning of the file. This is the default mode.\n', 'r+\n', '\n', 'Opens a file for both reading and writing. The file pointer will be at the beginning of the file.\n', 'rb+\n', '\n', 'Opens a file for both reading and writing in binary format. The file pointer will be at the beginning of the file.\n', 'w\n', '\n', 'Opens a file for writing only. Overwrites the file if the file exists. If the file does not exist, creates a new file for writing.\n', 'wb\n', '\n', 'Opens a file for writing only in binary format. Overwrites the file if the file exists. If the file does not exist, creates a new file for writing.\n', 'w+\n', '\n', 'Opens a file for both writing and reading. Overwrites the existing file if the file exists. If the file does not exist, creates a new file for reading and writing.\n', 'wb+\n', '\n', 'Opens a file for both writing and reading in binary format. Overwrites the existing file if the file exists. If the file does not exist, creates a new file for reading and writing.\n', 'a\n', '\n', 'Opens a file for appending. The file pointer is at the end of the file if the file exists. That is, the file is in the append mode. If the file does not exist, it creates a new file for writing.\n', 'ab\n', '\n', 'Opens a file for appending in binary format. The file pointer is at the end of the file if the file exists. That is, the file is in the append mode. If the file does not exist, it creates a new file for writing.\n', 'a+\n', '\n', 'Opens a file for both appending and reading. The file pointer is at the end of the file if the file exists. The file opens in the append mode. If the file does not exist, it creates a new file for reading and writing.\n', 'ab+\n', '\n', 'Opens a file for both appending and reading in binary format. The file pointer is at the end of the file if the file exists. The file opens in the append mode. If the file does not exist, it creates a new file for reading and writing.\n', 'To open a text file, use:\n', 'fh = open(""hello.txt"", ""r"")\n', '\n', 'To read a text file, use:\n', 'print fh.read()\n', '\n', 'To read one line at a time, use:\n', 'print fh.readline()\n', '\n', 'To read a list of lines use:\n', 'print fh.readlines()\n', '\n', 'To write to a file, use:\n', 'fh = open(""hello.txt"", ""w"")\n', 'lines_of_text = [""a line of text"", ""another line of text"", ""a third line""]\n', 'fh.writelines(lines_of_text)\n', 'fh.close()\n', '\n', 'To append to file, use:\n', 'fh = open(""Hello.txt"", ""a"")\n', 'fh.close()\n', '\n', ""    '''\n""]","['pth.find', 'b.lstrip']",2
utilmy/zarchive/util.py:os_path_norm,os_path_norm,function,8,21,19,174,8.29,0,1,['pth'],[None],[None],972,"['    \'\'\' #r""D:\\_devs\\Python01\\project\\03-Connect_Java_CPP_Excel\\PyBindGen\\examples"" \'\'\'\n']","['pth.find', 'b.lstrip']",2
utilmy/zarchive/util.py:os_path_change,os_path_change,function,4,4,4,34,8.5,0,0,['path1'],[None],[None],984,[],"['open', 'fh.read']",2
utilmy/zarchive/util.py:os_path_current,os_path_current,function,4,4,4,34,8.5,0,0,[],[],[],986,[],"['open', 'fh.read']",2
utilmy/zarchive/util.py:os_file_exist,os_file_exist,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],988,[],"['open', 'fh.read']",2
utilmy/zarchive/util.py:os_file_size,os_file_size,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],990,[],"['open', 'fh.read']",2
utilmy/zarchive/util.py:os_file_read,os_file_read,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],992,[],"['open', 'fh.read']",2
utilmy/zarchive/util.py:os_file_isame,os_file_isame,function,4,7,7,44,6.29,0,0,"['file1', 'file2']","[None, None]","[None, None]",997,[],['filecmp.cmp'],1
utilmy/zarchive/util.py:os_file_get_file_extension,os_file_get_file_extension,function,5,12,10,98,8.17,0,2,['file_path'],[None],[None],1004,"['    """"""\n', '    >>> get_file_extension(""/a/b/c"")\n', ""    ''\n"", '    >>> get_file_extension(""/a/b/c.tar.xz"")\n', ""    'xz'\n"", '    """"""\n']",['_ext.startswith'],1
utilmy/zarchive/util.py:os_file_normpath,os_file_normpath,function,2,8,8,67,8.38,0,0,['path'],[None],[None],1021,"['    """"""Normalize path.\n', '    - eliminating double slashes, etc. (os.path.normpath)\n', '    - ensure paths contain ~[user]/ expanded.\n', '\n', '    :param path: Path string :: str\n', '    """"""\n']",[],0
utilmy/zarchive/util.py:os_folder_is_path,os_folder_is_path,function,2,3,3,36,12.0,0,0,['path_or_stream'],[None],[None],1031,"['    """"""\n', '    Is given object `path_or_stream` a file path?\n', '    :param path_or_stream: file path or stream, file/file-like object\n', '    :return: True if `path_or_stream` is a file path\n', '    """"""\n']",['isinstance'],1
utilmy/zarchive/util.py:os_file_get_path_from_stream,os_file_get_path_from_stream,function,7,20,15,187,9.35,0,2,['maybe_stream'],[None],[None],1040,"['    """"""\n', '    Try to get file path from given stream `stream`.\n', '\n', '    :param maybe_stream: A file or file-like object\n', '    :return: Path of given file or file-like object or None\n', '\n', '    >>> __file__ == get_path_from_stream(__file__)\n', '    True\n', ""    >>> __file__ == get_path_from_stream(open(__file__, 'r'))\n"", '    True\n', '    >>> strm = anyconfig.compat.StringIO()\n', '    >>> get_path_from_stream(strm) is None\n', '    True\n', '    """"""\n']","['os_folder_is_path', 'getattr']",2
utilmy/zarchive/util.py:os_file_try_to_get_extension,os_file_try_to_get_extension,function,4,12,10,117,9.75,0,1,['path_or_strm'],[None],[None],1065,"['    """"""\n', '    Try to get file extension from given path or file object.\n', '    :return: File extension or None\n', '    """"""\n']","['os_file_get_path_from_stream', 'os_file_get_file_extension']",2
utilmy/zarchive/util.py:os_file_are_same_file_types,os_file_are_same_file_types,function,5,20,14,152,7.6,0,2,['paths'],[None],[None],1077,"['    """"""\n', '    Are given (maybe) file paths same type (extension) ?\n', '    :param paths: A list of file path or file(-like) objects\n', '\n', '    >>> are_same_file_types([])\n', '    False\n', '    >>> are_same_file_types([""a.conf""])\n', '    True\n', '    >>> are_same_file_types([""a.yml"", ""b.json""])\n', '    False\n', '    >>> strm = anyconfig.compat.StringIO()\n', '    >>> are_same_file_types([""a.yml"", ""b.yml"", strm])\n', '    False\n', '    """"""\n']","['os_file_try_to_get_extension', 'all']",2
utilmy/zarchive/util.py:os_file_norm_paths,os_file_norm_paths,function,18,56,37,397,7.09,2,3,"['paths', 'marker']","[None, None]","[None, ""'*'""]",1099,"['    """"""\n', '    :param paths:\n', '        A glob path pattern string, or a list consists of path strings or glob\n', '        path pattern strings or file objects\n', ""    :param marker: Glob marker character or string, e.g. '*'\n"", '    :return: List of path strings\n', '    >>> norm_paths([])\n', '    []\n', '    >>> norm_paths(""/usr/lib/a/b.conf /etc/a/b.conf /run/a/b.conf"".split())\n', ""    ['/usr/lib/a/b.conf', '/etc/a/b.conf', '/run/a/b.conf']\n"", '    >>> paths_s = os.path.join(os.path.dirname(__file__), ""u*.py"")\n', '    >>> ref = sglob(paths_s)\n', '    >>> ref = [""/etc/a.conf""] + ref\n', '    >>> assert norm_paths([""/etc/a.conf"", paths_s]) == ref\n', '    >>> strm = anyconfig.compat.StringIO()\n', '    >>> assert norm_paths([""/etc/a.conf"", strm]) == [""/etc/a.conf"", strm]\n', '    """"""\n']","['sglob', 'sorted', '_norm_paths_itr', 'os_folder_is_path', 'list']",5
utilmy/zarchive/util.py:os_file_mergeall,os_file_mergeall,function,10,20,20,185,9.25,1,0,"['nfile', 'dir1', 'pattern1', 'deepness']","[None, None, None, None]","[None, None, None, '2']",1144,[],"['os_file_listall', 'open', 'os_file_gettext', 'nfile1.write', 'nfile1.close']",5
utilmy/zarchive/util.py:os_file_extracttext,os_file_extracttext,function,14,31,29,286,9.23,1,0,"['output_file', 'dir1', 'pattern1', 'htmltag', 'deepness']","[None, None, None, None, None]","[None, None, '""*.html""', ""'p'"", '2']",1154,"["" ''' Extract text from html '''\n""]","['os_file_listall', 'open', 'os_file_gettext', 'BeautifulSoup', 'soup.find_all', 'txt2.strip', 'output_file1.write']",7
utilmy/zarchive/util.py:os_path_append,os_path_append,function,4,19,11,121,6.37,0,3,"['p1', 'p2', 'p3', 'p4']","[None, None, None, None]","[None, 'None', 'None', 'None']",1169,[],[],0
utilmy/zarchive/util.py:os_wait_cpu,os_wait_cpu,function,12,31,27,286,9.23,1,0,"['priority', 'cpu_min']","[None, None]","['300', '50']",1178,[],"['psutil.cpu_percent', 'print', 'arrow.utcnow', 'sleep']",4
utilmy/zarchive/util.py:os_split_dir_file,os_split_dir_file,function,6,15,15,112,7.47,0,1,['dirfile'],[None],[None],1187,[],"['dirfile.split', 'len']",2
utilmy/zarchive/util.py:os_process_run,os_process_run,function,13,31,31,321,10.35,0,1,"['cmd_list', ""'arg1'"", ""'arg2']"", 'capture_output']","[None, None, None, None]","[""['program'"", None, None, 'False']",1193,[],"['subprocess.Popen', 'proc.communicate', 'print', 'str']",4
utilmy/zarchive/util.py:os_process_2,os_process_2,function,0,1,1,4,4.0,0,0,[],[],[],1209,[],[],0
utilmy/zarchive/util.py:py_importfromfile,py_importfromfile,function,12,21,19,251,11.95,0,1,"['modulename', 'dir1']","[None, None]","[None, None]",1246,[],"['a_get_pythonversion', 'SourceFileLoader', 'foo.MyClass', 'imp.load_source']",4
utilmy/zarchive/util.py:py_memorysize,py_memorysize,function,16,56,38,312,5.57,0,4,"['o', 'ids', 'hint', 'set(']","[None, None, None, None]","[None, None, '"" deep_getsizeof(df_pd', None]",1258,"['    """""" deep_getsizeof(df_pd, set())\n', '    Find the memory footprint of a Python object\n', '    The sys.getsizeof function does a shallow size of only. It counts each\n', '    object inside a container as pointer only regardless of how big it\n', '    """"""\n']","['id', 'getsizeof', 'ids.add', 'isinstance', 'sum', 'd', 'o.items']",7
utilmy/zarchive/util.py:save,save,function,2,4,4,64,16.0,0,0,"['obj', 'folder', 'isabsolutpath']","[None, None, None]","[None, ""'/folder1/keyname'"", '0']",1286,[],['py_save_obj'],1
utilmy/zarchive/util.py:load,load,function,2,3,3,60,20.0,0,0,"['folder', 'isabsolutpath']","[None, None]","[""'/folder1/keyname'"", '0']",1289,[],['py_load_obj'],1
utilmy/zarchive/util.py:save_test,save_test,function,6,11,11,126,11.45,0,0,"['folder', 'isabsolutpath']","[None, None]","[""'/folder1/keyname'"", '0']",1292,[],"['py_load_obj', 'print', 'str', 'gc.collect']",4
utilmy/zarchive/util.py:py_save_obj,py_save_obj,function,27,58,53,549,9.47,0,1,"['obj1', 'keyname', 'otherfolder']","[None, None, None]","[None, ""''"", '0']",1298,"[""    '''def load_obj(name, encoding1='utf-8' ):\n"", ""         with open('D:/_devs/Python01/aaserialize/' + name + '.pkl', 'rb') as f:\n"", '            return pickle.load(f, encoding=encoding1)\n', ""    '''\n""]","['z_key_splitinto_dir_name', 'os_folder_create', 'open', 'dill.dumps', 'print']",5
utilmy/zarchive/util.py:py_load_obj,py_load_obj,function,16,30,28,265,8.83,0,1,"['folder', 'isabsolutpath', 'encoding1']","[None, None, None]","[""'/folder1/keyname'"", '0', ""'utf-8'""]",1311,"[""    '''def load_obj(name, encoding1='utf-8' ):\n"", ""         with open('D:/_devs/Python01/aaserialize/' + name + '.pkl', 'rb') as f:\n"", '            return pickle.load(f, encoding=encoding1)\n', ""    '''\n""]","['folder.find', 'z_key_splitinto_dir_name', 'os_folder_create', 'open', 'pickle.load']",5
utilmy/zarchive/util.py:z_key_splitinto_dir_name,z_key_splitinto_dir_name,function,6,15,15,112,7.47,0,1,['keyname'],[None],[None],1327,[],"['keyname.split', 'len']",2
utilmy/zarchive/util.py:sql_getdate,sql_getdate,function,0,1,1,4,4.0,0,0,[],[],[],1337,[],[],0
utilmy/zarchive/util.py:obj_getclass_of_method,obj_getclass_of_method,function,7,15,13,114,7.6,1,1,['meth'],[None],[None],1409,[],['inspect.getmro'],1
utilmy/zarchive/util.py:obj_getclass_property,obj_getclass_property,function,4,9,9,63,7.0,1,0,['pfi'],[None],[None],1417,[],"['vars', 'print']",2
utilmy/zarchive/util.py:print_topdf,print_topdf,function,27,114,95,912,8.0,0,0,[],[],[],1434,[],"['PdfPages', 'plt.figure', 'plt.plot', 'plt.title', 'pdf.savefig', 'plt.close', 'plt.rc', 'np.arange', 'np.sin', 'pdf.infodict', 'datetime.datetime']",11
utilmy/zarchive/util.py:os_config_setfile,os_config_setfile,function,9,39,26,229,5.87,1,1,"['dict_params', 'outfile', 'mode1']","[None, None, None]","[None, None, ""'w+'""]",1481,[],"['open', 'dict_params.items', 'isinstance', 'f1.write', 'str', 'f1.close', 'print']",7
utilmy/zarchive/util.py:os_config_getfile,os_config_getfile,function,7,16,15,73,4.56,1,0,['file1'],[None],[None],1493,[],"['open', 'f1.readlines', 'print']",3
utilmy/zarchive/util.py:os_csv_process,os_csv_process,function,1,2,2,9,4.5,0,0,['file1'],[None],[None],1501,[],['print'],1
utilmy/zarchive/util.py:read_funding_data,read_funding_data,function,5,13,12,79,6.08,1,0,['path'],[None],[None],1527,[],"['open', 'csv.DictReader']",2
utilmy/zarchive/util.py:read_funding_data,read_funding_data,function,5,13,12,79,6.08,1,0,['path'],[None],[None],1527,[],"['open', 'csv.DictReader']",2
utilmy/zarchive/util.py:read_funding_data,read_funding_data,function,5,13,12,79,6.08,1,0,['path'],[None],[None],1527,[],"['open', 'csv.DictReader']",2
utilmy/zarchive/util.py:str_match_fuzzy,str_match_fuzzy,function,4,12,11,64,5.33,1,1,"['xstring', 'list_string']","[None, None]","[None, None]",1687,"["" ''' if any of list_strinf elt matches partially xstring '''\n""]",['xstring.find'],1
utilmy/zarchive/util.py:str_parse_stringcalendar,str_parse_stringcalendar,function,14,39,29,278,7.13,3,1,['cal'],[None],[None],1694,"[""    '''----------Parse Calendar  --------'''\n""]","['cal.split', 'x.find', 'cal4.append', 'np.array', 'print']",5
utilmy/zarchive/util.py:str_make_unicode,str_make_unicode,function,7,16,13,102,6.38,0,1,"['input', 'errors']","[None, None]","[None, ""'replace'""]",1708,[],"['type', 'input.decode']",2
utilmy/zarchive/util.py:str_empty_string_array,str_empty_string_array,function,9,30,23,184,6.13,2,1,"['x', 'y']","[None, None]","[None, '1']",1714,[],"['range', 'str_empty_string_array_numpy', 'np.empty']",3
utilmy/zarchive/util.py:str_empty_string_array_numpy,str_empty_string_array_numpy,function,4,9,8,56,6.22,0,0,"['nx', 'ny']","[None, None]","[None, '1']",1718,[],['np.empty'],1
utilmy/zarchive/util.py:str_isfloat,str_isfloat,function,1,8,7,46,5.75,0,0,['value'],[None],[None],1723,[],['float'],1
utilmy/zarchive/util.py:str_is_azchar,str_is_azchar,function,1,8,7,42,5.25,0,0,['x'],[None],[None],1727,[],['float'],1
utilmy/zarchive/util.py:str_is_az09char,str_is_az09char,function,1,8,7,42,5.25,0,0,['x'],[None],[None],1731,[],['float'],1
utilmy/zarchive/util.py:str_reindent,str_reindent,function,1,3,3,28,9.33,0,0,"['s', 'numSpaces']","[None, None]","[None, None]",1735,"[""    '''\n"", '   if args:\n', ""       aux= name1+'.'+obj.__name__ +'('+ str(args) +')  \\n' + str(inspect.getdoc(obj))\n"", ""       aux= aux.replace('\\n', '\\n       ')\n"", '       aux= aux.rstrip()\n', ""       aux= aux + ' \\n'\n"", '       wi( aux)\n', ""    '''\n""]",['x.decode'],1
utilmy/zarchive/util.py:str_split2,str_split2,function,1,3,3,28,9.33,0,0,"['delimiters', 'string', 'maxsplit']","[None, None, None]","[None, None, '0']",1749,[],['x.decode'],1
utilmy/zarchive/util.py:str_split_pattern,str_split_pattern,function,1,3,3,28,9.33,0,0,"['sep2', 'll', 'maxsplit']","[None, None, None]","[None, None, '0']",1754,[],['x.decode'],1
utilmy/zarchive/util.py:pd_str_isascii,pd_str_isascii,function,1,3,3,28,9.33,0,0,['x'],[None],[None],1762,[],['x.decode'],1
utilmy/zarchive/util.py:str_to_utf8,str_to_utf8,function,2,2,2,23,11.5,0,0,['x'],[None],[None],1768,"['  """""" Do it before saving/output to external printer """"""\n']",['x.encode'],1
utilmy/zarchive/util.py:str_to_unicode,str_to_unicode,function,3,14,11,78,5.57,0,2,"['x', 'encoding']","[None, None]","[None, ""'utf-8'""]",1773,"[""  ''' Do it First after Loading some text '''\n""]","['isinstance', 'str']",2
utilmy/zarchive/util.py:np_minimize,np_minimize,function,12,41,37,379,9.24,1,0,"['fun_obj', 'x0', 'argext', '0']","[None, None, None, None]","[None, '[0.0]', '(0', None]",1846,[],"['penalty', 'enumerate', 'max', 'loss', 'fun_obj', 'np_minimizeDE']",6
utilmy/zarchive/util.py:np_minimizeDE,np_minimizeDE,function,16,51,41,418,8.2,1,2,"['fun_obj', 'bounds', 'name1', 'maxiter', 'popsize', 'solver']","[None, None, None, None, None, None]","[None, None, None, '10', '5', 'None']",1859,[],"['range', 'next', 'print', 'save', 'name1+date_now', 'np.mod', 'np.abs']",7
utilmy/zarchive/util.py:np_remove_NA_INF_2d,np_remove_NA_INF_2d,function,9,22,19,116,5.27,2,1,['X'],[None],[None],1876,[],"['np.shape', 'range', 'np.isnan', 'np.isinf']",4
utilmy/zarchive/util.py:np_addcolumn,np_addcolumn,function,8,11,11,86,7.82,0,0,"['arr', 'nbcol']","[None, None]","[None, None]",1883,[],"['np.shape', 'np.zeros', 'np.column_stack']",3
utilmy/zarchive/util.py:np_addrow,np_addrow,function,9,20,19,139,6.95,0,1,"['arr', 'nbrow']","[None, None]","[None, None]",1889,[],"['np.shape', 'len', 'np.zeros', 'np.row_stack', 'np.append']",5
utilmy/zarchive/util.py:np_int_tostr,np_int_tostr,function,3,17,12,69,4.06,0,1,['i'],[None],[None],1898,[],['str'],1
utilmy/zarchive/util.py:np_dictordered_create,np_dictordered_create,function,5,6,6,52,8.67,0,0,[],[],[],1904,[],['OrderedDict'],1
utilmy/zarchive/util.py:np_list_unique,np_list_unique,function,5,8,8,46,5.75,0,0,['seq'],[None],[None],1908,[],"['Set', 'list']",2
utilmy/zarchive/util.py:np_list_tofreqdict,np_list_tofreqdict,function,10,39,23,162,4.15,2,1,"['l1', 'wweight']","[None, None]","[None, '[]']",1914,[],"['dict', 'len', 'enumerate']",3
utilmy/zarchive/util.py:np_list_flatten,np_list_flatten,function,11,25,19,114,4.56,2,1,['seq'],[None],[None],1931,[],"['type', 'flatten', 'l.append']",3
utilmy/zarchive/util.py:np_dict_tolist,np_dict_tolist,function,5,22,15,100,4.55,2,1,"['dd', 'withkey']","[None, None]","[None, '0']",1942,[],['list'],1
utilmy/zarchive/util.py:np_dict_tostr_val,np_dict_tostr_val,function,1,9,9,52,5.78,0,0,['dd'],[None],[None],1948,[],"['str', 'list']",2
utilmy/zarchive/util.py:np_dict_tostr_key,np_dict_tostr_key,function,1,8,8,52,6.5,0,0,['dd'],[None],[None],1951,[],"['str', 'list']",2
utilmy/zarchive/util.py:np_removelist,np_removelist,function,6,15,15,71,4.73,1,1,"['x0', 'xremove']","[None, None]","[None, '[]']",1956,[],"['np_findfirst', 'xnew.append']",2
utilmy/zarchive/util.py:np_transform2d_int_1d,np_transform2d_int_1d,function,19,31,28,227,7.32,2,0,"['m2d', 'onlyhalf']","[None, None]","[None, 'False']",1962,[],"['np.shape', 'np.zeros', 'range', 'np.abs', 'np_sortbycol']",5
utilmy/zarchive/util.py:np_mergelist,np_mergelist,function,5,10,10,55,5.5,1,0,"['x0', 'x1']","[None, None]","[None, None]",1974,[],"['list', 'xnew.append']",2
utilmy/zarchive/util.py:np_enumerate2,np_enumerate2,function,6,14,14,83,5.93,1,0,['vec_1d'],[None],[None],1980,[],"['np.empty', 'enumerate']",2
utilmy/zarchive/util.py:np_pivottable_count,np_pivottable_count,function,12,24,23,170,7.08,1,0,['mylist'],[None],[None],1986,[],"['np_dict_tolist', 'sorted', 'int']",3
utilmy/zarchive/util.py:np_nan_helper,np_nan_helper,function,2,5,5,40,8.0,0,0,['y'],[None],[None],1994,"['    """""" Input:  - y, 1d numpy array with possible NaNs\n', '        Output - nans, logical indices of NaNs - index, a function, with signature indices= index(logical_indices),\n', ""              to convert logical indices of NaNs to 'equivalent' indices\n"", '    """"""\n']","['np.isnan', 'z.nonzero']",2
utilmy/zarchive/util.py:np_interpolate_nan,np_interpolate_nan,function,7,9,9,76,8.44,0,0,['y'],[None],[None],2001,[],"['np_nan_helper', 'np.interp', 'x']",3
utilmy/zarchive/util.py:np_and1,np_and1,function,8,55,22,356,6.47,0,6,"['x', 'y', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']","[None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None', 'None', 'None']",2006,[],[],0
utilmy/zarchive/util.py:np_sortcol,np_sortcol,function,6,8,8,77,9.62,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",2014,"[' """""" df.sort([\'A\', \'B\'], ascending=[1, 0])  """"""\n']","['pd.DataFrame', 'df.sort_values']",2
utilmy/zarchive/util.py:np_sort,np_sort,function,7,19,12,184,9.68,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",2020,[],"['pd.DataFrame', 'df.sort_values', 'np_sort']",3
utilmy/zarchive/util.py:np_ma,np_ma,function,2,3,3,45,15.0,0,0,"['vv', 'n']","[None, None]","[None, None]",2026,"[""  '''Moving average '''\n""]","['np.convolve', 'np.ones']",2
utilmy/zarchive/util.py:np_cleanmatrix,np_cleanmatrix,function,11,22,20,124,5.64,2,1,['m'],[None],[None],2032,[],"['np.nan_to_num', 'np.shape', 'range', 'abs']",4
utilmy/zarchive/util.py:np_torecarray,np_torecarray,function,2,6,6,105,17.5,0,0,"['arr', 'colname']","[None, None]","[None, None]",2040,[],['np.shape'],1
utilmy/zarchive/util.py:np_sortbycolumn,np_sortbycolumn,function,6,8,8,77,9.62,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",2043,[],"['pd.DataFrame', 'df.sort_values']",2
utilmy/zarchive/util.py:np_sortbycol,np_sortbycol,function,8,30,20,257,8.57,0,1,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",2048,[],"['pd.DataFrame', 'df.sort_values', 'np_sortbycol', 'len', 'np.reshape']",5
utilmy/zarchive/util.py:min_kpos,min_kpos,function,2,3,3,36,12.0,0,0,"['arr', 'kth']","[None, None]","[None, None]",2059,"[""   ''' return kth mininimun '''\n""]",['np.partition'],1
utilmy/zarchive/util.py:max_kpos,max_kpos,function,3,5,5,53,10.6,0,0,"['arr', 'kth']","[None, None]","[None, None]",2063,"[""   ''' return kth mininimun '''\n""]","['len', 'np.partition']",2
utilmy/zarchive/util.py:np_findfirst,np_findfirst,function,5,11,10,54,4.91,1,1,"['item', 'vec']","[None, None]","[None, None]",2070,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/util.py:np_find,np_find,function,5,11,10,54,4.91,1,1,"['item', 'vec']","[None, None]","[None, None]",2077,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/util.py:find,find,function,14,54,32,291,5.39,2,4,"['item', 'vec']","[None, None]","[None, None]",2084,"['    """"""return the index of the first occurence of item in vec""""""\n']","['range', 'findnone', 'findx', 'type', 'vec.index', 'len', 'finds']",7
utilmy/zarchive/util.py:findnone,findnone,function,4,11,10,54,4.91,1,1,['vec'],[None],[None],2090,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/util.py:findx,findx,function,6,27,18,119,4.41,0,2,"['item', 'vec']","[None, None]","[None, None]",2096,"['    """"""return the index of the first occurence of item in vec""""""\n']","['type', 'vec.index', 'len']",3
utilmy/zarchive/util.py:finds,finds,function,12,34,23,155,4.56,2,3,"['itemlist', 'vec']","[None, None]","[None, None]",2107,"['  """"""return the index of the first occurence of item in vec""""""\n']","['range', 'idlist.append']",2
utilmy/zarchive/util.py:findhigher,findhigher,function,5,13,12,50,3.85,1,1,"['x', 'vec']","[None, None]","[None, None]",2118,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/util.py:findlower,findlower,function,5,12,11,50,4.17,1,1,"['x', 'vec']","[None, None]","[None, None]",2124,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/util.py:np_find_minpos,np_find_minpos,function,4,7,5,95,13.57,0,0,['values'],[None],[None],2132,[],['min'],1
utilmy/zarchive/util.py:np_find_maxpos,np_find_maxpos,function,17,45,34,266,5.91,1,3,['values'],[None],[None],2136,[],"['max', 'np_find_maxpos_2nd', 'float', 'enumerate']",4
utilmy/zarchive/util.py:np_find_maxpos_2nd,np_find_maxpos_2nd,function,13,37,28,138,3.73,1,3,['numbers'],[None],[None],2140,[],"['float', 'enumerate']",2
utilmy/zarchive/util.py:np_findlocalmax2,np_findlocalmax2,function,45,116,77,690,5.95,2,5,"['v', 'trig']","[None, None]","[None, None]",2152,[],"['n=len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'range', 'len', 'findhigher', 'np.abs']",8
utilmy/zarchive/util.py:np_findlocalmin2,np_findlocalmin2,function,46,119,78,703,5.91,2,6,"['v', 'trig']","[None, None]","[None, None]",2186,[],"['n=len', 'np.zeros', 'np_find_minpos', 'enumerate', 'range', 'len', 'findlower', 'np.abs']",8
utilmy/zarchive/util.py:np_findlocalmax,np_findlocalmax,function,45,116,77,690,5.95,2,5,"['v', 'trig']","[None, None]","[None, None]",2220,[],"['n=len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'range', 'len', 'findhigher', 'np.abs']",8
utilmy/zarchive/util.py:np_findlocalmin,np_findlocalmin,function,46,119,78,703,5.91,2,6,"['v', 'trig']","[None, None]","[None, None]",2234,[],"['n=len', 'np.zeros', 'np_find_minpos', 'enumerate', 'range', 'len', 'findlower', 'np.abs']",8
utilmy/zarchive/util.py:np_stack,np_stack,function,12,53,25,323,6.09,0,7,"['v1', 'v2', 'v3', 'v4', 'v5']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",2250,[],"['np.shape', 'np.row_stack', 'np.column_stack']",3
utilmy/zarchive/util.py:np_uniquerows,np_uniquerows,function,6,8,8,148,18.5,0,0,['a'],[None],[None],2265,[],"['np.ascontiguousarray', 'np.unique', 'unique_a.view']",3
utilmy/zarchive/util.py:np_remove_zeros,np_remove_zeros,function,2,4,4,35,8.75,0,0,"['vv', 'axis1']","[None, None]","[None, '1']",2270,[],[],0
utilmy/zarchive/util.py:np_sort,np_sort,function,7,19,12,184,9.68,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",2273,"['    """"""\n', '    Creates a cross-tab or pivot table from a normalised input table. Use this\n', ""    function to 'denormalize' a table of normalized records.\n"", '\n', '    * The table argument can be a list of dictionaries or a Table object.\n', '    (http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/334621)\n', '    * The left argument is a tuple of headings which are displayed down the\n', '    left side of the new table.\n', '    * The top argument is a tuple of headings which are displayed across the\n', '    top of the new table.\n', '    Tuples are used so that multiple element headings and columns can be used.\n', '\n', '    E.g. To transform the list (listOfDicts):\n', '\n', '    Name,   Year,  Value\n', '    -----------------------\n', ""    'Simon', 2004, 32\n"", ""    'Simon', 2005, 128\n"", ""    'Russel', 2004, 64\n"", ""    'Eric', 2004, 52\n"", ""    'Russel', 2005, 32\n"", '\n', '    into the new list:\n', '\n', ""    'Name',   2004, 2005\n"", '    ------------------------\n', ""    'Simon',  32,     128\n"", ""    'Russel',  64,     32\n"", ""    'Eric',   52,     NA\n"", '\n', '    you would call pivot with the arguments:\n', '\n', ""    newList = pivot(listOfDicts, ('Name',), ('Year',), 'Value')\n"", '\n', '    """"""\n']","['pd.DataFrame', 'df.sort_values', 'np_sort']",3
utilmy/zarchive/util.py:np_memory_array_adress,np_memory_array_adress,function,2,2,2,38,19.0,0,0,['x'],[None],[None],2276,[],[],0
utilmy/zarchive/util.py:np_pivotable_create,np_pivotable_create,function,28,99,61,699,7.06,4,5,"['table', 'left', 'top', 'value']","[None, None, None, None]","[None, None, None, None]",2283,"['    """"""\n', '    Creates a cross-tab or pivot table from a normalised input table. Use this\n', ""    function to 'denormalize' a table of normalized records.\n"", '\n', '    * The table argument can be a list of dictionaries or a Table object.\n', '    (http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/334621)\n', '    * The left argument is a tuple of headings which are displayed down the\n', '    left side of the new table.\n', '    * The top argument is a tuple of headings which are displayed across the\n', '    top of the new table.\n', '    Tuples are used so that multiple element headings and columns can be used.\n', '\n', '    E.g. To transform the list (listOfDicts):\n', '\n', '    Name,   Year,  Value\n', '    -----------------------\n', ""    'Simon', 2004, 32\n"", ""    'Simon', 2005, 128\n"", ""    'Russel', 2004, 64\n"", ""    'Eric', 2004, 52\n"", ""    'Russel', 2005, 32\n"", '\n', '    into the new list:\n', '\n', ""    'Name',   2004, 2005\n"", '    ------------------------\n', ""    'Simon',  32,     128\n"", ""    'Russel',  64,     32\n"", ""    'Eric',   52,     NA\n"", '\n', '    you would call pivot with the arguments:\n', '\n', ""    newList = pivot(listOfDicts, ('Name',), ('Year',), 'Value')\n"", '\n', '    """"""\n']","['tuple', 'ysort.append', 'xsort.append', 'len', 'list', 'xsort.sort', 'headings.extend', 'sortedkeys.sort', 'row.extend', 't.append']",10
utilmy/zarchive/util.py:pd_info,pd_info,function,13,25,24,255,10.2,1,2,"['df', 'doreturn']","[None, None]","[None, '1']",2368,[],"['df.info', 'print', 'type', 'pd_info_memsize', 'pd_info', 'df.memory_usage']",6
utilmy/zarchive/util.py:pd_info_memsize,pd_info_memsize,function,5,6,6,84,14.0,0,1,"['df', 'memusage']","[None, None]","[None, '0']",2375,[],"['df.info', 'pd_info', 'df.memory_usage']",3
utilmy/zarchive/util.py:pd_row_findlast,pd_row_findlast,function,7,13,13,57,4.38,1,1,"['df', 'colid', 'emptyrowid']","[None, None, None]","[None, '0', 'None']",2382,[],['df.iterrows'],1
utilmy/zarchive/util.py:pd_row_select,pd_row_select,function,11,100,54,863,8.63,1,3,"['df', '**conditions']","[None, None]","[None, None]",2388,"[""    '''Select rows from a df according to conditions\n"", '    pdselect(data, a=2, b__lt=3) __gt __ge __lte  __in  __not_in\n', ""    will select all rows where 'a' is 2 and 'b' is less than 3\n"", ""    '''\n""]","['type', 'pd.DataFrame', 'list', 'c.endswith', 'np.in1d', 'ValueError']",6
utilmy/zarchive/util.py:pd_csv_randomread,pd_csv_randomread,function,11,41,37,266,6.49,0,1,"['filename', 'nsample', 'filemaxline', 'dtype']","[None, None, None, None]","[None, '10000', '-1', 'None']",2432,[],"['sum', 'open', 'np.sort', 'pd.read_csv']",4
utilmy/zarchive/util.py:pd_array_todataframe,pd_array_todataframe,function,14,46,33,313,6.8,0,2,"['array', 'colname', 'index1', 'dotranspose']","[None, None, None, None]","[None, 'None', 'None', 'False']",2442,[],"['np.shape', 'len', 'pd.DataFrame', 'np.array']",4
utilmy/zarchive/util.py:pd_dataframe_toarray,pd_dataframe_toarray,function,10,10,10,103,10.3,0,0,['df'],[None],[None],2452,[],['df.reset_index'],1
utilmy/zarchive/util.py:pd_createdf,pd_createdf,function,2,4,4,55,13.75,0,0,"['array1', 'col1', 'idx1']","[None, None, None]","[None, 'None', 'None']",2458,[],['pd.DataFrame'],1
utilmy/zarchive/util.py:pd_create_colmapdict_nametoint,pd_create_colmapdict_nametoint,function,9,12,12,77,6.42,1,0,['df'],[None],[None],2461,"[""  ''' 'close' ---> 5    '''\n""]",['enumerate'],1
utilmy/zarchive/util.py:pd_extract_col_idx_val,pd_extract_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],2471,[],[],0
utilmy/zarchive/util.py:pd_extract_col_uniquevalue_tocsv,pd_extract_col_uniquevalue_tocsv,function,5,10,9,101,10.1,0,0,"['df', 'colname', 'csvfile']","[None, None, None]","[None, ""''"", ""''""]",2475,"["" ''' Write one column into a file   '''\n""]","['np.array', 'pd.DataFrame', 'print']",3
utilmy/zarchive/util.py:pd_split_col_idx_val,pd_split_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],2484,[],[],0
utilmy/zarchive/util.py:pd_splitdf_inlist,pd_splitdf_inlist,function,15,34,26,254,7.47,3,2,"['df', 'colid', 'type1']","[None, None, None]","[None, None, '""dict""']",2487,"[""    ''' Split df into dictionnary of dict/list '''\n""]","['list', 'l1.append']",2
utilmy/zarchive/util.py:pd_find,pd_find,function,38,140,82,994,7.1,5,8,"['df', 'regex_pattern', 'col_restrict', 'isnumeric', 'doreturnposition']","[None, None, None, None, None]","[None, ""'*'"", '[]', 'False', 'False']",2502,"["" ''' Find string / numeric values inside df columns, return position where found\n"", ""     col_restrict : restrict to these columns '''\n""]","['str', 'print', 'enumerate', 'np.column_stack', 'len', 'gc.collect', 'np_dictordered_create', 'range', 'np.array']",9
utilmy/zarchive/util.py:pd_dtypes_totype2,pd_dtypes_totype2,function,6,9,9,58,6.44,1,0,"['df', 'columns', 'targetype']","[None, None, None]","[None, '[]', ""'category'""]",2548,[],[],0
utilmy/zarchive/util.py:pd_dtypes,pd_dtypes,function,7,12,11,91,7.58,1,0,"['df', 'columns', 'targetype']","[None, None, None]","[None, '[]', ""'category'""]",2553,[],['pd_dtypes'],1
utilmy/zarchive/util.py:pd_df_todict2,pd_df_todict2,function,16,31,27,247,7.97,1,2,"['df1', 'colkey', 'excludekey', 'onlyfirstelt']","[None, None, None, None]","[None, ""'table'"", ""['']"", ' True']",2568,[],"['df.drop_duplicates', 'range', 'len', 'dict0.setdefault']",4
utilmy/zarchive/util.py:pd_df_todict,pd_df_todict,function,17,35,31,306,8.74,1,2,"['df1', 'colkey', 'excludekey', 'onlyfirstelt']","[None, None, None, None]","[None, ""'table'"", ""['']"", ' True']",2581,[],"['df.drop_duplicates', 'range', 'len', 'dict0.setdefault', 'pd_df_todict']",5
utilmy/zarchive/util.py:pd_col_addfrom_dfmap,pd_col_addfrom_dfmap,function,12,44,34,229,5.2,0,1,"['df', 'dfmap', 'colkey', 'colval', 'df_colused', 'df_colnew', 'exceptval', 'inplace']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, '-1', ' True']",2594,"[""  ''' Add new columns based on df_map:  In Place Modification of df\n"", '    df:     Dataframe of transactions.\n', '    dfmap:  FSMaster Dataframe\n', '      colkey: colum used for dict key.  machine_code\n', '      colval: colum used for dict val.  adress\n', '      \n', '    df_colused  :     ""machine_code""\n', ""    exception val:  -1 or ''\n"", ""  '''\n""]","['pd_df_todict', 'map_dict_fun', 'df.apply']",3
utilmy/zarchive/util.py:pd_applyfun_col,pd_applyfun_col,function,5,11,9,107,9.73,0,1,"['df', 'newcol', 'ff', 'use_colname']","[None, None, None, None]","[None, None, None, '""all/[colname]""']",2652,"[""   ''' use all Columns to compute values '''\n""]",['ff'],1
utilmy/zarchive/util.py:pd_date_intersection,pd_date_intersection,function,7,18,15,157,8.72,1,0,['qlist'],[None],[None],2669,[],"['set', 'set.intersection', 'list', 'sorted']",4
utilmy/zarchive/util.py:pd_is_categorical,pd_is_categorical,function,2,12,10,108,9.0,0,1,['z'],[None],[None],2677,[],['isinstance'],1
utilmy/zarchive/util.py:pd_str_encoding_change,pd_str_encoding_change,function,6,8,8,76,9.5,1,0,"['df', 'cols', 'fromenc', 'toenc']","[None, None, None, None]","[None, None, ""'iso-8859-1'"", ""'utf-8'""]",2684,[],[],0
utilmy/zarchive/util.py:pd_str_unicode_tostr,pd_str_unicode_tostr,function,2,4,4,60,15.0,0,0,"['df', 'targetype']","[None, None]","[None, 'str']",2690,"["" '''\n"", ' https://www.azavea.com/blog/2014/03/24/solving-unicode-problems-in-python-2-7/\n', ' Nearly every Unicode problem can be solved by the proper application of these tools;\n', ' they will help you build an airlock to keep the inside of your code nice and clean:\n', '\n', 'encode(): Gets you from Unicode -> bytes\n', 'decode(): Gets you from bytes -> Unicode\n', 'codecs.open(encoding=utf-8): Read and write files directly to/from Unicode (you can use any encoding,\n', ' not just utf-8, but utf-8 is most common).\n', 'u: Makes your string literals into Unicode objects rather than byte sequences.\n', 'Warning: Dont use encode() on bytes or decode() on Unicode objects\n', '\n', '>>> uni_greeting % utf8_name\n', 'Traceback (most recent call last):\n', ' File ""<stdin>"", line 1, in <module>\n', ""UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 3: ordinal not in range(128)\n"", '# Solution:\n', "">>> uni_greeting % utf8_name.decode('utf-8')\n"", ""u'Hi, my name is Josxe9.'\n"", '\n', "" '''\n""]",['pd_dtypes_type1_totype2'],1
utilmy/zarchive/util.py:pd_dtypes_type1_totype2,pd_dtypes_type1_totype2,function,7,13,12,100,7.69,1,1,"['df', 'fromtype', 'targetype']","[None, None, None]","[None, 'str', 'str']",2714,[],['isinstance'],1
utilmy/zarchive/util.py:pd_resetindex,pd_resetindex,function,3,4,4,50,12.5,0,0,['df'],[None],[None],2723,[],['list'],1
utilmy/zarchive/util.py:pd_insertdatecol,pd_insertdatecol,function,8,11,11,192,17.45,0,0,"['df_insider', 'format1=""%Y-%m-%d %H']","[None, '']","[None, '""%Y-%m-%d %H:%M:%S:%f""']",2727,[],"['pd_addcol', 'date_nowtime']",2
utilmy/zarchive/util.py:pd_replacevalues,pd_replacevalues,function,11,14,14,101,7.21,1,0,"['df', 'matrix']","[None, None]","[None, None]",2733,"["" ''' Matrix replaces df.values  '''\n""]",['np.shape'],1
utilmy/zarchive/util.py:pd_removerow,pd_removerow,function,2,2,2,29,14.5,0,0,"['df', 'row_list_index', '45]']","[None, None, None]","[None, '[23', None]",2742,[],['df.drop'],1
utilmy/zarchive/util.py:pd_removecol,pd_removecol,function,2,3,3,28,9.33,0,0,"['df1', 'name1']","[None, None]","[None, None]",2745,[],['df1.drop'],1
utilmy/zarchive/util.py:pd_insertrow,pd_insertrow,function,6,10,10,102,10.2,0,0,"['df', 'rowval', 'index1', 'isreset']","[None, None, None, None]","[None, None, 'None', '1']",2749,[],"['pd_array_todataframe', 'df.append']",2
utilmy/zarchive/util.py:pd_h5_cleanbeforesave,pd_h5_cleanbeforesave,function,5,12,11,91,7.58,0,0,['df'],[None],[None],2757,"[""   '''Clean Column type before Saving in HDFS: Unicode, Datetime  '''\n""]","['pd_resetindex', 'pd_str_unicode_tostr']",2
utilmy/zarchive/util.py:pd_h5_addtable,pd_h5_addtable,function,5,18,17,148,8.22,0,1,"['df', 'tablename', ""dbfile='F""]","[None, None, '']","[None, None, ""'F:\\temp_pandas.h5'""]",2773,[],"['pd.HDFStore', 'find', 'list', 'print', 'store.append', 'store.close']",6
utilmy/zarchive/util.py:pd_h5_tableinfo,pd_h5_tableinfo,function,4,4,4,65,16.25,0,0,"['filenameh5', 'table']","[None, None]","[None, None]",2782,[],"['pd.HDFStore', 'store.get_storer']",2
utilmy/zarchive/util.py:pd_h5_dumpinfo,pd_h5_dumpinfo,function,12,23,23,304,13.22,1,0,"[""dbfile='E""]",[''],"[""'E:\\_data\\stock\\intraday_google.h5'""]",2787,[],"['pd.HDFStore', 'list', 'pd.read_hdf', 'extract.append', 'datetime_tostring', 'errsym.append', 'np.array']",7
utilmy/zarchive/util.py:pd_h5_save,pd_h5_save,function,3,6,6,64,10.67,0,0,"['df', ""filenameh5='E"", 'key']","[None, '', None]","[None, ""'E:/_data/_data_outlier.h5'"", ""'data'""]",2800,"["" ''' File is release after saving it'''\n""]","['pd.HDFStore', 'store.append', 'store.close']",3
utilmy/zarchive/util.py:pd_h5_load,pd_h5_load,function,4,22,16,185,8.41,0,2,"[""filenameh5='E"", 'table_id', 'exportype', 'rowstart', 'rowend', 'cols']","['', None, None, None, None, None]","[""'E:/_data/_data_outlier.h5'"", ""'data'"", '""pandas""', '-1', '-1', '[]']",2805,[],['pd.read_hdf'],1
utilmy/zarchive/util.py:pd_h5_fromcsv_tohdfs,pd_h5_fromcsv_tohdfs,function,30,78,65,670,8.59,3,2,"['dircsv', 'filepattern', 'tofilehdfs', 'tablename', 'col_category', 'dtype0', 'encoding', 'chunksize', 'mode', 'format', 'complib']","[None, None, None, None, None, None, None, None, None, None, None]","[""'dir1/dir2/'"", ""'*.csv'"", ""'file1.h5'"", ""'df'"", '[]', 'None', ""'utf-8'"", ' 2000000', ""'a'"", ""'table'"", 'None']",2811,[],"['os_file_listall', 'pd.HDFStore', 'enumerate', 'pd.read_csv', 'print', 'store.append', 'store.close', 'os_file_exist']",8
utilmy/zarchive/util.py:pd_np_toh5file,pd_np_toh5file,function,2,8,8,76,9.5,0,0,"['numpyarr', 'fileout', 'table1']","[None, None, None]","[None, '""file.h5""', ""'data'""]",2848,[],"['pd.DataFrame', 'pd.HDFStore', 'st.append']",3
utilmy/zarchive/util.py:date_allinfo,date_allinfo,function,0,0,0,0,0.0,0,0,[],[],[],2856,"[""   '''\n"", '\n', '   https://aboutsimon.com/blog/2016/08/04/datetime-vs-Arrow-vs-Pendulum-vs-Delorean-vs-udatetime.html\n', '\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/util.py:date_convert,date_convert,function,14,53,37,303,5.72,1,1,"['t1', 'fromtype', 'totype']","[None, None, None]","[None, None, None]",2865,[],"['len', 'isinstance', '_dateconvert_from', 'tlist.append']",4
utilmy/zarchive/util.py:datetime_tostring,datetime_tostring,function,8,26,21,244,9.38,1,2,['datelist1'],[None],[None],2889,[],"['isinstance', 'datelist1.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zarchive/util.py:date_remove_bdays,date_remove_bdays,function,16,41,28,351,8.56,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",2899,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/util.py:date_add_bdays,date_add_bdays,function,16,40,29,350,8.75,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",2913,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/util.py:datenumpy_todatetime,datenumpy_todatetime,function,11,36,22,384,10.67,0,3,"['tt', 'islocaltime']","[None, None]","[None, 'True']",2927,[],['type'],1
utilmy/zarchive/util.py:datetime_tonumpydate,datetime_tonumpydate,function,2,2,2,22,11.0,0,0,"['t', 'islocaltime']","[None, None]","[None, 'True']",2943,[],['np.datetime64'],1
utilmy/zarchive/util.py:datestring_todatetime,datestring_todatetime,function,6,15,13,125,8.33,1,1,"['datelist1', 'format1']","[None, None]","[None, ' ""%Y%m%d""']",2947,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/util.py:datetime_toint,datetime_toint,function,6,15,13,155,10.33,1,1,['datelist1'],[None],[None],2955,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/util.py:date_holiday,date_holiday,function,0,0,0,0,0.0,0,0,[],[],[],2962,"[""   '''\n"", '   https://jakevdp.github.io/blog/2015/07/23/learning-seattles-work-habits-from-bicycle-counts/\n', '\n', 'from pandas.tseries.holiday import USFederalHolidayCalendar\n', 'cal = USFederalHolidayCalendar()\n', ""holidays = cal.holidays('2012', '2016', return_name=True)\n"", 'holidays.head()\n', '\n', 'holidays_all = pd.concat([holidays, ""Day Before "" + holidays.shift(-1, \'D\'),  ""Day After "" + holidays.shift(1, \'D\')])\n', 'holidays_all = holidays_all.sort_index()\n', 'holidays_all.head()\n', '\n', 'from pandas.tseries.offsets import CustomBusinessDay\n', 'from pandas.tseries.holiday import USFederalHolidayCalendar\n', 'bday_us = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n', 'dateref[-1] - bday_us- bday_us\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/util.py:date_add_bday,date_add_bday,function,16,40,29,350,8.75,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",2982,"["" ''' str / stamp /  '''\n""]","['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/util.py:dateint_todatetime,dateint_todatetime,function,6,15,13,135,9.0,1,1,['datelist1'],[None],[None],2995,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/util.py:date_diffinday,date_diffinday,function,4,5,5,74,14.8,0,0,"['intdate1', 'intdate2']","[None, None]","[None, None]",3003,[],['dateint_todatetime'],1
utilmy/zarchive/util.py:date_diffinyear,date_diffinyear,function,2,3,3,53,17.67,0,0,"['startdate', 'enddate']","[None, None]","[None, None]",3007,[],['date_as_float'],1
utilmy/zarchive/util.py:date_diffinbday,date_diffinbday,function,7,11,9,112,10.18,0,0,"['intd2', 'intd1']","[None, None]","[None, None]",3010,[],"['dateint_todatetime', 'd1.date', 'd2.date', 'np.busday_count']",4
utilmy/zarchive/util.py:date_gencalendar,date_gencalendar,function,9,13,11,233,17.92,0,0,"['start', 'end', 'country']","[None, None, None]","[""'2010-01-01'"", ""'2010-01-15'"", ""'us'""]",3018,[],"['CustomBusinessDay', 'np.arrray']",2
utilmy/zarchive/util.py:date_finddateid,date_finddateid,function,5,75,20,437,5.83,0,10,"['date1', 'dateref']","[None, None]","[None, None]",3024,[],['np_findfirst'],1
utilmy/zarchive/util.py:datestring_toint,datestring_toint,function,6,16,14,106,6.62,1,1,['datelist1'],[None],[None],3038,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/util.py:date_now,date_now,function,15,45,31,396,8.8,0,2,['i'],[None],['0'],3044,[],"['datetime.now', 'date_add_bdays', 'date_remove_bdays', 'str', 'date_nowtime', 'datetime.today', 'd.strftime']",7
utilmy/zarchive/util.py:date_nowtime,date_nowtime,function,8,23,18,205,8.91,0,1,"['type1', 'format1= ""%Y-%m-%d %H']","[None, '']","[""'str'"", ' ""%Y-%m-%d %H:%M:%S:%f""']",3051,"["" ''' str / stamp /  '''\n""]","['datetime.today', 'd.strftime']",2
utilmy/zarchive/util.py:date_tofloat,date_tofloat,function,11,28,23,273,9.75,0,1,['dt'],[None],[None],3060,[],"['datetime.datetime', 'isleap', 'timedelta']",3
utilmy/zarchive/util.py:date_generatedatetime,date_generatedatetime,function,18,27,27,267,9.89,0,1,"['start', 'nbday', 'end']","[None, None, None]","['""20100101""', '10', '""""']",3068,[],"['datestring_todatetime', 'date_add_bdays', 'datetime.timedelta', 'list', 'np.array']",5
utilmy/zarchive/util.py:np_numexpr_vec_calc,np_numexpr_vec_calc,function,13,28,22,214,7.64,0,0,"['filename', 'expr', 'i0', 'imax', ""fileout='E""]","[None, None, None, None, '']","[None, None, '0', '1000', ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",3081,"["" ''' New= xx*xx  over very large series\n"", ' #numexpr_vect_calc(filename, 0, imax=16384*4096, ""xx*xx"", \'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5\'  ):\n', ""'''\n""]","['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/util.py:np_numexpr_tohdfs,np_numexpr_tohdfs,function,10,28,22,248,8.86,0,0,"['filename', 'expr', 'i0', 'imax', ""fileout='E""]","[None, None, None, None, '']","[None, None, '0', '1000', ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",3095,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/util.py:np_comoment,np_comoment,function,4,6,6,61,10.17,0,0,"['xx', 'yy', 'nsample', 'kx', 'ky']","[None, None, None, None, None]","[None, None, None, None, None]",3111,[],['ne.evaluate'],1
utilmy/zarchive/util.py:np_acf,np_acf,function,12,34,31,244,7.18,0,0,['data'],[None],[None],3117,[],"['len', 'np.mean', 'np.sum', 'float', 'r', 'np.arange', 'np.asarray']",7
utilmy/zarchive/util.py:plot_XY,plot_XY,function,25,83,60,688,8.29,0,3,"['xx', 'yy', 'zcolor', 'tsize', 'title1', 'xlabel', 'ylabel', 'figsize', '6']","[None, None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', ""''"", ""''"", ""''"", '(8', None]",3133,[],"['isinstance', 'np.array', 'np.abs', 'plt.get_cmap', 'np.min', 'plt.subplots', 'fig.set_size_inches', 'fig.set_dpi', 'fig.tight_layout', 'ax1.scatter', 'ax1.set_xlabel', 'ax1.set_ylabel', 'ax1.set_title', 'ax1.grid', 'plt.savefig', 'plt.clf', 'plt.show']",17
utilmy/zarchive/util.py:plot_heatmap,plot_heatmap,function,15,26,25,364,14.0,0,1,"['frame', 'ax', 'cmap', 'vmin', 'vmax', 'interpolation']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', ""'nearest'""]",3173,[],"['plt.gca', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_yticks', 'ax.set_yticklabels', 'ax.grid', 'ax.set_aspect', 'ax.imshow']",8
utilmy/zarchive/util.py:np_map_dict_to_bq_schema,np_map_dict_to_bq_schema,function,13,59,31,720,12.2,3,3,"['source_dict', 'schema', 'dest_dict']","[None, None, None]","[None, None, None]",3190,"[""    '''\n"", '     new_dict = {}\n', '     map_dict_to_bq_schema (my_dict, schema, new_dict)\n', '\n', '     new_dict = {}\n', '     map_dict_to_bq_schema (my_dict, schema, new_dict)\n', '\n', '    :param source_dict:\n', '    :param schema:\n', '    :param dest_dict:\n', '    :return:\n', ""    '''\n""]","['np_map_dict_to_bq_schema', 'format_value_bq']",2
utilmy/zarchive/util.py:googledrive_get,googledrive_get,function,0,1,1,4,4.0,0,0,[],[],[],3235,"[""   '''\n"", '   https://github.com/ctberthiaume/gdcp\n', '   ... I am using this now to transfer thousands of mp3 files from a ubuntu vps to google drive.\n', '\n', '\n', 'http://olivermarshall.net/how-to-upload-a-file-to-google-drive-from-the-command-line/\n', 'https://github.com/prasmussen/gdrive  : Super Complete\n', '\n', 'gdrive [global] upload [options] <path>\n', '\n', 'global:\n', '  -c, --config <configDir>         Application path, default: /Users/<user>/.gdrive\n', '  --refresh-token <refreshToken>   Oauth refresh token used to get access token (for advanced users)\n', '  --access-token <accessToken>     Oauth access token, only recommended for short-lived requests because of short lifetime (for advanced users)\n', '\n', 'options:\n', '  -r, --recursive           Upload directory recursively\n', '  -p, --parent <parent>     Parent id, used to upload file to a specific directory, can be specified multiple times to give many parents\n', '  --name <name>             Filename\n', '  --no-progress             Hide progress\n', '  --mime <mime>             Force mime type\n', '  --share                   Share file\n', '  --delete                  Delete local file when upload is successful\n', '  --chunksize <chunksize>   Set chunk size in bytes, default: 8388608\n', '\n', '   :return:\n', ""   '''\n""]",[],0
utilmy/zarchive/util.py:googledrive_put,googledrive_put,function,0,1,1,4,4.0,0,0,[],[],[],3265,"[""  '''\n"", '  100 GB: 2USD,  1TB: 10USD\n', '  https://gsuite.google.com/intl/en/pricing.html\n', '\n', '  :return:\n', ""  '''\n""]",[],0
utilmy/zarchive/util.py:googledrive_list,googledrive_list,function,0,1,1,4,4.0,0,0,[],[],[],3275,[],[],0
utilmy/zarchive/util.py:os_processify_fun,os_processify_fun,function,32,78,63,713,9.14,0,1,['func'],[None],[None],3280,"[""    '''Decorator to run a function as a process.\n"", '    Be sure that every argument and the return value is *pickable*.\n', '    The created process is joined, so the code does not  run in parallel.\n', '    @processify\n', '\n', '    def test():\n', '      return os.getpid()\n', '\n', '    @processify\n', '    def test_deadlock():\n', '      return range(30000)\n', '\n', '   @processify\n', '   def test_exception():\n', ""     raise RuntimeError('xyz')\n"", '\n', '   def test():\n', '     print os.getpid()\n', '     print test_function()\n', '     print len(test_deadlock())\n', '     test_exception()\n', '\n', ""   if __name__ == '__main__':\n"", '     test()\n', '\n', ""    '''\n""]","['process_func', 'func', 'sys.exc_info', 'q.put', 'setattr', 'wrapper', 'Queue', 'Process', 'list', 'p.start', 'q.get', 'ex_type']",12
utilmy/zarchive/util.py:ztest_processify,ztest_processify,function,2,2,2,17,8.5,0,0,[],[],[],3347,[],['os.getpid'],1
utilmy/zarchive/util.py:date_getspecificdate,date_getspecificdate,function,21,194,56,1312,6.76,8,19,"['datelist', 'datetype1', 'outputype1', 'includelastdate', 'includefirstdate', '']","[None, None, None, None, None, None]","[None, '""yearend""', '""intdate""', 'True', 'False', None]",3353,[],"['isint', 'dateint_todatetime', 'enumerate', 'vec2.append', 'np.mod', 'np.array']",6
utilmy/zarchive/util.py:py_exception_print,py_exception_print,function,16,27,26,281,10.41,0,0,[],[],[],3413,[],"['sys.exc_info', 'linecache.checkcache', 'linecache.getline', 'print', 'line.strip']",5
utilmy/zarchive/util.py:py_log_write,py_log_write,function,13,29,26,365,12.59,0,0,"['LOGFILE', 'prefix']","[None, None]","[None, None]",3424,[],"['print', 'arrow.utcnow', 'str', 'open']",4
utilmy/zarchive/util.py:testclass,testclass,class,23,50,40,392,7.84,0,0,[],[],[],148,[],[],0
utilmy/zarchive/util.py:FundingRecord,FundingRecord,class,14,59,48,394,6.68,0,0,[],[],[],1582,[],[],0
utilmy/zarchive/util.py:testclass:__init__,testclass:__init__,method,0,1,1,4,4.0,0,0,"['self', 'x']","[None, None]","[None, None]",149,[],[],0
utilmy/zarchive/util.py:testclass:z_autotest,testclass:z_autotest,method,21,43,36,345,8.02,0,0,['self'],[None],[None],152,[],"['io.StringIO', 'f', 'exec', 'print', 'codeErr.getvalue', 'codeOut.getvalue', 'codeOut.close', 'codeErr.close']",8
utilmy/zarchive/util.py:FundingRecord:parse,FundingRecord:parse,method,7,35,26,223,6.37,0,0,"['klass', 'row']","[None, None]","[None, None]",1585,[],"['list', 'int', 'datetime.strptime', 'klass']",4
utilmy/zarchive/util.py:FundingRecord:__str__,FundingRecord:__str__,method,6,18,18,118,6.56,0,0,['self'],[None],[None],1592,[],[],0
utilmy/zarchive/util_aws.py:aws_credentials,aws_credentials,function,3,7,7,126,18.0,0,0,['account'],[None],['None'],41,"['    """"""\n', '    Return a tuple of AWS credentials (access key id and secret access key) for\n', '    the given account.\n', '    """"""\n']",['INIConfig'],1
utilmy/zarchive/util_aws.py:aws_ec2_get_instanceid,aws_ec2_get_instanceid,function,4,7,6,110,15.71,0,1,"['con', 'ip_address']","[None, None]","[None, None]",283,[],['con.get_all_instances'],1
utilmy/zarchive/util_aws.py:aws_ec2_allocate_elastic_ip,aws_ec2_allocate_elastic_ip,function,7,31,23,358,11.55,0,1,"['con', 'instance_id', 'elastic_ip', 'region']","[None, None, None, None]","[None, '""""', ""''"", '""ap-northeast-2""']",288,[],"['con.allocate_address', 'con.associate_address']",2
utilmy/zarchive/util_aws.py:aws_ec2_printinfo,aws_ec2_printinfo,function,5,17,12,93,5.47,0,3,"['instance', 'ipadress', 'instance_id']","[None, None, None]","['None', '""""', '""""']",300,"[""   '''   Idenfiy instnance of\n"", '   :param instance: \n', '     ipadress\n', '   :param instance_id: \n', '   :return: return info on the instance : ip, ip_adress,  \n', ""   '''\n""]","['print', 'pprint']",2
utilmy/zarchive/util_aws.py:aws_ec2_spot_start,aws_ec2_spot_start,function,26,96,89,1014,10.56,1,1,"['con', 'region', 'key_name', 'inst_type', 'ami_id', 'pricemax', 'elastic_ip', 'pars= {""security_group""', '""disk_size""', '""disk_type""', '""volume_type""']","[None, None, None, None, None, None, None, '', ' 25', ' ""ssd""', ' ""gp2""}']","[None, None, '""ecsInstanceRole""', '""cx2.2""', '""""', '0.15', ""''"", ' {""security_group"": [""""]', None, None, None]",320,"[""    '''\n"", '   :param con:   Connector to Boto\n', '   :param region: AWS region (us-east-1,..) \n', '   :param key_name: AWS  SSH Key Name  (in EC2 webspage )\n', '   :param security_group: AWS security group id\n', '   :param inst_type:  AWS EC2 instance type (t1.micro, m1.small ...)\n', '   :param ami_id:  AWS AMI ID\n', '   :param pars: Disk Size, Volume type (General Purpose SSD - gp2, Magnetic etc)\n', '   :param pricemax: minmum spot instance bid price\n', ""    '''\n""]","['dict2', 'BlockDeviceMapping', 'EBSBlockDeviceType', 'int', 'print', 'con.request_spot_instances', 'con.get_all_spot_instance_requests', 'con.get_all_instances', 'aws_ec2_allocate_elastic_ip', 'aws_ec2_printinfo', 'sleep']",11
utilmy/zarchive/util_aws.py:aws_ec2_get_id,aws_ec2_get_id,function,4,11,9,69,6.27,0,2,"['ipadress', 'instance_id']","[None, None]","[""''"", ""''""]",368,[],[],0
utilmy/zarchive/util_aws.py:aws_ec2_spot_stop,aws_ec2_spot_stop,function,5,39,33,281,7.21,0,1,"['con', 'ipadress', 'instance_id']","[None, None, None]","[None, '""""', '""""']",379,"[""   '''\n"", '   :param con: connector \n', '   :param ipadress:   of the instance  to Identify the instance.\n', '   :param instance_id:  OR use instance ID....\n', '   :return: \n', ""   '''\n""]","['aws_ec2_get_instanceid', 'str', 'con.terminate_instances']",3
utilmy/zarchive/util_aws.py:aws_ec2_res_start,aws_ec2_res_start,function,26,97,90,972,10.02,1,1,"['con', 'region', 'key_name', 'ami_id', 'inst_type', 'min_count ', 'max_count ', 'pars= {""security_group""', '""disk_size""', '""disk_type""', '""volume_type""']","[None, None, None, None, None, None, None, '', ' 25', ' ""ssd""', ' ""gp2""}']","[None, None, None, None, '""cx2.2""', '1', '1', ' {""security_group"": [""""]', None, None, None]",405,"[""    '''  \n"", '        normal instance start\n', '        :param con:   Connector to Boto\n', '        :param region: AWS region (us-east-1,..) \n', '        :param key_name: AWS  SSH Key Name\n', '        :param security_group: AWS security group id\n', '        :param inst_type:  AWS EC2 instance type (t1.micro, m1.small ...)\n', '        :param ami_id:  AWS AMI ID\n', '        :param min_count: Minumum number of instances\n', '        :param max_count : Maximum number of instances\n', '        :param pars: Disk Size, Volume type (General Purpose SSD - gp2, Magnetic etc)\n', '        :return \n', ""    '''\n""]","['dict2', 'BlockDeviceMapping', 'EBSBlockDeviceType', 'int', 'print', 'con.run_instances', 'con.get_all_instances', 'aws_ec2_printinfo', 'sleep']",9
utilmy/zarchive/util_aws.py:aws_ec2_res_stop,aws_ec2_res_stop,function,7,44,37,309,7.02,0,1,"['con', 'ipadress', 'instance_id']","[None, None, None]","[None, '""""', '""""']",450,"[""   '''\n"", '   :param con: connector \n', '   :param ipadress:     Of the instance  to Identify the instance.\n', '   :param instance_id:  OR use instance ID....\n', '   :return: \n', ""   '''\n""]","['aws_ec2_get_instanceid', 'con.stop_instances', 'str']",3
utilmy/zarchive/util_aws.py:aws_accesskey_get,aws_accesskey_get,function,6,19,16,193,10.16,0,1,"['access', 'key']","[None, None]","[""''"", ""''""]",483,[],['print'],1
utilmy/zarchive/util_aws.py:aws_conn_do,aws_conn_do,function,2,3,3,39,13.0,0,0,"['action', 'region']","[None, None]","[""''"", '""ap-northeast-2""']",492,[],['aws_conn_create'],1
utilmy/zarchive/util_aws.py:aws_conn_getallregions,aws_conn_getallregions,function,2,2,2,28,14.0,0,0,['conn'],[None],['None'],497,[],['conn.get_all_regions'],1
utilmy/zarchive/util_aws.py:aws_conn_create,aws_conn_create,function,21,36,34,282,7.83,1,2,"['region', 'access', 'key']","[None, None, None]","['""ap-northeast-2""', ""''"", ""''""]",500,[],"['aws_accesskey_get', 'EC2Connection', 'aws_conn_getallregions', 'conn=EC2Connection', 'print']",5
utilmy/zarchive/util_aws.py:aws_conn_getinfo,aws_conn_getinfo,function,1,2,2,23,11.5,0,0,['conn'],[None],[None],513,[],['print'],1
utilmy/zarchive/util_aws.py:aws_s3_url_split,aws_s3_url_split,function,4,5,5,52,10.4,0,0,['url'],[None],[None],543,"[""  '''Split into Bucket, url '''\n""]",['url.split'],1
utilmy/zarchive/util_aws.py:aws_s3_getbucketconn,aws_s3_getbucketconn,function,13,17,16,221,13.0,0,0,['s3dir'],[None],[None],548,[],"['aws_s3_url_split', 'aws_accesskey_get', 'boto.connect_s3', 'conn.get_bucket']",4
utilmy/zarchive/util_aws.py:aws_s3_puto_s3,aws_s3_puto_s3,function,35,114,102,1162,10.19,3,2,"['fromdir_file', 'todir']","[None, None]","[""'dir/file.zip'"", ""'bucket/folder1/folder2'""]",556,"["" ''' Copy File or Folder to S3 '''\n""]","['aws_s3_getbucketconn', 'aws_s3_url_split', 'fromdir_file.find', 'util.os_file_getname', 'os.walk', 'uploadFileNames.extend', 'percent_cb', 'print', 'bucket.initiate_multipart_upload', 'open', 'mp.upload_part_from_file', 'mp.complete_upload', 'k.set_contents_from_filename']",13
utilmy/zarchive/util_aws.py:aws_s3_getfrom_s3,aws_s3_getfrom_s3,function,21,44,44,435,9.89,1,1,"['froms3dir', 'todir', 'bucket_name']","[None, None, None]","[""'task01/'"", ""''"", ""'zdisk'""]",600,"["" ''' Get from S3 file/folder  '''\n""]","['aws_s3_url_split', 'aws_s3_getbucketconn', 'bucket.list', 'str', 'util.os_file_getname', 'util.os_file_getpath', 'os.makedirs', 'l.get_contents_to_filename']",8
utilmy/zarchive/util_aws.py:aws_s3_folder_printtall,aws_s3_folder_printtall,function,12,19,18,234,12.32,1,0,['bucket_name'],[None],"[""'zdisk'""]",620,[],"['aws_accesskey_get', 'boto.connect_s3', 'conn.create_bucket', 'bucket.list']",4
utilmy/zarchive/util_aws.py:aws_s3_file_read,aws_s3_file_read,function,10,14,13,186,13.29,0,0,"['bucket1', 'filepath', 'isbinary']","[None, None, None]","[None, None, '1']",630,"[""  ''' s3_client = boto3.client('s3')\n"", '    #Download private key file from secure S3 bucket\n', ""  s3_client.download_file('s3-key-bucket','keys/keyname.pem', '/tmp/keyname.pem')\n"", ""  '''\n""]","['S3Connection', 'conn.get_object']",2
utilmy/zarchive/util_aws.py:aws_ec2_cmd_ssh,aws_ec2_cmd_ssh,function,23,82,72,560,6.83,1,4,"['cmdlist', 'host', 'doreturn', 'ssh', 'username', 'keyfilepath']","[None, None, None, None, None, None]","['  [""ls "" ]', ""'ip'"", '0', 'None', ""'ubuntu'"", ""''""]",642,"[""    ''' SSH Linux terminal Command\n"", '     https://www.siteground.com/tutorials/ssh/ssh_deleting.htm\n', '\n', '     rm -rf foldername/\n', '\n', '\n', '    fuser 8888/tcp     Check if Jupyter is running\n', '    ps -ef | grep python     :List of  PID Python process\n', '    kill -9 PID_number     (i.e. the pid returned)\n', '    top     : CPU usage\n', '\n', '      Run nohup python bgservice.py & to get the script to ignore the hangup signal and keep running.\n', '      Output will be put in nohup.out.\n', '        ""aws s3 cp s3://s3-bucket/scripts/HelloWorld.sh /home/ec2-user/HelloWorld.sh"",\n', '        ""chmod 700 /home/ec2-user/HelloWorld.sh"",\n', '        ""/home/ec2-user/HelloWorld.sh""\n', '\n', '    https://aws.amazon.com/blogs/compute/scheduling-ssh-jobs-using-aws-lambda/\n', ""   '''\n""]","['len', 'aws_ec2_create_con', 'print', 'isinstance', 'ssh.exec_command', 'stdout.read', 'stderr.read', 'readall.append', 'ssh.close']",9
utilmy/zarchive/util_aws.py:aws_ec2_python_script,aws_ec2_python_script,function,5,20,18,195,9.75,0,0,"['script_path', 'args1', 'host']","[None, None, None]","[None, None, None]",682,[],['aws_ec2_cmd_ssh'],1
utilmy/zarchive/util_aws.py:aws_ec2_create_con,aws_ec2_create_con,function,36,121,93,1016,8.4,1,9,"['contype', 'host', 'port', 'username', 'keyfilepath', 'password', 'keyfiletype', 'isprint']","[None, None, None, None, None, None, None, None]","[""'sftp/ssh'"", ""'ip'"", '22', ""'ubuntu'"", ""''"", ""''"", ""'RSA'"", '1']",687,"['    """""" Transfert File  host = \'52.79.79.1\'\n', ""        keyfilepath = 'D:/_devs/aws/keypairs/ec2_instanc'\n"", '\n', '# List files in the default directory on the remote computer.\n', ""dirlist = sftp.listdir('.')\n"", ""sftp.get('remote_file.txt', 'downloaded_file.txt')\n"", ""sftp.put('testfile.txt', 'remote_testfile.txt')\n"", '\n', 'http://docs.paramiko.org/en/2.1/api/sftp.html\n', '    """"""\n']","['paramiko.Transport', 'transport.add_server_key', 'transport.connect', 'print', 'sftp.listdir', 'paramiko.SSHClient', 'ssh.set_missing_host_key_policy', 'ssh.connect', 'ssh.exec_command', 'stdin.flush', 'stdout.read', 'sftp.close', 'transport.close', 'ssh.close']",14
utilmy/zarchive/util_aws.py:ztest_01,ztest_01,function,0,1,1,4,4.0,0,0,[],[],[],1069,[],[],0
utilmy/zarchive/util_aws.py:aws_ec2_ssh,aws_ec2_ssh,class,132,332,236,4141,12.47,9,10,[],[],[],54,[],[],0
utilmy/zarchive/util_aws.py:aws_ec2_ssh:__init__,aws_ec2_ssh:__init__,method,24,61,49,779,12.77,0,4,"['self', 'hostname', 'username', 'key_file', 'password']","[None, None, None, None, None]","[None, None, ""'ubuntu'"", 'None', 'None']",70,[],"['socket.socket', 'paramiko.Transport', 'isinstance', 'key_file=open', 'key_file.seek', 'Exception', 'print']",7
utilmy/zarchive/util_aws.py:aws_ec2_ssh:command,aws_ec2_ssh:command,method,12,19,17,242,12.74,1,0,"['self', 'cmd']","[None, None]","[None, None]",111,[],"['chan.get_pty', 'chan.invoke_shell', 'chan.settimeout', 'chan.send', 'cmd.split']",5
utilmy/zarchive/util_aws.py:aws_ec2_ssh:put,aws_ec2_ssh:put,method,17,33,29,648,19.64,2,2,"['self', 'localfile', 'remotefile']","[None, None, None]","[None, None, None]",131,[],"['put_all', 'os.chdir', 'os.walk', 'print', 'self.put']",5
utilmy/zarchive/util_aws.py:aws_ec2_ssh:put_all,aws_ec2_ssh:put_all,method,15,30,26,573,19.1,2,2,"['self', 'localpath', 'remotepath']","[None, None, None]","[None, None, None]",135,[],"['os.chdir', 'os.walk', 'print', 'self.put']",4
utilmy/zarchive/util_aws.py:aws_ec2_ssh:get,aws_ec2_ssh:get,method,1,1,1,35,35.0,0,0,"['self', 'remotefile', 'localfile']","[None, None, None]","[None, None, None]",151,[],[],0
utilmy/zarchive/util_aws.py:aws_ec2_ssh:sftp_walk,aws_ec2_ssh:sftp_walk,method,18,31,25,347,11.19,3,1,"['self', 'remotepath']","[None, None]","[None, None]",155,[],"['S_ISDIR', 'folders.append', 'files.append', 'self.sftp_walk']",4
utilmy/zarchive/util_aws.py:aws_ec2_ssh:get_all,aws_ec2_ssh:get_all,method,15,38,30,599,15.76,2,2,"['self', 'remotepath', 'localpath']","[None, None, None]","[None, None, None]",175,[],"['os.mkdir', 'self.sftp_walk', 'print', 'self.get']",4
utilmy/zarchive/util_aws.py:aws_ec2_ssh:write_command,aws_ec2_ssh:write_command,method,2,2,2,74,37.0,0,0,"['self', 'text', 'remotefile']","[None, None, None]","[None, None, None]",199,[],[],0
utilmy/zarchive/util_aws.py:aws_ec2_ssh:python_script,aws_ec2_ssh:python_script,method,5,12,10,119,9.92,0,0,"['self', 'script_path', 'args1']","[None, None, None]","[None, None, None]",206,[],['self.cmd2'],1
utilmy/zarchive/util_aws.py:aws_ec2_ssh:command_list,aws_ec2_ssh:command_list,method,5,13,13,119,9.15,1,0,"['self', 'cmdlist']","[None, None]","[None, None]",212,[],"['print', 'self.command']",2
utilmy/zarchive/util_aws.py:aws_ec2_ssh:listdir,aws_ec2_ssh:listdir,method,2,2,2,34,17.0,0,0,"['self', 'remotedir']","[None, None]","[None, None]",219,[],[],0
utilmy/zarchive/util_aws.py:aws_ec2_ssh:jupyter_kill,aws_ec2_ssh:jupyter_kill,method,3,10,10,136,13.6,0,0,['self'],[None],[None],222,[],['aws_ec2_cmd_ssh'],1
utilmy/zarchive/util_aws.py:aws_ec2_ssh:jupyter_start,aws_ec2_ssh:jupyter_start,method,0,1,1,4,4.0,0,0,['self'],[None],[None],226,[],[],0
utilmy/zarchive/util_aws.py:aws_ec2_ssh:cmd2,aws_ec2_ssh:cmd2,method,2,5,5,63,12.6,0,0,"['self', 'cmd1']","[None, None]","[None, None]",229,[],['aws_ec2_cmd_ssh'],1
utilmy/zarchive/util_aws.py:aws_ec2_ssh:_help_ssh,aws_ec2_ssh:_help_ssh,method,18,29,29,132,4.55,0,1,['self'],[None],[None],232,[],[],0
utilmy/zarchive/report.py:map_show,map_show,function,0,1,1,4,4.0,0,0,[],[],[],36,[],[],0
utilmy/zarchive/report.py:xl_create_pivot,xl_create_pivot,function,4,9,9,134,14.89,0,0,"['infile', 'index_list', '""Rep""', '""Product""]', 'value_list', '""Quantity""]']","[None, None, None, None, None, None]","[None, '[""Manager""', None, None, '[""Price""', None]",65,"[""   ''' Read in the Excel file, create a pivot table and return it as a DataFrame '''\n""]",[],0
utilmy/zarchive/report.py:xl_save_report,xl_save_report,function,6,10,10,167,16.7,1,0,"['report', 'outfile']","[None, None]","[None, None]",72,"[""   '''  Take a report and save it to a single Excel file\n"", '       sales_report = create_pivot(args.infile.name)\n', '       save_report(sales_report, args.outfile.name)\n', ""   '''\n""]","['temp_df.to_excel', 'writer.save']",2
utilmy/zarchive/report.py:xl_create_pdf,xl_create_pdf,function,23,44,39,743,16.89,1,0,[],[],[],97,[],"['sales_report=create_pivot', 'manager_df.append', 'sales_report.xs', 'env=Environment', 'get_summary_stats', 'sales_report.to_html', 'HTML']",7
utilmy/docs/generate_doc.py:markdown_create_function,markdown_create_function,function,22,83,67,776,9.35,2,0,"['uri', 'name', 'type', 'args_name', 'args_type', 'args_value', 'start_line', 'list_docs', 'prefix']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, '""""']",22,[],"['uri.split', 'literal_eval', 'zip', 'rsp.format']",4
utilmy/docs/generate_doc.py:markdown_create_file,markdown_create_file,function,1,2,2,7,3.5,0,0,"['list_info', 'prefix']","[None, None]","[None, ""''""]",58,[],[],0
utilmy/docs/generate_doc.py:markdown_createall,markdown_createall,function,5,29,27,290,10.0,0,0,"['dfi', 'prefix']","[None, None]","[None, '""""']",79,[],"['zip', 'markdown_create_file']",2
utilmy/docs/generate_doc.py:table_create_row,table_create_row,function,10,37,31,319,8.62,1,0,"['uri', 'name', 'type', 'start_line', 'list_funtions', 'prefix']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",91,[],"['literal_eval', 'print', 'uri.split', 'rsp.format']",4
utilmy/docs/generate_doc.py:table_all_row,table_all_row,function,1,2,2,7,3.5,0,0,['list_rows'],[None],[None],102,[],[],0
utilmy/docs/generate_doc.py:table_create,table_create,function,10,37,31,319,8.62,1,0,"['uri', 'name', 'type', 'start_line', 'list_funtions', 'prefix']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",112,"['    """""" \n', '        python generate_doc.py run_all <in_file> <out_file> <prefix>\n', '    Returns:\n', '    """"""\n']","['literal_eval', 'print', 'uri.split', 'rsp.format']",4
utilmy/docs/generate_doc.py:run_markdown,run_markdown,function,8,28,27,278,9.93,0,0,"['repo_stat_file', 'output', 'prefix=""https']","[None, None, '']","[None, ""'docs/doc_main.md'"", '""https://github.com/user/repo/tree/a""']",120,"['    """""" \n', '        python generate_doc.py run_all <in_file> <out_file> <prefix>\n', '    Returns:\n', '    """"""\n']","['print', 'pd.read_csv', 'markdown_createall', 'open', 'f.write']",5
utilmy/docs/generate_doc.py:run_table,run_table,function,8,26,26,269,10.35,0,0,"['repo_stat_file', 'output', 'prefix=""https']","[None, None, '']","[None, ""'docs/doc_table.md'"", '""https://github.com/user/repo/tree/a""']",137,"['    """""" \n', '        python generate_doc.py run_table <in_file> <out_file> <prefix>\n', '    Returns:\n', '    """"""\n']","['print', 'pd.read_csv', 'table_create', 'open', 'f.write']",5
utilmy/docs/generate_doc.py:test,test,function,10,23,23,315,13.7,0,0,[],[],[],155,[],"['pd.read_csv', 'table_create', 'open', 'f.write']",4
utilmy/docs/cli.py:os_remove,os_remove,function,1,5,5,36,7.2,0,0,['filepath'],[None],[None],57,[],['os.remove'],1
utilmy/docs/cli.py:run_cli,run_cli,function,35,272,97,2902,10.67,0,12,[],[],[],63,"['    """""" Usage\n', '    cd myutil\n', '    pip install -e  .\n', '\n', '    docs  help\n', '    docs markdown --repo_url  https://github.com/arita37/spacefusion.git   --out_dir docs/\n', '\n', '    docs  callgraph  --repo_dir utilmy/      --out_dir docs/\n', '    docs  csv        --repo_dir utilmy/      --out_dir docs/\n', '    docs  txt        --repo_dir utilmy/      --out_dir docs/\n', '\n', '\n', '    """"""\n']","['argparse.ArgumentParser', 'add', 'p.parse_args', 'print', 'os.makedirs', 'os_remove', 'cp.export_stats_repolink', 'cp.export_stats_repolink_txt', 'cp.export_stats_perrepo', 'cp.export_stats_perrepo_txt', 'Exception', 'gdoc.run_markdown', 'gdoc.run_table', 'cp.export_call_graph_url', 'cp.export_call_graph']",15
utilmy/docs/test.py:log,log,function,3,10,9,81,8.1,0,0,['data'],[None],[None],20,[],"['open', 'f.write', 'str']",3
utilmy/docs/test.py:list_buy_price,list_buy_price,function,8,32,28,256,8.0,1,1,"['start', 'bottom', 'delta']","[None, None, None]","[None, None, None]",27,[],"['while', 'list_output.insert', 'round', 'log', 'list']",5
utilmy/docs/test.py:calculateSellPrice,calculateSellPrice,function,2,3,3,40,13.33,0,0,"['enter', 'profit']","[None, None]","[None, None]",42,[],['round'],1
utilmy/docs/test.py:list_sell_price,list_sell_price,function,8,32,28,248,7.75,1,1,"['start', 'top', 'delta']","[None, None, None]","[None, None, None]",46,[],"['while', 'list_output.insert', 'round', 'log', 'list']",5
utilmy/docs/test.py:calculateBuyPrice,calculateBuyPrice,function,2,3,3,40,13.33,0,0,"['enter', 'profit']","[None, None]","[None, None]",61,[],['round'],1
utilmy/docs/test.py:get_list_price,get_list_price,function,19,151,84,1833,12.14,4,9,[],[],[],69,[],"['list_buy_price', 'str', 'print', 'gInfoTradingUp[str', 'calculateSellPrice', 'len', 'float', 'list_sell_price', 'gInfoTradingDown[str', 'calculateBuyPrice', 'threading.Timer', 't.start']",12
utilmy/docs/test.py:trading_up,trading_up,function,28,178,97,2240,12.58,1,11,[],[],[],144,[],"['exchange.fetch_orders', 'print', 'exchange.create_order', 'log', 'str', 'threading.Timer', 't.start']",7
utilmy/docs/test.py:trading_down,trading_down,function,28,175,95,2315,13.23,1,11,[],[],[],218,[],"['exchange.fetch_orders', 'print', 'exchange.create_order', 'log', 'str', 'threading.Timer', 't.start']",7
utilmy/docs/test.py:update_price,update_price,function,5,8,8,89,11.12,0,0,[],[],[],289,[],"['threading.Timer', 't.start']",2
utilmy/docs/code_parser.py:export_stats_pertype,export_stats_pertype,function,9,46,22,381,8.28,0,4,"['in_path', 'type', 'out_path']","['str', 'str', 'str']","['None', 'None', 'None']",55,"['    """"""\n', '        python code_parser.py type <in_path> <type> <out_path>\n', '    Returns:\n', '\n', '    """"""\n']","['get_list_function_stats', 'print', 'df.to_csv', 'get_list_class_stats', 'get_list_method_stats']",5
utilmy/docs/code_parser.py:export_stats_perfile,export_stats_perfile,function,8,36,16,336,9.33,0,3,"['in_path', 'out_path']","['str', 'str']","['None', 'None']",81,"['    """"""\n', '        python code_parser.py  export_stats_perfile <in_path> <out_path>\n', '\n', '    Returns:\n', '\n', '    """"""\n']","['get_list_function_stats', 'print', 'df.to_csv', 'get_list_class_stats', 'get_list_method_stats']",5
utilmy/docs/code_parser.py:export_stats_perrepo_txt,export_stats_perrepo_txt,function,1,4,4,59,14.75,0,0,"['in_path', 'out_path', 'repo_name']","['str', 'str', 'str']","['None', 'None', 'None']",105,"['    """"""\n', '        python code_parser.py  repo_txt   parser/test3    parser/output/output_repo.csv\n', '\n', '    Returns:\n', '        1  txt file\n', '    """"""\n']",['export_stats_perrepo'],1
utilmy/docs/code_parser.py:export_stats_perrepo,export_stats_perrepo,function,31,235,87,2231,9.49,5,15,"['in_path', 'out_path', 'repo_name']","['str', 'str', 'str']","['None', 'None', 'None']",115,[],"['export_stats_perrepo', 'glob.glob', 'range', 'get_list_function_stats', 'print', 'open', 'f.write', 'x.replace', 'df.to_csv', 'df.iterrows', 'zip', 'get_list_class_stats', 'get_list_method_stats']",13
utilmy/docs/code_parser.py:export_stats_repolink_txt,export_stats_repolink_txt,function,1,3,3,52,17.33,0,0,"['repo_link', 'out_path']","[' str', 'str']","[None, 'None']",202,"['    """"""\n', '        python code_parser.py repo_url_txt https://github.com/lucidrains/DALLE-pytorch.git docs/test_example1.csv\n', '\n', '    Returns:\n', '        1  txt   --->  data info detail\n', '    """"""\n']",['export_stats_repolink'],1
utilmy/docs/code_parser.py:export_stats_repolink,export_stats_repolink,function,9,39,35,479,12.28,0,2,"['repo_link', 'out_path']","[' str', 'str']","[None, 'None']",212,[],"['export_stats_repolink', 'repo_link.split', 'shutil.rmtree', 'print', 'os.system', 'export_stats_perrepo']",6
utilmy/docs/code_parser.py:export_call_graph_url,export_call_graph_url,function,6,17,16,219,12.88,0,1,"['repo_link', 'out_path']","[' str', 'str']","[None, 'None']",238,"['    """"""\n', '        python code_parser.py  export_call_graph_url <repo_link> <out_path>\n', '    Returns:\n', '        1  csv output\n', '    """"""\n']","['repo_link.split', 'print', 'os.system', 'export_call_graph']",4
utilmy/docs/code_parser.py:export_call_graph,export_call_graph,function,34,220,93,2216,10.07,9,8,"['repo_link', 'out_path']","[' str', 'str']","[None, 'None']",256,[],"['repo_link.split', 'print', 'os.system', 'export_call_graph', 'glob.glob', 'range', 'get_list_class_stats', 'zip', 'list_classes.append', 'get_list_function_stats', 'get_list_imported_func', 'get_list_import_class_as', 'open', 'f.write', 'write_to_file', 'get_list_method_stats']",16
utilmy/docs/code_parser.py:get_list_function_name,get_list_function_name,function,17,27,26,352,13.04,1,1,['file_path'],[None],[None],366,"['    """"""The function use to get all functions of the python file\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: list_functions   - List all python functions in the input file\n', '    Example Output:\n', ""        ['func1', 'func2']\n"", '    """"""\n']","['_get_all_line', '_get_and_clean_all_lines', 'list', 're.match', 'line.rstrip', 're_response.group', 'all_lines.index', 'list_functions.append']",8
utilmy/docs/code_parser.py:get_list_class_name,get_list_class_name,function,17,27,26,347,12.85,1,1,['file_path'],[None],[None],388,"['    """"""The function use to get all classes of the python file\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: list_classes     - List all python classes in the input file\n', '    Example Output:\n', ""        ['Class1', 'Class1']\n"", '    """"""\n']","['_get_all_line', '_get_and_clean_all_lines', 'list', 're.match', 'line.rstrip', 're_response.group', 'all_lines.index', 'list_classes.append']",8
utilmy/docs/code_parser.py:get_list_class_methods,get_list_class_methods,function,26,42,37,620,14.76,2,1,['file_path'],[None],[None],413,"['    """"""The function use to get all classes and all methods in this class of the python file\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: An array of class info [{dict}, {dict}, ...]\n', '    Example Output:\n', '    [\n', '        {""class_name"": ""Class1"", ""listMethods"": [""method1"", ""method2"", ""method3""]},\n', '        {""class_name"": ""Class2"", ""listMethods"": [""method4"", ""method5"", ""method6""]},\n', '    ]\n', '    """"""\n']","['_get_all_line', '_get_and_clean_all_lines', 'get_list_class_name', '_get_all_lines_in_class', 're.match', 'line.rstrip', 're_response.group', 'all_lines.index', 'list_names.append']",9
utilmy/docs/code_parser.py:get_list_variable_global,get_list_variable_global,function,12,26,24,357,13.73,1,2,['file_path'],[None],[None],445,"['    """"""The function use to get all global variable of the python file\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: list_var         - Array of all global variable\n', '    Example Output:\n', ""        ['Var1', 'Var2']\n"", '    """"""\n']","['_get_and_clean_all_lines', 're.match', 'line.rstrip', 'list_var.append', 'list']",5
utilmy/docs/code_parser.py:_get_docs,_get_docs,function,12,77,32,669,8.69,1,6,"['all_lines', 'index_1', 'func_lines']","[None, None, None]","[None, None, None]",467,[],"['line.strip', 'len', 'response.append']",3
utilmy/docs/code_parser.py:get_list_function_info,get_list_function_info,function,29,55,48,787,14.31,2,0,['file_path'],[None],[None],507,"['    """"""The function use to get functions stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Array of functions, lines of the function, and variable in function\n', '    Example Output:\n', '        [\n', '            {""name"": ""function_name1"", ""lines"": 20, ""variables"": [""a"", ""b"", ""c""]},\n', '            {""name"": ""function_name2"", ""lines"": 30, ""variables"": []},\n', '        ]\n', '    """"""\n']","['_get_all_line', '_get_and_clean_all_lines', 'get_list_function_name', '_get_all_lines_in_function', 'len', '_get_function_stats', '_get_docs', '_get_all_lines_define_function', '_get_define_function_stats', 'output.append']",10
utilmy/docs/code_parser.py:get_list_class_info,get_list_class_info,function,20,49,42,584,11.92,2,0,['file_path'],[None],[None],545,"['    """"""The class use to get functions stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Array of functions, lines of the function, and variable in function\n', '    Example Output:\n', '        [\n', '            {""function"": ""function_name1"", ""lines"": 20, ""variables"": [""a"", ""b"", ""c""]},\n', '            {""function"": ""function_name2"", ""lines"": 30, ""variables"": []},\n', '        ]\n', '    """"""\n']","['_get_and_clean_all_lines', 'get_list_class_name', '_get_all_lines_in_class', 'len', '_get_function_stats', 'output.append']",6
utilmy/docs/code_parser.py:get_list_method_info,get_list_method_info,function,35,65,54,966,14.86,3,0,['file_path'],[None],[None],581,"['    """"""get_list_method_info\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Array of methods in class\n', '    Example Output:\n', '        [\n', '            {""function"": ""function_name1"", ""lines"": 20, ""variables"": [""a"", ""b"", ""c""]},\n', '            {""function"": ""function_name2"", ""lines"": 30, ""variables"": []},\n', '        ]\n', '    """"""\n']","['_get_all_line', '_get_and_clean_all_lines', 'get_list_class_methods', '_get_all_lines_in_class', '_get_all_lines_in_function', 'len', '_get_function_stats', '_get_docs', '_get_all_lines_define_function', '_get_define_function_stats', 'output.append']",11
utilmy/docs/code_parser.py:get_list_method_stats,get_list_method_stats,function,6,12,11,148,12.33,0,1,['file_path'],[None],[None],623,"['    """"""The function use to get methods stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Dataframe with bellow fields\n', '            uri:   path1/path2/filename.py:function1\n', '            name: function1\n', '            n_lines\n', '            n_words\n', '            n_words_unqiue\n', '            n_characters\n', '            avg_char_per_word = n_charaecter / n_words\n', '            n_loop  : nb of for, while loop\n', '            n_ifthen  : nb of if_then\n', '        \n', '        **** return None if no function in file\n', '    Example Output:\n', '                                                    uri                                               name    type  n_variable  n_words  n_words_unique  n_characters  avg_char_per_word  n_loop  n_ifthen\n', '    0   d:/Project/job/test2/zz936/parser/test/keys.py...                              VerifyingKey:__init__  method           2       11              11           100           9.090909       0         1      \n', '    1   d:/Project/job/test2/zz936/parser/test/keys.py...                     VerifyingKey:from_public_point  method          10       13              12           185          14.230769       0         0      \n', '    2   d:/Project/job/test2/zz936/parser/test/keys.py...                           VerifyingKey:from_string  method          17       45              39           504          11.200000       0         1      \n', '    3   d:/Project/job/test2/zz936/parser/test/keys.py...                              VerifyingKey:from_pem  method           2        2               2            39          19.500000       0         0      \n', '    4   d:/Project/job/test2/zz936/parser/test/keys.py...                              VerifyingKey:from_der  method          19       64              38           683          10.671875       0         3      \n', '    5   d:/Project/job/test2/zz936/parser/test/keys.py...              VerifyingKey:from_public_key_recovery  method           4        8               8           137          17.125000       0         0      \n', '    6   d:/Project/job/test2/zz936/parser/test/keys.py...  VerifyingKey:from_public_key_recovery_with_digest  method          13       24              23           288          12.000000       0         0      \n', '    7   d:/Project/job/test2/zz936/parser/test/keys.py...                             VerifyingKey:to_string  method           6       11               8           145          13.181818       0         0      \n', '    8   d:/Project/job/test2/zz936/parser/test/keys.py...                                VerifyingKey:to_pem  method           2        4               4            42          10.500000       0         0  \n', '    """"""\n']","['get_list_method_info', 'get_stats']",2
utilmy/docs/code_parser.py:get_list_class_stats,get_list_class_stats,function,6,12,11,147,12.25,0,1,['file_path'],[None],[None],659,"['    """"""The class use to get functions stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Dataframe with bellow fields\n', '            uri:   path1/path2/filename.py:function1\n', '            name: function1\n', '            n_lines\n', '            n_words\n', '            n_words_unqiue\n', '            n_characters\n', '            avg_char_per_word = n_charaecter / n_words\n', '            n_loop  : nb of for, while loop\n', '            n_ifthen  : nb of if_then\n', '        \n', '        **** return None if no function in file\n', '    Example Output:\n', '                                                    uri               name   type  n_variable  n_words  n_words_unique  n_characters  avg_char_per_word  n_loop  n_ifthen\n', '    0  d:/Project/job/test2/zz936/parser/test/keys.py...  BadSignatureError  class           0        1               1             4           4.000000       0         0\n', '    1  d:/Project/job/test2/zz936/parser/test/keys.py...     BadDigestError  class           0        1               1             4           4.000000       0         0\n', '    2  d:/Project/job/test2/zz936/parser/test/keys.py...       VerifyingKey  class          84      301             189          3584          11.906977       0         7\n', '    3  d:/Project/job/test2/zz936/parser/test/keys.py...         SigningKey  class         138      482             310          4615           9.574689       3         9\n', '    """"""\n']","['get_list_class_info', 'get_stats']",2
utilmy/docs/code_parser.py:get_list_function_stats,get_list_function_stats,function,6,12,11,150,12.5,0,1,['file_path'],[None],[None],690,"['    """"""The function use to get functions stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Dataframe with bellow fields\n', '            uri:   path1/path2/filename.py:function1\n', '            name: function1\n', '            n_lines\n', '            n_words\n', '            n_words_unqiue\n', '            n_characters\n', '            avg_char_per_word = n_charaecter / n_words\n', '            n_loop  : nb of for, while loop\n', '            n_ifthen  : nb of if_then\n', '        \n', '        **** return None if no function in file\n', '    Example Output:\n', '            uri                                 name  n_variable  n_words  n_words_unique  n_characters  avg_char_per_word  n_loop  n_ifthen\n', '        0   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...     prepare_target_and_clean_up_test           8       92              32           535           5.815217       0         0\n', '        1   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...                 clean_up_config_test           6       55              19           241           4.381818       0         1\n', '        2   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...         check_default_network_config          22      388              74           955           2.461340       1         5\n', '        3   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...                     check_module_env           9      250              54           553           2.212000       1         1\n', '        4   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...     provision_certificates_to_target           7      101              29           384           3.801980       0         3\n', '        5   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...            config_session_connection           2       14               8            97           6.928571       0         0\n', '        6   d:\\Project\\job\\test2\\zz936\\parser/test/test2.p...  config_cipher_suite_and_tcps_action           8      101              30           335           3.316832       0         3\n', '    """"""\n']","['get_list_function_info', 'get_stats']",2
utilmy/docs/code_parser.py:get_stats,get_stats,function,19,84,52,808,9.62,0,0,"['df', 'file_path']","['pd.DataFrame', 'str']","[None, None]",730,"['    """""" Calculate stats from datafaframe\n', '    Args:\n', '        df: pandas DataFrame\n', '\n', '    Returns:\n', '        pandas DataFrame\n', '\n', '    """"""\n']","['len', 'df.apply', '_get_words', '_get_avg_char_per_word', '_get_functions']",5
utilmy/docs/code_parser.py:get_file_stats,get_file_stats,function,11,21,19,244,11.62,1,0,['file_path'],[None],[None],763,"['    """"""The function use to get file stars\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: Dict of file stars\n', '    Example Output:\n', '        {\n', '            ""total_functions"": 22,\n', '            ""avg_lines"" : 110.2,\n', '            ""total_class"": 3\n', '        }\n', '    """"""\n']","['get_list_function_stats', 'len', 'avg_lines/len']",3
utilmy/docs/code_parser.py:get_list_imported_func,get_list_imported_func,function,13,44,30,467,10.61,2,3,['file_path'],[' str'],[None],786,"['    """"""Get list funtions was imported in python file.\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: List function\n', '    Example Output:\n', '        [\n', ""            {'func1': 'zc.Class'},\n"", ""            {'func2': 'Hola'}\n"", '        ]\n', '    """"""\n']","['dict', '_get_and_clean_all_lines', 're.search', 'line.rstrip', 'line.split', 'functions.split']",6
utilmy/docs/code_parser.py:get_list_import_class_as,get_list_import_class_as,function,18,41,37,463,11.29,1,1,['file_path'],[' str'],[None],820,"['    """"""Get list funtions was imported in python file.\n', '    Args:\n', '        IN: file_path         - the file path input\n', '        OUT: List function\n', '    Example Output:\n', '        [\n', ""            {'class': 'zc.Class'},\n"", ""            {'class': 'Hola'}\n"", '        ]\n', '    """"""\n']","['dict', '_get_and_clean_all_lines', 're.match', 'line.rstrip', 'print', 'line.split', 'importlib.import_module']",7
utilmy/docs/code_parser.py:_get_words,_get_words,function,6,23,21,154,6.7,1,1,['row'],[None],[None],857,[],[],0
utilmy/docs/code_parser.py:_get_functions,_get_functions,function,10,32,26,389,12.16,1,2,['row'],[None],[None],867,[],"['re.match', 'list_funcs.append', 'list', 'return']",4
utilmy/docs/code_parser.py:_get_avg_char_per_word,_get_avg_char_per_word,function,1,9,9,76,8.44,0,0,['row'],[None],[None],887,[],[],0
utilmy/docs/code_parser.py:_validate_file,_validate_file,function,5,34,18,227,6.68,0,3,['file_path'],[None],[None],891,"['    """"""Check if the file is existed and it\'s a python file\n', '    """"""\n']",['print'],1
utilmy/docs/code_parser.py:_clean_data,_clean_data,function,12,132,40,1162,8.8,3,13,['array'],[None],[None],906,"['    """"""Remove empty lines and comment lines start with #\n', '    """"""\n']","['array.copy', '_remove_empty_line', '_remmove_commemt_line', 'response.remove', 'response.copy', 'line.strip', 'len', 're.search']",8
utilmy/docs/code_parser.py:_remove_empty_line,_remove_empty_line,function,2,7,7,37,5.29,0,1,['line'],[None],[None],979,[],['line.strip'],1
utilmy/docs/code_parser.py:_remmove_commemt_line,_remmove_commemt_line,function,2,17,14,97,5.71,0,1,['line'],[None],[None],983,[],['line.strip'],1
utilmy/docs/code_parser.py:_get_and_clean_all_lines,_get_and_clean_all_lines,function,5,11,8,124,11.27,0,1,['file_path'],[None],[None],989,"['    """"""Prepare all lines of the file\n', '    """"""\n']","['_validate_file', '_get_all_line', '_clean_data']",3
utilmy/docs/code_parser.py:_get_all_line,_get_all_line,function,33,247,85,2442,9.89,7,17,['file_path'],[None],[None],999,[],"['open', '_get_all_lines_in_function', 'array.copy', 're.match', 'line.rstrip', 'response.remove', 'len', 'range', 'list', 'list_lines.append', '_get_all_lines_in_class', '_get_all_lines_define_function']",12
utilmy/docs/code_parser.py:_get_all_lines_in_function,_get_all_lines_in_function,function,21,87,52,810,9.31,3,7,"['function_name', 'array', 'indentMethod']","[None, None, None]","[None, None, ""''""]",1005,"['    """"""The function use to get all lines of the function\n', '    Args:\n', '        IN: function_name - name of the function will be used to get all line\n', '        IN: array         - list all lines of the file have this input function\n', '        OUT: list_lines   - Array of all line of this function\n', '        OUT: indent       - The indent of this function (this will be used for another calculation)\n', '    """"""\n']","['array.copy', 're.match', 'line.rstrip', 'response.remove', 'len', 'range', 'list', 'list_lines.append']",8
utilmy/docs/code_parser.py:_get_all_lines_in_class,_get_all_lines_in_class,function,16,71,46,670,9.44,2,4,"['class_name', 'array']","[None, None]","[None, None]",1057,"['    """"""The function use to get all lines of the class\n', '    Args:\n', '        IN: class_name    - name of the class will be used to get all line\n', '        IN: array         - list all lines of the file have this input class\n', '        OUT: list_lines   - Array of all line of this class\n', '    """"""\n']","['array.copy', 're.match', 'line.rstrip', 'response.remove', 'len', 'list', 'list_lines.append']",7
utilmy/docs/code_parser.py:_get_all_lines_define_function,_get_all_lines_define_function,function,20,71,46,687,9.68,2,6,"['function_name', 'array', 'indentMethod']","[None, None, None]","[None, None, ""''""]",1099,"['    """"""The function use to get all lines define_function\n', '    Args:\n', '        IN: function_name - name of the function will be used to get all line\n', '        IN: array         - list all lines of the file have this input function\n', '        OUT: list_lines   - Array of all line used to define the function\n', '        OUT: indent       - The indent of this function (this will be used for another calculation)\n', '    """"""\n']","['list', 'array.copy', 're.match', 'line.rstrip', 'response.remove', 'len', 'list_lines.append', 'range']",8
utilmy/docs/code_parser.py:_get_define_function_stats,_get_define_function_stats,function,25,156,76,1487,9.53,4,9,['array'],[None],[None],1143,"['    """"""The function use to get define function stats: arg_name, arg_type, arg_value\n', '    Args:\n', '        IN: array         - list all lines of function to get variables\n', '        OUT: function stats: arg_name, arg_type, arg_value\n', '    """"""\n']","['len', 're.search', 'line.strip', 'print', 'i.start', 're.finditer', 'range', 'data.split', 'arg.replace', 'arg.strip', 'arg.find', 'arg_name.append', 'arg_type.append', 'arg_value.append']",14
utilmy/docs/code_parser.py:_get_function_stats,_get_function_stats,function,16,206,105,1958,9.5,4,11,"['array', 'indent']","[None, None]","[None, None]",1209,"['    """"""The function use to get all lines of the function\n', '    Args:\n', '        IN: indent        - indent string\n', '        IN: array         - list all lines of function to get variables\n', '        OUT: list_var     - Array of all variables\n', '    """"""\n']","['array.copy', 'check_array.copy', 'line.rstrip', 'check_array.remove', 'line.split', 're.match', 'list_var.append', 'list']",8
utilmy/docs/code_parser.py:write_to_file,write_to_file,function,25,163,71,1808,11.09,1,8,"['uri', 'type', 'list_functions', 'list_classes', 'list_imported', 'dict_functions', 'list_class_as', 'out_path']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",1298,[],"['importlib.import_module', 'function.split', 'print', 'open', 'f.write']",5
utilmy/docs/code_parser.py:test_example,test_example,function,3,7,7,233,33.29,0,0,[],[],[],1377,[],"['export_stats_pertype', 'export_stats_perfile', 'export_stats_perrepo']",3
utilmy/excel/xlvba.py:load_csv,load_csv,function,14,20,19,243,12.15,0,0,['csvfile'],[None],[None],4,"['    """"""Reads input csv file and populates the caller""""""\n']","['pd.read_csv', 'xw.Range']",2
utilmy/excel/xlvba.py:invokenumpy,invokenumpy,function,12,15,15,177,11.8,0,0,[],[],[],19,"['    """"""Prints 9 equally spaced numbers b/w 0 and 2 using numpy""""""\n']","['xw.Range', 'np.linspace']",2
utilmy/excel/xlvba.py:invokesklearn,invokesklearn,function,21,34,31,395,11.62,0,0,[],[],[],32,"['    """"""Loads 10 rows from iris dataset to excel sheet""""""\n']","['datasets.load_iris', 'pd.DataFrame', 'xw.Range']",3
utilmy/excel/xlvba.py:loaddf,loaddf,function,12,18,17,235,13.06,0,0,[],[],[],52,"['    """"""Load data from excel to pandas dataframe""""""\n']","['ws.range', 'xw.Range']",2
utilmy/logs/util_log.py:logger_setup,logger_setup,function,33,105,82,1013,9.65,2,3,"['log_config_path', 'log_template', '**kwargs']","[' str ', ' str ', None]","[' None', ' ""default""', None]",34,"['    """""" Generic Logging setup\n', '      Overide logging using loguru setup\n', '      1) Custom config from log_config_path .yaml file\n', '      2) Use shortname log, log2, logw, loge for logging output\n', '\n', '    Args:\n', '        log_config_path:\n', '        template_name:\n', '        **kwargs:\n', '    Returns:None\n', '\n', '    TODO:\n', '\n', '\n', '    """"""\n']","['open', 'yaml.safe_load', 'print', 'cfg.pop', 'globals_.pop', 'int', 'SocketHandler', 'handler.get', 'handler.items', 'handler.update', 'logger.configure', 'logger.level']",12
utilmy/logs/util_log.py:log,log,function,33,105,82,1013,9.65,2,3,"['log_config_path', 'log_template', '**kwargs']","[' str ', ' str ', None]","[' None', ' ""default""', None]",115,[],"['open', 'yaml.safe_load', 'print', 'cfg.pop', 'globals_.pop', 'int', 'SocketHandler', 'handler.get', 'handler.items', 'handler.update', 'logger.configure', 'logger.level']",12
utilmy/logs/util_log.py:log2,log2,function,1,6,6,62,10.33,0,0,['*s'],[None],[None],119,[],['logger.opt'],1
utilmy/logs/util_log.py:log3,log3,function,1,6,6,64,10.67,0,0,['*s'],[None],[None],123,[],['logger.opt'],1
utilmy/logs/util_log.py:logw,logw,function,1,6,6,64,10.67,0,0,['*s'],[None],[None],128,[],['logger.opt'],1
utilmy/logs/util_log.py:logc,logc,function,1,6,6,65,10.83,0,0,['*s'],[None],[None],132,[],['logger.opt'],1
utilmy/logs/util_log.py:loge,loge,function,1,6,6,66,11.0,0,0,['*s'],[None],[None],136,[],['logger.opt'],1
utilmy/logs/util_log.py:logr,logr,function,1,6,6,62,10.33,0,0,['*s'],[None],[None],140,[],['logger.opt'],1
utilmy/logs/util_log.py:test,test,function,8,18,18,151,8.39,0,0,[],[],[],145,[],"['log3', 'log2', 'log', 'logw', 'loge', 'logc', 'logr']",7
utilmy/logs/util_log.py:z_logger_stdout_override,z_logger_stdout_override,function,18,36,33,384,10.67,1,0,[],[],[],162,"['    """""" Redirect stdout --> logger\n', '    Returns:\n', '    """"""\n']","['__init__', 'write', 'buffer.rstrip', 'logger.opt', 'line.rstrip', 'flush', 'logger.remove', 'logger.add', 'StreamToLogger', 'contextlib.redirect_stdout', 'print']",11
utilmy/logs/util_log.py:z_logger_custom_1,z_logger_custom_1,function,38,110,92,1250,11.36,2,1,[],[],[],187,[],"['InterceptHandler', 'emit', 'logger.level', 'str', 'logging.currentframe', 'logger.opt', 'record.getMessage', 'format_record', 'pformat', 'setup_logging', 'logging.getLogger', 'logger.configure']",12
utilmy/logs/test_log.py:test1,test1,function,17,28,28,206,7.36,0,0,[],[],[],9,[],"['log3', 'log2', 'log', 'logw', 'logc', 'logr', 'loge']",7
utilmy/logs/test_log.py:test2,test2,function,19,35,34,331,9.46,0,0,[],[],[],28,[],"['print', 'util_log.logger_setup', 'log3', 'log2', 'log', 'logw', 'logc', 'logr', 'loge']",9
utilmy/logs/test_log.py:test_launch_server,test_launch_server,function,12,16,16,155,9.69,0,0,[],[],[],81,"[""\t'''\n"", '\tServer code from loguru.readthedocs.io\n', '\tUse to test network logging\n', '\n', '     python   test.py test_launch_server\n', '\n', '\n', ""\t'''\n""]",['socketserver.TCPServer'],1
utilmy/logs/test_log.py:test_server,test_server,function,19,35,34,337,9.63,0,0,[],[],[],95,[],"['print', 'util_log.logger_setup', 'log3', 'log2', 'log', 'logw', 'logc', 'logr', 'loge']",9
utilmy/logs/test_log.py:LoggingStreamHandler,LoggingStreamHandler,class,13,34,28,381,11.21,2,1,[],[],[],65,[],[],0
utilmy/logs/test_log.py:LoggingStreamHandler:handle,LoggingStreamHandler:handle,method,12,32,26,364,11.38,2,1,['self'],[None],[None],66,[],"['len', 'struct.unpack', 'pickle.loads', 'json.loads', 'logger.patch', 'record.update']",6
utilmy/zzarchive/zutil.py:session_load_function,session_load_function,function,6,9,9,88,9.78,0,0,['name'],[None],"['""test_20160815""']",66,[],"['dill.load_session', 'print']",2
utilmy/zzarchive/zutil.py:session_save_function,session_save_function,function,6,11,10,111,10.09,0,0,['name'],[None],"['""test""']",75,[],"['date_now', 'dill.dump_session', 'print']",3
utilmy/zzarchive/zutil.py:py_save_obj_dill,py_save_obj_dill,function,16,52,42,502,9.65,0,1,"['obj1', 'keyname', 'otherfolder']","[None, None, None]","[None, '""""', '0']",83,[],"['z_key_splitinto_dir_name', 'os_folder_create', 'open', 'dill.dumps', 'print']",5
utilmy/zzarchive/zutil.py:aa_unicode_ascii_utf8_issue,aa_unicode_ascii_utf8_issue,function,0,0,0,0,0.0,0,0,[],[],[],111,"['    """"""Take All csv in a folder and provide Table, Column Schema, type\n', '\n', ' METHOD FOR Unicode / ASCII issue\n', '1. Decode early\n', ""Decode to <type 'unicode'> ASAP\n"", ""df['PREF_NAME']=       df['PREF_NAME'].apply(to_unicode)\n"", '\n', '2. Unicode everywhere\n', '\n', '\n', '3. Encode late\n', ""# >>> f = open('/tmp/ivan_out.txt','w')\n"", ""# >>> f.write(ivan_uni.encode('utf-8'))\n"", '\n', 'Important methods\n', ""s.decode(encoding)  <type 'str'> to <type 'unicode'>\n"", ""u.encode(encoding)  <type 'unicode'> to <type 'str'>\n"", '\n', 'http://farmdev.com/talks/unicode/\n', '\n', '   """"""\n']",[],0
utilmy/zzarchive/zutil.py:isfloat,isfloat,function,4,13,11,79,6.08,0,1,['x'],[None],[None],135,[],['float'],1
utilmy/zzarchive/zutil.py:isint,isint,function,2,6,6,50,8.33,0,0,['x'],[None],[None],145,[],['isinstance'],1
utilmy/zzarchive/zutil.py:isanaconda,isanaconda,function,4,10,9,65,6.5,0,1,[],[],[],149,[],['txt.find'],1
utilmy/zzarchive/zutil.py:a_run_ipython,a_run_ipython,function,1,1,1,33,33.0,0,0,['cmd1'],[None],[None],157,"['    """""" Execute Ipython Command in python code\n', '     run -i :  run including current interprete variable\n', ' """"""\n']",['IPython.get_ipython'],1
utilmy/zzarchive/zutil.py:py_autoreload,py_autoreload,function,1,4,4,64,16.0,0,0,[],[],[],164,[],['a_run_ipython'],1
utilmy/zzarchive/zutil.py:os_platform,os_platform,function,1,2,2,8,4.0,0,0,[],[],[],169,[],[],0
utilmy/zzarchive/zutil.py:a_start_log,a_start_log,function,3,15,14,103,6.87,0,0,"['id1', 'folder']","[None, None]","['""""', '""aaserialize/log/""']",173,[],"['a_run_ipython', 'str', 'os_platform', 'date_now']",4
utilmy/zzarchive/zutil.py:a_cleanmemory,a_cleanmemory,function,3,3,3,21,7.0,0,0,[],[],[],187,[],['gc.collect'],1
utilmy/zzarchive/zutil.py:a_info_conda_jupyter,a_info_conda_jupyter,function,1,2,2,5,2.5,0,0,[],[],[],193,[],[],0
utilmy/zzarchive/zutil.py:a_run_cmd,a_run_cmd,function,1,1,1,20,20.0,0,0,['cmd1'],[None],[None],221,[],['os_process_run'],1
utilmy/zzarchive/zutil.py:a_help,a_help,function,1,2,2,13,6.5,0,0,[],[],[],228,[],[],0
utilmy/zzarchive/zutil.py:print_object_tofile,print_object_tofile,function,15,45,35,236,5.24,2,2,"['vv', 'txt', 'file1=""d']","[None, None, '']","[None, None, '""d:/regression_output.py""']",355,"['    """""" #Print to file Object   Table   """"""\n']","['open', 'file1.write', 'np.shape', 'range', 'str']",5
utilmy/zzarchive/zutil.py:print_progressbar,print_progressbar,function,9,32,29,316,9.88,0,1,"['iteration', 'total', 'prefix', 'suffix', 'decimals', 'bar_length']","[None, None, None, None, None, None]","[None, None, '""""', '""""', '1', '100']",372,"['    """"""# Print iterations progress\n', '     Call in a loop to create terminal progress bar\n', '    @params:\n', '        iteration   - Required  : current iteration (Int)\n', '        total       - Required  : total iterations (Int)\n', '        prefix      - Optional  : prefix string (Str)\n', '        suffix      - Optional  : suffix string (Str)\n', '        decimals    - Optional  : positive number of decimals in percent complete (Int)\n', '        bar_length   - Optional  : character length of bar (Int)\n', '    """"""\n']","['str', 'format_str.format', 'float', 'int']",4
utilmy/zzarchive/zutil.py:os_zip_checkintegrity,os_zip_checkintegrity,function,7,27,24,173,6.41,0,1,['filezip1'],[None],[None],401,[],"['zipfile.ZipFile', 'zip_file.testzip', 'print']",3
utilmy/zzarchive/zutil.py:os_zipfile,os_zipfile,function,19,37,32,376,10.16,2,2,"['folderin', 'folderzipname', 'iscompress']","[None, None, None]","[None, None, 'True']",414,[],"['zipfile.ZipFile', 'os.walk', 'zf.write', 'zf.close', 'os_zip_checkintegrity', 'print']",6
utilmy/zzarchive/zutil.py:os_zipfolder,os_zipfolder,function,2,8,5,116,14.5,0,0,"['dir_tozip', 'zipname', 'dir_prefix', 'iscompress=Trueimport shutil_ = iscompressdir_tozip = dir_tozip if dir_tozip[-1] != ""/"" else dir_tozip[']","[None, None, None, '']","['""/zdisks3/output""', '""/zdisk3/output.zip""', 'True', 'Trueimport shutil_ = iscompressdir_tozip = dir_tozip if dir_tozip[-1] != ""/"" else dir_tozip[:-1]if dir_prefix:']",432,"['    """"""\n', "" shutil.make_archive('/zdisks3/results/output', 'zip',\n"", ""                     root_dir=/zdisks3/results/',\n"", ""                     base_dir='output')\n"", ' os_zipfolder(\'zdisk/test/aapackage\', \'zdisk/test/aapackage.zip\', \'zdisk/test\')""""""\n']",['dir_tozip.split'],1
utilmy/zzarchive/zutil.py:os_zipextractall,os_zipextractall,function,23,61,51,536,8.79,1,3,"['filezip_or_dir', 'tofolderextract', 'isprint']","[None, None, None]","['""folder1/*.zip""', '""zdisk/test""', '1']",484,"['    """"""os_zipextractall( \'aapackage.zip\',\'zdisk/test/\'      )  """"""\n']","['filezip_or_dir.find', 'os_file_listall', 'os_file_getname', 'zipfile.ZipFile', 'zip_ref.extractall', 'zip_ref.close', 'print']",7
utilmy/zzarchive/zutil.py:os_folder_copy,os_folder_copy,function,15,39,36,391,10.03,0,2,"['src', 'dst', 'symlinks', 'pattern1', 'fun_file_toignore']","[None, None, None, None, None]","[None, None, 'False', '""*.py""', 'None']",518,"['    """"""\n', '       callable(src, names) -> ignored_names\n', ""       'src' parameter, which is the directory being visited by copytree(), and\n"", ""       'names' which is the list of `src` contents, as returned by os.listdir():\n"", '\n', '    Since copytree() is called recursively, the callable will be called once for each\n', '    directory that is copied.\n', '    It returns a  list of names relative to the `src` directory that should not be copied.\n', '   """"""\n']","['_default_fun_file_toignore', 'fnmatch.filter', 'shutil.copytree', 'shutil.copy']",4
utilmy/zzarchive/zutil.py:os_folder_create,os_folder_create,function,5,7,7,86,12.29,0,1,['directory'],[None],[None],548,[],"['os.getcwd', 'os.makedirs', 'os.chdir']",3
utilmy/zzarchive/zutil.py:os_folder_robocopy,os_folder_robocopy,function,3,15,15,163,10.87,0,1,"['from_folder', 'to_folder', 'my_log=""H']","[None, None, '']","['""""', '""""', '""H:/robocopy_log.txt""']",555,"['    """"""\n', '    Copy files to working directory\n', '    robocopy <Source> <Destination> [<File>[ ...]] [<Options>]\n', '    We want to copy the files to a fast SSD drive\n', '    """"""\n']","['subprocess.call', 'print']",2
utilmy/zzarchive/zutil.py:os_file_replace,os_file_replace,function,27,73,56,824,11.29,3,0,"['source_file_path', 'pattern', 'substring']","[None, None, None]","[None, None, None]",568,[],"['mkstemp', 'open', 'target_file.write', 'remove', 'move', 'os_file_replacestring1', 'fileinput.FileInput', 'line.replace', 'file1.close', 'print', 'format', 'os_file_replacestring2', 'os_file_listall']",13
utilmy/zzarchive/zutil.py:os_file_replacestring1,os_file_replacestring1,function,9,18,17,201,11.17,1,0,"['find_str', 'rep_str', 'file_path']","[None, None, None]","[None, None, None]",582,"['    """"""replaces all find_str by rep_str in file file_path""""""\n']","['fileinput.FileInput', 'line.replace', 'file1.close', 'print', 'format']",5
utilmy/zzarchive/zutil.py:os_file_replacestring2,os_file_replacestring2,function,5,13,12,162,12.46,1,0,"['findstr', 'replacestr', 'some_dir', 'pattern', 'dirlevel']","[None, None, None, None, None]","[None, None, None, '""*.*""', '1']",594,"['    """""" #fil_replacestring_files(""logo.png"", ""logonew.png"", r""D:/__Alpaca__details/aiportfolio"",\n', '    pattern=""*.html"", dirlevel=5  )\n', '  """"""\n']","['os_file_listall', 'os_file_replacestring1']",2
utilmy/zzarchive/zutil.py:os_file_getname,os_file_getname,function,8,9,8,75,8.33,0,0,['path'],[None],[None],604,[],"['ntpath.split', 'ntpath.basename']",2
utilmy/zzarchive/zutil.py:os_file_getpath,os_file_getpath,function,7,7,7,52,7.43,0,0,['path'],[None],[None],611,[],['ntpath.split'],1
utilmy/zzarchive/zutil.py:os_file_gettext,os_file_gettext,function,4,8,8,55,6.88,0,0,['file1'],[None],[None],618,[],"['open', 'f.read']",2
utilmy/zzarchive/zutil.py:os_file_listall,os_file_listall,function,23,40,34,476,11.9,2,1,"['dir1', 'pattern', 'dirlevel', 'onlyfolder']","[None, None, None, None]","[None, '""*.*""', '1', '0']",625,[],"['dir1.rstrip', 'dir1.count', 'os.walk', 'root.count', 'fnmatch.filter']",5
utilmy/zzarchive/zutil.py:_os_file_search_fast,_os_file_search_fast,function,27,120,70,884,7.37,4,4,"['fname', 'texts', 'mode']","[None, None, None]","[None, 'None', '""regex/str""']",695,[],"['re.compile', 'enumerate', 're.search', 'line.decode', 'res.append', 'found.start', 'text.encode', 'line.find', 'print']",9
utilmy/zzarchive/zutil.py:os_file_search_content,os_file_search_content,function,10,33,29,296,8.97,1,1,"['srch_pattern', 'mode', 'dir1', 'file_pattern', 'dirlevel']","[None, None, None, None, None]","['None', '""str""', '""""', '""*.*""', '1']",740,[],"['os_file_listall', '_os_file_search_fast', 'pd.DataFrame']",3
utilmy/zzarchive/zutil.py:os_file_rename,os_file_rename,function,33,53,45,644,12.15,2,1,"['some_dir', 'pattern', 'pattern2', 'dirlevel']","[None, None, None, None]","[None, '""*.*""', '""""', '1']",758,[],"['some_dir.rstrip', 'some_dir.count', 'os.walk', 'root.count', 'matches.append', 'fnmatch.filter', 're.sub', 'os.rename', 'np.array']",9
utilmy/zzarchive/zutil.py:os_gui_popup_show,os_gui_popup_show,function,25,37,36,408,11.03,0,0,['txt'],[None],[None],788,[],"['os_gui_popup_show2', 'Tk', 'Scrollbar', 'Text', 'scrollbar.pack', 'text.pack', 'scrollbar.config', 'text.config', 'text.insert', 'root.attributes', 'mainloop']",11
utilmy/zzarchive/zutil.py:os_print_tofile,os_print_tofile,function,1,1,1,24,24.0,0,0,"['vv', 'file1', 'mode1']","[None, None, None]","[None, None, '""a""']",808,"['    """"""\n', '    Here is a list of the different modes of opening a file:\n', 'r\n', 'Opens a file for reading only. The file pointer is placed at the beginning of the file.\n', 'This is the default mode.\n', '\n', 'rb\n', '\n', 'Opens a file for reading only in binary format. The file pointer is placed at the\n', 'beginning of the file. This is the default mode.\n', 'r+\n', '\n', 'Opens a file for both reading and writing. The file pointer will be at the beginning of the file.\n', 'rb+\n', '\n', 'Opens a file for both reading and writing in binary format. The file pointer will be at the\n', 'beginning of the file.\n', 'w\n', '\n', 'Opens a file for writing only. Overwrites the file if the file exists. If the file does not exist,\n', 'creates a new file for writing.\n', 'wb\n', '\n', 'Opens a file for writing only in binary format. Overwrites the file if the file exists.\n', 'If the file does not exist, creates a new file for writing.\n', 'w+\n', '\n', 'Opens a file for both writing and reading. Overwrites the existing file if the file exists.\n', 'If the file does not exist, creates a new file for reading and writing.\n', 'wb+\n', '\n', 'Opens a file for both writing and reading in binary format. Overwrites the existing file if\n', 'the file exists. If the file does not exist, creates a new file for reading and writing.\n', 'a\n', '\n', 'Opens a file for appending. The file pointer is at the end of the file if the file exists. That is,\n', 'the file is in the append mode. If the file does not exist, it creates a new file for writing.\n', 'ab\n', '\n', 'Opens a file for appending in binary format. The file pointer is at the end of the file if the file\n', 'exists. That is, the file is in the append mode. If the file does not exist, it creates a new file\n', 'for writing.\n', 'a+\n', '\n', 'Opens a file for both appending and reading. The file pointer is at the end of the file if the\n', 'file exists. The file opens in the append mode. If the file does not exist, it creates a new file\n', 'for reading and writing.\n', 'ab+\n', '\n', 'Opens a file for both appending and reading in binary format. The file pointer is at the end of\n', 'the file if the file exists. The file opens in the append mode. If the file does not exist,\n', 'it creates a new file for reading and writing.\n', 'To open a text file, use:\n', 'fh = open(""hello.txt"", ""r"")\n', '\n', 'To read a text file, use:\n', 'print fh.read()\n', '\n', 'To read one line at a time, use:\n', 'print fh.readline()\n', '\n', 'To read a list of lines use:\n', 'print fh.readlines()\n', '\n', 'To write to a file, use:\n', 'fh = open(""hello.txt"", ""w"")\n', 'lines_of_text = [""a line of text"", ""another line of text"", ""a third line""]\n', 'fh.writelines(lines_of_text)\n', 'fh.close()\n', '\n', 'To append to file, use:\n', 'fh = open(""Hello.txt"", ""a"")\n', 'fh.close()\n', '\n', '    """"""\n']",['text_file.write'],1
utilmy/zzarchive/zutil.py:os_path_norm,os_path_norm,function,8,20,18,175,8.75,0,1,['pth'],[None],[None],888,[],"['pth.find', 'b.lstrip']",2
utilmy/zzarchive/zutil.py:os_path_change,os_path_change,function,3,8,8,68,8.5,0,0,['path1'],[None],[None],902,[],"['os_path_norm', 'os.chdir']",2
utilmy/zzarchive/zutil.py:os_path_current,os_path_current,function,2,2,2,17,8.5,0,0,[],[],[],907,[],['os.getcwd'],1
utilmy/zzarchive/zutil.py:os_file_exist,os_file_exist,function,2,2,2,27,13.5,0,0,['file1'],[None],[None],911,[],[],0
utilmy/zzarchive/zutil.py:os_file_size,os_file_size,function,2,2,2,28,14.0,0,0,['file1'],[None],[None],915,[],[],0
utilmy/zzarchive/zutil.py:os_file_read,os_file_read,function,4,5,5,34,6.8,0,0,['file1'],[None],[None],919,[],"['open', 'fh.read']",2
utilmy/zzarchive/zutil.py:os_file_isame,os_file_isame,function,4,5,5,44,8.8,0,0,"['file1', 'file2']","[None, None]","[None, None]",924,[],['filecmp.cmp'],1
utilmy/zzarchive/zutil.py:os_file_get_extension,os_file_get_extension,function,5,12,10,98,8.17,0,2,['file_path'],[None],[None],930,"['    """"""\n', '    # >>> get_file_extension(""/a/b/c"")\n', ""    ''\n"", '    # >>> get_file_extension(""/a/b/c.tar.xz"")\n', ""    'xz'\n"", '    """"""\n']",['_ext.startswith'],1
utilmy/zzarchive/zutil.py:os_file_normpath,os_file_normpath,function,2,8,8,67,8.38,0,0,['path'],[None],[None],944,"['    """"""Normalize path.\n', '    - eliminating double slashes, etc. (os.path.normpath)\n', '    - ensure paths contain ~[user]/ expanded.\n', '\n', '    :param path: Path string :: str\n', '    """"""\n']",[],0
utilmy/zzarchive/zutil.py:os_folder_is_path,os_folder_is_path,function,2,3,3,36,12.0,0,0,['path_or_stream'],[None],[None],954,"['    """"""\n', '    Is given object `path_or_stream` a file path?\n', '    :param path_or_stream: file path or stream, file/file-like object\n', '    :return: True if `path_or_stream` is a file path\n', '    """"""\n']",['isinstance'],1
utilmy/zzarchive/zutil.py:os_file_get_path_from_stream,os_file_get_path_from_stream,function,7,20,15,187,9.35,0,2,['maybe_stream'],[None],[None],963,"['    """"""\n', '    Try to get file path from given stream `stream`.\n', '\n', '    :param maybe_stream: A file or file-like object\n', '    :return: Path of given file or file-like object or None\n', '\n', '    # >>> __file__ == get_path_from_stream(__file__)\n', '    True\n', ""    # >>> __file__ == get_path_from_stream(open(__file__, 'r'))\n"", '    True\n', '    # >>> strm = anyconfig.compat.StringIO()\n', '    # >>> get_path_from_stream(strm) is None\n', '    True\n', '    """"""\n']","['os_folder_is_path', 'getattr']",2
utilmy/zzarchive/zutil.py:os_file_try_to_get_extension,os_file_try_to_get_extension,function,4,12,10,112,9.33,0,1,['path_or_strm'],[None],[None],988,"['    """"""\n', '    Try to get file extension from given path or file object.\n', '    :return: File extension or None\n', '    """"""\n']","['os_file_get_path_from_stream', 'os_file_get_extension']",2
utilmy/zzarchive/zutil.py:os_file_are_same_file_types,os_file_are_same_file_types,function,5,20,14,154,7.7,0,2,['paths'],[None],[None],1000,"['    """"""\n', '    Are given (maybe) file paths same type (extension) ?\n', '    :param paths: A list of file path or file(-like) objects\n', '\n', '    # >>> are_same_file_types([])\n', '    False\n', '    # >>> are_same_file_types([""a.conf""])\n', '    True\n', '    # >>> are_same_file_types([""a.yml"", ""b.json""])\n', '    False\n', '    # >>> strm = anyconfig.compat.StringIO()\n', '    # >>> are_same_file_types([""a.yml"", ""b.yml"", strm])\n', '    False\n', '    """"""\n']","['os_file_try_to_get_extension', 'all']",2
utilmy/zzarchive/zutil.py:os_file_norm_paths,os_file_norm_paths,function,18,54,35,387,7.17,2,3,"['paths', 'marker']","[None, None]","[None, '""*""']",1024,"['    """"""\n', '    :param paths:\n', '        A glob path pattern string, or a list consists of path strings or glob\n', '        path pattern strings or file objects\n', ""    :param marker: Glob marker character or string, e.g. '*'\n"", '    :return: List of path strings\n', '    # >>> norm_paths([])\n', '    []\n', '    # >>> norm_paths(""/usr/lib/a/b.conf /etc/a/b.conf /run/a/b.conf"".split())\n', ""    ['/usr/lib/a/b.conf', '/etc/a/b.conf', '/run/a/b.conf']\n"", '    # >>> paths_s = os.path.join(os.path.dirname(__file__), ""u*.py"")\n', '    # >>> ref = sglob(paths_s)\n', '    # >>> ref = [""/etc/a.conf""] + ref\n', '    # >>> assert norm_paths([""/etc/a.conf"", paths_s]) == ref\n', '    # >>> strm = anyconfig.compat.StringIO()\n', '    # >>> assert norm_paths([""/etc/a.conf"", strm]) == [""/etc/a.conf"", strm]\n', '    """"""\n']","['sglob', 'sorted', '_norm_paths_itr', 'os_folder_is_path', 'list']",5
utilmy/zzarchive/zutil.py:os_file_mergeall,os_file_mergeall,function,10,19,19,173,9.11,1,0,"['nfile', 'dir1', 'pattern1', 'deepness']","[None, None, None, None]","[None, None, None, '2']",1068,[],"['os_file_listall', 'open', 'os_file_gettext', 'nfile1.write', 'nfile1.close']",5
utilmy/zzarchive/zutil.py:os_file_extracttext,os_file_extracttext,function,16,32,30,285,8.91,1,0,"['output_file', 'dir1', 'pattern1', 'htmltag', 'deepness']","[None, None, None, None, None]","[None, None, '""*.html""', '""p""', '2']",1077,"['    """""" Extract text from html """"""\n']","['os_file_listall', 'open', 'os_file_gettext', 'BeautifulSoup', 'soup.find_all', 'txt2.strip', 'output_file1.write']",7
utilmy/zzarchive/zutil.py:os_path_append,os_path_append,function,4,19,11,124,6.53,0,3,"['p1', 'p2', 'p3', 'p4']","[None, None, None, None]","[None, 'None', 'None', 'None']",1093,[],[],0
utilmy/zzarchive/zutil.py:os_wait_cpu,os_wait_cpu,function,5,21,16,257,12.24,1,0,"['priority', 'cpu_min']","[None, None]","['300', '50']",1104,[],"['psutil.cpu_percent', 'print', 'arrow.utcnow', 'time.sleep']",4
utilmy/zzarchive/zutil.py:os_split_dir_file,os_split_dir_file,function,7,15,13,114,7.6,0,1,['dirfile'],[None],[None],1116,[],"['dirfile.split', 'len']",2
utilmy/zzarchive/zutil.py:os_process_run,os_process_run,function,11,31,30,281,9.06,0,1,"['cmd_list', 'capture_output']","[None, None]","[None, 'False']",1126,"['    """"""os_process_run\n', '    \n', '    Args:\n', '         cmd_list: list [""program"", ""arg1"", ""arg2""]\n', '         capture_output: bool\n', '    """"""\n']","['subprocess.Popen', 'proc.communicate', 'print', 'str']",4
utilmy/zzarchive/zutil.py:os_process_2,os_process_2,function,0,1,1,4,4.0,0,0,[],[],[],1149,[],[],0
utilmy/zzarchive/zutil.py:py_importfromfile,py_importfromfile,function,12,24,20,269,11.21,0,1,"['modulename', 'dir1']","[None, None]","[None, None]",1183,[],"['a_get_pythonversion', 'SourceFileLoader', 'foo.MyClass', 'importlib.import_module']",4
utilmy/zzarchive/zutil.py:py_memorysize,py_memorysize,function,17,58,39,319,5.5,0,4,"['o', 'ids', 'hint', 'set(']","[None, None, None, None]","[None, None, '"" deep_getsizeof(df_pd', None]",1199,"['    """""" deep_getsizeof(df_pd, set())\n', '    Find the memory footprint of a Python object\n', '    The sys.getsizeof function does a shallow size of only. It counts each\n', '    object inside a container as pointer only regardless of how big it\n', '    """"""\n']","['id', 'getsizeof', 'ids.add', 'isinstance', 'sum', 'd', 'o.items']",7
utilmy/zzarchive/zutil.py:save,save,function,2,4,4,64,16.0,0,0,"['obj', 'folder', 'isabsolutpath']","[None, None, None]","[None, '""/folder1/keyname""', '0']",1229,[],['py_save_obj'],1
utilmy/zzarchive/zutil.py:load,load,function,2,3,3,60,20.0,0,0,"['folder', 'isabsolutpath']","[None, None]","['""/folder1/keyname""', '0']",1233,[],['py_load_obj'],1
utilmy/zzarchive/zutil.py:save_test,save_test,function,5,10,10,126,12.6,0,0,"['folder', 'isabsolutpath']","[None, None]","['""/folder1/keyname""', '0']",1237,[],"['py_load_obj', 'print', 'str', 'gc.collect']",4
utilmy/zzarchive/zutil.py:py_save_obj,py_save_obj,function,16,52,42,502,9.65,0,1,"['obj1', 'keyname', 'otherfolder']","[None, None, None]","[None, '""""', '0']",1244,[],"['z_key_splitinto_dir_name', 'os_folder_create', 'open', 'dill.dumps', 'print']",5
utilmy/zzarchive/zutil.py:py_load_obj,py_load_obj,function,18,33,30,277,8.39,0,1,"['folder', 'isabsolutpath', 'encoding1']","[None, None, None]","['""/folder1/keyname""', '0', '""utf-8""']",1257,"['    """"""def load_obj(name, encoding1=\'utf-8\' ):\n', ""         with open('D:/_devs/Python01/aaserialize/' + name + '.pkl', 'rb') as f:\n"", '            return pickle.load(f, encoding=encoding1)\n', '    """"""\n']","['folder.find', 'z_key_splitinto_dir_name', 'os_folder_create', 'open', 'pickle.load']",5
utilmy/zzarchive/zutil.py:z_key_splitinto_dir_name,z_key_splitinto_dir_name,function,7,15,13,114,7.6,0,1,['keyname'],[None],[None],1277,[],"['keyname.split', 'len']",2
utilmy/zzarchive/zutil.py:os_config_setfile,os_config_setfile,function,9,32,23,223,6.97,1,1,"['dict_params', 'outfile', 'mode1']","[None, None, None]","[None, None, '""w+""']",1287,[],"['open', 'dict_params.items', 'isinstance', 'f1.write', 'str', 'f1.close', 'print']",7
utilmy/zzarchive/zutil.py:os_config_getfile,os_config_getfile,function,6,14,14,73,5.21,1,0,['file1'],[None],[None],1299,[],"['open', 'f1.readlines', 'print']",3
utilmy/zzarchive/zutil.py:os_csv_process,os_csv_process,function,2,2,2,7,3.5,0,0,['file1'],[None],[None],1307,[],[],0
utilmy/zzarchive/zutil.py:pd_toexcel,pd_toexcel,function,24,91,60,862,9.47,0,8,"['df', 'outfile', 'sheet_name', 'append', 'returnfile']","[None, None, None, None, None]","[None, '""file.xlsx""', '""sheet1""', '1', '1']",1427,"['    """"""\n', '# Create a Pandas Excel writer using XlsxWriter as the engine.\n', ""writer = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n"", ""df.to_excel(writer, sheet_name='Sheet1')\n"", 'writer.save()\n', '\n', '# Get the xlsxwriter objects from the dataframe writer object.\n', 'workbook  = writer.book\n', ""worksheet = writer.sheets['Sheet1']\n"", '\n', '# Add some cell formats.\n', ""format1 = workbook.add_format({'num_format': '#,##0.00'})\n"", ""format2 = workbook.add_format({'num_format': '0%'})\n"", ""format3 = workbook.add_format({'num_format': 'h:mm:ss AM/PM'})\n"", '\n', '# Set the column width and format.\n', ""worksheet.set_column('B:B', 18, format1)\n"", '\n', '# Set the format but not the column width.\n', ""worksheet.set_column('C:C', None, format2)\n"", '\n', ""worksheet.set_column('D:D', 16, format3)\n"", '\n', '# Close the Pandas Excel writer and output the Excel file.\n', 'writer.save()\n', '\n', 'from openpyxl import load_workbook\n', 'wb = load_workbook(outfile)\n', 'ws = wb.active\n', ""ws.title = 'Table 1'\n"", '\n', 'tableshape = np.shape(table)\n', 'alph = list(string.ascii_uppercase)\n', '\n', 'for i in range(tableshape[0]):\n', '    for j in range(tableshape[1]):\n', '        ws[alph[i]+str(j+1)] = table[i, j]\n', '\n', ""for cell in ws['A'] + ws[1]:\n"", ""    cell.style = 'Pandas'\n"", '\n', ""wb.save('Scores.xlsx')\n"", '\n', '   """"""\n']","['os_file_exist', 'load_workbook', 'pd.ExcelWriter', 'dict', 'df.to_excel', 'writer.save', 'pd_toexcel_many', 'pd_toexcel']",8
utilmy/zzarchive/zutil.py:pd_toexcel_many,pd_toexcel_many,function,1,3,3,40,13.33,0,0,"['outfile', 'df1', 'df2', 'df3', 'df4', 'df5', 'df6', 'outfile', 'sheet_name=""df1"")if df2 is not None']","[None, None, None, None, None, None, None, None, '']","['""file1.xlsx""', 'None', 'None', 'None', 'None', 'None', 'Nonedf1', None, '""df1"")if df2 is not None:']",1492,[],['pd_toexcel'],1
utilmy/zzarchive/zutil.py:find_fuzzy,find_fuzzy,function,4,10,10,49,4.9,1,1,"['xstring', 'list_string']","[None, None]","[None, None]",1509,"['    """""" if xstring matches partially, add to the list   """"""\n']",['xi.find'],1
utilmy/zzarchive/zutil.py:str_match_fuzzy,str_match_fuzzy,function,4,12,11,65,5.42,1,1,"['xstring', 'list_string']","[None, None]","[None, None]",1514,"['    """""" if any of list_strinf elt matches partially xstring """"""\n']",['xstring.find'],1
utilmy/zzarchive/zutil.py:str_parse_stringcalendar,str_parse_stringcalendar,function,14,40,30,282,7.05,3,1,['cal'],[None],[None],1522,"['    """"""----------Parse Calendar  --------""""""\n']","['cal.split', 'x.find', 'cal4.append', 'np.array', 'print']",5
utilmy/zzarchive/zutil.py:str_make_unicode,str_make_unicode,function,6,13,10,123,9.46,0,1,"['input_str', 'errors']","[None, None]","[None, '""replace""']",1539,[],"['type', 'input_str.decode']",2
utilmy/zzarchive/zutil.py:str_empty_string_array,str_empty_string_array,function,8,33,23,182,5.52,2,1,"['x', 'y']","[None, None]","[None, '1']",1548,[],"['range', 'str_empty_string_array_numpy', 'np.empty']",3
utilmy/zzarchive/zutil.py:str_empty_string_array_numpy,str_empty_string_array_numpy,function,4,9,8,56,6.22,0,0,"['nx', 'ny']","[None, None]","[None, '1']",1555,[],['np.empty'],1
utilmy/zzarchive/zutil.py:str_isfloat,str_isfloat,function,2,8,7,58,7.25,0,0,['value'],[None],[None],1561,[],['float'],1
utilmy/zzarchive/zutil.py:str_is_azchar,str_is_azchar,function,2,8,7,54,6.75,0,0,['x'],[None],[None],1569,[],['float'],1
utilmy/zzarchive/zutil.py:str_is_az09char,str_is_az09char,function,2,8,7,54,6.75,0,0,['x'],[None],[None],1577,[],['float'],1
utilmy/zzarchive/zutil.py:str_reindent,str_reindent,function,3,8,7,65,8.12,0,0,"['s', 'num_spaces']","[None, None]","[None, None]",1585,"['    """"""\n', '   if args:\n', ""       aux= name1+'.'+obj.__name__ +'('+ str(args) +')  \\n' + str(inspect.getdoc(obj))\n"", ""       aux= aux.replace('\\n', '\\n       ')\n"", '       aux= aux.rstrip()\n', ""       aux= aux + ' \\n'\n"", '       wi( aux)\n', '    """"""\n']",['x.decode'],1
utilmy/zzarchive/zutil.py:str_split2,str_split2,function,3,8,7,65,8.12,0,0,"['delimiters', 'string', 'maxsplit']","[None, None, None]","[None, None, '0']",1600,[],['x.decode'],1
utilmy/zzarchive/zutil.py:str_split_pattern,str_split_pattern,function,3,8,7,65,8.12,0,0,"['sep2', 'll', 'maxsplit']","[None, None, None]","[None, None, '0']",1607,[],['x.decode'],1
utilmy/zzarchive/zutil.py:pd_str_isascii,pd_str_isascii,function,3,8,7,65,8.12,0,0,['x'],[None],[None],1619,[],['x.decode'],1
utilmy/zzarchive/zutil.py:str_to_utf8,str_to_utf8,function,2,2,2,23,11.5,0,0,['x'],[None],[None],1627,"['    """""" Do it before saving/output to external printer """"""\n']",['x.encode'],1
utilmy/zzarchive/zutil.py:str_to_unicode,str_to_unicode,function,4,13,9,80,6.15,0,2,"['x', 'encoding']","[None, None]","[None, '""utf-8""']",1632,"['    """""" Do it First after Loading some text """"""\n']","['isinstance', 'str']",2
utilmy/zzarchive/zutil.py:np_minimize,np_minimize,function,28,116,91,902,7.78,2,5,"['fun_obj', 'x0', 'argext', '0']","[None, None, None, None]","[None, 'None', '(0', None]",1642,[],"['penalty', 'enumerate', 'max', 'loss', 'fun_obj', 'np_minimize_de', 'range', 'next', 'print', 'save', 'date_now', 'np_int_tostr', 'np.mod', 'np.abs']",14
utilmy/zzarchive/zutil.py:np_minimize_de,np_minimize_de,function,17,56,47,452,8.07,1,3,"['fun_obj', 'bounds', 'name1', 'maxiter', 'popsize', 'solver']","[None, None, None, None, None, None]","[None, None, None, '10', '5', 'None']",1662,[],"['range', 'next', 'print', 'save', 'date_now', 'np_int_tostr', 'np.mod', 'np.abs']",8
utilmy/zzarchive/zutil.py:np_remove_na_inf_2d,np_remove_na_inf_2d,function,12,26,23,117,4.5,2,1,['x'],[None],[None],1686,[],"['np.shape', 'range', 'np.isnan', 'np.isinf']",4
utilmy/zzarchive/zutil.py:np_addcolumn,np_addcolumn,function,7,10,9,85,8.5,0,0,"['arr', 'nbcol']","[None, None]","[None, None]",1695,[],"['np.shape', 'np.zeros', 'np.column_stack']",3
utilmy/zzarchive/zutil.py:np_addrow,np_addrow,function,8,18,17,138,7.67,0,1,"['arr', 'nbrow']","[None, None]","[None, None]",1702,[],"['np.shape', 'len', 'np.zeros', 'np.row_stack', 'np.append']",5
utilmy/zzarchive/zutil.py:np_int_tostr,np_int_tostr,function,3,17,12,72,4.24,0,1,['i'],[None],[None],1712,[],['str'],1
utilmy/zzarchive/zutil.py:np_dictordered_create,np_dictordered_create,function,5,6,6,52,8.67,0,0,[],[],[],1721,[],['OrderedDict'],1
utilmy/zzarchive/zutil.py:np_list_unique,np_list_unique,function,1,2,2,20,10.0,0,0,['seq'],[None],[None],1727,[],['list'],1
utilmy/zzarchive/zutil.py:np_list_tofreqdict,np_list_tofreqdict,function,10,41,25,214,5.22,2,2,"['l1', 'wweight']","[None, None]","[None, 'None']",1731,[],"['dict', 'len', 'enumerate']",3
utilmy/zzarchive/zutil.py:np_list_flatten,np_list_flatten,function,11,25,19,132,5.28,2,1,['seq'],[None],[None],1754,[],"['type', 'np_list_flatten', 'ret.append']",3
utilmy/zzarchive/zutil.py:np_dict_tolist,np_dict_tolist,function,6,18,14,100,5.56,2,1,"['dd', 'withkey']","[None, None]","[None, '0']",1766,[],['list'],1
utilmy/zzarchive/zutil.py:np_dict_tostr_val,np_dict_tostr_val,function,1,7,7,52,7.43,0,0,['dd'],[None],[None],1773,[],['list'],1
utilmy/zzarchive/zutil.py:np_dict_tostr_key,np_dict_tostr_key,function,1,7,7,52,7.43,0,0,['dd'],[None],[None],1777,[],['list'],1
utilmy/zzarchive/zutil.py:np_removelist,np_removelist,function,7,20,16,100,5.0,1,2,"['x0', 'xremove']","[None, None]","[None, 'None']",1781,[],"['np_findfirst', 'xnew.append']",2
utilmy/zzarchive/zutil.py:np_transform2d_int_1d,np_transform2d_int_1d,function,18,47,39,238,5.06,2,0,"['m2d', 'onlyhalf']","[None, None]","[None, 'False']",1792,[],"['np.shape', 'np.zeros', 'range', 'np.abs', 'np_sortbycol']",5
utilmy/zzarchive/zutil.py:np_mergelist,np_mergelist,function,5,9,9,55,6.11,1,0,"['x0', 'x1']","[None, None]","[None, None]",1809,[],"['list', 'xnew.append']",2
utilmy/zzarchive/zutil.py:np_enumerate2,np_enumerate2,function,8,16,14,84,5.25,1,0,['vec_1d'],[None],[None],1816,[],"['np.empty', 'enumerate']",2
utilmy/zzarchive/zutil.py:np_pivottable_count,np_pivottable_count,function,10,24,21,170,7.08,1,0,['mylist'],[None],[None],1825,[],"['np_dict_tolist', 'sorted', 'int']",3
utilmy/zzarchive/zutil.py:np_nan_helper,np_nan_helper,function,2,5,5,40,8.0,0,0,['y'],[None],[None],1834,"['    """""" Input:  - y, 1d numpy array with possible NaNs\n', '        Output - nans, logical indices of NaNs - index, a function, with signature\n', '              indices= index(logical_indices),\n', ""              to convert logical indices of NaNs to 'equivalent' indices\n"", '    """"""\n']","['np.isnan', 'z.nonzero']",2
utilmy/zzarchive/zutil.py:np_interpolate_nan,np_interpolate_nan,function,7,9,9,76,8.44,0,0,['y'],[None],[None],1843,[],"['np_nan_helper', 'np.interp', 'x']",3
utilmy/zzarchive/zutil.py:np_and1,np_and1,function,8,69,25,362,5.25,0,6,"['x', 'y', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']","[None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None', 'None', 'None']",1849,[],[],0
utilmy/zzarchive/zutil.py:np_sortcol,np_sortcol,function,6,7,7,77,11.0,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",1864,"['    """""" df.sort([\'A\', \'B\'], ascending=[1, 0])  """"""\n']","['pd.DataFrame', 'df.sort_values']",2
utilmy/zzarchive/zutil.py:np_ma,np_ma,function,2,6,6,45,7.5,0,0,"['vv', 'n']","[None, None]","[None, None]",1871,"['    """"""Moving average """"""\n']","['np.convolve', 'np.ones']",2
utilmy/zzarchive/zutil.py:np_cleanmatrix,np_cleanmatrix,function,12,25,21,125,5.0,2,1,['m'],[None],[None],1877,[],"['np.nan_to_num', 'np.shape', 'range', 'abs']",4
utilmy/zzarchive/zutil.py:np_torecarray,np_torecarray,function,2,6,6,105,17.5,0,0,"['arr', 'colname']","[None, None]","[None, None]",1887,[],['np.shape'],1
utilmy/zzarchive/zutil.py:np_sortbycolumn,np_sortbycolumn,function,6,7,7,77,11.0,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",1893,[],"['pd.DataFrame', 'df.sort_values']",2
utilmy/zzarchive/zutil.py:np_sortbycol,np_sortbycol,function,8,26,19,257,9.88,0,1,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",1899,[],"['pd.DataFrame', 'df.sort_values', 'np_sortbycol', 'len', 'np.reshape']",5
utilmy/zzarchive/zutil.py:np_min_kpos,np_min_kpos,function,2,5,5,36,7.2,0,0,"['arr', 'kth']","[None, None]","[None, None]",1912,"['    """""" return kth mininimun """"""\n']",['np.partition'],1
utilmy/zzarchive/zutil.py:np_max_kpos,np_max_kpos,function,3,10,8,53,5.3,0,0,"['arr', 'kth']","[None, None]","[None, None]",1917,"['    """""" return kth mininimun """"""\n']","['len', 'np.partition']",2
utilmy/zzarchive/zutil.py:np_findfirst,np_findfirst,function,5,11,10,55,5.0,1,1,"['item', 'vec']","[None, None]","[None, None]",1924,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zzarchive/zutil.py:np_find,np_find,function,5,11,10,55,5.0,1,1,"['item', 'vec']","[None, None]","[None, None]",1933,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zzarchive/zutil.py:find,find,function,4,10,10,49,4.9,1,1,"['xstring', 'list_string']","[None, None]","[None, None]",1941,"['    """"""return the index of the first occurence of item in vec""""""\n']",['xi.find'],1
utilmy/zzarchive/zutil.py:findnone,findnone,function,4,12,11,55,4.58,1,1,['vec'],[None],[None],1949,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zzarchive/zutil.py:findx,findx,function,8,24,19,130,5.42,0,2,"['item', 'vec']","[None, None]","[None, None]",1957,"['    """"""return the index of the first occurence of item in vec""""""\n']","['type', 'vec.index', 'np.where', 'len']",4
utilmy/zzarchive/zutil.py:finds,finds,function,9,30,20,158,5.27,2,3,"['itemlist', 'vec']","[None, None]","[None, None]",1970,"['    """"""return the index of the first occurence of item in vec""""""\n']","['range', 'idlist.append']",2
utilmy/zzarchive/zutil.py:findhigher,findhigher,function,5,12,11,51,4.25,1,1,"['x', 'vec']","[None, None]","[None, None]",1987,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zzarchive/zutil.py:findlower,findlower,function,5,12,11,51,4.25,1,1,"['x', 'vec']","[None, None]","[None, None]",1995,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zzarchive/zutil.py:np_find_minpos,np_find_minpos,function,4,7,5,95,13.57,0,0,['values'],[None],[None],2003,[],['min'],1
utilmy/zzarchive/zutil.py:np_find_maxpos,np_find_maxpos,function,16,47,35,274,5.83,1,3,['values'],[None],[None],2008,[],"['max', 'np_find_maxpos_2nd', 'float', 'enumerate']",4
utilmy/zzarchive/zutil.py:np_find_maxpos_2nd,np_find_maxpos_2nd,function,12,39,29,146,3.74,1,3,['numbers'],[None],[None],2013,[],"['float', 'enumerate']",2
utilmy/zzarchive/zutil.py:np_findlocalmax2,np_findlocalmax2,function,32,141,83,689,4.89,2,5,"['v', 'trig']","[None, None]","[None, None]",2028,[],"['len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'np_sortbycolumn', 'range', 'findhigher', 'np.abs']",8
utilmy/zzarchive/zutil.py:np_findlocalmin2,np_findlocalmin2,function,32,145,83,702,4.84,2,6,"['v', 'trig']","[None, None]","[None, None]",2064,[],"['len', 'np.zeros', 'np_find_minpos', 'enumerate', 'np_sortbycolumn', 'range', 'findlower', 'np.abs']",8
utilmy/zzarchive/zutil.py:np_findlocalmax,np_findlocalmax,function,32,141,83,689,4.89,2,5,"['v', 'trig']","[None, None]","[None, None]",2100,[],"['len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'np_sortbycolumn', 'range', 'findhigher', 'np.abs']",8
utilmy/zzarchive/zutil.py:np_findlocalmin,np_findlocalmin,function,32,145,83,702,4.84,2,6,"['v', 'trig']","[None, None]","[None, None]",2116,[],"['len', 'np.zeros', 'np_find_minpos', 'enumerate', 'np_sortbycolumn', 'range', 'findlower', 'np.abs']",8
utilmy/zzarchive/zutil.py:np_stack,np_stack,function,11,63,21,347,5.51,0,7,"['v1', 'v2', 'v3', 'v4', 'v5']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",2134,[],"['np.shape', 'np.row_stack', 'np.column_stack']",3
utilmy/zzarchive/zutil.py:np_uniquerows,np_uniquerows,function,6,9,9,148,16.44,0,0,['a'],[None],[None],2156,[],"['np.ascontiguousarray', 'np.unique', 'unique_a.view']",3
utilmy/zzarchive/zutil.py:np_remove_zeros,np_remove_zeros,function,2,4,4,35,8.75,0,0,"['vv', 'axis1']","[None, None]","[None, '1']",2162,[],[],0
utilmy/zzarchive/zutil.py:np_sort,np_sort,function,6,7,7,77,11.0,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",2166,"['    """"""\n', '    Creates a cross-tab or pivot table from a normalised input table. Use this\n', ""    function to 'denormalize' a table of normalized records.\n"", '\n', '    * The table argument can be a list of dictionaries or a Table object.\n', '    (http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/334621)\n', '    * The left argument is a tuple of headings which are displayed down the\n', '    left side of the new table.\n', '    * The top argument is a tuple of headings which are displayed across the\n', '    top of the new table.\n', '    Tuples are used so that multiple element headings and columns can be used.\n', '\n', '    E.g. To transform the list (listOfDicts):\n', '\n', '    Name,   Year,  Value\n', '    -----------------------\n', ""    'Simon', 2004, 32\n"", ""    'Simon', 2005, 128\n"", ""    'Russel', 2004, 64\n"", ""    'Eric', 2004, 52\n"", ""    'Russel', 2005, 32\n"", '\n', '    into the new list:\n', '\n', ""    'Name',   2004, 2005\n"", '    ------------------------\n', ""    'Simon',  32,     128\n"", ""    'Russel',  64,     32\n"", ""    'Eric',   52,     NA\n"", '\n', '    you would call pivot with the arguments:\n', '\n', ""    newList = pivot(listOfDicts, ('Name',), ('Year',), 'Value')\n"", '\n', '    """"""\n']","['pd.DataFrame', 'df.sort_values']",2
utilmy/zzarchive/zutil.py:np_memory_array_adress,np_memory_array_adress,function,2,2,2,38,19.0,0,0,['x'],[None],[None],2170,[],[],0
utilmy/zzarchive/zutil.py:np_pivotable_create,np_pivotable_create,function,28,100,60,701,7.01,4,5,"['table', 'left', 'top', 'value']","[None, None, None, None]","[None, None, None, None]",2175,"['    """"""\n', '    Creates a cross-tab or pivot table from a normalised input table. Use this\n', ""    function to 'denormalize' a table of normalized records.\n"", '\n', '    * The table argument can be a list of dictionaries or a Table object.\n', '    (http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/334621)\n', '    * The left argument is a tuple of headings which are displayed down the\n', '    left side of the new table.\n', '    * The top argument is a tuple of headings which are displayed across the\n', '    top of the new table.\n', '    Tuples are used so that multiple element headings and columns can be used.\n', '\n', '    E.g. To transform the list (listOfDicts):\n', '\n', '    Name,   Year,  Value\n', '    -----------------------\n', ""    'Simon', 2004, 32\n"", ""    'Simon', 2005, 128\n"", ""    'Russel', 2004, 64\n"", ""    'Eric', 2004, 52\n"", ""    'Russel', 2005, 32\n"", '\n', '    into the new list:\n', '\n', ""    'Name',   2004, 2005\n"", '    ------------------------\n', ""    'Simon',  32,     128\n"", ""    'Russel',  64,     32\n"", ""    'Eric',   52,     NA\n"", '\n', '    you would call pivot with the arguments:\n', '\n', ""    newList = pivot(listOfDicts, ('Name',), ('Year',), 'Value')\n"", '\n', '    """"""\n']","['tuple', 'ysort.append', 'xsort.append', 'len', 'list', 'xsort.sort', 'headings.extend', 'sortedkeys.sort', 'row.extend', 't.append']",10
utilmy/zzarchive/zutil.py:pd_info,pd_info,function,12,24,22,257,10.71,1,2,"['df', 'doreturn']","[None, None]","[None, '1']",2259,[],"['df.info', 'print', 'type', 'pd_info_memsize', 'pd_info', 'df.memory_usage']",6
utilmy/zzarchive/zutil.py:pd_info_memsize,pd_info_memsize,function,5,7,7,85,12.14,0,1,"['df', 'memusage']","[None, None]","[None, '0']",2268,[],"['df.info', 'pd_info', 'df.memory_usage']",3
utilmy/zzarchive/zutil.py:pd_row_findlast,pd_row_findlast,function,7,11,11,58,5.27,1,1,"['df', 'colid', 'emptyrowid']","[None, None, None]","[None, '0', 'None']",2275,[],['df.iterrows'],1
utilmy/zzarchive/zutil.py:pd_row_select,pd_row_select,function,13,112,53,862,7.7,1,3,"['df', '**conditions']","[None, None]","[None, None]",2282,"['    """"""Select rows from a df according to conditions\n', '    pdselect(data, a=2, b__lt=3) __gt __ge __lte  __in  __not_in\n', ""    will select all rows where 'a' is 2 and 'b' is less than 3\n"", '    """"""\n']","['type', 'pd.DataFrame', 'list', 'c.endswith', 'np.in1d', 'ValueError']",6
utilmy/zzarchive/zutil.py:pd_csv_randomread,pd_csv_randomread,function,13,47,38,267,5.68,0,1,"['filename', 'nsample', 'filemaxline', 'dtype']","[None, None, None, None]","[None, '10000', '-1', 'None']",2328,[],"['sum', 'open', 'np.sort', 'pd.read_csv']",4
utilmy/zzarchive/zutil.py:pd_array_todataframe,pd_array_todataframe,function,14,37,30,313,8.46,0,2,"['array', 'colname', 'index1', 'dotranspose']","[None, None, None, None]","[None, 'None', 'None', 'False']",2341,[],"['np.shape', 'len', 'pd.DataFrame']",3
utilmy/zzarchive/zutil.py:pd_dataframe_toarray,pd_dataframe_toarray,function,9,11,10,103,9.36,0,0,['df'],[None],[None],2352,[],['df.reset_index'],1
utilmy/zzarchive/zutil.py:pd_createdf,pd_createdf,function,2,4,4,55,13.75,0,0,"['array1', 'col1', 'idx1']","[None, None, None]","[None, 'None', 'None']",2359,[],['pd.DataFrame'],1
utilmy/zzarchive/zutil.py:pd_create_colmapdict_nametoint,pd_create_colmapdict_nametoint,function,9,13,12,77,5.92,1,0,['df'],[None],[None],2363,"['    """""" \'close\' ---> 5    """"""\n']",['enumerate'],1
utilmy/zzarchive/zutil.py:pd_extract_col_idx_val,pd_extract_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],2373,[],[],0
utilmy/zzarchive/zutil.py:pd_extract_col_uniquevalue_tocsv,pd_extract_col_uniquevalue_tocsv,function,5,7,6,101,14.43,0,0,"['df', 'colname', 'csvfile']","[None, None, None]","[None, '""""', '""""']",2377,"['    """""" Write one column into a file   """"""\n']","['np.array', 'pd.DataFrame', 'print']",3
utilmy/zzarchive/zutil.py:pd_split_col_idx_val,pd_split_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],2385,[],[],0
utilmy/zzarchive/zutil.py:pd_splitdf_inlist,pd_splitdf_inlist,function,14,34,25,254,7.47,3,2,"['df', 'colid', 'type1']","[None, None, None]","[None, None, '""dict""']",2389,"['    """""" Split df into dictionnary of dict/list """"""\n']","['list', 'l1.append']",2
utilmy/zzarchive/zutil.py:pd_find,pd_find,function,35,142,88,1051,7.4,4,8,"['df', 'regex_pattern', 'col_restrict', 'isnumeric', 'doreturnposition']","[None, None, None, None, None]","[None, '""*""', 'None', 'False', 'False']",2405,"['    """""" Find string / numeric values inside df columns, return position where found\n', '     col_restrict : restrict to these columns """"""\n']","['str', 'print', 'enumerate', 'np.column_stack', 'len', 'gc.collect', 'np_dictordered_create', 'range', 'np.array']",9
utilmy/zzarchive/zutil.py:pd_dtypes_totype2,pd_dtypes_totype2,function,6,8,8,59,7.38,1,0,"['df', 'columns']","[None, None]","[None, '(']",2466,[],[],0
utilmy/zzarchive/zutil.py:pd_dtypes,pd_dtypes,function,28,76,63,490,6.45,2,2,"['df', 'columns']","[None, None]","[None, '(']",2472,[],"['pd_dtypes', 'OrderedDict', 'enumerate', 'eval', 'print', 'df.astype']",6
utilmy/zzarchive/zutil.py:pd_df_todict2,pd_df_todict2,function,14,31,26,249,8.03,1,2,"['df', 'colkey', 'excludekey', '']","[None, None, None, None]","[None, '""table""', '(""""', None]",2492,[],"['df.drop_duplicates', 'range', 'len', 'dict0.setdefault']",4
utilmy/zzarchive/zutil.py:pd_df_todict,pd_df_todict,function,21,49,39,414,8.45,2,2,"['df', 'colkey', 'excludekey', '']","[None, None, None, None]","[None, '""table""', '(""""', None]",2507,[],"['df.drop_duplicates', 'range', 'len', 'dict0.setdefault', 'pd_df_todict', 'df.iterrows']",6
utilmy/zzarchive/zutil.py:pd_col_addfrom_dfmap,pd_col_addfrom_dfmap,function,4,7,6,69,9.86,0,0,"['df', 'dfmap', 'colkey', 'colval', 'df_colused', 'df_colnew', 'exceptval', 'inplace', 'colkey', 'colval=colval)rowi)']","[None, None, None, None, None, None, None, None, None, '']","[None, None, None, None, None, None, '-1', 'Truedfmap', 'colkey', 'colval)rowi):']",2518,"['    """""" Add new columns based on df_map:  In Place Modification of df\n', '    df:     Dataframe of transactions.\n', '    dfmap:  FSMaster Dataframe\n', '      colkey: colum used for dict key.  machine_code\n', '      colval: colum used for dict val.  adress\n', '\n', '    df_colused  :     ""machine_code""\n', ""    exception val:  -1 or ''\n"", '  """"""\n']",[],0
utilmy/zzarchive/zutil.py:pd_applyfun_col,pd_applyfun_col,function,5,10,9,109,10.9,0,1,"['df', 'newcol', 'ff', 'use_colname']","[None, None, None, None]","[None, None, None, '""all/[colname]""']",2578,"['    """""" use all Columns to compute values """"""\n']",['ff'],1
utilmy/zzarchive/zutil.py:pd_date_intersection,pd_date_intersection,function,7,17,13,157,9.24,1,0,['qlist'],[None],[None],2599,[],"['set', 'set.intersection', 'list', 'sorted']",4
utilmy/zzarchive/zutil.py:pd_is_categorical,pd_is_categorical,function,3,13,11,118,9.08,0,1,['z'],[None],[None],2609,[],['isinstance'],1
utilmy/zzarchive/zutil.py:pd_str_encoding_change,pd_str_encoding_change,function,6,8,8,76,9.5,1,0,"['df', 'cols', 'fromenc', 'toenc']","[None, None, None, None]","[None, None, '""iso-8859-1""', '""utf-8""']",2618,[],[],0
utilmy/zzarchive/zutil.py:pd_str_unicode_tostr,pd_str_unicode_tostr,function,4,7,7,76,10.86,0,0,"['df', 'targetype']","[None, None]","[None, 'str']",2625,"['    """"""\n', ' https://www.azavea.com/blog/2014/03/24/solving-unicode-problems-in-python-2-7/\n', ' Nearly every Unicode problem can be solved by the proper application of these tools;\n', ' they will help you build an airlock to keep the inside of your code nice and clean:\n', '\n', 'encode(): Gets you from Unicode -> bytes\n', 'decode(): Gets you from bytes -> Unicode\n', 'codecs.open(encoding=""utf-8""): Read and write files directly to/from Unicode (you can use any\n', 'encoding,\n', ' not just utf-8, but utf-8 is most common).\n', 'u"": Makes your string literals into Unicode objects rather than byte sequences.\n', ""Warning: Don't use encode() on bytes or decode() on Unicode objects\n"", '\n', '# >>> uni_greeting % utf8_name\n', 'Traceback (most recent call last):\n', ' File ""<stdin>"", line 1, in <module>\n', ""UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 3: ordinal not in range(128)\n"", '# Solution:\n', ""# >>> uni_greeting % utf8_name.decode('utf-8')\n"", ""u'Hi, my name is Josxe9.'\n"", '\n', ' """"""\n']",['pd_dtypes_type1_totype2'],1
utilmy/zzarchive/zutil.py:pd_dtypes_type1_totype2,pd_dtypes_type1_totype2,function,7,11,11,100,9.09,1,1,"['df', 'fromtype', 'targetype']","[None, None, None]","[None, 'str', 'str']",2653,[],['isinstance'],1
utilmy/zzarchive/zutil.py:pd_resetindex,pd_resetindex,function,3,5,5,50,10.0,0,0,['df'],[None],[None],2661,[],"['list', 'len']",2
utilmy/zzarchive/zutil.py:pd_insertdatecol,pd_insertdatecol,function,4,7,7,71,10.14,0,0,"['df', 'col', 'format1=""%Y-%m-%d %H']","[None, None, '']","[None, None, '""%Y-%m-%d %H:%M:%S:%f""']",2666,[],['date_nowtime'],1
utilmy/zzarchive/zutil.py:pd_replacevalues,pd_replacevalues,function,11,14,14,104,7.43,1,0,"['df', 'matrix']","[None, None]","[None, None]",2671,"['    """""" Matrix replaces df.values  """"""\n']",['np.shape'],1
utilmy/zzarchive/zutil.py:pd_removerow,pd_removerow,function,2,2,2,29,14.5,0,0,"['df', 'row_list_index', '45']","[None, None, None]","[None, '(23', None]",2681,[],['df.drop'],1
utilmy/zzarchive/zutil.py:pd_removecol,pd_removecol,function,2,3,3,28,9.33,0,0,"['df1', 'name1']","[None, None]","[None, None]",2685,[],['df1.drop'],1
utilmy/zzarchive/zutil.py:pd_insertrow,pd_insertrow,function,7,11,10,112,10.18,0,0,"['df', 'rowval', 'index1', 'isreset']","[None, None, None, None]","[None, None, 'None', '1']",2689,[],"['pd_array_todataframe', 'df.append']",2
utilmy/zzarchive/zutil.py:pd_h5_cleanbeforesave,pd_h5_cleanbeforesave,function,4,13,11,91,7.0,0,0,['df'],[None],[None],2699,"['    """"""Clean Column type before Saving in HDFS: Unicode, Datetime  """"""\n']","['pd_resetindex', 'pd_str_unicode_tostr']",2
utilmy/zzarchive/zutil.py:pd_h5_addtable,pd_h5_addtable,function,6,16,16,148,9.25,0,1,"['df', 'tablename', 'dbfile=""F']","[None, None, '']","[None, None, '""F:\\temp_pandas.h5""']",2716,[],"['pd.HDFStore', 'find', 'list', 'print', 'store.append', 'store.close']",6
utilmy/zzarchive/zutil.py:pd_h5_tableinfo,pd_h5_tableinfo,function,4,4,4,65,16.25,0,0,"['filenameh5', 'table']","[None, None]","[None, None]",2726,[],"['pd.HDFStore', 'store.get_storer']",2
utilmy/zzarchive/zutil.py:pd_h5_dumpinfo,pd_h5_dumpinfo,function,13,28,26,334,11.93,1,0,"['dbfile=r""E']",[''],"['r""E:\\_data\\stock\\intraday_google.h5""']",2731,[],"['pd.HDFStore', 'list', 'pd.DataFrame', 'extract.append', 'datetime_tostring', 'errsym.append', 'np.array']",7
utilmy/zzarchive/zutil.py:pd_h5_save,pd_h5_save,function,4,5,5,64,12.8,0,0,"['df', 'filenameh5=""E', 'key']","[None, '', None]","[None, '""E:/_data/_data_outlier.h5""', '""data""']",2753,"['    """""" File is release after saving it""""""\n']","['pd.HDFStore', 'store.append', 'store.close']",3
utilmy/zzarchive/zutil.py:pd_h5_load,pd_h5_load,function,9,24,20,210,8.75,0,2,"['filenameh5=""E', 'table_id', 'exportype', 'rowstart', 'rowend', ')', '']","['', None, None, None, None, None, None]","['""E:/_data/_data_outlier.h5""', '""data""', '""pandas""', '-1', '-1', None, None]",2760,[],"['pd.read_hdf', 'pd.DataFrame']",2
utilmy/zzarchive/zutil.py:pd_h5_fromcsv_tohdfs,pd_h5_fromcsv_tohdfs,function,29,80,63,661,8.26,3,2,"['dircsv', 'filepattern', 'tofilehdfs', 'tablename', ')', 'dtype0', 'encoding', 'chunksize', 'mode', 'form', 'complib', '']","[None, None, None, None, None, None, None, None, None, None, None, None]","['""dir1/dir2/""', '""*.csv""', '""file1.h5""', '""df""', None, 'None', '""utf-8""', '2000000', '""a""', '""table""', 'None', None]",2780,[],"['os_file_listall', 'pd.HDFStore', 'enumerate', 'pd.read_csv', 'print', 'store.append', 'store.close', 'os_file_exist']",8
utilmy/zzarchive/zutil.py:pd_np_toh5file,pd_np_toh5file,function,6,8,7,82,10.25,0,0,"['numpyarr', 'fileout', 'table1']","[None, None, None]","[None, '""file.h5""', '""data""']",2837,[],"['pd.DataFrame', 'pd.HDFStore', 'st.append']",3
utilmy/zzarchive/zutil.py:date_allinfo,date_allinfo,function,0,0,0,0,0.0,0,0,[],[],[],2844,"['    """"""\n', '\n', 'https://aboutsimon.com/blog/2016/08/04/datetime-vs-Arrow-vs-Pendulum-vs-Delorean-vs-udatetime.html\n', '\n', '\n', '   """"""\n']",[],0
utilmy/zzarchive/zutil.py:datetime_tostring,datetime_tostring,function,9,21,15,245,11.67,1,2,['datelist1'],[None],[None],2853,[],"['isinstance', 'datelist1.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zzarchive/zutil.py:date_remove_bdays,date_remove_bdays,function,14,36,26,354,9.83,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",2866,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zzarchive/zutil.py:date_add_bdays,date_add_bdays,function,14,36,27,353,9.81,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",2884,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zzarchive/zutil.py:datenumpy_todatetime,datenumpy_todatetime,function,9,40,24,384,9.6,0,3,"['tt', 'islocaltime']","[None, None]","[None, 'True']",2902,[],['type'],1
utilmy/zzarchive/zutil.py:datetime_tonumpydate,datetime_tonumpydate,function,4,4,4,36,9.0,0,0,"['t', 'islocaltime']","[None, None]","[None, 'True']",2919,[],['np.datetime64'],1
utilmy/zzarchive/zutil.py:datestring_todatetime,datestring_todatetime,function,9,16,14,136,8.5,1,1,"['datelist1', 'format1']","[None, None]","[None, '""%Y%m%d""']",2925,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zzarchive/zutil.py:datetime_toint,datetime_toint,function,6,14,12,156,11.14,1,1,['datelist1'],[None],[None],2937,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zzarchive/zutil.py:date_holiday,date_holiday,function,0,0,0,0,0.0,0,0,[],[],[],2946,"['    """"""\n', '   https://jakevdp.github.io/blog/2015/07/23/learning-seattles-work-habits-from-bicycle-counts/\n', '\n', 'from pandas.tseries.holiday import USFederalHolidayCalendar\n', 'cal = USFederalHolidayCalendar()\n', ""holidays = cal.holidays('2012', '2016', return_name=True)\n"", 'holidays.head()\n', '\n', 'holidays_all = pd.concat([holidays, ""Day Before "" + holidays.shift(-1, \'D\'),  ""Day After ""\n', ""+ holidays.shift(1, 'D')])\n"", 'holidays_all = holidays_all.sort_index()\n', 'holidays_all.head()\n', '\n', 'from pandas.tseries.offsets import CustomBusinessDay\n', 'from pandas.tseries.holiday import USFederalHolidayCalendar\n', 'bday_us = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n', 'dateref[-1] - bday_us- bday_us\n', '\n', '   """"""\n']",[],0
utilmy/zzarchive/zutil.py:date_add_bday,date_add_bday,function,14,36,27,353,9.81,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",2968,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zzarchive/zutil.py:dateint_todatetime,dateint_todatetime,function,7,14,12,136,9.71,1,1,['datelist1'],[None],[None],2983,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zzarchive/zutil.py:date_diffinday,date_diffinday,function,4,5,5,74,14.8,0,0,"['intdate1', 'intdate2']","[None, None]","[None, None]",2993,[],['dateint_todatetime'],1
utilmy/zzarchive/zutil.py:date_diffinbday,date_diffinbday,function,7,11,9,112,10.18,0,0,"['intd2', 'intd1']","[None, None]","[None, None]",2998,[],"['dateint_todatetime', 'd1.date', 'd2.date', 'np.busday_count']",4
utilmy/zzarchive/zutil.py:date_gencalendar,date_gencalendar,function,11,16,14,242,15.12,0,0,"['start', 'end', 'country']","[None, None, None]","['""2010-01-01""', '""2010-01-15""', '""us""']",3007,[],"['CustomBusinessDay', 'np.array']",2
utilmy/zzarchive/zutil.py:date_finddateid,date_finddateid,function,3,75,12,447,5.96,0,10,"['date1', 'dateref']","[None, None]","[None, None]",3017,[],['np_findfirst'],1
utilmy/zzarchive/zutil.py:datestring_toint,datestring_toint,function,6,14,12,108,7.71,1,1,['datelist1'],[None],[None],3042,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zzarchive/zutil.py:date_now,date_now,function,14,44,29,401,9.11,0,2,['i'],[None],['0'],3051,[],"['datetime.now', 'date_add_bdays', 'date_remove_bdays', 'str', 'date_nowtime', 'datetime.today', 'd.strftime']",7
utilmy/zzarchive/zutil.py:date_nowtime,date_nowtime,function,8,22,18,208,9.45,0,1,"['type1', 'format1=""%Y-%m-%d %H']","[None, '']","['""str""', '""%Y-%m-%d %H:%M:%S:%f""']",3062,"['    """""" str / stamp /  """"""\n']","['datetime.today', 'd.strftime']",2
utilmy/zzarchive/zutil.py:date_generatedatetime,date_generatedatetime,function,17,32,28,268,8.38,0,1,"['start', 'nbday', 'end']","[None, None, None]","['""20100101""', '10', '""""']",3076,[],"['datestring_todatetime', 'date_add_bdays', 'datetime.timedelta', 'list', 'np.array']",5
utilmy/zzarchive/zutil.py:np_numexpr_vec_calc,np_numexpr_vec_calc,function,5,12,12,110,9.17,0,0,[],[],[],3088,[],[],0
utilmy/zzarchive/zutil_features.py:log,log,function,6,30,20,188,6.27,0,2,"['*s', 'n', 'm', '**kw']","[None, None, None, None]","[None, '0', '1', None]",12,[],"['print', 'log2', 'log3']",3
utilmy/zzarchive/zutil_features.py:log2,log2,function,2,7,7,40,5.71,0,1,"['*s', '**kw']","[None, None]","[None, None]",18,[],['print'],1
utilmy/zzarchive/zutil_features.py:log3,log3,function,2,7,7,40,5.71,0,1,"['*s', '**kw']","[None, None]","[None, None]",21,[],['print'],1
utilmy/zzarchive/zutil_features.py:os_get_function_name,os_get_function_name,function,4,4,4,47,11.75,0,0,[],[],[],32,[],['sys._getframe'],1
utilmy/zzarchive/zutil_features.py:os_getcwd,os_getcwd,function,3,6,5,66,11.0,0,0,[],[],[],37,[],[],0
utilmy/zzarchive/zutil_features.py:pa_read_file,pa_read_file,function,38,129,74,709,5.5,2,9,"['path', 'cols', 'n_rows', 'file_start', 'file_end', 'verbose', '']","[None, None, None, None, None, None, None]","[""  'folder_parquet/'"", 'None', '1000', '0', '100000', '1', None]",43,"['    """"""Requied HDFS connection\n', '       http://arrow.apache.org/docs/python/parquet.html\n', '\n', '       conda install libhdfs3 pyarrow\n', '       in your script.py:\n', '        import os\n', ""        os.environ['ARROW_LIBHDFS_DIR'] = '/opt/cloudera/parcels/CDH/lib64/'\n"", '\n', '       https://stackoverflow.com/questions/18123144/missing-server-jvm-java-jre7-bin-server-jvm-dll\n', '\n', '    """"""\n']","['hdfs.ls', 'glob.glob', 'fi.split', 'print', 'pq.read_table', 'arr_table.to_pandas', 'gc.collect', 'pd.concat', 'len', 'dfall.head']",10
utilmy/zzarchive/zutil_features.py:pa_write_file,pa_write_file,function,23,62,45,599,9.66,0,4,"['df', 'path', 'cols', 'n_rows', 'partition_cols', 'overwrite', 'verbose', 'filesystem ']","[None, None, None, None, None, None, None, None]","[None, ""  'folder_parquet/'"", 'None', '1000', 'None', 'True', '1', "" 'hdfs'""]",97,"['    """""" Pandas to HDFS\n', ""      pyarrow.parquet.write_table(table, where, row_group_size=None, version='1.0',\n"", ""      use_dictionary=True, compression='snappy', write_statistics=True, use_deprecated_int96_timestamps=None,\n"", '      coerce_timestamps=None, allow_truncated_timestamps=False, data_page_size=None,\n', ""      flavor=None, filesystem=None, compression_level=None, use_byte_stream_split=False, data_page_version='1.0', **kwargs)\n"", '\n', '      https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html#pyarrow.parquet.write_to_dataset\n', '\n', '    """"""\n']","['hdfs.rm', 'hdfs.mkdir', 'pq.write_to_dataset', 'hdfs.ls', 'print', 'os.removedirs', 'os.makedirs', 'os.listdir']",8
utilmy/zzarchive/zutil_features.py:test_get_classification_data,test_get_classification_data,function,20,44,40,470,10.68,1,0,['name'],[None],['None'],143,[],"['make_classification', 'range', 'pd.DataFrame', 'np.arange', 'len', 'dfX.set_index', 'dfy.set_index']",7
utilmy/zzarchive/zutil_features.py:params_check,params_check,function,7,56,26,259,4.62,1,5,"['pars', 'check_list', 'name']","[None, None, None]","[None, None, '""""']",160,"['    """"""\n', '      Validate a dict parans\n', '    :param pars:\n', '    :param check_list:\n', '    :param name:\n', '    :return:\n', '    """"""\n']","['isinstance', 'Exception']",2
utilmy/zzarchive/zutil_features.py:save_features,save_features,function,7,30,26,275,9.17,0,2,"['df', 'name', 'path']","[None, None, None]","[None, None, 'None']",186,"['    """""" Save dataframe on disk\n', '    :param df:\n', '    :param name:\n', '    :param path:\n', '    :return:\n', '    """"""\n']","['os.makedirs', 'isinstance', 'log', 'list', 'df0.to_parquet']",5
utilmy/zzarchive/zutil_features.py:load_features,load_features,function,3,10,9,110,11.0,0,0,"['name', 'path']","[None, None]","[None, None]",206,[],"['pd.read_parquet', 'log']",2
utilmy/zzarchive/zutil_features.py:save_list,save_list,function,8,21,20,166,7.9,1,0,"['path', 'name_list', 'glob']","[None, None, None]","[None, None, None]",214,[],"['os.makedirs', 'log', 'pickle.dump', 'open']",4
utilmy/zzarchive/zutil_features.py:save,save,function,7,30,26,275,9.17,0,2,"['df', 'name', 'path']","[None, None, None]","[None, None, 'None']",221,"['  """"""\n', '      Read file in parallel from disk : very Fast\n', '  :param path_glob:\n', '  :param ignore_index:\n', '  :param cols:\n', '  :param verbose:\n', '  :param nrows:\n', '  :param concat_sort:\n', '  :param n_pool:\n', '  :param drop_duplicates:\n', '  :param shop_id:\n', '  :param kw:\n', '  :return:\n', '  """"""\n']","['os.makedirs', 'isinstance', 'log', 'list', 'df0.to_parquet']",5
utilmy/zzarchive/zutil_features.py:load,load,function,3,10,9,110,11.0,0,0,"['name', 'path']","[None, None]","[None, None]",228,"['  """"""\n', '      Read file in parallel from disk : very Fast\n', '  :param path_glob:\n', '  :param ignore_index:\n', '  :param cols:\n', '  :param verbose:\n', '  :param nrows:\n', '  :param concat_sort:\n', '  :param n_pool:\n', '  :param drop_duplicates:\n', '  :param shop_id:\n', '  :param kw:\n', '  :return:\n', '  """"""\n']","['pd.read_parquet', 'log']",2
utilmy/zzarchive/zutil_features.py:pd_read_file,pd_read_file,function,52,161,100,1083,6.73,3,10,"['path_glob', 'ignore_index', 'cols', 'verbose', 'nrows', 'concat_sort', 'n_pool', 'drop_duplicates', 'col_filter', 'col_filter_val', '**kw']","[None, None, None, None, None, None, None, None, None, None, None]","['""*.pkl""', 'True', 'None', 'False', '-1', 'True', '1', 'None', 'None', 'None', None]",233,"['  """"""\n', '      Read file in parallel from disk : very Fast\n', '  :param path_glob:\n', '  :param ignore_index:\n', '  :param cols:\n', '  :param verbose:\n', '  :param nrows:\n', '  :param concat_sort:\n', '  :param n_pool:\n', '  :param drop_duplicates:\n', '  :param shop_id:\n', '  :param kw:\n', '  :return:\n', '  """"""\n']","['ThreadPool', 'glob.glob', 'pd.DataFrame', 'len', 'log', 'range', 'job_list.append', 'pool.apply_async', 'dfi.drop_duplicates', 'gc.collect', 'pd.concat']",11
utilmy/zzarchive/zutil_features.py:load_dataset,load_dataset,function,35,207,126,1727,8.34,3,11,"['path_data_x', 'path_data_y', 'colid', 'n_sample']","[None, None, None, None]","[None, ""''"", '""jobId""', '-1']",300,"['    """"""\n', '      return a datraframe\n', '      https://raw.github.com/someguy/brilliant/master/somefile.txt\n', '\n', '    :param path_data_x:\n', '    :param path_data_y:\n', '    :param colid:\n', '    :param n_sample:\n', '    :return:\n', '    """"""\n']","['log', 'fetch_spark_koalas', 'fetch_dataset', 'glob.glob', 'ntpath.dirname', 'ntpath.basename', 'len', 'print', 'pd.read_csv', 'fi.endswith', 'pd.read_parquet', 'pd.read_pickle', 'pd.concat', 'df.head', 'list', 'np.arange', 'df.set_index', 'pd_read_file', 'dfy.head', 'df.join']",20
utilmy/zzarchive/zutil_features.py:fetch_spark_koalas,fetch_spark_koalas,function,9,11,11,109,9.91,0,0,"['path_data_x', 'path_data_y', 'colid', 'n_sample']","[None, None, None, None]","[None, ""''"", '""jobId""', '-1']",378,[],"['path_data_x.replace', 'ks.read_parquet']",2
utilmy/zzarchive/zutil_features.py:fetch_dataset,fetch_dataset,function,52,177,125,1978,11.18,1,6,"['url_dataset', 'path_target', 'file_target']","[None, None, None]","[None, 'None', 'None']",386,"['    """"""Fetch dataset from a given URL and save it.\n', '\n', '    Currently `github`, `gdrive` and `dropbox` are the only supported sources of\n', '    data. Also only zip files are supported.\n', '\n', '    :param url_dataset:   URL to send\n', '    :param path_target:   Path to save dataset\n', '    :param file_target:   File to save dataset\n', '\n', '    """"""\n']","['log', 'mkdtemp', 'pathlib.Path', 'mktemp', 'url_dataset.replace', 'urlx.replace', 'urlpath.split', 'os.makedirs', 'requests.Session', 's.get', 'print', 'open', 'f.write', 'res.raise_for_status', 'urlparse', 'parse_qs', 'download_googledrive', 'download_dtopbox', 'os.listdir', 'os.unlink', 'os.link']",21
utilmy/zzarchive/zutil_features.py:load_function_uri,load_function_uri,function,25,66,59,691,10.47,0,0,"['uri_name=""myfolder/myfile.py']",[''],"['""myfolder/myfile.py::myFunction""']",491,"['    """"""\n', '    #load dynamically function from URI pattern\n', '    #""dataset""        : ""mlmodels.preprocess.generic:pandasDataset""\n', '    ###### External File processor :\n', '    #""dataset""        : ""MyFolder/preprocess/myfile.py:pandasDataset""\n', '    """"""\n']","['uri_name.split', 'len', 'package_path.replace', 'getattr', 'str', 'log', 'Path', 'NameError']",8
utilmy/zzarchive/zutil_features.py:metrics_eval,metrics_eval,function,23,71,52,902,12.7,2,3,"['metric_list', 'ytrue', 'ypred', 'ypred_proba', 'return_dict']","[None, None, None, None, None]","['[""mean_squared_error""]', 'None', 'None', 'None', 'False']",531,"['    """"""\n', '      Generic metrics calculation, using sklearn naming pattern\n', '    """"""\n']","['len', 'isinstance', 'getattr', 'range', 'mval_.append', 'np.mean', 'np.sqrt', 'metric_scorer', 'pd.DataFrame']",9
utilmy/zzarchive/zutil_features.py:pd_stat_dataset_shift,pd_stat_dataset_shift,function,14,24,23,328,13.67,1,0,"['dftrain', 'dftest', 'colused', 'nsample', 'buckets', 'axis']","[None, None, None, None, None, None]","[None, None, None, '10000', '5', '0']",572,[],"['print', 'pd_stat_datashift_psi', 'pd.DataFrame']",3
utilmy/zzarchive/zutil_features.py:pd_stat_datashift_psi,pd_stat_datashift_psi,function,25,122,83,1188,9.74,1,5,"['expected', 'actual', 'buckettype', 'buckets', 'axis']","[None, None, None, None, None]","[None, None, ""'bins'"", '10', '0']",588,"[""    '''Calculate the PSI (population stability index) across all variables\n"", '    Args:\n', '       expected: numpy matrix of original values\n', '       actual: numpy matrix of new values, same size as expected\n', '       buckettype: type of strategy for creating buckets, bins splits into even splits, quantiles splits into quantile buckets\n', '       buckets: number of quantiles to use in bucketing variables\n', '       axis: axis by which variables are defined, 0 for vertical, 1 for horizontal\n', '    Returns:\n', '       psi_values: ndarray of psi values for each variable\n', ""    '''\n""]","['psi', 'scale_range', 'np.max', 'np.arange', 'np.min', 'np.stack', 'np.histogram', 'len', 'sub_psi', 'np.log', 'np.sum', 'range', 'np.empty']",13
utilmy/zzarchive/zutil_features.py:feature_importance_perm,feature_importance_perm,function,31,57,54,761,13.35,1,1,"['clf', 'Xtrain', 'ytrain', 'cols', 'n_repeats', 'scoring', 'show_graph']","[None, None, None, None, None, None, None]","[None, None, None, None, '8', ""'neg_root_mean_squared_error'"", '1']",660,[],"['permutation_importance', 'list', 'pd.DataFrame', 'np.arange', 'min', 'len', 'plt.subplots', 'ax1.boxplot', 'ax1.set_yticklabels', 'ax1.set_ylim', 'fig.tight_layout', 'plt.show']",12
utilmy/zzarchive/zutil_features.py:feature_selection_multicolinear,feature_selection_multicolinear,function,25,44,36,510,11.59,3,0,"['df', 'threshold']","[None, None]","[None, '1.0']",694,[],"['list', 'spearmanr', 'hierarchy.ward', 'hierarchy.fcluster', 'defaultdict', 'enumerate', 'cluster_id_to_feature_ids.values']",7
utilmy/zzarchive/zutil_features.py:feature_correlation_cat,feature_correlation_cat,function,24,45,41,582,12.93,0,0,"['df', 'colused']","[None, None]","[None, None]",712,[],"['plt.subplots', 'spearmanr', 'hierarchy.ward', 'hierarchy.dendrogram', 'np.arange', 'len', 'ax2.imshow', 'ax2.set_xticks', 'ax2.set_yticks', 'ax2.set_xticklabels', 'ax2.set_yticklabels', 'fig.tight_layout', 'plt.show']",13
utilmy/zzarchive/zutil_features.py:pd_feature_generate_cross,pd_feature_generate_cross,function,33,70,59,485,6.93,2,2,"['df', 'cols', 'cols_cross_input', 'pct_threshold', 'm_combination']","[None, None, None, None, None]","[None, None, 'None', '0.2', '2']",735,"['    """"""\n', '       Generate Xi.Xj features and filter based on stats threshold\n', '    """"""\n']","['len', 'itertools.combinations', 'range', 'y.sum', 'col_cross.append']",5
utilmy/zzarchive/zutil_features.py:pd_col_to_onehot,pd_col_to_onehot,function,20,94,61,616,6.55,3,6,"['dfref', 'colname', 'colonehot', 'return_val']","[None, None, None, None]","[None, 'None', 'None', '""dataframe,column""']",770,"['    """"""\n', '    :param df:\n', '    :param colname:\n', '    :param colonehot: previous one hot columns\n', '    :param returncol:\n', '    :return:\n', '    """"""\n']","['copy.deepcopy', 'list', 'len', 'print', 'pd.concat', 'pd.get_dummies', 'coladded.append']",7
utilmy/zzarchive/zutil_features.py:pd_colcat_mergecol,pd_colcat_mergecol,function,17,41,32,255,6.22,2,1,"['df', 'col_list', 'x0', 'colid']","[None, None, None, None]","[None, None, None, '""easy_id""']",813,"['    """"""\n', '       Merge category onehot column\n', '    :param df:\n', '    :param l:\n', '    :param x0:\n', '    :return:\n', '    """"""\n']","['pd.DataFrame', 't.rfind', 'int', 'print', 'dfz.set_index']",5
utilmy/zzarchive/zutil_features.py:pd_colcat_tonum,pd_colcat_tonum,function,21,62,43,534,8.61,1,4,"['df', 'colcat', 'drop_single_label', 'drop_fact_dict']","[None, None, None, None]","[None, '""all""', 'False', 'True']",837,"['    """"""\n', '    Encoding a data-set with mixed data (numerical and categorical) to a numerical-only data-set,\n', '    using the following logic:\n', '    * categorical with only a single value will be marked as zero (or dropped, if requested)\n', '    * categorical with two values will be replaced with the result of Pandas `factorize`\n', '    * categorical with more than two values will be replaced with the result of Pandas `get_dummies`\n', '    * numerical columns will not be modified\n', '    **Returns:** DataFrame or (DataFrame, dict). If `drop_fact_dict` is True, returns the encoded DataFrame.\n', '    else, returns a tuple of the encoded DataFrame and dictionary, where each key is a two-value column, and the\n', '    value is the original labels, as supplied by Pandas `factorize`. Will be empty if no two-value columns are\n', '    present in the data-set\n', '    Parameters\n', '    ----------\n', '    df : NumPy ndarray / Pandas DataFrame\n', '        The data-set to encode\n', '    colcat : sequence / string\n', ""        A sequence of the nominal (categorical) columns in the dataset. If string, must be 'all' to state that\n"", ""        all columns are nominal. If None, nothing happens. Default: 'all'\n"", '    drop_single_label : Boolean, default = False\n', '        If True, nominal columns with a only a single value will be dropped.\n', '    drop_fact_dict : Boolean, default = True\n', '        If True, the return value will be the encoded DataFrame alone. If False, it will be a tuple of\n', '        the DataFrame and the dictionary of the binary factorization (originating from pd.factorize)\n', '    """"""\n']","['pd.DataFrame', 'dict', 'pd.unique', 'len', 'pd.factorize', 'pd.get_dummies', 'pd.concat']",7
utilmy/zzarchive/zutil_features.py:pd_colcat_mapping,pd_colcat_mapping,function,9,35,20,271,7.74,4,0,"['df', 'colname']","[None, None]","[None, None]",889,"['    """"""\n', '       map category to integers\n', '    :param df:\n', '    :param colname:\n', '    :return:\n', '    """"""\n']",['enumerate'],1
utilmy/zzarchive/zutil_features.py:pd_colcat_toint,pd_colcat_toint,function,28,78,50,628,8.05,4,2,"['dfref', 'colname', 'colcat_map', 'suffix']","[None, None, None, None]","[None, None, 'None', 'None']",909,[],"['isinstance', 'pd.DataFrame', 'print', 'ddict.get', 'colname_new.append', 'enumerate']",6
utilmy/zzarchive/zutil_features.py:pd_colnum_tocat,pd_colnum_tocat,function,42,147,98,1134,7.71,2,6,"['df', 'colname', 'colexclude', 'colbinmap', 'bins', 'suffix', 'method', 'na_value', 'return_val', 'params={""KMeans_n_clusters""', '""KMeans_init""', '""KMeans_n_init""']","[None, None, None, None, None, None, None, None, None, '', "" 'k-means++'"", ' 10,""KMeans_max_iter"": 300, ""KMeans_tol"": 0.0001, ""KMeans_precompute_distances"": \'auto\',""KMeans_verbose"": 0, ""KMeans_random_state"": None,""KMeans_copy_x"": True, ""KMeans_n_jobs"": None, ""KMeans_algorithm"": \'auto\'}']","[None, 'None', 'None', 'None', '5', '""_bin""', '""uniform""', '-1', '""dataframe,param""', '{""KMeans_n_clusters"": 8', None, None]",948,"['    """"""\n', '    colbinmap = for each column, definition of bins\n', '    https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n', '       :param df:\n', '       :param method:\n', '       :return:\n', '    """"""\n']","['list', 'OrderedDict', 'bin_create', 'dfc.min', 'dfc.max', 'range', 'bin_create_quantile', 'np.arange', 'dfc.quantile', 'print', 'colbinmap.get', 'len', 'pd.cut', 'df.groupby', 'colnew.append']",15
utilmy/zzarchive/zutil_features.py:pd_colnum_normalize,pd_colnum_normalize,function,29,134,64,840,6.27,3,6,"['df0', 'colname', 'pars', 'suffix', 'return_val']","[None, None, None, None, None]","[None, None, None, '""_norm""', ""'dataframe,param'""]",1039,"['    """"""\n', '    :param df:\n', '    :param colnum_log:\n', '    :param colproba:\n', '    :return:\n', '    """"""\n']","['np.log', 'max', 'min', 'log', 'list']",5
utilmy/zzarchive/zutil_features.py:pd_col_merge_onehot,pd_col_merge_onehot,function,11,26,20,136,5.23,2,1,"['df', 'colname']","[None, None]","[None, None]",1085,"['    """"""\n', '      Merge columns into single (hotn\n', '    :param df:\n', '    :param colname:\n', '    :return :\n', '    """"""\n']","['t[len', 'len', 'merge_array.append']",3
utilmy/zzarchive/zutil_features.py:pd_col_to_num,pd_col_to_num,function,9,27,24,184,6.81,1,0,"['df', 'colname', 'default']","[None, None, None]","[None, 'None', 'np.nan']",1102,[],"['to_float', 'float', 'list']",3
utilmy/zzarchive/zutil_features.py:pd_col_filter,pd_col_filter,function,12,28,23,186,6.64,1,2,"['df', 'filter_val', 'iscol']","[None, None, None]","[None, 'None', '1']",1115,"['    """"""\n', '   # Remove Columns where Index Value is not in the filter_value\n', ""   # filter1= X_client['client_id'].values\n"", '   :param df:\n', '   :param filter_val:\n', '   :param iscol:\n', '   :return:\n', '   """"""\n']","['col_delete.append', 'df.drop']",2
utilmy/zzarchive/zutil_features.py:pd_col_fillna,pd_col_fillna,function,22,70,49,586,8.37,1,6,"['dfref', 'colname', 'method', 'value', 'colgroupby', 'return_val', '']","[None, None, None, None, None, None, None]","[None, 'None', '""frequent""', 'None', 'None', '""dataframe,param""', None]",1134,"['    """"""\n', '    Function to fill NaNs with a specific value in certain columns\n', '    Arguments:\n', '        df:            dataframe\n', '        colname:      list of columns to remove text\n', '        value:         value to replace NaNs with\n', '    Returns:\n', '        df:            new dataframe with filled values\n', '    """"""\n']","['list', 'df.groupby', 'print']",3
utilmy/zzarchive/zutil_features.py:pd_pipeline_apply,pd_pipeline_apply,function,8,29,22,259,8.93,1,0,"['df', 'pipeline']","[None, None]","[None, None]",1180,"['    """"""\n', '      pipe_preprocess_colnum = [\n', '      (pd_col_to_num, {""val"": ""?"", })\n', '    , (pd_colnum_tocat, {""colname"": None, ""colbinmap"": colnum_binmap, \'bins\': 5,\n', '                         ""method"": ""uniform"", ""suffix"": ""_bin"",\n', '                         ""return_val"": ""dataframe""})\n', '    , (pd_col_to_onehot, {""colname"": None, ""colonehot"": colnum_onehot,\n', '                          ""return_val"": ""dataframe""})\n', '      ]\n', '    :param df:\n', '    :param pipeline:\n', '    :return:\n', '    """"""\n']","['copy.deepcopy', 'enumerate', 'print', 'str']",4
utilmy/zzarchive/zutil_features.py:pd_stat_correl_pair,pd_stat_correl_pair,function,12,33,30,316,9.58,1,1,"['df', 'coltarget', 'colname']","[None, None, None]","[None, 'None', 'None']",1204,"['    """"""\n', '      Genearte correletion between the column and target column\n', '      df represents the dataframe comprising the column and colname comprising the target column\n', '    :param df:\n', '    :param colname: list of columns\n', '    :param coltarget : target column\n', '    :return:\n', '    """"""\n']","['list', 'target_corr.append', 'pd.DataFrame', 'len']",4
utilmy/zzarchive/zutil_features.py:pd_stat_pandas_profile,pd_stat_pandas_profile,function,7,9,8,175,19.44,0,0,"['df', 'savefile', 'title']","[None, None, None]","[None, '""report.html""', '""Pandas Profile""']",1225,"['    """""" Describe the tables\n', '        #Pandas-Profiling 2.0.0\n', '        df.profile_report()\n', '    """"""\n']","['print', 'df.profile_report', 'profile.to_file', 'profile.get_rejected_variables']",4
utilmy/zzarchive/zutil_features.py:pd_stat_distribution_colnum,pd_stat_distribution_colnum,function,23,84,72,646,7.69,1,3,"['df', 'nrows', 'verbose']","[None, None, None]","[None, '2000', 'False']",1238,"['    """""" Stats the tables\n', '    """"""\n']","['df.sample', 'getstat', 'str', 'len', 'pd.DataFrame', 'pd.concat', 'print', 'np.arange']",8
utilmy/zzarchive/zutil_features.py:pd_stat_histogram,pd_stat_histogram,function,8,19,18,210,11.05,0,0,"['df', 'bins', 'coltarget']","[None, None, None]","[None, '50', '""diff""']",1278,"['    """"""\n', '    :param df:\n', '    :param bins:\n', '    :param coltarget:\n', '    :return:\n', '    """"""\n']","['np.histogram', 'pd.DataFrame']",2
utilmy/zzarchive/zutil_features.py:col_extractname,col_extractname,function,7,37,23,207,5.59,1,5,['col_onehot'],[None],[None],1293,"['    """"""\n', '    Column extraction from onehot name\n', '    :param col_onehot\n', '    :return:\n', '    """"""\n']","['len', 'colnew.append']",2
utilmy/zzarchive/zutil_features.py:col_remove,col_remove,function,12,42,26,214,5.1,3,4,"['cols', 'colsremove', 'mode']","[None, None, None]","[None, None, '""exact""']",1316,"['    """"""\n', '    """"""\n']","['cols.remove', 'cols3.append']",2
utilmy/zzarchive/zutil_features.py:pd_colnum_tocat_stat,pd_colnum_tocat_stat,function,53,152,107,1986,13.07,1,5,"['df', 'feature', 'target_col', 'bins', 'cuts']","[None, None, None, None, None]","[None, None, None, None, '0']",1341,"['    """"""\n', '    Bins continuous features into equal sample size buckets and returns the target mean in each bucket. Separates out\n', '    nulls into another bucket.\n', '    :param df: dataframe containg features and target column\n', '    :param feature: feature column name\n', '    :param target_col: target column\n', '    :param bins: Number bins required\n', '    :param cuts: if buckets of certain specific cuts are required. Used on test data to use cuts from train.\n', '    :return: If cuts are passed only df_grouped data is returned, else cuts and df_grouped data is returned\n', '    """"""\n']","['pd.isnull', 'df.reset_index', 'min', 'range', 'np.percentile', 'cuts.append', 'pd.cut', 'df.groupby', 'df_grouped.reset_index', 'list', 'df_grouped.rename', 'str', 'len', 'pd.concat']",14
utilmy/zzarchive/zutil_features.py:pd_stat_shift_trend_changes,pd_stat_shift_trend_changes,function,20,41,35,660,16.1,0,0,"['df', 'feature', 'target_col', 'threshold']","[None, None, None, None]","[None, None, None, '0.03']",1410,"['    """"""\n', '    Calculates number of times the trend of feature wrt target changed direction.\n', '    :param df: df_grouped dataset\n', '    :param feature: feature column name\n', '    :param target_col: target column\n', '    :param threshold: minimum % difference required to count as trend change\n', '    :return: number of trend chagnes for the feature\n', '    """"""\n']","['target_diffs.fillna', 'target_diffs.divide', 'target_diffs_norm.diff', 'target_diffs_lvl2.fillna', 'int']",5
utilmy/zzarchive/zutil_features.py:pd_stat_shift_trend_correlation,pd_stat_shift_trend_correlation,function,19,63,55,788,12.51,0,2,"['df', 'df_test', 'colname', 'target_col']","[None, None, None, None]","[None, None, None, None]",1434,"['    """"""\n', '    Calculates correlation between train and test trend of colname wrt target.\n', '    :param df: train df data\n', '    :param df_test: test df data\n', '    :param colname: colname column name\n', '    :param target_col: target column name\n', '    :return: trend correlation between train and test\n', '    """"""\n']","['df.merge', 'pd.isnull', 'len', 'np.corrcoef', 'print']",5
utilmy/zzarchive/zutil_features.py:pd_stat_shift_changes,pd_stat_shift_changes,function,33,99,81,1236,12.48,1,4,"['df', 'target_col', 'features_list', 'bins', 'df_test']","[None, None, None, None, None]","[None, None, '0', '10', '0']",1465,"['    """"""\n', '    Calculates trend changes and correlation between train/test for list of features\n', '    :param df: dfframe containing features and target columns\n', '    :param target_col: target column name\n', '    :param features_list: by default creates plots for all features. If list passed, creates plots of only those features.\n', '    :param bins: number of bins to be created from continuous colname\n', '    :param df_test: test df which has to be compared with input df for correlation\n', '    :return: dfframe with trend changes and trend correlation (if test df passed)\n', '    """"""\n']","['type', 'list', 'features_list.remove', 'ignored.append', 'pd_colnum_tocat_stat', 'pd_stat_shift_trend_correlation', 'pd_stat_shift_changes', 'stats_all.append', 'pd.DataFrame', 'len', 'print', 'str']",12
utilmy/zzarchive/zutil_features.py:np_conv_to_one_col,np_conv_to_one_col,function,5,11,10,137,12.45,0,0,"['np_array', 'sep_char']","[None, None]","[None, '""_""']",1510,"['    """"""\n', '    converts string/numeric columns to one string column\n', '    :param np_array: the numpy array with more than one column\n', '    :param sep_char: the separator character\n', '    """"""\n']","['row2string', 'sep_char.join']",2
utilmy/zzarchive/zutil_features.py:dict2,dict2,class,3,5,5,36,7.2,0,0,[],[],[],28,[],[],0
utilmy/zzarchive/zutil_features.py:dict2:__init__,dict2:__init__,method,2,2,2,15,7.5,0,0,"['self', 'd']","[None, None]","[None, None]",29,[],[],0
utilmy/configs/test.py:create_fixtures_data,create_fixtures_data,function,5,8,7,165,20.62,0,0,['tmp_path'],[None],[None],55,[],"['good_data_yaml.write_text', 'bad_data_yaml.write_text']",2
utilmy/configs/test.py:test_validate_yaml_types,test_validate_yaml_types,function,15,122,90,892,7.31,0,0,['tmp_path'],[None],[None],62,[],"['config_validate', 'isinstance', 'datetime.date', 'test_validate_yaml_types_failed', 'pytest.raises', 'sorted']",6
utilmy/configs/test.py:test_validate_yaml_types_failed,test_validate_yaml_types_failed,function,10,75,55,469,6.25,0,0,['tmp_path'],[None],[None],83,[],"['pytest.raises', 'config_validate', 'sorted']",3
utilmy/configs/test.py:test_validate_yaml_failed_silent,test_validate_yaml_failed_silent,function,5,13,12,128,9.85,0,0,['tmp_path'],[None],[None],105,[],['config_validate'],1
utilmy/configs/util_config.py:log,log,function,2,5,4,54,10.8,0,0,['*s'],[None],[None],18,[],"['print', 'loge']",2
utilmy/configs/util_config.py:loge,loge,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],22,[],['print'],1
utilmy/configs/util_config.py:test_yamlschema,test_yamlschema,function,5,6,6,104,17.33,0,0,[],[],[],29,[],"['config_load', 'config_isvalid_yamlschema', 'log']",3
utilmy/configs/util_config.py:test_pydanticgenrator,test_pydanticgenrator,function,6,26,20,373,14.35,0,0,[],[],[],35,[],"['pydantic_model_generator', 'Path']",2
utilmy/configs/util_config.py:test4,test4,function,9,12,12,183,15.25,0,0,[],[],[],50,[],"['config_load', 'convert_dict_to_pydantic', 'isinstance']",3
utilmy/configs/util_config.py:test_example,test_example,function,14,27,23,145,5.37,0,0,[],[],[],57,[],[],0
utilmy/configs/util_config.py:config_load,config_load,function,31,125,89,1236,9.89,0,5,"['config_path', 'path_default', 'config_default', 'save_default', 'to_dataclass', '']","['    str  ', '   str  ', ' dict ', '   bool ', '   bool ', None]","[' None', ' None', ' None', ' False', ' True', None]",79,"['    """"""Load Config file into a dict\n', '    1) load config_path\n', '    2) If not, load in USER/.myconfig/.config.yaml\n', '    3) If not, create default save in USER/.myconfig/.config.yaml\n', '    Args:\n', ""        config_path:   path of config or 'default' tag value\n"", '        path_default : path of default config\n', '        config_default: dict value of default config\n', '        save_default: save default config on disk\n', '    Returns: dict config\n', '    """"""\n']","['log', 'pathlib.Path', 'yaml.safe_load', 'json.loads', 'SafeConfigParser', 'cfg.read', 'toml.loads', 'Exception', 'Box', 'os.makedirs', 'open', 'yaml.dump']",12
utilmy/configs/util_config.py:config_isvalid_yamlschema,config_isvalid_yamlschema,function,13,37,33,370,10.0,2,1,"['config_dict', 'schema_path', 'silent']","[' dict', ' str ', ' bool ']","[None, "" 'config_val.yaml'"", ' False']",152,"['    """"""Validate using a  yaml file\n', '    Args:\n', '        config_dict:\n', '        schema_path:\n', '        silent:\n', '    Returns: True/False\n', '    """"""\n']","['yamale.make_schema', 'schema.validate', 'result.isValid', 'yamale.YamaleError', 'loge']",5
utilmy/configs/util_config.py:config_isvalid_pydantic,config_isvalid_pydantic,function,4,11,10,69,6.27,0,0,"['config_dict', 'pydanctic_schema', 'silent']","[' dict', ' str ', ' bool ']","[None, "" 'config_py.yaml'"", ' False']",177,"['    """"""Validate using a pydantic files\n', '    Args:\n', '        config_dict:\n', '        pydanctic_schema:\n', '        silent:\n', '    Returns: True/False\n', '    """"""\n']",[],0
utilmy/configs/util_config.py:convert_yaml_to_box,convert_yaml_to_box,function,6,8,8,57,7.12,0,0,['yaml_path'],[' str'],[None],196,[],"['open', 'yaml.load', 'Box']",3
utilmy/configs/util_config.py:convert_dict_to_pydantic,convert_dict_to_pydantic,function,10,13,13,253,19.46,0,0,"['config_dict', 'schema_name']","[' dict', ' str']","[None, None]",202,[],"['SchemaGen', 'generated.to_file', 'importlib.import_module', 'pydantic_module.MainSchema']",4
utilmy/configs/util_config.py:pydantic_model_generator,pydantic_model_generator,function,8,34,32,299,8.79,0,0,"['input_file', 'str]', 'input_file_type', 'output_file', '**kwargs', '']","[' Union[Path', None, None, ' Path', None, None]","[None, None, None, None, None, None]",215,"['    """"""\n', '    Args:\n', '        input_file:\n', '        input_file_type:\n', '        output_file:\n', '        **kwargs:\n', '\n', '    Returns:\n', '    # https://github.com/koxudaxi/datamodel-code-generator\n', '    # pip install datamodel-code-generator\n', '\n', '    """"""\n']","['generate', 'loge', 'log']",3
utilmy/configs/util_config.py:global_verbosity,global_verbosity,function,12,58,38,471,8.12,0,2,"['cur_path', 'path_relative', 'default', 'key', '']","[None, None, None, None, None]","[None, '""/../../config.json""', '5', ""'verbosity'"", None]",251,"['    """""" Get global verbosity\n', '    verbosity = global_verbosity(__file__, ""/../../config.json"", default=5 )\n', '\n', '    verbosity = global_verbosity(""repo_root"", ""config/config.json"", default=5 )\n', '\n', '    :param cur_path:\n', '    :param path_relative:\n', '    :param key:\n', '    :param default:\n', '    :return:\n', '    """"""\n']","['utilmy.git_repo_root', 'json.load', 'yaml.load', 'Exception', 'int']",5
utilmy/configs/util_config.py:zzz_config_load_validate,zzz_config_load_validate,function,16,37,34,374,10.11,2,1,"['config_path', 'schema_path', 'silent']","[' str', ' str', ' bool ']","[None, None, ' False']",300,[],"['yamale.make_schema', 'yamale.make_data', 'yamale.validate', 'convert_yaml_to_box', 'print']",5
utilmy/deeplearning/utils.py:log,log,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],19,[],['print'],1
utilmy/deeplearning/utils.py:metric_accuracy,metric_accuracy,function,10,28,23,255,9.11,1,0,"['y_test', 'y_pred', 'dd']","[None, None, None]","[None, None, None]",24,[],"['enumerate', 'np.argmax', 'accuracy_score', 'log']",4
utilmy/deeplearning/utils.py:clf_loss_macro_soft_f1,clf_loss_macro_soft_f1,function,17,47,36,317,6.74,0,0,"['y', 'y_hat']","[None, None]","[None, None]",38,"['    """"""Compute the macro soft F1-score as a cost.\n', '    Average (1 - soft-F1) across all labels.\n', '    Use probability values instead of binary predictions.\n', '    Args:\n', '        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n', '        y_hat (float32 Tensor): probability matrix of shape (BATCH_SIZE, N_LABELS)\n', '    Returns:\n', '        cost (scalar Tensor): value of the cost function for the batch\n', '    """"""\n']","['tf.cast', 'tf.reduce_sum', 'tf.reduce_mean']",3
utilmy/deeplearning/utils.py:save_best,save_best,function,8,30,27,224,7.47,0,1,"['model', 'model_dir2', 'valid_loss', 'best_loss', 'counter']","[None, None, None, None, None]","[None, None, None, None, None]",62,[],"['save_model_state', 'print']",2
utilmy/deeplearning/utils.py:save_model_state,save_model_state,function,2,6,6,95,15.83,0,0,"['model', 'model_dir2']","[None, None]","[None, None]",76,[],"['os.makedirs', 'model.save_weights']",2
utilmy/deeplearning/utils.py:train_stop,train_stop,function,4,16,15,145,9.06,0,1,"['counter', 'patience']","[None, None]","[None, None]",82,[],['log'],1
utilmy/deeplearning/utils.py:data_get_sample,data_get_sample,function,12,33,28,305,9.24,1,0,"['batch_size', 'x_train', 'labels_val']","[None, None, None]","[None, None, None]",90,[],"['np.array', 'y_label_list.append']",2
utilmy/deeplearning/utils.py:data_to_y_onehot_list,data_to_y_onehot_list,function,22,48,35,362,7.54,2,1,"['df', 'dfref', 'labels_col']","[None, None, None]","[None, None, None]",110,[],"['df.merge', 'pd.get_dummies', 'print']",3
utilmy/deeplearning/utils.py:data_add_onehot,data_add_onehot,function,26,84,55,529,6.3,3,2,"['dfref', 'img_dir', 'labels_col']","[None, None, None]","[None, None, None]",205,"['    """"""\n', '       id, uri, cat1, cat2, .... , cat1_onehot\n', '\n', '    """"""\n']","['glob.glob', 'fi.split', 'log', 'pd.DataFrame', 'x.split', 'int', 'df.merge', 'pd.get_dummies', 'dfi_1hot.apply', 'str']",10
utilmy/deeplearning/utils.py:image_check_npz,image_check_npz,function,15,59,41,336,5.69,2,1,"['path_npz', 'keys', 'path', 'tag', 'n_sample', 'renorm']","[None, None, None, None, None, None]","[None, ""['train']"", '""""', '""""', '3', 'True']",338,[],"['os.makedirs', 'np.load', 'data_npz.keys', 'print', 'str', 'range', 'cv2.imwrite']",7
utilmy/deeplearning/utils.py:padding_generate,padding_generate,function,2,5,5,80,16.0,0,0,"['paddings_number', 'min_padding', 'max_padding']","[' int ', ' int ', ' int ']","[' 1', ' 1', ' 1']",359,"['    """"""\n', '    Args:\n', '        paddings_number:  4\n', '        min_padding:      1\n', '        max_padding:    100\n', '    Returns: padding list\n', '    """"""\n']",[],0
utilmy/deeplearning/utils.py:image_center_crop,image_center_crop,function,19,25,23,308,12.32,0,2,"['img', 'dim']","[None, None]","[None, None]",374,"['\t""""""Returns center cropped image\n', '\tArgs:\n', '\timg: image to be center cropped\n', '\tdim: dimensions (width, height) to be cropped\n', '\t""""""\n']",['int'],1
utilmy/deeplearning/utils.py:image_resize_pad,image_resize_pad,function,32,127,88,991,7.8,0,3,"['img', 'size', '256']","[None, None, None]","[None, '(256', None]",392,"['    """"""\n', '      resize and keep into the target Box\n', '    \n', '    """"""\n']","['float', 'np.round', 'np.floor', 'np.ceil', 'len', 'isinstance', 'cv2.resize', 'cv2.copyMakeBorder']",8
utilmy/deeplearning/utils.py:image_merge,image_merge,function,32,73,51,554,7.59,1,3,"['image_list', 'n_dim', 'padding_size', 'max_height', 'total_width']","[None, None, None, None, None]","[None, None, None, None, None]",438,"['    """"""\n', '    Args:\n', '        image_list:  list of image\n', '        n_dim:\n', '        padding_size: padding size max\n', '        max_height:   max height\n', '        total_width:  total width\n', '    Returns:\n', '    """"""\n']","['np.zeros', 'len', 'enumerate']",3
utilmy/deeplearning/utils.py:image_remove_extra_padding,image_remove_extra_padding,function,31,73,63,518,7.1,0,2,"['img', 'inverse', 'removedot']","[None, None, None]","[None, 'False', 'True']",472,"['    """"""TODO: Issue with small dot noise points : noise or not ?\n', '              Padding calc has also issues with small blobs.\n', '    Args:\n', '        img: image\n', '    Returns: image cropped of extra padding\n', '    """"""\n']","['cv2.cvtColor', 'max', 'int', 'np.where', 'morphology.remove_small_objects', 'graybin.astype', 'cv2.findNonZero', 'cv2.boundingRect']",8
utilmy/deeplearning/utils.py:image_resize,image_resize,function,32,127,88,991,7.8,0,3,"['img', 'size', '256']","[None, None, None]","[None, '(256', None]",502,"['    """"""Resizes a image and maintains aspect ratio.\n', '    Args:\n', '        image:\n', '        width:\n', '        height:\n', '        inter:\n', '    Returns:\n', '    """"""\n']","['float', 'np.round', 'np.floor', 'np.ceil', 'len', 'isinstance', 'cv2.resize', 'cv2.copyMakeBorder']",8
utilmy/deeplearning/utils.py:image_read,image_read,function,12,45,33,584,12.98,0,3,"['filepath_or_buffer', 'io.BytesIO]']","[' Union[str', None]","[None, None]",534,"['    """"""\n', '    Read a file into an image object\n', '    Args:\n', '        filepath_or_buffer: The path to the file, a URL, or any object\n', '            with a `read` method (such as `io.BytesIO`)\n', '    """"""\n']","['isinstance', 'hasattr', 'np.asarray', 'cv2.imdecode', 'filepath_or_buffer.endswith', 'tifffile.imread', 'cv2.imread', 'cv2.cvtColor']",8
utilmy/deeplearning/utils.py:CustomDataGenerator,CustomDataGenerator,class,18,52,43,505,9.71,1,1,[],[],[],131,[],[],0
utilmy/deeplearning/utils.py:CustomDataGenerator_img,CustomDataGenerator_img,class,36,106,77,916,8.64,2,1,[],[],[],156,[],[],0
utilmy/deeplearning/utils.py:SprinklesTransform,SprinklesTransform,class,10,35,32,458,13.09,0,1,[],[],[],237,[],[],0
utilmy/deeplearning/utils.py:CustomDataGenerator:__init__,CustomDataGenerator:__init__,method,8,8,8,71,8.88,0,0,"['self', 'x', 'y', 'batch_size', 'augmentations']","[None, None, None, None, None]","[None, None, None, '32', 'None']",132,[],[],0
utilmy/deeplearning/utils.py:CustomDataGenerator:__len__,CustomDataGenerator:__len__,method,1,3,3,54,18.0,0,0,['self'],[None],[None],138,[],"['int', 'float']",2
utilmy/deeplearning/utils.py:CustomDataGenerator:__getitem__,CustomDataGenerator:__getitem__,method,10,30,25,278,9.27,1,1,"['self', 'idx']","[None, None]","[None, None]",141,[],"['batch_y.append', 'np.stack']",2
utilmy/deeplearning/utils.py:CustomDataGenerator_img:__init__,CustomDataGenerator_img:__init__,method,12,14,14,187,13.36,0,0,"['self', 'img_dir', 'label_path', 'class_list', 'split', 'batch_size', 'transforms']","[None, None, None, None, None, None, None]","[None, None, None, None, ""'train'"", '8', 'None']",157,"['        """"""    \n', '           df_label format :\n', '               id, uri, cat1, cat2, cat3, cat1_onehot, cat1_onehot, ....\n', '\n', '        """"""\n']","['pd.read_csv', 'data_add_onehot']",2
utilmy/deeplearning/utils.py:CustomDataGenerator_img:on_epoch_end,CustomDataGenerator_img:on_epoch_end,method,2,2,2,49,24.5,0,0,['self'],[None],[None],172,[],[],0
utilmy/deeplearning/utils.py:CustomDataGenerator_img:__len__,CustomDataGenerator_img:__len__,method,1,3,3,59,19.67,0,0,['self'],[None],[None],138,"['        """"""    \n', '           df_label format :\n', '               id, uri, cat1, cat2, cat3, cat1_onehot, cat1_onehot, ....\n', '\n', '        """"""\n']","['int', 'float']",2
utilmy/deeplearning/utils.py:CustomDataGenerator_img:__getitem__,CustomDataGenerator_img:__getitem__,method,21,72,50,458,6.36,2,1,"['self', 'idx']","[None, None]","[None, None]",141,[],"['df_batch.iterrows', 'np.array', 'batch_x.append', 'x.split', 'batch_y.append', 'np.stack']",6
utilmy/deeplearning/utils.py:SprinklesTransform:__init__,SprinklesTransform:__init__,method,6,10,10,157,15.7,0,0,"['self', 'num_holes', 'side_length', 'always_apply', 'p']","[None, None, None, None, None]","[None, '30', '5', 'False', '1.0']",238,[],"['super', 'Sprinkles']",2
utilmy/deeplearning/utils.py:SprinklesTransform:apply,SprinklesTransform:apply,method,3,15,13,198,13.2,0,1,"['self', 'image', '**params']","[None, None, None]","[None, None, None]",243,[],"['isinstance', 'tf.constant', 'self.sprinkles']",3
utilmy/deeplearning/prepro.py:log,log,function,1,2,2,20,10.0,0,0,['*s'],[None],[None],21,[],['print'],1
utilmy/deeplearning/prepro.py:prepro_images,prepro_images,function,9,21,17,124,5.9,1,1,"['image_paths', 'nmax']","[None, None]","[None, '10000000']",26,[],"['range', 'prepro_image', 'images.append']",3
utilmy/deeplearning/prepro.py:prepro_image0,prepro_image0,function,14,46,35,388,8.43,0,0,['image_path'],[None],[None],35,[],"['str', 'fname.split', 'util_image.image_read', 'util_image.image_resize_pad', 'util_image.image_center_crop', 'image.astype']",6
utilmy/deeplearning/prepro.py:prepro_images_multi,prepro_images_multi,function,20,52,40,338,6.5,2,1,"['image_paths', 'npool', 'prepro_image']","[None, None, None]","[None, '30', 'None']",55,"['    """""" Parallel processing\n', '    \n', '    """"""\n']","['Pool', 'pool.map', 'pool.close', 'pool.join', 'print', 'len', 'images.append', 'labels.append']",8
utilmy/deeplearning/prepro.py:run_multiprocess,run_multiprocess,function,17,29,24,192,6.62,1,0,"['myfun', 'list_args', 'npool', '**kwargs']","[None, None, None, None]","[None, None, '10', None]",79,"['    """"""\n', '       res = run_multiprocess(prepro, image_paths, npool=10, )\n', '    """"""\n']","['Pool', 'pool.map', 'partial', 'pool.close', 'pool.join']",5
utilmy/deeplearning/prepro.py:create_train_npz,create_train_npz,function,26,151,88,1693,11.21,2,2,[],[],[],94,[],"['log', 'pd.read_csv', 'set', 'len', 'sorted', 'int', 'prepro_images_multi', 'log5', 'np.array', 'np.savez_compressed', 'util_image.image_check_npz']",11
utilmy/deeplearning/prepro.py:image_resize,image_resize,function,35,101,87,990,9.8,0,0,['out_dir'],[None],"['""""']",171,"['    """"""     python prepro.py  image_resize\n', '\n', '          image white color padded\n', '    \n', '    """"""\n']","['os.makedirs', 'log', 'time.sleep', 'prepro_image3b', 'str', 'fname.split', 'cv2.cvtColor', 'util_image.image_resize_pad', 'cv2.imwrite', 'sorted', 'len', 'prepro_images_multi', 'os_path_check']",13
utilmy/deeplearning/prepro.py:image_check,image_check,function,29,68,63,626,9.21,1,1,[],[],[],220,"['    """"""     python prepro.py  image_check \n', '\n', '          image white color padded\n', '    \n', '    """"""    \n']","['log', 'dc.Cache', 'list', 'print', 'len', 'os.makedirs', 'enumerate', 'key.split', 'cv2.imwrite']",9
utilmy/deeplearning/prepro.py:create_train_parquet,create_train_parquet,function,23,134,72,1545,11.53,2,2,[],[],[],265,[],"['log', 'pd.read_csv', 'set', 'len', 'sorted', 'int', 'prepro_images_multi', 'pd.DataFrame', 'df2.merge', 'df2.to_parquet']",10
utilmy/deeplearning/prepro.py:image_remove_bg,image_remove_bg,function,12,48,41,388,8.08,1,1,"['in_dir', 'out_dir', 'level']","[None, None, None]","['""""', '""""', '1']",331,"['    """""" #### remove background\n', '    \n', '         source activate py38 &&  sleep 5 && python prepro.py   image_remove_bg  \n', '    \n', '    \n', '        python prepro.py rembg  --in_dir  /data/workspaces/noelkevin01/img/data/bing/v4     --out_dir  /data/workspaces/noelkevin01/img/data/bing/v4_nobg &>> /data/workspaces/noelkevin01/img/data/zlog_rembg.py  &\n', '\n', '        rembg  -ae 15 -p  /data/workspaces/noelkevin01/img/data/fashion/test2/  /data/workspaces/noelkevin01/img/data/fashion/test_nobg/  \n', '        \n', '        mkdir /data/workspaces/noelkevin01/img/data/fashion/train_nobg/  \n', '        \n', '    """"""    \n']","['glob.glob', 'log', 'str', 'fp.split', 'fp.replace', 'os.makedirs', 'os.system']",7
utilmy/deeplearning/prepro.py:image_create_cache,image_create_cache,function,60,188,137,1755,9.34,3,2,[],[],[],361,[],"['log', 'sorted', 'len', 'prepro_image2b', 'str', 'fname.split', 'cv2.imread', 'cv2.cvtColor', 'util_image.image_center_crop', 'dc.Cache', 'size_limit=int', 'prepro_images_multi', 'set_async', 'asyncio.get_running_loop', 'loop.run_in_executor', 'zip', 'key.split', 'print', 'enumerate', 'cv2.imwrite']",20
utilmy/deeplearning/prepro.py:os_path_check,os_path_check,function,5,23,19,128,5.57,0,0,"['path', 'n']","[None, None]","[None, '5']",454,[],"['print', 'os_system']",2
utilmy/deeplearning/prepro.py:image_face_blank,image_face_blank,function,41,90,78,784,8.71,2,0,"['in_dir', 'level ', 'out_dir', 'npool']","[None, None, None, None]","['""""', ' ""/*""', 'f""""', '30']",461,"['    """"""  Remove face\n', '\n', '     python prepro.py  image_face_blank\n', '     \n', '     python prepro.py  image_face_blank  --in_dir img/data/fashion/test_nobg   --out_dir img/data/fashion/test_nobg_noface\n', '\n', '     python prepro.py  image_face_blank  --in_dir img/data/fashion/train_nobg   --out_dir img/data/fashion/train_nobg_noface\n', '\n', '\n', '      five elements are [xmin, ymin, xmax, ymax, detection_confidence]\n', '\n', '    """"""\n']","['glob.glob', 'face_detection.build_detector', 'log', 'myfun', 'cv2.imread', 'detector.detect', 'int', 'fp.replace', 'os.makedirs', 'cv2.imwrite', 'Pool', 'pool.map', 'pool.close', 'pool.join']",14
utilmy/deeplearning/prepro.py:image_text_blank,image_text_blank,function,26,67,56,503,7.51,2,0,"['in_dir', 'out_dir', 'level']","[None, None, None]","[None, None, '""/*""']",522,"['    """"""\n', '        Not working well\n', '        python prepro.py  image_text_blank  --in_dir img/data/fashion/ztest   --out_dir img/data/fashion/ztest_noface\n', '        \n', '    \n', '    """"""\n']","['glob.glob', 'log', 'cv2.imread', 'detect_text_regions', 'int', 'fp.replace', 'os.makedirs', 'cv2.imwrite']",8
utilmy/deeplearning/prepro.py:model_deletes,model_deletes,function,16,82,54,512,6.24,4,3,['dry'],[None],['0'],557,"['    """"""  ## Delete files on disk\n', '        python prepro.py model_deletes  --dry 0\n', '        \n', '    """"""\n']","['glob.glob', 'print', 'sorted', 'int', 'os.system']",5
utilmy/deeplearning/prepro.py:image_save,image_save,function,22,50,46,482,9.64,1,1,[],[],[],587,[],"['dc.Cache', 'print', 'len', 'log', 'os.makedirs', 'enumerate', 'key.split', 'cv2.imwrite']",8
utilmy/deeplearning/prepro.py:topk_predict,topk_predict,function,3,9,9,149,16.56,0,0,[],[],[],606,[],['os.system'],1
utilmy/deeplearning/prepro.py:topk,topk,function,69,265,187,2377,8.97,3,5,[],[],[],616,[],"['os.system', 'topk', 'dname.replace', 'os.makedirs', 'log', 'pd_read_file', 'x.split', 'df.drop_duplicates', 'list', 'len', 'np.array', 'copy.deepcopy', 'x0.reshape', 'topk_nearest_vector', 'np.arange', 'df1.to_csv', 'dc.Cache', 'print', 'enumerate', 'key.split', 'cv2.imwrite']",21
utilmy/deeplearning/prepro.py:topk_nearest_vector,topk_nearest_vector,function,9,14,12,132,9.43,0,0,"['x0', 'vector_list', 'topk']","[None, None, None]","[None, None, '3']",746,"['   """"""\n', '      Retrieve top k nearest vectors using FAISS\n', '  \n', '   """"""\n']","['faiss.index_factory', 'index.add', 'index.search']",3
utilmy/deeplearning/prepro.py:topk_export,topk_export,function,8,25,24,171,6.84,0,1,[],[],[],758,"['    """"""   /user/scoupon/zexport/z/fashion_emb_100k  \n', '                     id gender masterCategory subCategory  ... masterCategory_pred subCategory_pred articleType_pred  baseColour_pred\n', '0     cn3357-01_1-11.png  women        apparel     topwear  ...                   1                1               32                4\n', '1      cs6481-01_1-4.png   kids          shoes       shoes  ...                   5               20              151                7\n', '\n', '\n', '     hdfs dfs -put  /data/workspaces/noelkevin01/img/models/fashion/dcf_vae/m_train9pred/res/m_train9b_g3_-img_train_r2p2_200k_clean_nobg_256_256-500000-cache_best_best_good_epoch_313/fashion_emb_500k/                /user/scoupon/zexport/z/\n', '\n', '    """"""\n']","['log', 'pd.read_parquet', 'dfi.to_parquet', 'fi.split']",4
utilmy/deeplearning/prepro.py:data_add_onehot,data_add_onehot,function,26,84,55,529,6.3,3,2,"['dfref', 'img_dir', 'labels_col']","[None, None, None]","[None, None, None]",787,"['    """"""\n', '       id, uri, cat1, cat2, .... , cat1_onehot\n', '\n', '    """"""\n']","['glob.glob', 'fi.split', 'log', 'pd.DataFrame', 'x.split', 'int', 'df.merge', 'pd.get_dummies', 'dfi_1hot.apply', 'str']",10
utilmy/deeplearning/prepro.py:test,test,function,8,20,20,204,10.2,0,0,[],[],[],815,"['    """"""\n', '       python prepro.py test\n', '       \n', '    """"""\n']","['pd.read_csv', 'data_add_onehot', 'log']",3
utilmy/deeplearning/prepro.py:unzip,unzip,function,6,10,9,93,9.3,0,0,"['in_dir', 'out_dir']","[None, None]","[None, None]",827,[],"['zipfile.ZipFile', 'zip_ref.extractall']",2
utilmy/deeplearning/prepro.py:gzip,gzip,function,7,14,14,309,22.07,0,0,[],[],[],835,[],"['print', 'os.system']",2
utilmy/deeplearning/prepro.py:predict,predict,function,2,13,12,141,10.85,0,1,['name'],[None],['None'],850,[],['os.system'],1
utilmy/deeplearning/prepro.py:folder_size,folder_size,function,1,12,12,78,6.5,0,0,[],[],[],857,[],['os.system'],1
utilmy/deeplearning/prepro.py:gpu_usage,gpu_usage,function,7,12,12,122,10.17,0,0,[],[],[],861,[],"['os_system', 'print']",2
utilmy/deeplearning/prepro.py:gpu_free,gpu_free,function,16,52,42,308,5.92,1,1,[],[],[],873,[],"['os_system', 'ss.split', 'x.split', 'len', 'print', 'enumerate', 'deviceid_free.append']",7
utilmy/deeplearning/prepro.py:down_ichiba,down_ichiba,function,25,94,74,722,7.68,2,1,[],[],[],892,"['    """"""  python prepro.py down_ichiba\n', '    \n', '     TODO :\n', '        Map queries ---> Categories        \n', '        Cateogries  ---> Generate Ichiba Queries\n', '        \n', '    \n', '      https://search.rakuten.co.jp/search/mall/-/566028/tg1003435/?max=4000&min=3000\n', '      \n', '    ### reverse Images  \n', '    https://search.rakuten.co.jp/search/mall/blue/566028/tg1004015/?max=4000&min=3000\n', '    \n', '    \n', '    get random images\n', '       https://search.rakuten.co.jp/search/mall/blue/566028/tg1004015/?max=9000&p=4\n', '       \n', '       from generating URL and queries\n', '       \n', '       \n', '    \n', '    \n', '    """"""\n']","['down2', 'down_page', 'pd.read_csv', 'log', 'df.iterrows', 'll.append', 'run_multiprocess']",7
utilmy/deeplearning/prepro.py:down_page,down_page,function,61,174,128,1891,10.87,8,1,"['query', 'out_dir', 'genre_en', 'id0', 'cat', 'npage']","[None, None, None, None, None, None]","[None, '""query1""', ""''"", '""""', '""""', '1']",941,"['    """"""\n', ""        python prepro.py down_page  '++'    --out_dir men_fashion_topshirts_blue  \n"", '\n', '\n', '    """"""\n']","['os.makedirs', 'print', 'open', 'csv.writer', 'csv_writer.writerow', 'Request', 'urlopen', 'bs', 'str', 'soup.find_all', 'individual_item.find_all', 'time.sleep']",12
utilmy/deeplearning/prepro.py:check_tf,check_tf,function,4,6,6,61,10.17,0,0,[],[],[],1039,[],['print'],1
utilmy/deeplearning/prepro.py:prepro_images2,prepro_images2,function,22,55,46,553,10.05,1,4,['image_paths'],[None],[None],1148,[],"['range', 'str', 'fname.split', 'print', 'temp.astype', 'cv2.resize', 'images.append']",7
utilmy/deeplearning/util_train.py:np_remove_duplicates,np_remove_duplicates,function,6,17,15,74,4.35,1,1,['seq'],[None],[None],100,[],"['set', 'seen_add']",2
utilmy/deeplearning/util_train.py:clean1,clean1,function,5,14,13,53,3.79,1,1,['ll'],[None],[None],106,[],"['len', 'np_remove_duplicates']",2
utilmy/deeplearning/util_train.py:log3,log3,function,5,14,14,111,7.93,0,1,['*s'],[None],[None],182,[],"['print', 'open', 'fp.write']",3
utilmy/deeplearning/util_train.py:log2,log2,function,4,10,10,90,9.0,0,0,['*s'],[None],[None],188,[],"['print', 'open', 'fp.write']",3
utilmy/deeplearning/util_train.py:log,log,function,6,26,17,215,8.27,0,1,['*s'],[None],[None],254,"['    """"""Log decorator""""""\n']","['print', 'open', 'fp.write', 'log2']",4
utilmy/deeplearning/util_train.py:config_save,config_save,function,4,10,10,61,6.1,0,0,"['cc', 'path']","[None, None]","[None, None]",259,[],"['json.dump', 'str', 'open', 'print']",4
utilmy/deeplearning/util_train.py:os_path_copy,os_path_copy,function,12,24,23,261,10.88,1,1,"['in_dir', 'path', 'ext']","[None, None, None]","[None, None, '""*.py""']",266,"['  """""" Copy folder recursively \n', '  """"""\n']","['glob.glob', 'print', 'os.makedirs', 'shutil.copytree', 'shutil.copy2']",5
utilmy/deeplearning/util_train.py:metric_accuracy,metric_accuracy,function,11,26,21,260,10.0,2,0,"['y_val', 'y_pred_head', 'class_dict']","[None, None, None]","[None, None, None]",282,[],"['enumerate', 'np.argmax', 'print']",3
utilmy/deeplearning/util_train.py:valid_image_original,valid_image_original,function,17,55,42,469,8.53,1,2,"['img_list', 'path', 'tag', 'y_labels', 'n_sample']","[None, None, None, None, None]","[None, None, None, None, 'None']",293,"['    """"""Assess image validity""""""\n']","['os.makedirs', 'isinstance', 'range', 'img.numpy', 'np.clip', 'cv2.imwrite']",6
utilmy/deeplearning/util_train.py:valid_image_check,valid_image_check,function,17,57,43,460,8.07,1,3,"['img_list', 'path', 'tag', 'y_labels', 'n_sample', 'renorm']","[None, None, None, None, None, None]","[None, '""""', '""""', '""""', '3', 'True']",313,"['    """"""Assess image validity""""""\n']","['os.makedirs', 'isinstance', 'range', 'img.numpy', 'cv2.imwrite']",5
utilmy/deeplearning/util_train.py:save_best,save_best,function,9,42,38,357,8.5,0,1,"['model', 'model_dir2', 'curr_loss', 'best_loss', 'counter', 'epoch', 'dd']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",335,"['    """"""Save the best model""""""\n']","['save_model_state', 'config_save', 'print']",3
utilmy/deeplearning/util_train.py:save_model_state,save_model_state,function,2,6,6,95,15.83,0,0,"['model', 'model_dir2']","[None, None]","[None, None]",354,"['    """"""Save the model""""""\n']","['os.makedirs', 'model.save_weights']",2
utilmy/deeplearning/util_train.py:train_stop,train_stop,function,4,16,15,145,9.06,0,1,"['counter', 'patience']","[None, None]","[None, None]",360,"['    """"""Stop the training if meet the condition""""""\n']",['log'],1
utilmy/deeplearning/util_train.py:model_reload,model_reload,function,6,23,21,250,10.87,0,0,"['model_reload_name', 'cc', '']","[None, None, None]","[None, None, None]",369,[],"['DFC_VAE', 'x_train.reshape', 'model2.build', 'model2.load_weights']",4
utilmy/deeplearning/util_train.py:image_check,image_check,function,4,19,17,96,5.05,0,1,"['name', 'img', 'renorm']","[None, None, None]","[None, None, 'False']",378,[],['cv2.imwrite'],1
utilmy/deeplearning/util_train.py:pd_get_dummies,pd_get_dummies,function,24,89,46,544,6.11,4,5,"['df', 'cols_cat', 'cat_dict', 'only_onehot']","[None, None, 'dict', None]","[None, None, None, 'True']",385,"['   """""" dfi_onehot = pd_get_dummies( df, cols_cat = [\'articleType\'  ], cat_dict= cc.labels_map, only_onehot= False)\n', '      dfi_onehot.sum()\n', '      dfi_onehot.dtypes\n', '\n', '   """""" \n']","['cat_dict.keys', 'pd.get_dummies', 'df1.astype', 'pd.concat', 'print']",5
utilmy/deeplearning/util_train.py:make_encoder,make_encoder,function,22,85,63,1289,15.16,0,0,['n_outputs'],[None],['1'],467,[],"['functools.partial', 'Input', 'Conv2D', 'BatchNormalization', 'layers.Dropout', 'Flatten', 'Dense']",7
utilmy/deeplearning/util_train.py:make_decoder,make_decoder,function,20,91,50,1558,17.12,0,2,[],[],[],528,"['    """"""\n', '    ValueError: Dimensions must be equal, but are 3 and 4\n', ""    for '{{node sub}} = Sub[T=DT_FLOAT](x, sequential_1/conv2d_transpose_3/Relu)' with input shapes: [8,256,256,3], [8,256,256,4].\n"", '\n', '    """"""\n']","['functools.partial', 'Input', 'Dense', 'layers.Dropout', 'Reshape', 'Conv2DTranspose']",6
utilmy/deeplearning/util_train.py:make_classifier,make_classifier,function,18,37,36,645,17.43,0,0,['class_dict'],[None],[None],590,"['    """""" Supervised multi class\n', ""            self.gender         = nn.Linear(self.inter_features, self.num_classes['gender'])\n"", ""            self.masterCategory = nn.Linear(self.inter_features, self.num_classes['masterCategory'])\n"", '    """"""\n']","['functools.partial', 'Input', 'Dense', 'class_dict.items']",4
utilmy/deeplearning/util_train.py:learning_rate_schedule,learning_rate_schedule,function,10,101,46,480,4.75,0,5,"['mode', 'epoch', 'cc']","[None, None, None]","['""step""', '1', 'None']",668,[],"['np.exp', 'np.array']",2
utilmy/deeplearning/util_train.py:loss_schedule,loss_schedule,function,9,137,44,739,5.39,0,3,"['mode', 'epoch']","[None, None]","['""step""', '1']",689,[],[],0
utilmy/deeplearning/util_train.py:perceptual_loss_function,perceptual_loss_function,function,25,110,79,1234,11.22,1,2,"['x', 'x_recon', 'z_mean', 'z_logsigma', 'kl_weight', 'y_label_heads', 'y_pred_heads', 'clf_loss_fn']","[None, None, None, None, None, None, None, None]","[None, None, None, None, '0.00005', 'None', 'None', 'None']",724,[],"['recons_loss_global', 'tf.reduce_mean', 'tf.reduce_sum', 'tf.square', 'percep_loss_global', 'tf.stop_gradient', 'percep_model', 'loss_schedule', '6*triplet_loss_global', '4*triplet_loss_global', 'triplet_loss_global', 'range', 'clf_loss_fn', 'loss_clf.append']",14
utilmy/deeplearning/util_train.py:label_get_data,label_get_data,function,18,91,61,766,8.42,2,1,[],[],[],777,[],"['pd_read_file', 'df.fillna', 'log', 'df.drop_duplicates', 'set', 'len', 'list', 'print', 'sys.exit', 'time.sleep']",10
utilmy/deeplearning/util_train.py:pd_category_filter,pd_category_filter,function,12,28,23,255,9.11,1,1,"['df', 'category_map']","[None, None]","[None, None]",831,[],"['cat_filter', 's.lower', 'list', 'category_map.items']",4
utilmy/deeplearning/util_train.py:image_load,image_load,function,5,8,7,90,11.25,0,0,"['pathi', 'mode']","[None, None]","[None, ""'cache'""]",873,[],['pathi.split'],1
utilmy/deeplearning/util_train.py:train_step,train_step,function,13,38,35,432,11.37,0,0,"['x', 'model', 'y_label_list']","[None, None, None]","[None, None, 'None']",1044,[],"['tf.GradientTape', 'model', 'perceptual_loss_function', 'tape.gradient', 'optimizer.apply_gradients']",5
utilmy/deeplearning/util_train.py:validation_step,validation_step,function,9,29,24,263,9.07,0,0,"['x', 'model', 'y_label_list']","[None, None, None]","[None, None, 'None']",1060,[],"['model', 'perceptual_loss_function']",2
utilmy/deeplearning/util_train.py:metric_accuracy2,metric_accuracy2,function,13,29,26,295,10.17,1,0,"['y_test', 'y_pred', 'dd']","[None, None, None]","[None, None, None]",1293,[],"['enumerate', 'np.argmax', 'accuracy_score', 'log']",4
utilmy/deeplearning/util_train.py:LearningRateDecay,LearningRateDecay,class,1,8,8,65,8.12,0,0,[],[],[],416,[],[],0
utilmy/deeplearning/util_train.py:DFC_VAE,DFC_VAE,class,38,82,65,867,10.57,0,1,[],[],[],427,[],[],0
utilmy/deeplearning/util_train.py:RealCustomDataGenerator,RealCustomDataGenerator,class,63,180,132,1831,10.17,5,2,[],[],[],886,[],[],0
utilmy/deeplearning/util_train.py:StepDecay,StepDecay,class,11,64,39,372,5.81,0,1,[],[],[],1270,[],[],0
utilmy/deeplearning/util_train.py:SprinklesTransform,SprinklesTransform,class,9,30,26,426,14.2,0,1,[],[],[],1308,[],[],0
utilmy/deeplearning/util_train.py:LearningRateDecay:plot,LearningRateDecay:plot,method,0,1,1,4,4.0,0,0,"['self', 'epochs', 'title', 'path']","[None, None, None, None]","[None, None, '""Learning Rate Schedule""', 'None']",417,[],[],0
utilmy/deeplearning/util_train.py:DFC_VAE:__init__,DFC_VAE:__init__,method,9,10,10,157,15.7,0,0,"['self', 'latent_dim', 'class_dict']","[None, None, None]","[None, None, None]",429,[],"['super', 'make_encoder', 'make_decoder', 'make_classifier']",4
utilmy/deeplearning/util_train.py:DFC_VAE:encode,DFC_VAE:encode,method,4,8,6,95,11.88,0,0,"['self', 'x']","[None, None]","[None, None]",437,[],['tf.split'],1
utilmy/deeplearning/util_train.py:DFC_VAE:reparameterize,DFC_VAE:reparameterize,method,4,7,6,84,12.0,0,0,"['self', 'z_mean', 'z_logsigma']","[None, None, None]","[None, None, None]",441,[],['tf.exp'],1
utilmy/deeplearning/util_train.py:DFC_VAE:decode,DFC_VAE:decode,method,6,10,8,104,10.4,0,1,"['self', 'z', 'apply_sigmoid']","[None, None, None]","[None, None, 'False']",445,[],"['self.decoder', 'tf.sigmoid']",2
utilmy/deeplearning/util_train.py:DFC_VAE:call,DFC_VAE:call,method,18,26,23,222,8.54,0,0,"['self', 'x', 'training', 'mask', 'y_label_list']","[None, None, None, None, None]","[None, None, 'True', 'None', ' None']",452,[],"['self.encode', 'self.reparameterize', 'self.decode', 'self.classifier']",4
utilmy/deeplearning/util_train.py:RealCustomDataGenerator:__init__,RealCustomDataGenerator:__init__,method,17,19,19,254,13.37,0,0,"['self', 'image_dir', 'label_path', 'class_dict', 'split', 'batch_size', 'transforms', 'shuffle', 'img_suffix']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, ""'train'"", '8', 'None', 'False', '"".png""']",887,[],"['self._load_data', 'len']",2
utilmy/deeplearning/util_train.py:RealCustomDataGenerator:_load_data,RealCustomDataGenerator:_load_data,method,17,36,32,394,10.94,1,0,"['self', 'label_path']","[None, None]","[None, None]",906,[],"['pd.read_csv', 'list', 'df.dropna', 'df.drop', 'pd_get_dummies', 'log2', 'dfi_onehot.sum', 'labels.append']",8
utilmy/deeplearning/util_train.py:RealCustomDataGenerator:on_epoch_end,RealCustomDataGenerator:on_epoch_end,method,10,15,15,189,12.6,1,1,['self'],[None],[None],923,[],['np.arange'],1
utilmy/deeplearning/util_train.py:RealCustomDataGenerator:__len__,RealCustomDataGenerator:__len__,method,1,3,3,62,20.67,0,0,['self'],[None],[None],931,[],"['int', 'float']",2
utilmy/deeplearning/util_train.py:RealCustomDataGenerator:__getitem__,RealCustomDataGenerator:__getitem__,method,21,87,61,702,8.07,3,1,"['self', 'idx']","[None, None]","[None, None]",934,[],"['image_load', 'batch_x.append', 'np.concatenate', 'np.stack', 'range', 'batch_y.append']",6
utilmy/deeplearning/util_train.py:StepDecay:__init__,StepDecay:__init__,method,6,6,6,66,11.0,0,0,"['self', 'init_lr', 'factor', 'drop_every']","[None, None, None, None]","[None, '0.01', '0.25', '5']",1271,[],[],0
utilmy/deeplearning/util_train.py:StepDecay:__call__,StepDecay:__call__,method,3,50,26,223,4.46,0,1,"['self', 'epoch']","[None, None]","[None, None]",1277,[],['np.exp'],1
utilmy/deeplearning/util_train.py:SprinklesTransform:__init__,SprinklesTransform:__init__,method,3,6,6,125,20.83,0,0,"['self', 'num_holes', 'side_length', 'always_apply', 'p']","[None, None, None, None, None]","[None, '100', '10', 'False', '1.0']",1309,[],"['super', 'Sprinkles']",2
utilmy/deeplearning/util_train.py:SprinklesTransform:apply,SprinklesTransform:apply,method,5,14,11,196,14.0,0,1,"['self', 'image', '**params']","[None, None, None]","[None, None, None]",1313,[],"['isinstance', 'tf.constant', 'self.sprinkles']",3
utilmy/deeplearning/utils_dl.py:np_remove_duplicates,np_remove_duplicates,function,6,17,15,74,4.35,1,1,['seq'],[None],[None],131,[],"['set', 'seen_add']",2
utilmy/deeplearning/utils_dl.py:clean1,clean1,function,5,14,13,53,3.79,1,1,['ll'],[None],[None],137,[],"['len', 'np_remove_duplicates']",2
utilmy/deeplearning/utils_dl.py:log,log,function,7,29,19,249,8.59,0,1,['*s'],[None],[None],210,[],"['print', 'log3', 'open', 'fp.write', 'log2']",5
utilmy/deeplearning/utils_dl.py:log3,log3,function,5,14,14,111,7.93,0,1,['*s'],[None],[None],214,[],"['print', 'open', 'fp.write']",3
utilmy/deeplearning/utils_dl.py:log2,log2,function,4,10,10,90,9.0,0,0,['*s'],[None],[None],220,[],"['print', 'open', 'fp.write']",3
utilmy/deeplearning/utils_dl.py:prepro_images,prepro_images,function,9,20,16,124,6.2,1,1,"['image_paths', 'nmax']","[None, None]","[None, '10000000']",233,[],"['range', 'prepro_image', 'images.append']",3
utilmy/deeplearning/utils_dl.py:image_center_crop,image_center_crop,function,19,31,28,308,9.94,0,2,"['img', 'dim']","[None, None]","[None, None]",242,"[' \t""""""Returns center cropped image\n', ' \tArgs:\n', ' \timg: image to be center cropped\n', ' \tdim: dimensions (width, height) to be cropped\n', ' \t""""""\n']",['int'],1
utilmy/deeplearning/utils_dl.py:image_resize_pad,image_resize_pad,function,32,127,88,991,7.8,0,3,"['img', 'size', '256']","[None, None, None]","[None, '(256', None]",259,"['     """"""\n', '       resize and keep into the target Box\n', '     \n', '     """"""\n']","['float', 'np.round', 'np.floor', 'np.ceil', 'len', 'isinstance', 'cv2.resize', 'cv2.copyMakeBorder']",8
utilmy/deeplearning/utils_dl.py:prepro_image0,prepro_image0,function,14,46,35,355,7.72,0,0,['image_path'],[None],[None],304,[],"['str', 'fname.split', 'image_read', 'image_resize_pad', 'image_center_crop', 'image.astype']",6
utilmy/deeplearning/utils_dl.py:prepro_images_multi,prepro_images_multi,function,20,52,40,338,6.5,2,1,"['image_paths', 'npool', 'prepro_image']","[None, None, None]","[None, '30', 'None']",324,"['    """""" Parallel processing\n', '    \n', '    """"""\n']","['Pool', 'pool.map', 'pool.close', 'pool.join', 'print', 'len', 'images.append', 'labels.append']",8
utilmy/deeplearning/utils_dl.py:run_multiprocess,run_multiprocess,function,17,29,24,192,6.62,1,0,"['myfun', 'list_args', 'npool', '**kwargs']","[None, None, None, None]","[None, None, '10', None]",347,"['    """"""\n', '       res = run_multiprocess(prepro, image_paths, npool=10, )\n', '    """"""\n']","['Pool', 'pool.map', 'partial', 'pool.close', 'pool.join']",5
utilmy/deeplearning/utils_dl.py:pd_get_dummies,pd_get_dummies,function,24,89,46,544,6.11,4,5,"['df', 'cols_cat', 'cat_dict', 'only_onehot']","[None, None, 'dict', None]","[None, None, None, 'True']",361,"['   """""" dfi_onehot = pd_get_dummies( df, cols_cat = [\'articleType\'  ], cat_dict= cc.labels_map, only_onehot= False)\n', '      dfi_onehot.sum()\n', '      dfi_onehot.dtypes\n', '   """""" \n']","['cat_dict.keys', 'pd.get_dummies', 'df1.astype', 'pd.concat', 'print']",5
utilmy/deeplearning/utils_dl.py:label_get_data,label_get_data,function,18,91,61,766,8.42,2,1,[],[],[],396,[],"['pd_read_file', 'df.fillna', 'log', 'df.drop_duplicates', 'set', 'len', 'list', 'print', 'sys.exit', 'time.sleep']",10
utilmy/deeplearning/utils_dl.py:pd_category_filter,pd_category_filter,function,12,28,23,255,9.11,1,1,"['df', 'category_map']","[None, None]","[None, None]",442,[],"['cat_filter', 's.lower', 'list', 'category_map.items']",4
utilmy/deeplearning/utils_dl.py:image_load,image_load,function,5,8,7,90,11.25,0,0,"['pathi', 'mode']","[None, None]","[None, ""'cache'""]",460,[],['pathi.split'],1
utilmy/deeplearning/utils_dl.py:data_add_onehot,data_add_onehot,function,26,84,55,529,6.3,3,2,"['dfref', 'img_dir', 'labels_col']","[None, None, None]","[None, None, None]",473,"['    """"""\n', '       id, uri, cat1, cat2, .... , cat1_onehot\n', '    """"""\n']","['glob.glob', 'fi.split', 'log', 'pd.DataFrame', 'x.split', 'int', 'df.merge', 'pd.get_dummies', 'dfi_1hot.apply', 'str']",10
utilmy/deeplearning/utils_dl.py:image_check_npz,image_check_npz,function,15,59,41,336,5.69,2,1,"['path_npz', 'keys', 'path', 'tag', 'n_sample', 'renorm']","[None, None, None, None, None, None]","[None, ""['train']"", '""""', '""""', '3', 'True']",642,[],"['os.makedirs', 'np.load', 'data_npz.keys', 'print', 'str', 'range', 'cv2.imwrite']",7
utilmy/deeplearning/utils_dl.py:image_resize,image_resize,function,32,127,88,991,7.8,0,3,"['img', 'size', '256']","[None, None, None]","[None, '(256', None]",662,"['    """"""     python prepro.py  image_resize\n', '          image white color padded\n', '    \n', '    """"""\n']","['float', 'np.round', 'np.floor', 'np.ceil', 'len', 'isinstance', 'cv2.resize', 'cv2.copyMakeBorder']",8
utilmy/deeplearning/utils_dl.py:image_resize2,image_resize2,function,8,38,27,219,5.76,0,2,"['image', 'width', 'height', 'inter']","[None, None, None, None]","[None, 'None', 'None', 'cv2.INTER_AREA']",709,"['    """"""Resizes a image and maintains aspect ratio.\n', '    Args:\n', '        image:\n', '        width:\n', '        height:\n', '        inter:\n', '    Returns:\n', '    """"""\n']","['float', 'int', 'cv2.resize']",3
utilmy/deeplearning/utils_dl.py:image_check,image_check,function,15,59,41,336,5.69,2,1,"['path_npz', 'keys', 'path', 'tag', 'n_sample', 'renorm']","[None, None, None, None, None, None]","[None, ""['train']"", '""""', '""""', '3', 'True']",742,"['    """"""     python prepro.py  image_check \n', '          image white color padded\n', '    \n', '    """"""    \n']","['os.makedirs', 'np.load', 'data_npz.keys', 'print', 'str', 'range', 'cv2.imwrite']",7
utilmy/deeplearning/utils_dl.py:padding_generate,padding_generate,function,2,5,5,80,16.0,0,0,"['paddings_number', 'min_padding', 'max_padding']","[' int ', ' int ', ' int ']","[' 1', ' 1', ' 1']",787,"['     """"""\n', '     Args:\n', '         paddings_number:  4\n', '         min_padding:      1\n', '         max_padding:    100\n', '     Returns: padding list\n', '     """"""\n']",[],0
utilmy/deeplearning/utils_dl.py:image_merge,image_merge,function,32,73,51,554,7.59,1,3,"['image_list', 'n_dim', 'padding_size', 'max_height', 'total_width']","[None, None, None, None, None]","[None, None, None, None, None]",803,"['    """"""\n', '    Args:\n', '        image_list:  list of image\n', '        n_dim:\n', '        padding_size: padding size max\n', '        max_height:   max height\n', '        total_width:  total width\n', '    Returns:\n', '    """"""\n']","['np.zeros', 'len', 'enumerate']",3
utilmy/deeplearning/utils_dl.py:image_remove_extra_padding,image_remove_extra_padding,function,31,73,63,518,7.1,0,2,"['img', 'inverse', 'removedot']","[None, None, None]","[None, 'False', 'True']",839,"['    """"""TODO: Issue with small dot noise points : noise or not ?\n', '              Padding calc has also issues with small blobs.\n', '    Args:\n', '        img: image\n', '    Returns: image cropped of extra padding\n', '    """"""\n']","['cv2.cvtColor', 'max', 'int', 'np.where', 'morphology.remove_small_objects', 'graybin.astype', 'cv2.findNonZero', 'cv2.boundingRect']",8
utilmy/deeplearning/utils_dl.py:image_remove_bg,image_remove_bg,function,12,48,41,388,8.08,1,1,"['in_dir', 'out_dir', 'level']","[None, None, None]","['""""', '""""', '1']",868,"['    """""" #### remove background\n', '    \n', '         source activate py38 &&  sleep 5 && python prepro.py   image_remove_bg  \n', '    \n', '    \n', '        python prepro.py rembg  --in_dir  /data/workspaces/noelkevin01/img/data/bing/v4     --out_dir  /data/workspaces/noelkevin01/img/data/bing/v4_nobg &>> /data/workspaces/noelkevin01/img/data/zlog_rembg.py  &\n', '        rembg  -ae 15 -p  /data/workspaces/noelkevin01/img/data/fashion/test2/  /data/workspaces/noelkevin01/img/data/fashion/test_nobg/  \n', '        \n', '        mkdir /data/workspaces/noelkevin01/img/data/fashion/train_nobg/  \n', '        \n', '    """"""    \n']","['glob.glob', 'log', 'str', 'fp.split', 'fp.replace', 'os.makedirs', 'os.system']",7
utilmy/deeplearning/utils_dl.py:image_create_cache,image_create_cache,function,60,188,137,1744,9.28,3,2,[],[],[],898,[],"['log', 'sorted', 'len', 'prepro_image2b', 'str', 'fname.split', 'cv2.imread', 'cv2.cvtColor', 'image_center_crop', 'dc.Cache', 'size_limit=int', 'prepro_images_multi', 'set_async', 'asyncio.get_running_loop', 'loop.run_in_executor', 'zip', 'key.split', 'print', 'enumerate', 'cv2.imwrite']",20
utilmy/deeplearning/utils_dl.py:os_path_check,os_path_check,function,5,23,19,128,5.57,0,0,"['path', 'n']","[None, None]","[None, '5']",992,[],"['print', 'os_system']",2
utilmy/deeplearning/utils_dl.py:image_face_blank,image_face_blank,function,41,90,78,784,8.71,2,0,"['in_dir', 'level ', 'out_dir', 'npool']","[None, None, None, None]","['""""', ' ""/*""', 'f""""', '30']",998,"['    """"""  Remove face\n', '     python prepro.py  image_face_blank\n', '     \n', '     python prepro.py  image_face_blank  --in_dir img/data/fashion/test_nobg   --out_dir img/data/fashion/test_nobg_noface\n', '     python prepro.py  image_face_blank  --in_dir img/data/fashion/train_nobg   --out_dir img/data/fashion/train_nobg_noface\n', '      five elements are [xmin, ymin, xmax, ymax, detection_confidence]\n', '    """"""\n']","['glob.glob', 'face_detection.build_detector', 'log', 'myfun', 'cv2.imread', 'detector.detect', 'int', 'fp.replace', 'os.makedirs', 'cv2.imwrite', 'Pool', 'pool.map', 'pool.close', 'pool.join']",14
utilmy/deeplearning/utils_dl.py:image_text_blank,image_text_blank,function,39,209,101,1461,6.99,8,3,"['in_dir', 'out_dir', 'level']","[None, None, None]","[None, None, '""/*""']",1054,"['    """"""\n', '        Not working well\n', '        python prepro.py  image_text_blank  --in_dir img/data/fashion/ztest   --out_dir img/data/fashion/ztest_noface\n', '        \n', '    \n', '    """"""\n']","['glob.glob', 'log', 'cv2.imread', 'detect_text_regions', 'int', 'fp.replace', 'os.makedirs', 'cv2.imwrite', 'print', 'sorted', 'os.system']",11
utilmy/deeplearning/utils_dl.py:image_read,image_read,function,12,45,33,584,12.98,0,3,"['filepath_or_buffer', 'io.BytesIO]']","[' Union[str', None]","[None, None]",1146,"['    """"""\n', '    Read a file into an image object\n', '    Args:\n', '        filepath_or_buffer: The path to the file, a URL, or any object\n', '            with a `read` method (such as `io.BytesIO`)\n', '    """"""\n']","['isinstance', 'hasattr', 'np.asarray', 'cv2.imdecode', 'filepath_or_buffer.endswith', 'tifffile.imread', 'cv2.imread', 'cv2.cvtColor']",8
utilmy/deeplearning/utils_dl.py:image_save,image_save,function,22,50,46,482,9.64,1,1,[],[],[],1175,[],"['dc.Cache', 'print', 'len', 'log', 'os.makedirs', 'enumerate', 'key.split', 'cv2.imwrite']",8
utilmy/deeplearning/utils_dl.py:create_train_npz,create_train_npz,function,26,151,88,1682,11.14,2,2,[],[],[],1199,[],"['log', 'pd.read_csv', 'set', 'len', 'sorted', 'int', 'prepro_images_multi', 'log5', 'np.array', 'np.savez_compressed', 'image_check_npz']",11
utilmy/deeplearning/utils_dl.py:create_train_parquet,create_train_parquet,function,23,134,72,1545,11.53,2,2,[],[],[],1272,[],"['log', 'pd.read_csv', 'set', 'len', 'sorted', 'int', 'prepro_images_multi', 'pd.DataFrame', 'df2.merge', 'df2.to_parquet']",10
utilmy/deeplearning/utils_dl.py:model_deletes,model_deletes,function,16,82,54,512,6.24,4,3,['dry'],[None],['0'],1336,"['    """"""  ## Delete files on disk\n', '        python prepro.py model_deletes  --dry 0\n', '        \n', '    """"""\n']","['glob.glob', 'print', 'sorted', 'int', 'os.system']",5
utilmy/deeplearning/utils_dl.py:topk_predict,topk_predict,function,3,9,9,149,16.56,0,0,[],[],[],1366,[],['os.system'],1
utilmy/deeplearning/utils_dl.py:topk,topk,function,69,265,187,2377,8.97,3,5,[],[],[],1375,[],"['os.system', 'topk', 'dname.replace', 'os.makedirs', 'log', 'pd_read_file', 'x.split', 'df.drop_duplicates', 'list', 'len', 'np.array', 'copy.deepcopy', 'x0.reshape', 'topk_nearest_vector', 'np.arange', 'df1.to_csv', 'dc.Cache', 'print', 'enumerate', 'key.split', 'cv2.imwrite']",21
utilmy/deeplearning/utils_dl.py:topk_nearest_vector,topk_nearest_vector,function,9,14,12,132,9.43,0,0,"['x0', 'vector_list', 'topk']","[None, None, None]","[None, None, '3']",1503,"['   """"""\n', '      Retrieve top k nearest vectors using FAISS\n', '  \n', '   """"""\n']","['faiss.index_factory', 'index.add', 'index.search']",3
utilmy/deeplearning/utils_dl.py:topk_export,topk_export,function,8,25,24,171,6.84,0,1,[],[],[],1515,"['    """"""   /user/scoupon/zexport/z/fashion_emb_100k  \n', '                     id gender masterCategory subCategory  ... masterCategory_pred subCategory_pred articleType_pred  baseColour_pred\n', '0     cn3357-01_1-11.png  women        apparel     topwear  ...                   1                1               32                4\n', '1      cs6481-01_1-4.png   kids          shoes       shoes  ...                   5               20              151                7\n', '     hdfs dfs -put  /data/workspaces/noelkevin01/img/models/fashion/dcf_vae/m_train9pred/res/m_train9b_g3_-img_train_r2p2_200k_clean_nobg_256_256-500000-cache_best_best_good_epoch_313/fashion_emb_500k/                /user/scoupon/zexport/z/\n', '    """"""\n']","['log', 'pd.read_parquet', 'dfi.to_parquet', 'fi.split']",4
utilmy/deeplearning/utils_dl.py:data_get_sample,data_get_sample,function,12,33,28,305,9.24,1,0,"['batch_size', 'x_train', 'labels_val']","[None, None, None]","[None, None, None]",1540,[],"['np.array', 'y_label_list.append']",2
utilmy/deeplearning/utils_dl.py:data_to_y_onehot_list,data_to_y_onehot_list,function,22,48,35,362,7.54,2,1,"['df', 'dfref', 'labels_col']","[None, None, None]","[None, None, None]",1560,[],"['df.merge', 'pd.get_dummies', 'print']",3
utilmy/deeplearning/utils_dl.py:test,test,function,8,20,20,204,10.2,0,0,[],[],[],1577,"['    """"""\n', '       python prepro.py test\n', '       \n', '    """"""\n']","['pd.read_csv', 'data_add_onehot', 'log']",3
utilmy/deeplearning/utils_dl.py:unzip,unzip,function,5,8,8,83,10.38,0,0,"['in_dir', 'out_dir']","[None, None]","[None, None]",1590,[],"['zipfile.ZipFile', 'zip_ref.extractall']",2
utilmy/deeplearning/utils_dl.py:gzip,gzip,function,5,12,12,299,24.92,0,0,[],[],[],1597,[],"['print', 'os.system']",2
utilmy/deeplearning/utils_dl.py:folder_size,folder_size,function,1,12,12,78,6.5,0,0,[],[],[],1611,[],['os.system'],1
utilmy/deeplearning/utils_dl.py:down_ichiba,down_ichiba,function,25,94,74,722,7.68,2,1,[],[],[],1616,"['    """"""  python prepro.py down_ichiba\n', '    \n', '     TODO :\n', '        Map queries ---> Categories        \n', '        Cateogries  ---> Generate Ichiba Queries\n', '        \n', '    \n', '      https://search.rakuten.co.jp/search/mall/-/566028/tg1003435/?max=4000&min=3000\n', '      \n', '    ### reverse Images  \n', '    https://search.rakuten.co.jp/search/mall/blue/566028/tg1004015/?max=4000&min=3000\n', '    \n', '    \n', '    get random images\n', '       https://search.rakuten.co.jp/search/mall/blue/566028/tg1004015/?max=9000&p=4\n', '       \n', '       from generating URL and queries\n', '       \n', '       \n', '    \n', '    \n', '    """"""\n']","['down2', 'down_page', 'pd.read_csv', 'log', 'df.iterrows', 'll.append', 'run_multiprocess']",7
utilmy/deeplearning/utils_dl.py:down_page,down_page,function,58,171,125,1873,10.95,8,1,"['query', 'out_dir', 'genre_en', 'id0', 'cat', 'npage']","[None, None, None, None, None, None]","[None, '""query1""', ""''"", '""""', '""""', '1']",1665,"['    """"""\n', ""        python prepro.py down_page  '++'    --out_dir men_fashion_topshirts_blue  \n"", '    """"""\n']","['os.makedirs', 'print', 'open', 'csv.writer', 'csv_writer.writerow', 'Request', 'urlopen', 'bs', 'str', 'soup.find_all', 'individual_item.find_all', 'time.sleep']",12
utilmy/deeplearning/utils_dl.py:config_save,config_save,function,2,7,7,50,7.14,0,0,"['cc', 'path']","[None, None]","[None, None]",1751,[],"['json.dump', 'str', 'open', 'print']",4
utilmy/deeplearning/utils_dl.py:os_path_copy,os_path_copy,function,12,24,23,261,10.88,1,1,"['in_dir', 'path', 'ext']","[None, None, None]","[None, None, '""*.py""']",1758,"['  """""" Copy folder recursively \n', '  """"""\n']","['glob.glob', 'print', 'os.makedirs', 'shutil.copytree', 'shutil.copy2']",5
utilmy/deeplearning/utils_dl.py:save_best,save_best,function,9,42,38,357,8.5,0,1,"['model', 'model_dir2', 'curr_loss', 'best_loss', 'counter', 'epoch', 'dd']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",1795,"['    """"""Save the best model""""""\n']","['save_model_state', 'config_save', 'print']",3
utilmy/deeplearning/utils_dl.py:save_model_state,save_model_state,function,2,6,6,95,15.83,0,0,"['model', 'model_dir2']","[None, None]","[None, None]",1814,"['    """"""Save the model""""""\n']","['os.makedirs', 'model.save_weights']",2
utilmy/deeplearning/utils_dl.py:train_stop,train_stop,function,4,16,15,145,9.06,0,1,"['counter', 'patience']","[None, None]","[None, None]",1820,"['    """"""Stop the training if meet the condition""""""\n']",['log'],1
utilmy/deeplearning/utils_dl.py:model_reload,model_reload,function,6,23,21,250,10.87,0,0,"['model_reload_name', 'cc', '']","[None, None, None]","[None, None, None]",1829,[],"['DFC_VAE', 'x_train.reshape', 'model2.build', 'model2.load_weights']",4
utilmy/deeplearning/utils_dl.py:learning_rate_schedule,learning_rate_schedule,function,10,101,46,480,4.75,0,5,"['mode', 'epoch', 'cc']","[None, None, None]","['""step""', '1', 'None']",1845,[],"['np.exp', 'np.array']",2
utilmy/deeplearning/utils_dl.py:loss_schedule,loss_schedule,function,9,137,44,739,5.39,0,3,"['mode', 'epoch']","[None, None]","['""step""', '1']",1866,[],[],0
utilmy/deeplearning/utils_dl.py:perceptual_loss_function,perceptual_loss_function,function,25,110,79,1234,11.22,1,2,"['x', 'x_recon', 'z_mean', 'z_logsigma', 'kl_weight', 'y_label_heads', 'y_pred_heads', 'clf_loss_fn']","[None, None, None, None, None, None, None, None]","[None, None, None, None, '0.00005', 'None', 'None', 'None']",1909,[],"['recons_loss_global', 'tf.reduce_mean', 'tf.reduce_sum', 'tf.square', 'percep_loss_global', 'tf.stop_gradient', 'percep_model', 'loss_schedule', '6*triplet_loss_global', '4*triplet_loss_global', 'triplet_loss_global', 'range', 'clf_loss_fn', 'loss_clf.append']",14
utilmy/deeplearning/utils_dl.py:make_encoder,make_encoder,function,22,85,63,1289,15.16,0,0,['n_outputs'],[None],['1'],2030,[],"['functools.partial', 'Input', 'Conv2D', 'BatchNormalization', 'layers.Dropout', 'Flatten', 'Dense']",7
utilmy/deeplearning/utils_dl.py:make_decoder,make_decoder,function,16,87,46,1472,16.92,0,2,[],[],[],2088,"['    """"""\n', '    ValueError: Dimensions must be equal, but are 3 and 4\n', ""    for '{{node sub}} = Sub[T=DT_FLOAT](x, sequential_1/conv2d_transpose_3/Relu)' with input shapes: [8,256,256,3], [8,256,256,4].\n"", '    """"""\n']","['functools.partial', 'Input', 'Dense', 'layers.Dropout', 'Reshape', 'Conv2DTranspose']",6
utilmy/deeplearning/utils_dl.py:make_classifier,make_classifier,function,14,33,32,559,16.94,0,0,['class_dict'],[None],[None],2149,"['    """""" Supervised multi class\n', ""            self.gender         = nn.Linear(self.inter_features, self.num_classes['gender'])\n"", ""            self.masterCategory = nn.Linear(self.inter_features, self.num_classes['masterCategory'])\n"", '    """"""\n']","['functools.partial', 'Input', 'Dense', 'class_dict.items']",4
utilmy/deeplearning/utils_dl.py:predict,predict,function,2,13,12,141,10.85,0,1,['name'],[None],['None'],2210,[],['os.system'],1
utilmy/deeplearning/utils_dl.py:metric_accuracy_test,metric_accuracy_test,function,10,28,23,255,9.11,1,0,"['y_test', 'y_pred', 'dd']","[None, None, None]","[None, None, None]",2221,[],"['enumerate', 'np.argmax', 'accuracy_score', 'log']",4
utilmy/deeplearning/utils_dl.py:metric_accuracy_val,metric_accuracy_val,function,11,26,21,260,10.0,2,0,"['y_val', 'y_pred_head', 'class_dict']","[None, None, None]","[None, None, None]",2234,[],"['enumerate', 'np.argmax', 'print']",3
utilmy/deeplearning/utils_dl.py:valid_image_original,valid_image_original,function,17,55,42,469,8.53,1,2,"['img_list', 'path', 'tag', 'y_labels', 'n_sample']","[None, None, None, None, None]","[None, None, None, None, 'None']",2245,"['    """"""Assess image validity""""""\n']","['os.makedirs', 'isinstance', 'range', 'img.numpy', 'np.clip', 'cv2.imwrite']",6
utilmy/deeplearning/utils_dl.py:valid_image_check,valid_image_check,function,17,57,43,460,8.07,1,3,"['img_list', 'path', 'tag', 'y_labels', 'n_sample', 'renorm']","[None, None, None, None, None, None]","[None, '""""', '""""', '""""', '3', 'True']",2265,"['    """"""Assess image validity""""""\n']","['os.makedirs', 'isinstance', 'range', 'img.numpy', 'cv2.imwrite']",5
utilmy/deeplearning/utils_dl.py:metric_accuracy2,metric_accuracy2,function,13,29,26,295,10.17,1,0,"['y_test', 'y_pred', 'dd']","[None, None, None]","[None, None, None]",2288,[],"['enumerate', 'np.argmax', 'accuracy_score', 'log']",4
utilmy/deeplearning/utils_dl.py:clf_loss_macro_soft_f1,clf_loss_macro_soft_f1,function,17,47,36,317,6.74,0,0,"['y', 'y_hat']","[None, None]","[None, None]",2302,"['    """"""Compute the macro soft F1-score as a cost.\n', '    Average (1 - soft-F1) across all labels.\n', '    Use probability values instead of binary predictions.\n', '    Args:\n', '        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n', '        y_hat (float32 Tensor): probability matrix of shape (BATCH_SIZE, N_LABELS)\n', '    Returns:\n', '        cost (scalar Tensor): value of the cost function for the batch\n', '    """"""\n']","['tf.cast', 'tf.reduce_sum', 'tf.reduce_mean']",3
utilmy/deeplearning/utils_dl.py:check_tf,check_tf,function,4,6,6,61,10.17,0,0,[],[],[],2361,[],['print'],1
utilmy/deeplearning/utils_dl.py:gpu_usage,gpu_usage,function,7,12,12,122,10.17,0,0,[],[],[],2367,[],"['os_system', 'print']",2
utilmy/deeplearning/utils_dl.py:gpu_free,gpu_free,function,16,52,42,308,5.92,1,1,[],[],[],2379,[],"['os_system', 'ss.split', 'x.split', 'len', 'print', 'enumerate', 'deviceid_free.append']",7
utilmy/deeplearning/utils_dl.py:train_step,train_step,function,13,38,35,432,11.37,0,0,"['x', 'model', 'y_label_list']","[None, None, None]","[None, None, 'None']",2467,[],"['tf.GradientTape', 'model', 'perceptual_loss_function', 'tape.gradient', 'optimizer.apply_gradients']",5
utilmy/deeplearning/utils_dl.py:validation_step,validation_step,function,9,29,24,263,9.07,0,0,"['x', 'model', 'y_label_list']","[None, None, None]","[None, None, 'None']",2483,[],"['model', 'perceptual_loss_function']",2
utilmy/deeplearning/utils_dl.py:RealCustomDataGenerator,RealCustomDataGenerator,class,63,180,132,1831,10.17,5,2,[],[],[],501,[],[],0
utilmy/deeplearning/utils_dl.py:CustomDataGenerator_img,CustomDataGenerator_img,class,36,106,77,916,8.64,2,1,[],[],[],582,[],[],0
utilmy/deeplearning/utils_dl.py:SprinklesTransform,SprinklesTransform,class,12,34,30,458,13.47,0,1,[],[],[],1775,[],[],0
utilmy/deeplearning/utils_dl.py:LearningRateDecay,LearningRateDecay,class,1,8,8,65,8.12,0,0,[],[],[],1837,[],[],0
utilmy/deeplearning/utils_dl.py:StepDecay,StepDecay,class,11,64,39,372,5.81,0,1,[],[],[],1960,[],[],0
utilmy/deeplearning/utils_dl.py:DFC_VAE,DFC_VAE,class,38,82,65,867,10.57,0,1,[],[],[],1990,[],[],0
utilmy/deeplearning/utils_dl.py:RealCustomDataGenerator:__init__,RealCustomDataGenerator:__init__,method,17,19,19,254,13.37,0,0,"['self', 'image_dir', 'label_path', 'class_dict', 'split', 'batch_size', 'transforms', 'shuffle', 'img_suffix']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, ""'train'"", '8', 'None', 'False', '"".png""']",502,[],"['self._load_data', 'len']",2
utilmy/deeplearning/utils_dl.py:RealCustomDataGenerator:_load_data,RealCustomDataGenerator:_load_data,method,17,36,32,394,10.94,1,0,"['self', 'label_path']","[None, None]","[None, None]",521,[],"['pd.read_csv', 'list', 'df.dropna', 'df.drop', 'pd_get_dummies', 'log2', 'dfi_onehot.sum', 'labels.append']",8
utilmy/deeplearning/utils_dl.py:RealCustomDataGenerator:on_epoch_end,RealCustomDataGenerator:on_epoch_end,method,10,15,15,189,12.6,1,1,['self'],[None],[None],538,[],['np.arange'],1
utilmy/deeplearning/utils_dl.py:RealCustomDataGenerator:__len__,RealCustomDataGenerator:__len__,method,1,3,3,62,20.67,0,0,['self'],[None],[None],546,[],"['int', 'float']",2
utilmy/deeplearning/utils_dl.py:RealCustomDataGenerator:__getitem__,RealCustomDataGenerator:__getitem__,method,21,87,61,702,8.07,3,1,"['self', 'idx']","[None, None]","[None, None]",549,[],"['image_load', 'batch_x.append', 'np.concatenate', 'np.stack', 'range', 'batch_y.append']",6
utilmy/deeplearning/utils_dl.py:CustomDataGenerator_img:__init__,CustomDataGenerator_img:__init__,method,12,14,14,187,13.36,0,0,"['self', 'img_dir', 'label_path', 'class_list', 'split', 'batch_size', 'transforms']","[None, None, None, None, None, None, None]","[None, None, None, None, ""'train'"", '8', 'None']",583,"['        """"""    \n', '           df_label format :\n', '               id, uri, cat1, cat2, cat3, cat1_onehot, cat1_onehot, ....\n', '        """"""\n']","['pd.read_csv', 'data_add_onehot']",2
utilmy/deeplearning/utils_dl.py:CustomDataGenerator_img:on_epoch_end,CustomDataGenerator_img:on_epoch_end,method,2,2,2,49,24.5,0,0,['self'],[None],[None],538,[],[],0
utilmy/deeplearning/utils_dl.py:CustomDataGenerator_img:__len__,CustomDataGenerator_img:__len__,method,1,3,3,59,19.67,0,0,['self'],[None],[None],546,"['        """"""    \n', '           df_label format :\n', '               id, uri, cat1, cat2, cat3, cat1_onehot, cat1_onehot, ....\n', '        """"""\n']","['int', 'float']",2
utilmy/deeplearning/utils_dl.py:CustomDataGenerator_img:__getitem__,CustomDataGenerator_img:__getitem__,method,21,72,50,458,6.36,2,1,"['self', 'idx']","[None, None]","[None, None]",549,[],"['df_batch.iterrows', 'np.array', 'batch_x.append', 'x.split', 'batch_y.append', 'np.stack']",6
utilmy/deeplearning/utils_dl.py:SprinklesTransform:__init__,SprinklesTransform:__init__,method,6,10,10,157,15.7,0,0,"['self', 'num_holes', 'side_length', 'always_apply', 'p']","[None, None, None, None, None]","[None, '100', '10', 'False', '1.0']",1776,[],"['super', 'Sprinkles']",2
utilmy/deeplearning/utils_dl.py:SprinklesTransform:apply,SprinklesTransform:apply,method,5,14,11,196,14.0,0,1,"['self', 'image', '**params']","[None, None, None]","[None, None, None]",1781,[],"['isinstance', 'tf.constant', 'self.sprinkles']",3
utilmy/deeplearning/utils_dl.py:LearningRateDecay:plot,LearningRateDecay:plot,method,0,1,1,4,4.0,0,0,"['self', 'epochs', 'title', 'path']","[None, None, None, None]","[None, None, '""Learning Rate Schedule""', 'None']",1838,[],[],0
utilmy/deeplearning/utils_dl.py:StepDecay:__init__,StepDecay:__init__,method,6,6,6,66,11.0,0,0,"['self', 'init_lr', 'factor', 'drop_every']","[None, None, None, None]","[None, '0.01', '0.25', '5']",1961,[],[],0
utilmy/deeplearning/utils_dl.py:StepDecay:__call__,StepDecay:__call__,method,3,50,26,223,4.46,0,1,"['self', 'epoch']","[None, None]","[None, None]",1967,[],['np.exp'],1
utilmy/deeplearning/utils_dl.py:DFC_VAE:__init__,DFC_VAE:__init__,method,9,10,10,157,15.7,0,0,"['self', 'latent_dim', 'class_dict']","[None, None, None]","[None, None, None]",1992,[],"['super', 'make_encoder', 'make_decoder', 'make_classifier']",4
utilmy/deeplearning/utils_dl.py:DFC_VAE:encode,DFC_VAE:encode,method,4,8,6,95,11.88,0,0,"['self', 'x']","[None, None]","[None, None]",2000,[],['tf.split'],1
utilmy/deeplearning/utils_dl.py:DFC_VAE:reparameterize,DFC_VAE:reparameterize,method,4,7,6,84,12.0,0,0,"['self', 'z_mean', 'z_logsigma']","[None, None, None]","[None, None, None]",2004,[],['tf.exp'],1
utilmy/deeplearning/utils_dl.py:DFC_VAE:decode,DFC_VAE:decode,method,6,10,8,104,10.4,0,1,"['self', 'z', 'apply_sigmoid']","[None, None, None]","[None, None, 'False']",2008,[],"['self.decoder', 'tf.sigmoid']",2
utilmy/deeplearning/utils_dl.py:DFC_VAE:call,DFC_VAE:call,method,18,26,23,222,8.54,0,0,"['self', 'x', 'training', 'mask', 'y_label_list']","[None, None, None, None, None]","[None, None, 'True', 'None', ' None']",2015,[],"['self.encode', 'self.reparameterize', 'self.decode', 'self.classifier']",4
utilmy/viz/vizhtml.py:test1,test1,function,8,20,20,265,13.25,0,1,['verbose'],[None],['False'],86,[],"['htmlDoc', 'doc.add_css', 'doc.table', 'doc.print', 'doc.save']",5
utilmy/viz/vizhtml.py:test2,test2,function,6,7,7,40,5.71,0,0,['verbose'],[None],['False'],100,"['    """"""\n', '      # pip install --upgrade utilmy\n', '      from util.viz import vizhtml as vi\n', '      vi.test2()\n', '    """"""\n']",['vi.test2'],1
utilmy/viz/vizhtml.py:mlpd3_add_tooltip,mlpd3_add_tooltip,function,3,10,10,147,14.7,0,0,"['fig', 'points', 'labels']","[None, None, None]","[None, None, None]",569,[],['mpld3_TopToolbar'],1
utilmy/viz/vizhtml.py:pd_plot_scatter_get_data,pd_plot_scatter_get_data,function,39,143,88,951,6.65,1,1,"['df0', 'colx', 'coly', 'collabel', 'colclass1', 'colclass2', 'nmax', '**kw']","['pd.DataFrame', ' str', ' str', ' str', ' str', ' str', ' int', None]","[None, 'None', 'None', 'None', 'None', 'None', '20000', None]",577,[],"['min', 'len', 'df0.sample', 'range', 'hash', 'np.arange']",6
utilmy/viz/vizhtml.py:pd_plot_scatter_matplot,pd_plot_scatter_matplot,function,28,145,95,1194,8.23,0,1,"['df', 'colx', 'coly', 'collabel', 'colclass1', 'colclass2', 'cfg', 'mode', 'save_path', 'verbose', '**kw']","['pd.DataFrame', ' str', ' str', ' str', ' str', ' str', ' dict ', None, ' str', None, None]","[None, 'None', 'None', 'None', 'None', 'None', ' {}', ""'d3'"", ""''"", 'True', None]",622,[],"['Box', 'cc.get', 'pd_plot_scatter_get_data', 'plt.subplots', 'ax.margins', 'ax.scatter', 'ax.grid', 'ax.set_aspect', 'ax.tick_params', 'len', 'plt.savefig', 'mpld3_TopToolbar', 'mpld3.fig_to_html']",13
utilmy/viz/vizhtml.py:pd_plot_histogram_matplot,pd_plot_histogram_matplot,function,25,55,45,514,9.35,1,2,"['df', 'col', 'colormap', 'title', 'nbin', 'q5', 'q95', 'nsample', 'save_img', 'xlabel', 'ylabel', 'verbose', '**kw']","['pd.DataFrame', ' str', 'str', ' str', None, None, None, None, ' str', ' str', ' str', None, None]","[None, ""''"", ""'RdYlBu'"", ""''"", '20.0', '0.005', '0.995', '-1', '""""', 'None', 'None', 'True', None]",695,"['    """"""\n', '    fig = plt.figure()\n', '    ax = fig.add_subplot(111)\n', ""    ax.hist(df[config['x']].values,\n"", ""    bins=config['bins'], color='red', alpha=0.5)\n"", ""    ax.set_xlabel(config['x'])\n"", ""    ax.set_ylabel(config['y'])\n"", ""    ax.set_title(config['title'])\n"", ""    ax.set_xlim(config['xlim'])\n"", ""    ax.set_ylim(config['ylim'])\n"", '    return fig\n', '    """"""\n']","['dfi.quantile', 'plt.figure', 'plt.hist', 'enumerate', 'plt.setp', 'cm', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'len', 'os.makedirs', 'plt.savefig', 'print']",13
utilmy/viz/vizhtml.py:pd_plot_tseries_matplot,pd_plot_tseries_matplot,function,42,114,83,1053,9.24,2,3,"['df', 'plot_type', 'coly1', 'coly2', '8', '4)', 'spacing', 'verbose', '**kw)']","['pd.DataFrame', ' str', ' list ', ' list ', None, None, None, None, '']","[None, 'None', ' []', ' []', None, None, '0.1', 'True', None]",738,"['    """"""\n', '    """"""\n']","['plt.figure', 'len', 'getattr', 'num_colors=len', 'df.plot', 'mpld3.fig_to_html', 'ax.set_ylabel', 'range', 'ax.get_legend_handles_labels', 'ax.twinx', 'ax_new.set_ylabel', 'ax_new.get_legend_handles_labels', 'ax.legend']",13
utilmy/viz/vizhtml.py:mpld3_server_start,mpld3_server_start,function,1,5,5,24,4.8,0,0,[],[],[],797,[],['mpld3.show'],1
utilmy/viz/vizhtml.py:pd_plot_highcharts,pd_plot_highcharts,function,10,21,20,340,16.19,0,0,['df'],[None],[None],808,"['    """"""\n', '    # Basic line plot\n', '   chart = serialize(df, render_to=""my-chart"", title=""My Chart"")\n', '   # Basic column plot\n', '   chart = serialize(df, render_to=""my-chart"", title=""Test"", kind=""bar"")\n', '   # Plot C on secondary axis\n', '   chart = serialize(df, render_to=""my-chart"", title=""Test"", secondary_y = [""C""])\n', '   # Plot on a 1000x700 div\n', '   chart = serialize(df, render_to=""my-chart"", title=""Test"", figsize = (1000, 700))\n', '    """"""\n']","['pandas_highcharts.serialize', 'Highcharts.StockChart']",2
utilmy/viz/vizhtml.py:pd_plot_scatter_highcharts,pd_plot_scatter_highcharts,function,64,247,164,1889,7.65,2,2,"['df0', 'colx', 'coly', 'collabel', 'colclass1', 'colclass2', 'colclass3', 'nsample', 'cfg', 'mode', 'save_img', 'verbose', '**kw']","['pd.DataFrame', 'str', 'str', ' str', ' str', ' str', ' str', None, 'dict', None, None, None, None]","[None, 'None', 'None', 'None', 'None', 'None', 'None', '10000', '{}', ""'d3'"", ""''"", 'True', None]",831,"['    """""" Plot Highcharts X=Y Scatter\n', '    from utilmy.viz import vizhtml\n', '    vizhtml.pd_plot_scatter_highcharts(df, colx:str=None, coly:str=None, collabel=None,\n', '                               colclass1=None, colclass2=None, colclass3=None, nsample=10000,\n', ""                               cfg:dict={}, mode='d3', save_img=False,  verbose=True )\n"", '    """"""\n']","['Box', 'cc.get', 'print', 'min', 'len', 'df0.sample', 'hash', 'vmax=max', 'np.arange', 'str', 'Highchart', 'chart.set_dict_options', 'float', 'int', 'range', 'chart.add_data_set', 'chart.buildcontent']",17
utilmy/viz/vizhtml.py:pd_plot_tseries_highcharts,pd_plot_tseries_highcharts,function,36,213,141,1732,8.13,2,1,"['df', 'coldate', 'date_format', 'coly1', 'coly2', 'figsize', 'title', 'xlabel', 'y1label', 'y2label', 'cfg', 'mode', 'save_img', 'verbose', '**kw']","[None, 'str', 'str', 'list ', 'list ', 'tuple ', 'str', 'str', 'str', 'str', 'dict', None, None, None, None]","[None, 'None', ""'%m/%d/%Y'"", '[]', '[]', '  None', 'None', 'None', 'None', 'None', '{}', ""'d3'"", '""""', 'True', None]",914,"[""    '''\n"", '        function to return highchart json cord for time_series.\n', '        input parameter\n', '        df : panda dataframe on which you want to apply time_series\n', '        coly1: column name for y-axis one\n', '        coly2: column name for y-axis second\n', '        xlabel : label of x-axis\n', '        cols_y1label : label for yaxis 1\n', '        cols_y2label : label for yaxis 2\n', '        date_format : %m for moth , %d for day and %Y for Year.\n', ""    '''\n""]","['Box', 'cc.get', 'str', 'pd.to_datetime', 'Highchart', 'H.set_dict_options', 'float', 'range', 'H.add_data_set', 'H.buildcontent']",10
utilmy/viz/vizhtml.py:pd_plot_histogram_highcharts,pd_plot_histogram_highcharts,function,25,128,98,1192,9.31,0,0,"['df', 'colname', 'binsNumber', 'binWidth', 'title', 'xaxis_label', 'yaxis_label', 'cfg', 'mode', 'save_img', 'show', 'verbose', '**kw']","['pd.DataFrame', 'str', None, None, 'str', 'str', 'str', 'dict', None, None, None, None, None]","[None, 'None', 'None', 'None', '""""', ' ""x-axis""', '""y-axis""', '{}', ""'d3'"", '""""', 'False', 'True', None]",988,"[""    ''' function to return highchart json code for histogram.\n"", '        input parameter\n', '        df : panda dataframe on which you want to apply histogram\n', '        colname : column name from dataframe in which histogram will apply\n', '        xaxis_label: label for x-axis\n', '        yaxis_label: label for y-axis\n', '        binsNumber: Number of bin in bistogram.\n', '        binWidth : width of each bin in histogram\n', '        title : title of histogram\n', '        cols_y2label : label for yaxis 2\n', '        date_format : %m for moth , %d for day and %Y for Year.\n', '\n', ""        df        = data['housing.csv']\n"", '        html_code = pd_plot_histogram_hcharts(df,colname=""median_income"",xaxis_label= ""x-axis"",yaxis_label=""y-axis"",cfg={}, mode=\'d3\', save_img=False)\n', '        # highcharts_show_chart(html_code)\n', ""    '''\n""]","['Box', 'cc.get', 'str']",3
utilmy/viz/vizhtml.py:html_show_chart_highchart,html_show_chart_highchart,function,14,19,17,194,10.21,0,1,"['html_code', 'verbose']","[None, None]","[None, 'True']",1071,[],"['Highchart', 'hc.buildhtmlheader', 'print', 'display']",4
utilmy/viz/vizhtml.py:html_show,html_show,function,15,28,21,300,10.71,0,1,"['html_code', 'verbose']","[None, None]","[None, 'True']",1083,[],"['Highchart', 'hc.buildhtmlheader', 'print', 'display', 'html_show']",5
utilmy/viz/vizhtml.py:images_to_html,images_to_html,function,22,42,36,337,8.02,1,1,"['dir_input', 'title', 'verbose']","[None, None, None]","['""*.png""', '""""', 'False']",1093,"['    """"""\n', '        images_to_html( model_path + ""/graph_shop_17_past/*.png"" , model_path + ""shop_17.html"" )\n', '    """"""\n']","['glob.glob', 'flist.sorted', 'print', 'open', 'fp2.read', 'base64.b64encode']",6
utilmy/viz/vizhtml.py:colormap_get_names,colormap_get_names,function,24,144,131,1397,9.7,3,0,[],[],[],1115,[],"['np.linspace', 'np.vstack', 'plot_color_gradients', 'len', 'plt.subplots', 'fig.subplots_adjust', 'zip', 'ax.imshow', 'ax.text', 'ax.set_axis_off', 'cmaps.items', 'plt.show']",12
utilmy/viz/vizhtml.py:pd_plot_network,pd_plot_network,function,50,137,106,1488,10.86,3,4,"['df', 'cola', 'colb', 'coledge', 'colweight', 'html_code']","['pd.DataFrame', ' str', ' str', ' str', ' str', 'bool ']","[None, ""'col_node1'"", ""'col_node2'"", ""'col_edge'"", '""weight""', ' True']",1165,"['    """"""\n', '        https://pyviz.org/tools.html\n', '    """"""\n']","['convert_to_networkx', 'nx.Graph', 'df.iterrows', 'g.add_edge', 'nx.draw', 'draw_graph', 'net.Network', 'networkx_graph.nodes', 'pyvis_graph.add_node', 'networkx_graph.edges', 'pyvis_graph.add_edge', 'str', 'pyvis_graph.show_buttons', 'pyvis_graph.show', 'extract_text', 're.findall', 'open', 'f.read']",18
utilmy/viz/vizhtml.py:help_get_codesource,help_get_codesource,function,7,16,15,181,11.31,0,0,['func'],[None],[None],1327,"['    """""" Extract code source from func name\n', '    """"""\n']","['len', 'inspect.getsourcelines']",2
utilmy/viz/vizhtml.py:zz_css_get_template,zz_css_get_template,function,33,121,79,1017,8.4,0,3,['css_name'],['str'],"[' ""A4_size""']",1355,[],"['rgba', 'rgb']",2
utilmy/viz/vizhtml.py:zz_test_get_random_data,zz_test_get_random_data,function,10,38,30,290,7.63,1,0,['n'],[None],['100'],1394,[],"['pd.DataFrame', 'size=len']",2
utilmy/viz/vizhtml.py:zz_pd_plot_histogram_highcharts_old,zz_pd_plot_histogram_highcharts_old,function,19,64,56,533,8.33,0,0,"['df', 'col', 'figsize', 'title', 'cfg', 'mode', 'save_img']","[None, None, None, None, 'dict', None, None]","[None, None, 'None', 'None', '{}', ""'d3'"", ""''""]",1408,[],"['Box', 'cc.get', 'np.histogram', 'range', 'hist.tolist', 'pd_plot_histogram_highcharts_base']",6
utilmy/viz/vizhtml.py:mpld3_TopToolbar,mpld3_TopToolbar,class,37,72,60,623,8.65,0,2,[],[],[],542,[],[],0
utilmy/viz/vizhtml.py:mpld3_TopToolbar:__init__,mpld3_TopToolbar:__init__,method,1,3,3,32,10.67,0,0,['self'],[None],[None],565,[],[],0
utilmy/viz/embedding.py:log,log,function,1,2,2,23,11.5,0,0,['*s'],[None],[None],34,[],['print'],1
utilmy/viz/embedding.py:embedding_load_word2vec,embedding_load_word2vec,function,29,74,57,561,7.58,1,3,"['model_vector_path', 'nmax ']","[None, None]","['""model.vec""', ' 500']",256,[],"['isvalid', 'KeyedVectors.load_word2vec_format', 'len', 'list', 'OrderedDict', 'np.zeros', 'enumerate', 'pd.DataFrame']",8
utilmy/viz/embedding.py:embedding_load_parquet,embedding_load_parquet,function,19,50,43,261,5.22,1,1,"['path', 'nmax ']","[None, None]","['""df.parquet""', ' 500']",286,[],"['pd.read_parquet', 'len', 'np.vstack', 'enumerate']",4
utilmy/viz/embedding.py:tokenize_text,tokenize_text,function,1,20,15,126,6.3,0,0,['text'],[None],[None],309,[],['nlp'],1
utilmy/viz/embedding.py:run,run,function,3,6,6,72,12.0,0,0,"['dir_in', 'dir_out', 'nmax']","[None, None, None]","['""in/model.vec""', '""ztmp/""', '100']",323,[],"['vizEmbedding', 'myviz.run_all']",2
utilmy/viz/embedding.py:vizEmbedding,vizEmbedding,class,96,488,308,4895,10.03,7,9,[],[],[],50,[],[],0
utilmy/viz/embedding.py:vizEmbedding:__init__,vizEmbedding:__init__,method,7,8,8,73,9.12,0,0,"['self', 'path', 'num_clusters', 'sep', 'config']","[None, None, None, None, 'dict']","[None, '""myembed.parquet""', '5', '"";""', 'None']",51,"['        """"""\n', '           self = Box({})\n', '           self.path = ""C:/D/gitdev/cpa/data/model.vec""\n', '\n', '           from utilmy.viz.embedding import vizEmbedding\n', '           myviz = vizEmbedding(path = ""C:/D/gitdev/cpa/data/model.vec"")\n', '           myviz.run_all(nsample=100)\n', '\n', '\n', ""           myviz.dim_reduction(mode='mds')\n"", '           myviz.create_visualization(dir_out=""ztmp/vis/"")        \n', '        \n', '\n', '        """"""\n']",[],0
utilmy/viz/embedding.py:vizEmbedding:run_all,vizEmbedding:run_all,method,3,12,12,196,16.33,0,0,"['self', 'mode', 'col_embed', 'ndim', 'nmax', 'dir_out']","[None, None, None, None, None, None]","[None, '""mds""', ""'embed'"", '2', ' 5000', '""ztmp/""']",71,[],"['self.dim_reduction', 'self.create_clusters', 'self.create_visualization']",3
utilmy/viz/embedding.py:vizEmbedding:dim_reduction,vizEmbedding:dim_reduction,method,28,127,103,1420,11.18,0,5,"['self', 'mode', 'col_embed', 'ndim', 'nmax', 'dir_out']","[None, None, None, None, None, None]","[None, '""mds""', ""'embed'"", '2', ' 5000', 'None']",77,[],"['embedding_load_word2vec', 'embedding_load_parquet', 'cosine_similarity', 'MDS', 'mds.fit_transform', 'UMAP', 'clf.fit_transform', 'os.makedirs', 'pd.DataFrame']",9
utilmy/viz/embedding.py:vizEmbedding:create_clusters,vizEmbedding:create_clusters,method,11,29,25,292,10.07,1,1,"['self', 'after_dim_reduction']","[None, None]","[None, 'True']",128,[],"['KMeans', 'km.fit', 'range']",3
utilmy/viz/embedding.py:vizEmbedding:create_visualization,vizEmbedding:create_visualization,method,41,245,140,2135,8.71,6,3,"['self', 'dir_out', 'mode', 'cols_label', 'show_server', '**kw']","[None, None, None, None, None, None]","[None, '""ztmp/""', ""'d3'"", 'None', 'False', None]",143,"['        """"""\n', '\n', '        """"""\n']","['os.makedirs', 'text_label_and_text.append', 'pd.DataFrame', 'df.to_parquet', 'df.groupby', 'plt.subplots', 'ax.margins', 'ax.plot', 'ax.set_aspect', 'ax.tick_params', 'ax.legend', 'range', 'ax.text', 'plt.savefig', 'TopToolbar', 'mpld3.save_html', 'os.system', 'mpld3.show']",18
utilmy/viz/embedding.py:vizEmbedding:draw_hiearchy,vizEmbedding:draw_hiearchy,method,9,30,27,358,11.93,0,0,['self'],[None],[None],242,[],"['ward', 'plt.subplots', 'dendrogram', 'plt.tick_params', 'plt.tight_layout', 'plt.savefig']",6
utilmy/zarchive/py2to3/fast_parallel.py:task_summary,task_summary,function,4,17,17,142,8.35,1,0,['tasks'],[None],[None],76,[],"['print', 'enumerate', 't.get']",3
utilmy/zarchive/py2to3/fast_parallel.py:task_progress,task_progress,function,8,26,23,161,6.19,1,1,['tasks'],[None],[None],83,"[""  ''' Monitor progress '''\n""]","['np.mean', 'print']",2
utilmy/zarchive/py2to3/fast_parallel.py:task_find_best,task_find_best,function,3,11,11,80,7.27,0,0,"['tasks', 'n_top']","[None, None]","[None, '5']",94,"['    """"""Compute the best score of tasks_folder""""""\n']","['t.ready', 'sorted']",2
utilmy/zarchive/py2to3/fast_parallel.py:task_parallel_job_01,task_parallel_job_01,function,19,33,32,336,10.18,0,0,"['name', 'param', 'datadict']","[None, None, None]","[None, None, None]",101,"[""   ''' Sample task run in Parallel '''\n""]","['os.chdir', 'util.a_run_ipython', 'str']",3
utilmy/zarchive/py2to3/filelock.py:FileLock,FileLock,class,41,133,94,1246,9.37,2,6,[],[],[],36,[],[],0
utilmy/zarchive/py2to3/filelock.py:FileLock:__init__,FileLock:__init__,method,11,27,23,279,10.33,1,1,"['self', 'protected_file_path', 'timeout', 'delay', 'lock_file_contents']","[None, None, None, None, None]","[None, None, 'None', '1', 'None']",53,"['        """""" Prepare the file locker. Specify the file to lock and optionally\n', '            the maximum timeout and the delay between each attempt to lock.\n', '        """"""\n']",[],0
utilmy/zarchive/py2to3/filelock.py:FileLock:locked,FileLock:locked,method,2,2,2,20,10.0,0,0,['self'],[None],[None],67,"['        """"""\n', '        Returns True iff the file is owned by THIS FileLock instance.\n', '        (Even if this returns false, the file could be owned by another FileLock instance, possibly in a different thread or process).\n', '        """"""\n']",[],0
utilmy/zarchive/py2to3/filelock.py:FileLock:available,FileLock:available,method,2,3,3,38,12.67,0,0,['self'],[None],[None],74,"['        """"""\n', '        Returns True iff the file is currently available to be locked.\n', '        """"""\n']",[],0
utilmy/zarchive/py2to3/filelock.py:FileLock:acquire,FileLock:acquire,method,18,55,48,410,7.45,1,3,"['self', 'blocking']","[None, None]","[None, 'True']",80,"['        """""" Acquire the lock, if possible. If the lock is in use, and `blocking` is False, return False.\n', '            Otherwise, check again every `self.delay` seconds until it either gets the lock or\n', '            exceeds `timeout` number of seconds, in which case it raises an exception.\n', '        """"""\n']","['time.time', 'os.open', 'os.fdopen', 'f.write', 'FileLock.FileLockException', 'time.sleep']",6
utilmy/zarchive/py2to3/filelock.py:FileLock:release,FileLock:release,method,2,3,3,45,15.0,0,0,['self'],[None],[None],106,"['        """""" Get rid of the lock by deleting the lockfile.\n', '            When working in a `with` statement, this gets automatically\n', '            called at the end.\n', '        """"""\n']",['os.unlink'],1
utilmy/zarchive/py2to3/filelock.py:FileLock:__enter__,FileLock:__enter__,method,2,3,3,25,8.33,0,0,['self'],[None],[None],115,"['        """""" Activated when used in the with statement.\n', '            Should automatically acquire a lock to be used in the with block.\n', '        """"""\n']",['self.acquire'],1
utilmy/zarchive/py2to3/filelock.py:FileLock:__exit__,FileLock:__exit__,method,1,1,1,14,14.0,0,0,"['self', 'type', 'value', 'traceback']","[None, None, None, None]","[None, None, None, None]",123,"['        """""" Activated at the end of the with statement.\n', ""            It automatically releases the lock if it isn't locked.\n"", '        """"""\n']",['self.release'],1
utilmy/zarchive/py2to3/filelock.py:FileLock:__del__,FileLock:__del__,method,2,3,3,32,10.67,0,1,['self'],[None],[None],130,"['        """""" Make sure this ``FileLock`` instance doesn\'t leave a .lock file\n', '            lying around.\n', '        """"""\n']",['self.release'],1
utilmy/zarchive/py2to3/filelock.py:FileLock:purge,FileLock:purge,method,3,7,6,70,10.0,0,1,['self'],[None],[None],137,"['        """"""\n', '        For debug purposes only.  Removes the lock file from the hard disk.\n', '        """"""\n']",['self.release'],1
utilmy/zarchive/py2to3/excel.py:get_workbook_name,get_workbook_name,function,3,5,5,48,9.6,0,0,[],[],[],9,"['    """"""Writes the name of the Workbook into Range(""D3"") of Sheet 1""""""\n']","['Workbook.caller', 'Range']",2
utilmy/zarchive/py2to3/excel.py:double_sum,double_sum,function,1,4,4,13,3.25,0,0,"['x', 'y']","[None, None]","[None, None]",16,"['    """"""Returns twice the sum of the two arguments""""""\n']",[],0
utilmy/zarchive/py2to3/excel.py:add_one,add_one,function,5,11,9,40,3.64,2,0,['data'],[None],[None],23,"['    """"""Adds 1 to every cell in Range""""""\n']",[],0
utilmy/zarchive/py2to3/excel.py:matrix_mult,matrix_mult,function,2,2,2,14,7.0,0,0,"['x', 'y']","[None, None]","[None, None]",31,"['    """"""Alternative implementation of Excel\'s MMULT, requires NumPy""""""\n']",['x.dot'],1
utilmy/zarchive/py2to3/excel.py:npdot,npdot,function,0,0,0,0,0.0,0,0,[],[],[],43,[],[],0
utilmy/zarchive/py2to3/util_ml.py:create_weight_variable,create_weight_variable,function,5,7,6,129,18.43,0,0,"['name', 'shape']","[None, None]","[None, None]",11,[],['tf.Variable'],1
utilmy/zarchive/py2to3/util_ml.py:create_bias_variable,create_bias_variable,function,4,6,6,112,18.67,0,0,"['name', 'shape']","[None, None]","[None, None]",17,[],"['tf.constant_initializer', 'tf.Variable']",2
utilmy/zarchive/py2to3/util_ml.py:create_adam_optimizer,create_adam_optimizer,function,2,3,3,70,23.33,0,0,"['learning_rate', 'momentum']","[None, None]","[None, None]",22,[],[],0
utilmy/zarchive/py2to3/util_ml.py:tf_check,tf_check,function,32,91,51,915,10.05,0,0,[],[],[],28,[],"['print', 'tf.constant', 'tf.matmul', 'tf.Session', 'sess.run']",5
utilmy/zarchive/py2to3/util_ml.py:parse_args,parse_args,function,11,26,24,196,7.54,1,1,"['ppa', 'args']","[None, None]","['None', ' {}']",59,[],"['argparse.ArgumentParser', 'args.items', 'ppa.add_argument', 'type=type', 'ppa.parse_args', 'parse_args2']",6
utilmy/zarchive/py2to3/util_ml.py:parse_args2,parse_args2,function,5,133,98,1435,10.79,0,1,['ppa'],[None],['None'],70,[],"['argparse.ArgumentParser', 'e:print', 'ppa.add_argument']",3
utilmy/zarchive/py2to3/util_ml.py:tf_global_variables_initializer,tf_global_variables_initializer,function,27,65,46,866,13.32,3,1,['sess'],[None],['None'],108,"['    """"""Initializes all uninitialized variables in correct order. Initializers\n', ""    are only run for uninitialized variables, so it's safe to run this multiple times.\n"", '    Args:   sess: session to use. Use default session if None.\n', '    """"""\n']","['make_initializer', 'f', 'tf.assign', 'make_noop', 'tf.no_op', 'make_safe_initializer', 'tf.cond', 'tf.get_default_session', 'tf.get_default_graph', 'tf.global_variables', 'g.get_operation_by_name', 'sess.run']",12
utilmy/zarchive/py2to3/util_ml.py:visualize_result,visualize_result,function,21,142,111,1132,7.97,2,0,[],[],[],218,[],"['range', 'enumerate', 'pd.read_csv', 'plt.plot', 'train_loss.tolist', 'plt.legend', 'plt.xlabel', 'plt.ylabel', 'plt.show']",9
utilmy/zarchive/py2to3/util_ml.py:TextLoader,TextLoader,class,65,159,120,2143,13.48,0,1,[],[],[],151,[],[],0
utilmy/zarchive/py2to3/util_ml.py:TextLoader:__init__,TextLoader:__init__,method,15,34,31,491,14.44,0,1,"['self', 'data_dir', 'batch_size', 'seq_length']","[None, None, None, None]","[None, None, None, None]",152,[],"['print', 'self.preprocess', 'self.load_preprocessed', 'self.create_batches', 'self.reset_batch_pointer']",5
utilmy/zarchive/py2to3/util_ml.py:TextLoader:preprocess,TextLoader:preprocess,method,17,34,31,404,11.88,0,0,"['self', 'input_file', 'vocab_file', 'tensor_file']","[None, None, None, None]","[None, None, None, None]",170,[],"['codecs.open', 'f.read', 'collections.Counter', 'sorted', 'list', 'len', 'dict', 'range', 'open', 'pickle.dump', 'np.array', 'np.save']",12
utilmy/zarchive/py2to3/util_ml.py:TextLoader:load_preprocessed,TextLoader:load_preprocessed,method,10,19,19,245,12.89,0,0,"['self', 'vocab_file', 'tensor_file']","[None, None, None]","[None, None, None]",183,[],"['open', 'pickle.load', 'len', 'dict', 'range', 'np.load']",6
utilmy/zarchive/py2to3/util_ml.py:TextLoader:create_batches,TextLoader:create_batches,method,20,40,33,665,16.62,0,0,['self'],[None],[None],191,[],"['np.copy', 'np.split', 'int', 'zip']",4
utilmy/zarchive/py2to3/util_ml.py:TextLoader:next_batch,TextLoader:next_batch,method,6,10,8,87,8.7,0,0,['self'],[None],[None],207,[],[],0
utilmy/zarchive/py2to3/util_ml.py:TextLoader:reset_batch_pointer,TextLoader:reset_batch_pointer,method,1,2,2,14,7.0,0,0,['self'],[None],[None],212,[],[],0
utilmy/zarchive/py2to3/multiprocessfunc.py:multigbm_paralell_func,multigbm_paralell_func,function,34,117,81,913,7.8,1,4,"['nbsimul', 'ww', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'price', 'type1', 'strike', 'cp']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, '0', '0', '1']",20,[],"['np.zeros', 'range', 'np.dot', 'np.multiply', 'np.exp', 'np.cumprod', 'np.maximum', 'sum', 'sum1/float']",9
utilmy/zarchive/py2to3/multiprocessfunc.py:func,func,function,6,10,10,56,5.6,1,0,"['val', 'lock']","[None, None]","[None, None]",73,[],"['range', 'time.sleep']",2
utilmy/zarchive/py2to3/multiprocessfunc.py:multigbm_processfast7,multigbm_processfast7,function,24,48,44,419,8.73,3,0,"['nbsimul', 's0', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'price']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",150,[],"['range', 'np.dot', 'bm_generator', 'np.exp', 'sum', 'sum1/float']",6
utilmy/zarchive/py2to3/multiprocessfunc.py:bm_generator,bm_generator,function,5,9,8,43,4.78,0,1,"['bm', 'dt', 'n', 'type1']","[None, None, None, None]","[None, None, None, None]",177,[],[],0
utilmy/zarchive/py2to3/multiprocessfunc.py:merge,merge,function,6,20,17,106,5.3,1,1,['d2'],[None],[None],211,[],"['time.sleep', 'd2.keys']",2
utilmy/zarchive/py2to3/multiprocessfunc.py:integratenp2,integratenp2,function,10,29,25,184,6.34,1,0,"['its', 'nchunk']","[None, None]","[None, None]",222,[],"['range', 'ne.evaluate']",2
utilmy/zarchive/py2to3/multiprocessfunc.py:integratenp,integratenp,function,27,77,47,454,5.9,2,0,"['its', 'nchunk']","[None, None]","[None, None]",234,[],"['range', 'ne.evaluate', 'integratenp', 'np.sum', 'float']",5
utilmy/zarchive/py2to3/multiprocessfunc.py:integratene,integratene,function,12,44,38,275,6.25,1,0,['its'],[None],[None],247,[],"['range', 'ne.evaluate', 'float']",3
utilmy/zarchive/py2to3/multiprocessfunc.py:parzen_estimation,parzen_estimation,function,7,30,26,185,6.17,2,1,"['x_samples', 'point_x', 'h']","[None, None, None]","[None, None, None]",268,[],"['np.abs', 'len']",2
utilmy/zarchive/py2to3/multiprocessfunc.py:init2,init2,function,2,4,4,13,3.25,0,0,['d'],[None],[None],282,[],[],0
utilmy/zarchive/py2to3/multiprocessfunc.py:init_global1,init_global1,function,2,2,2,37,18.5,0,0,"['l', 'r']","[None, None]","[None, None]",286,[],[],0
utilmy/zarchive/py2to3/multiprocessfunc.py:np_sin,np_sin,function,2,2,2,19,9.5,0,0,['value'],[None],[None],291,[],['np.sin'],1
utilmy/zarchive/py2to3/multiprocessfunc.py:ne_sin,ne_sin,function,2,2,2,27,13.5,0,0,['x'],[None],[None],294,[],['ne.evaluate'],1
utilmy/zarchive/py2to3/multiprocessfunc.py:res_shared2,res_shared2,function,4,16,14,94,5.88,0,0,[],[],[],297,[],['float'],1
utilmy/zarchive/py2to3/multiprocessfunc.py:list_append,list_append,function,3,4,4,54,13.5,1,0,"['count', 'id', 'out_list']","[None, None, None]","[None, None, None]",261,"['\t""""""Creates an empty list and then appends a random number to the list \'count\' number of times. A CPU-heavy operation!""""""\n']",['range'],1
utilmy/zarchive/py2to3/function_custom.py:mapping_calc_risk_elvis_v03,mapping_calc_risk_elvis_v03,function,4,9,9,63,7.0,0,0,"['ss', 'tr', 't', 'riskout']","[None, None, None, None]","[None, None, None, None]",18,"[""     ''' ss: state,  tr:trigger level,  risk0: previous risk value '''\n""]",['mapping_calc_risk_v01'],1
utilmy/zarchive/py2to3/function_custom.py:mapping_calc_risk_v02,mapping_calc_risk_v02,function,18,190,75,704,3.71,0,2,"['ss', 'tr', 't', 'risk0']","[None, None, None, None]","[None, None, None, None]",31,"[""     ''' ss: state,  tr:trigger level,  risk0: previous risk value '''\n""]",[],0
utilmy/zarchive/py2to3/function_custom.py:mapping_calc_risk_v01,mapping_calc_risk_v01,function,19,130,56,532,4.09,0,2,"['ss', 'tr', 't', 'risk0']","[None, None, None, None]","[None, None, None, None]",72,"[""     ''' ss: state,  tr:trigger level,  risk0: previous risk value '''\n""]",[],0
utilmy/zarchive/py2to3/function_custom.py:mapping_risk_ww_v01,mapping_risk_ww_v01,function,12,56,29,245,4.38,0,1,"['risk', 'wwmat', 'ww2']","[None, None, None]","[None, None, None]",98,[],"['np.sum', 'np.array']",2
utilmy/zarchive/py2to3/function_custom.py:mapping_calc_risk_v00,mapping_calc_risk_v00,function,16,68,45,286,4.21,0,1,"['self', 'ss', 'tr', 't', 'risk0']","[None, None, None, None, None]","[None, None, None, None, None]",107,[],[],0
utilmy/zarchive/py2to3/function_custom.py:getweight,getweight,function,7,13,12,70,5.38,0,1,"['ww', 'size', '3']","[None, None, None]","[None, '(9', None]",133,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/py2to3/function_custom.py:fun_obj,fun_obj,function,6,11,11,120,10.91,0,0,"['vv', 'ext']","[None, None]","[None, None]",139,[],"['pf.folio_volta', 'int', 'pf.volhisto_fromprice', 'len']",4
utilmy/zarchive/py2to3/datanalysis.py:pd_filter_column,pd_filter_column,function,12,25,22,218,8.72,1,2,"['df_client_product', 'filter_val', 'iscol']","[None, None, None]","[None, '[]', '1']",52,"[""   '''\n"", '   # Remove Columns where Index Value is not in the filter_value\n', ""   # filter1= X_client['client_id'].values\n"", '   :param df_client_product:\n', '   :param filter_val:\n', '   :param iscol:\n', '   :return:\n', ""   '''\n""]",['col_delete1.append'],1
utilmy/zarchive/py2to3/datanalysis.py:pd_missing_show,pd_missing_show,function,0,1,1,4,4.0,0,0,[],[],[],69,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:pd_validation_struct,pd_validation_struct,function,0,1,1,4,4.0,0,0,[],[],[],98,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:pd_checkpoint,pd_checkpoint,function,0,1,1,4,4.0,0,0,[],[],[],108,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:xl_setstyle,xl_setstyle,function,32,63,47,610,9.68,2,0,['file1'],[None],[None],167,"["" '''\n"", '   http://openpyxl.readthedocs.io/en/default/styles.html#cell-styles-and-named-styles\n', '  import openpyxl.styles.builtins  as bi\n', '  import openpyxl.styles.builtins\n', '\n', ""  col = ws.column_dimensions['A']\n"", '  col.font = Font(bold=True)\n', '\n', ""  for cell in ws['A'] + ws[1]:\n"", ""    cell.style = 'data01'\n"", '\n', '  bd = Side(style=\'thick\', color=""000000"")\n', '  highlight.border = BORDER_NONE\n', '  from openpyxl.styles import\n', "" '''\n""]","['load_workbook', 'print', 'named_styles.NamedStyle', 'Font', 'wb.get_sheet_names', 'wb.get_sheet_by_name', 'xrange', 'wb.save']",8
utilmy/zarchive/py2to3/datanalysis.py:xl_val,xl_val,function,7,14,13,107,7.64,0,0,"['ws', 'colj', 'rowi']","[None, None, None]","[None, None, None]",207,[],"['ws[gcol', 'str']",2
utilmy/zarchive/py2to3/datanalysis.py:isnull,isnull,function,3,5,5,20,4.0,0,0,['x'],[None],[None],214,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:xl_get_rowcol,xl_get_rowcol,function,8,44,37,222,5.05,1,1,"['ws', 'i0', 'j0', 'imax', 'jmax']","[None, None, None, None, None]","[None, None, None, None, None]",217,[],"['xrange', 'isnull', 'rmat.append']",3
utilmy/zarchive/py2to3/datanalysis.py:pd_stack_dflist,pd_stack_dflist,function,8,29,24,155,5.34,1,1,['df_list'],[None],[None],227,[],"['enumerate', 'df0.append', 'print']",3
utilmy/zarchive/py2to3/datanalysis.py:xl_getschema,xl_getschema,function,73,258,174,1953,7.57,4,8,"['dirxl', 'filepattern', 'dirlevel', 'outfile']","[None, None, None, None]","['""""', ""'*.xlsx'"", '1', ""'.xlsx'""]",237,"["" '''Take All excel in a folder and provide Table, Column Schema, type into master file\n"", "" '''\n""]","['xl_is_data_block_start', 'isnull', 'xl_find_start_block', 'xrange', '_xl_getschema', 'util.os_file_getname', 'load_workbook', 'print', 'wb.get_sheet_names', 'wb.get_sheet_by_name', 'gcol', 'str', 'np.array', 'unicode', 'np.column_stack', 'df_list.append', 'pd_stack_dflist', 'util.os_file_listall', 'enumerate', 'df0.append', 'util.pd_toexcel']",21
utilmy/zarchive/py2to3/datanalysis.py:str_to_unicode,str_to_unicode,function,4,16,12,99,6.19,0,2,"['x', 'encoding']","[None, None]","[None, ""'utf-8'""]",318,[],"['isinstance', 'unicode']",2
utilmy/zarchive/py2to3/datanalysis.py:csv_dtypes_getdict,csv_dtypes_getdict,function,6,20,16,122,6.1,0,1,"['df', 'csvfile']","[None, None]","['None', 'None']",327,[],['pd.read_csv'],1
utilmy/zarchive/py2to3/datanalysis.py:csv_fast_processing,csv_fast_processing,function,0,0,0,0,0.0,0,0,[],[],[],334,"[""   '''\n"", '   http://word.bitly.com/post/74069870671/optimizing-text-processing\n', '\n', 'import sys\n', 'from collections import defaultdict\n', 'OUT_FILES = defaultdict(dict)\n', '\n', 'open_outfiles()  # open all files I could possibly need\n', '\n', 'for line in sys.stdin:\n', '    # 1. parse line for account_id and metric_type\n', ""    key = line.split(',')\n"", '    account_id = key[ACCOUNT_ID_INDEX][1:] # strip leading quote\n', '\n', '    # 2. write to appropriate file for account_id and metric_type\n', '    OUT_FILES[account_id][key[METRIC_TYPE_INDEX]].write(line)\n', '\n', '   close_outfiles()  # close all the files we opened\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/py2to3/datanalysis.py:csv_col_schema_toexcel,csv_col_schema_toexcel,function,87,292,206,2613,8.95,3,8,"['dircsv', 'filepattern', 'outfile', 'returntable', 'maxrow', 'maxcol_pertable', 'maxstrlen']","[None, None, None, None, None, None, None]","['""""', ""'*.csv'"", ""'.xlsx'"", '1', '5000000', '90', ""'U80'""]",356,"["" '''Take All csv in a folder and provide Table, Column Schema, type\n"", ' str(df[col].dtype)  USE str always, otherwise BIG Issue\n', '\n', 'METHOD FOR Unicode / ASCII issue\n', ""1. Decode early:  Decode to <type 'unicode'> ASAP\n"", ""    df['PREF_NAME']=       df['PREF_NAME'].apply(to_unicode)\n"", '2. Unicode everywhere\n', ""3. Encode late :f = open('/tmp/ivan_out.txt','w')\n"", ""                f.write(ivan_uni.encode('utf-8'))\n"", "" '''\n""]","['util.os_file_listall', 'len', 'np.array', 'enumerate', 'pd.read_csv', 'str', 'util.pd_toexcel', 'df.apply', 'np.zeros', 'xrange', 'float', 'gc.collect', 'pd.DataFrame', 'util.save', 'outfile.replace', 'print']",16
utilmy/zarchive/py2to3/datanalysis.py:csv_col_get_dict_categoryfreq,csv_col_get_dict_categoryfreq,function,39,81,63,551,6.8,4,4,"['dircsv', 'filepattern', 'category_cols', 'maxline', 'fileencoding']","[None, None, None, None, None]","[None, '""*.csv""', '[]', '-1', '""utf-8""']",447,"[""  ''' Find Category Freq in large CSV Transaction Column   '''\n""]","['datetime.now', 'defaultdict', 'util.os_file_listall', 'enumerate', 'line.split']",5
utilmy/zarchive/py2to3/datanalysis.py:csv_row_reduce_line,csv_row_reduce_line,function,23,82,62,474,5.78,1,5,"['fromfile', 'tofile', 'condfilter', 'catval_tokeep', 'maxline']","[None, None, None, None, None]","[None, None, None, None, '-1']",472,"[""  ''' Reduce Data Row by filtering on some Category\n"", '    file_category=  in1+ ""offers.csv""\n', '    ncol= 8\n', '    catval_tokeep=[ {} for i in xrange(0, ncol)]\n', '    for i, line in enumerate(open(file_category)):\n', '      ll=  line.split("","")\n', '      catval_tokeep[3][  ll[1] ]  = 1  # Offer_file_col1 --> Transact_file_col_4\n', '      catval_tokeep[4][  ll[3] ] =  1  # Offer_file_col3 --> Transact_file_col_4\n', '\n', '  def condfilter(colk, catval_tokeep) :\n', '    if colk[3] in catval_tokeep[3] or colk[4] in catval_tokeep[4]: return True\n', '    else: return False\n', ""  '''\n""]","['datetime.now', 'open', 'f.next', 'outfile.write', 'enumerate', 'line.split', 'condfilter']",7
utilmy/zarchive/py2to3/datanalysis.py:csv_analysis,csv_analysis,function,0,0,0,0,0.0,0,0,[],[],[],511,"[""   '''\n"", '   https://csvkit.readthedocs.io/en/540/tutorial/1_getting_started.html\n', '\n', '   sudo pip install csvkit\n', '\n', '   :return:\n', ""   '''\n""]",[],0
utilmy/zarchive/py2to3/datanalysis.py:csv_row_reduce_line_manual,csv_row_reduce_line_manual,function,26,79,53,495,6.27,2,4,"['file_category', 'file_transact', 'file_reduced']","[None, None, None]","[None, None, None]",520,"[""  ''' Reduce Data by filtering on some Category '''\n""]","['datetime.now', 'enumerate', 'line.split', 'open', 'outfile.write']",5
utilmy/zarchive/py2to3/datanalysis.py:csv_row_mapreduce,csv_row_mapreduce,function,19,29,26,278,9.59,2,0,"['dircsv', 'outfile', 'type_mapreduce', 'chunk']","[None, None, None, None]","['""""', '""""', ""'sum'"", ' 5000000']",549,"["" '''Take All csv in a folder and provide Table, Column Schema'''\n""]","['util.os_file_listall', 'pd.DataFrame', 'enumerate', 'xrange', 'pd.read_csv']",5
utilmy/zarchive/py2to3/datanalysis.py:csv_pivotable,csv_pivotable,function,13,57,38,576,10.11,1,4,"['dircsv', 'filepattern', 'fileh5', 'leftX', 'topY', 'centerZ', 'mapreduce', 'chunksize', 'tablename']","[None, None, None, None, None, None, None, None, None]","['""""', ""'*.csv'"", ""'.h5'"", ""'col0'"", ""'col2'"", ""'coli'"", ""'sum'"", ' 500000', ""'df'""]",565,"["" ''' return df Pivot Table from series of csv file (transfer to d5 temporary)\n"", '\n', 'Edit: you can groupby/sum from the store iteratively since this ""map-reduces"" over the chunks:\n', '\n', 'reduce(lambda x, y: x.add(y, fill_value=0),\n', ""       (df.groupby().sum() for df in store.select('df', chunksize=50000)))\n"", '\n', "" '''\n""]","['util.pd_h5_fromcsv_tohdfs', 'pd.HDFStore', 'store.select', 'pd.concat']",4
utilmy/zarchive/py2to3/datanalysis.py:csv_bigcompute,csv_bigcompute,function,0,1,1,4,4.0,0,0,[],[],[],594,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:db_getdata,db_getdata,function,0,1,1,4,4.0,0,0,[],[],[],601,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:db_sql,db_sql,function,0,1,1,4,4.0,0,0,[],[],[],604,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:db_meta_add,db_meta_add,function,26,85,61,762,8.96,1,4,"['metadb', 'dbname', 'new_table', '[]']","[None, None, None, None]","[None, None, ""(''"", None]",607,"[""   ''' Create Meta database to store infos on the tables : csv, zip, HFS, Postgres\n"", ""ALL_DB['japancoupon']= {}\n"", ""ALL_DB['japancoupon']['schema']=    df_schema\n"", ""ALL_DB['japancoupon']['df_table_uri']= df_schema_dictionnary\n"", ""ALL_DB['japancoupon']['df_table_columns']= df_schema_dict\n"", '        DBname, db_schema, db_table_uri, db_table_columns(dict_table->colum_list),\n', ""   '''\n""]","['pd_df_todict', 'df.drop_duplicates', 'xrange', 'dict0.setdefault']",4
utilmy/zarchive/py2to3/datanalysis.py:db_meta_find,db_meta_find,function,24,86,67,605,7.03,3,4,"['ALLDB', 'query', 'filter_db', 'filter_table', 'filter_column']","[None, None, None, None, None]","[None, ""''"", '[]', '[]', '[]']",644,"[""  ''' Find string in all the meta table name, column\n"", ""  db_meta_find(ALLDB, query='bottler', filter_db=['cokeon'],   filter_table=['table'], filter_column=['table'] )\n"", '  dbname: should be exact name\n', '  fitler_table: partial match is ok\n', '  fitler_column : partial name is ok\n', '  return   (dbname, meta_table_name,  meta_table_filtered_by_row_containing query)\n', ""  '''\n""]","['ALLDB.items', 'len', 'dbi.items', 'isinstance', 'util.str_match_fuzzy', 'list', 'util.find_fuzzy', 'util.pd_find', 'rs.append']",9
utilmy/zarchive/py2to3/datanalysis.py:col_study_getcategorydict_freq,col_study_getcategorydict_freq,function,17,26,25,402,15.46,1,0,['catedict'],[None],[None],675,"[""  ''' Generate Frequency of category : Id, Freq, Freqin%, CumSum%, ZScore\n"", '      given a dictionnary of category parsed previously\n', ""  '''\n""]","['catedict.items', 'util.pd_array_todataframe', 'df.sort_values', 'np.arange', 'catlist.append']",5
utilmy/zarchive/py2to3/datanalysis.py:col_feature_importance,col_feature_importance,function,0,1,1,4,4.0,0,0,"['Xcol', 'Ytarget']","[None, None]","[None, None]",690,"[""   ''' random forest for column importance '''\n""]",[],0
utilmy/zarchive/py2to3/datanalysis.py:col_study_distribution_show,col_study_distribution_show,function,22,96,71,982,10.23,3,3,"['df', 'col_include', 'col_exclude', ""pars={'binsize'""]","[None, None, None, '']","[None, 'None', 'None', ""{'binsize':20}""]",694,"["" '''  Perfom Full Study of the pandas columns'''\n""]","['np.count_nonzero', 'sorted', 'print', 'len', 'np.max', 'np.min', 'np.median', 'np.mean', 'np.std', 'plot_Y', 'plt.title', 'plt.plot', 'plt.hist', 'plt.show', 'plt.figure', 'np.percentile', 'range']",17
utilmy/zarchive/py2to3/datanalysis.py:col_study_summary,col_study_summary,function,18,57,53,604,10.6,1,3,"['Xmat', '0.0]', 'Xcolname', ""'col2']"", 'Xcolselect', '9]', 'isprint']","[None, None, None, None, None, None, None]","['[0.0', None, ""['col1'"", None, '[9', None, '0']",724,[],"['np.arange', 'len', 'print', 'np.min', 'np.max', 'np.median', 'np.mean', 'np.std', 'colanalysis.append', 'pd.DataFrame', 'np.shape']",11
utilmy/zarchive/py2to3/datanalysis.py:col_pair_plot,col_pair_plot,function,14,34,32,261,7.68,1,1,"['dfX', 'Xcolname_selectlist', 'dfY', 'Ycolname']","[None, None, None, None]","[None, 'None', 'None', 'None']",741,[],"['str', 'plt.scatter', 'plt.autoscale', 'plt.title', 'plt.show']",5
utilmy/zarchive/py2to3/datanalysis.py:col_pair_correl,col_pair_correl,function,0,1,1,4,4.0,0,0,"['Xcol', 'Ytarget']","[None, None]","[None, None]",755,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:col_pair_interaction,col_pair_interaction,function,0,1,1,4,4.0,0,0,"['Xcol', 'Ytarget']","[None, None]","[None, None]",758,"[""   ''' random forest for pairwise interaction '''\n""]",[],0
utilmy/zarchive/py2to3/datanalysis.py:plot_col_pair,plot_col_pair,function,1,4,4,66,16.5,0,0,"['dfX', 'Xcolname_selectlist', 'dfY', 'Ycolname']","[None, None, None, None]","[None, 'None', 'None', 'None']",762,[],['col_pair_plot'],1
utilmy/zarchive/py2to3/datanalysis.py:tf_transform_catlabel_toint,tf_transform_catlabel_toint,function,13,18,18,219,12.17,1,0,['Xmat'],[None],[None],768,"[""   '''\n"", '     # [""paris"", ""paris"", ""tokyo"", ""amsterdam""]  --> 2 ,5,6\n', '     # np.array(le.inverse_transform([2, 2, 1]))\n', '     le = preprocessing.LabelEncoder()\n', '     le.fit([""paris"", ""paris"", ""tokyo"", ""amsterdam""])\n', 'LabelEncoder()\n', 'list(le.classes_)\n', ""['amsterdam', 'paris', 'tokyo']\n"", 'le.transform([""tokyo"", ""tokyo"", ""paris""])\n', 'array([2, 2, 1]...)\n', 'list(le.inverse_transform([2, 2, 1]))\n', ""['tokyo', 'tokyo', 'paris']\n"", ""   '''\n""]","['xrange', 'le.get_params']",2
utilmy/zarchive/py2to3/datanalysis.py:tf_transform_pca,tf_transform_pca,function,3,4,4,78,19.5,0,0,"['Xmat', 'dimpca', 'whiten']","[None, None, None]","[None, '2', 'True']",793,"[""   '''Project ndim data into dimpca sub-space  '''\n""]","['pca=PCA', 'pca.transform']",2
utilmy/zarchive/py2to3/datanalysis.py:plot_distance_heatmap,plot_distance_heatmap,function,19,30,29,363,12.1,0,0,"['Xmat_dist', 'Xcolname']","[None, None]","[None, None]",802,[],"['pd.DataFrame', 'plt.figure', 'fig.add_subplot', 'ax.imshow', 'pyplot.get_cmap', 'ax.set_xlabel', 'ax.set_ylabel', 'ax.set_title', 'plt.colorbar']",9
utilmy/zarchive/py2to3/datanalysis.py:plot_cluster_2D,plot_cluster_2D,function,10,20,20,238,11.9,1,0,"['X_2dim', 'target_class', 'target_names']","[None, None, None]","[None, None, None]",817,"[""   ''' Plot 2d of Clustering Class,\n"", '       X2d: Nbsample x 2 dim  (projection on 2D sub-space)\n', ""   '''\n""]","['target_ids=xrange', 'len', 'pl.figure', 'zip', 'pl.scatter', 'pl.legend', 'pl.show']",7
utilmy/zarchive/py2to3/datanalysis.py:plot_cluster_tsne,plot_cluster_tsne,function,23,53,46,590,11.13,0,3,"['Xmat', 'Xcluster_label', 'metric', 'perplexity', 'ncomponent', 'savefile', 'isprecompute', 'returnval']","[None, None, None, None, None, None, None, None]","[None, 'None', ""'euclidean'"", '50', '2', ""''"", 'False', 'True']",829,"["" '''Plot High dimemnsionnal State using TSNE method\n"", ""   'euclidean, 'minkowski', 'cityblock', 'seuclidean', 'sqeuclidean, 'cosine, 'correlation, 'hamming, 'jaccard, 'chebyshev,\n"", ""   'canberra, 'braycurtis, 'mahalanobis', VI=None) 'yule, 'matching, 'dice, 'kulsinski, 'rogerstanimoto, 'russellrao, 'sokalmichener, 'sokalsneath,\n"", '\n', '   Xtsne= da.plot_cluster_tsne(Xtrain_dist, Xcluster_label=None, perplexity=40, ncomponent=2, isprecompute=True)\n', '\n', '   Xtrain_dist= sci.spatial.distance.squareform(sci.spatial.distance.pdist(Xtrain_d,\n', ""               metric='cityblock', p=2, w=None, V=None, VI=None))\n"", ""   '''\n""]","['np.set_printoptions', 'model.fit_transform', 'np.arange', 'plot_XY']",4
utilmy/zarchive/py2to3/datanalysis.py:plot_cluster_pca,plot_cluster_pca,function,22,51,44,480,9.41,0,3,"['Xmat', 'Xcluster_label', 'metric', 'dimpca', 'whiten', 'isprecompute', 'savefile', 'doreturn']","[None, None, None, None, None, None, None, None]","[None, 'None', ""'euclidean'"", '2', 'True', 'False', ""''"", '1']",857,[],"['PCA', 'model.fit_transform', 'np.zeros', 'plot_XY']",4
utilmy/zarchive/py2to3/datanalysis.py:plot_cluster_hiearchy,plot_cluster_hiearchy,function,29,84,76,941,11.2,1,3,"['Xmat_dist', 'p', 'truncate_mode', 'color_threshold', 'get_leaves', 'orientation', 'labels', 'count_sort', 'distance_sort', 'show_leaf_counts', 'no_plot', 'no_labels', 'leaf_font_size', 'leaf_rotation', 'leaf_label_func', 'show_contracted', 'link_color_func', 'ax', 'above_threshold_color']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, '30', 'None', 'None', 'True', ""'top'"", 'None', 'False', 'False', 'True', 'False', 'False', 'None', 'None', 'None', 'False', 'None', 'None', ""'b'""]",874,[],"['dendrogram', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'zip', 'sum', 'plt.plot', 'plt.annotate', 'plt.axhline']",9
utilmy/zarchive/py2to3/datanalysis.py:plot_distribution_density,plot_distribution_density,function,28,58,52,631,10.88,0,0,"['Xsample', 'kernel', 'N', 'bandwith']","[None, None, None, None]","[None, ""'gaussian'"", '10', '1 / 10.0']",902,[],"['plt.subplots', 'len', 'np.min', 'np.max', 'np.linspace', 'np.ones_like', 'np.ones', 'ax.hist', 'kde.score_samples', 'np.log', 'ax.plot', 'np.exp', 'ax.set_xlim', 'plt.show']",14
utilmy/zarchive/py2to3/datanalysis.py:plot_Y,plot_Y,function,4,7,7,88,12.57,0,0,"['Yval', 'typeplot', 'tsize', 'labels', 'title', 'xlabel', 'ylabel', 'zcolor_label', 'figsize', '6']","[None, None, None, None, None, None, None, None, None, None]","[None, ""'.b'"", 'None', 'None', ""''"", ""''"", ""''"", ""''"", '(8', None]",944,[],"['plt.figure', 'plt.title', 'plt.plot', 'plt.show']",4
utilmy/zarchive/py2to3/datanalysis.py:plot_XY,plot_XY,function,72,197,147,1805,9.16,0,7,"['xx', 'yy', 'zcolor', 'tsize', 'labels', 'title', 'xlabel', 'ylabel', 'zcolor_label', 'figsize', '6']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', ""''"", ""''"", ""''"", ""''"", '(8', None]",950,"[""  '''\n"", '      labels= numpy array, ---> Generate HTML File with the labels interactives\n', '      Color: Plasma\n', ""  '''\n""]","['isinstance', 'np.array', 'np.abs', 'plt.get_cmap', 'np.min', 'plt.subplots', 'fig.set_size_inches', 'fig.set_dpi', 'fig.tight_layout', 'ax1.scatter', 'ax1.set_xlabel', 'ax1.set_ylabel', 'ax1.set_title', 'ax1.grid', 'np.max', 'scatter.set_clim', 'fig.colorbar', 'cb.set_label', 'list', 'mpld3.save_html', 'plt.show', 'util.os_folder_create', 'plt.savefig', 'plot_XY_plotly', 'go.Scatter', 'py.iplot', 'py.plot', 'plot_XY_seaborn', 'sns.set_context', 'sns.set_color_codes', 'sns.color_palette', 'np.unique', 'plt.scatter', 'plt.gca', 'plt.title']",35
utilmy/zarchive/py2to3/datanalysis.py:plot_XY_plotly,plot_XY_plotly,function,15,31,26,261,8.42,0,1,"['xx', 'yy', 'towhere']","[None, None, None]","[None, None, ""'url'""]",1016,"[""  ''' Create Interactive Plotly   '''\n""]","['go.Scatter', 'py.iplot', 'py.plot']",3
utilmy/zarchive/py2to3/datanalysis.py:plot_XY_seaborn,plot_XY_seaborn,function,14,44,42,400,9.09,0,1,"['X', 'Y', 'Zcolor']","[None, None, None]","[None, None, 'None']",1038,[],"['sns.set_context', 'sns.set_color_codes', 'sns.color_palette', 'np.unique', 'plt.scatter', 'plt.gca', 'plt.title']",7
utilmy/zarchive/py2to3/datanalysis.py:optim_is_pareto_efficient,optim_is_pareto_efficient,function,15,35,31,304,8.69,1,2,"['Xmat_cost', 'epsilon', 'ret_boolean']","[None, None, None]","[None, ' 0.01', '1']",1080,"['    """""" Calculate Pareto Frontier of Multi-criteria Optimization program\n', '    c1, c2  has to be minimized : -Sharpe, -Perf, +Drawdown\n', '    :param Xmat_cost: An (n_points, k_costs) array\n', '    :return: A (n_points, ) boolean array, indicating whether each point is Pareto efficient\n', '    """"""\n']","['np.ones', 'enumerate', 'np.any']",3
utilmy/zarchive/py2to3/datanalysis.py:sk_model_auto_tpot,sk_model_auto_tpot,function,21,41,36,560,13.66,0,1,"['Xmat', 'y', 'outfolder', 'model_type', 'train_size', 'generation', 'population_size', 'verbosity']","[None, None, None, None, None, None, None, None]","[None, None, ""'aaserialize/'"", ""'regressor/classifier'"", '0.5', '1', '5', '2']",1098,"[""  ''' Automatic training of Xmat--->Y, Generate SKlearn code in outfile\n"", '      Very Slow Process, use lower number of Sample\n', '  :param Xmat:\n', '  :param y:\n', '  :param outfolder:\n', '  :param model_type:\n', '  :param train_size:\n', '  :param generation: \n', '  :param population_size:\n', '  :param verbosity:\n', '  :return:\n', ""  '''\n""]","['train_test_split', 'TPOTRegressor', 'TPOTClassifier', 'tpot.fit', 'print', 'str', 'tpot.export']",7
utilmy/zarchive/py2to3/datanalysis.py:sk_params_search_best,sk_params_search_best,function,30,61,51,826,13.54,0,3,"['Xmat', 'Ytarget', 'model1', ""param_grid={'alpha'"", '1', '5) }', 'method', ""param_search= {'scoretype'"", ""'cv'"", ""'population_size'"", ""'generations_number'""]","[None, None, None, '', None, None, None, '', '5', '5', '3 }']","[None, None, None, ""{'alpha':  np.linspace(0"", None, None, ""'gridsearch'"", "" {'scoretype':'r2'"", None, None, None]",1126,"[""  '''\n"", '   genetic: population_size=5, ngene_mutation_prob=0.10,,gene_crossover_prob=0.5, tournament_size=3,  generations_number=3\n', '\n', '  :param Xmat:\n', '  :param Ytarget:\n', '  :param model1:\n', '  :param param_grid:\n', '  :param method:\n', '  :param param_search:\n', '  :return:\n', ""  '''\n""]","['make_scorer', 'GridSearchCV', 'grid.fit', 'EvolutionaryAlgorithmSearchCV', 'cv=StratifiedKFold', 'cv.fit']",6
utilmy/zarchive/py2to3/datanalysis.py:sk_distribution_kernel_bestbandwidth,sk_distribution_kernel_bestbandwidth,function,8,17,17,176,10.35,0,0,['kde'],[None],[None],1193,"["" '''Find best Bandwidht for a  given kernel\n"", '  :param kde:\n', '  :return:\n', "" '''\n""]","['GridSearchCV', 'np.linspace', 'grid.fit']",3
utilmy/zarchive/py2to3/datanalysis.py:sk_distribution_kernel_sample,sk_distribution_kernel_sample,function,15,31,29,212,6.84,1,0,"['kde', 'n']","[None, None]","['None', '1']",1204,"["" ''' \n"", '  kde = sm.nonparametric.KDEUnivariate(np.array(Y[Y_cluster==0],dtype=np.float64))\n', '  kde = sm.nonparametric.KDEMultivariate()  # ... you already did this\n', "" '''\n""]","['np.zeros', 'func', 'kde.cdf', 'xrange', 'brentq']",5
utilmy/zarchive/py2to3/datanalysis.py:sk_correl_rank,sk_correl_rank,function,16,38,31,291,7.66,2,0,"['correl', '0]', '[0', '1]]']","[None, None, None, None]","['[[1', None, None, None]",1222,"[' """""" Correl Ranking:  Col i, Col j, Correl_i_j, Abs_Correl_i_j    """"""\n']","['np.shape', 'np.zeros', 'xrange', 'abs', 'util.sortcol']",5
utilmy/zarchive/py2to3/datanalysis.py:sk_error_r2,sk_error_r2,function,8,19,16,175,9.21,0,1,"['Ypred', 'y_true', 'sample_weight', 'multioutput']","[None, None, None, None]","[None, None, 'None', 'None']",1237,[],"['r2_score', 'np.sign']",2
utilmy/zarchive/py2to3/datanalysis.py:sk_error_rmse,sk_error_rmse,function,6,10,10,100,10.0,0,0,"['Ypred', 'Ytrue']","[None, None]","[None, None]",1244,[],"['np.sqrt', 'len', 'np.std']",3
utilmy/zarchive/py2to3/datanalysis.py:sk_cluster_distance_pair,sk_cluster_distance_pair,function,5,16,14,195,12.19,0,2,"['Xmat', 'metric']","[None, None]","[None, ""'jaccard'""]",1248,"[""   '''\n"", ""    'euclidean, 'minkowski', 'cityblock', 'seuclidean', 'sqeuclidean, 'cosine, 'correlation, 'hamming, 'jaccard, 'chebyshev, 'canberra, 'braycurtis, 'mahalanobis', VI=None) 'yule, 'matching, 'dice, 'kulsinski, 'rogerstanimoto, 'russellrao, 'sokalmichener, 'sokalsneath,\n"", '\n', ""    'braycurtis': hdbscan.dist_metrics.BrayCurtisDistance,\n"", "" 'canberra': hdbscan.dist_metrics.CanberraDistance,\n"", "" 'chebyshev': hdbscan.dist_metrics.ChebyshevDistance,\n"", "" 'cityblock': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'dice': hdbscan.dist_metrics.DiceDistance,\n"", "" 'euclidean': hdbscan.dist_metrics.EuclideanDistance,\n"", "" 'hamming': hdbscan.dist_metrics.HammingDistance,\n"", "" 'haversine': hdbscan.dist_metrics.HaversineDistance,\n"", "" 'infinity': hdbscan.dist_metrics.ChebyshevDistance,\n"", "" 'jaccard': hdbscan.dist_metrics.JaccardDistance,\n"", "" 'kulsinski': hdbscan.dist_metrics.KulsinskiDistance,\n"", "" 'l1': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'l2': hdbscan.dist_metrics.EuclideanDistance,\n"", "" 'mahalanobis': hdbscan.dist_metrics.MahalanobisDistance,\n"", "" 'manhattan': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'matching': hdbscan.dist_metrics.MatchingDistance,\n"", "" 'minkowski': hdbscan.dist_metrics.MinkowskiDistance,\n"", "" 'p': hdbscan.dist_metrics.MinkowskiDistance,\n"", "" 'pyfunc': hdbscan.dist_metrics.PyFuncDistance,\n"", "" 'rogerstanimoto': hdbscan.dist_metrics.RogersTanimotoDistance,\n"", "" 'russellrao': hdbscan.dist_metrics.RussellRaoDistance,\n"", "" 'seuclidean': hdbscan.dist_metrics.SEuclideanDistance,\n"", "" 'sokalmichener': hdbscan.dist_metrics.SokalMichenerDistance,\n"", "" 'sokalsneath': hdbscan.dist_metrics.SokalSneathDistance,\n"", "" 'wminkowski': hdbscan.dist_metrics.WMinkowskiDistance}\n"", '   #Visualize discretization scheme\n', '\n', '   Xtrain_dist= sci.spatial.distance.squareform(sci.spatial.distance.pdist(Xtrain_d,\n', ""             metric='cityblock', p=2, w=None, V=None, VI=None))\n"", '\n', ""   Xtsne= da.plot_cluster_tsne(Xtrain_dist, metric='', perplexity=40, ncomponent=2, isprecompute=True)\n"", '\n', ""   '''\n""]",['fast.distance_jaccard_X'],1
utilmy/zarchive/py2to3/datanalysis.py:sk_cluster,sk_cluster,function,52,174,123,1637,9.41,2,11,"['Xmat', 'metric']","[None, None]","[None, ""'jaccard'""]",1290,[],"['fast.distance_jaccard_X', 'sk_cluster', 'hdbscan.HDBSCAN', 'print', 'len', 'np.std', 'np.mean', 'Xmat.reshape', 'k_means', 'KMeans', 'kmeans.fit', 'range', 'plt.plot', 'plt.show', 'sk_cluster_algo_custom', 'algorithm']",16
utilmy/zarchive/py2to3/datanalysis.py:sk_cluster_algo_custom,sk_cluster_algo_custom,function,9,17,15,172,10.12,0,1,"['Xmat', 'algorithm', 'args', 'kwds', 'returnval']","[None, None, None, None, None]","[None, None, None, None, '1']",1353,"[""    ''' Plot the cLuster using specific Algo\n"", '    distance_matrix = pairwise_distances(blobs)\n', ""    clusterer = hdbscan.HDBSCAN(metric='precomputed')\n"", '    clusterer.fit(distance_matrix)\n', '    clusterer.labels_\n', '\n', ""    {'braycurtis': hdbscan.dist_metrics.BrayCurtisDistance,\n"", "" 'canberra': hdbscan.dist_metrics.CanberraDistance,\n"", "" 'chebyshev': hdbscan.dist_metrics.ChebyshevDistance,\n"", "" 'cityblock': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'dice': hdbscan.dist_metrics.DiceDistance,\n"", "" 'euclidean': hdbscan.dist_metrics.EuclideanDistance,\n"", "" 'hamming': hdbscan.dist_metrics.HammingDistance,\n"", "" 'haversine': hdbscan.dist_metrics.HaversineDistance,\n"", "" 'infinity': hdbscan.dist_metrics.ChebyshevDistance,\n"", "" 'jaccard': hdbscan.dist_metrics.JaccardDistance,\n"", "" 'kulsinski': hdbscan.dist_metrics.KulsinskiDistance,\n"", "" 'l1': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'l2': hdbscan.dist_metrics.EuclideanDistance,\n"", "" 'mahalanobis': hdbscan.dist_metrics.MahalanobisDistance,\n"", "" 'manhattan': hdbscan.dist_metrics.ManhattanDistance,\n"", "" 'matching': hdbscan.dist_metrics.MatchingDistance,\n"", "" 'minkowski': hdbscan.dist_metrics.MinkowskiDistance,\n"", "" 'p': hdbscan.dist_metrics.MinkowskiDistance,\n"", "" 'pyfunc': hdbscan.dist_metrics.PyFuncDistance,\n"", "" 'rogerstanimoto': hdbscan.dist_metrics.RogersTanimotoDistance,\n"", "" 'russellrao': hdbscan.dist_metrics.RussellRaoDistance,\n"", "" 'seuclidean': hdbscan.dist_metrics.SEuclideanDistance,\n"", "" 'sokalmichener': hdbscan.dist_metrics.SokalMichenerDistance,\n"", "" 'sokalsneath': hdbscan.dist_metrics.SokalSneathDistance,\n"", "" 'wminkowski': hdbscan.dist_metrics.WMinkowskiDistance}\n"", '\n', ""    '''\n""]","['algorithm', 'print', 'len']",3
utilmy/zarchive/py2to3/datanalysis.py:sk_optim_de,sk_optim_de,function,37,127,96,1179,9.28,1,4,"['obj_fun', 'bounds', 'maxiter', 'name1', 'solver1', 'isreset', 'popsize']","[None, None, None, None, None, None, None]","[None, None, '1', ""''"", 'None', '1', '15']",1426,"[""    ''' Optimization and Save Data into file'''\n""]","['print', 'load_obj', 'int', 'copy.deepcopy', 'xrange', 'next', 'util.save_obj', 'util.date_now', 'util.np_int_tostr', 'np.mod', 'np.abs']",11
utilmy/zarchive/py2to3/datanalysis.py:sk_feature_importance,sk_feature_importance,function,9,20,20,228,11.4,1,1,"['clfrf', 'feature_name']","[None, None]","[None, None]",1529,[],"['np.argsort', 'range', 'len', 'str']",4
utilmy/zarchive/py2to3/datanalysis.py:sk_showconfusion,sk_showconfusion,function,11,22,21,244,11.09,0,1,"['clfrf', 'X_train', 'Y_train', 'isprint']","[None, None, None, None]","[None, None, None, 'True']",1537,[],"['clfrf.predict', 'cm.astype', 'cm.sum', 'print']",4
utilmy/zarchive/py2to3/datanalysis.py:sk_tree,sk_tree,function,13,33,32,445,13.48,0,1,"['Xtrain', 'Ytrain', 'nbtree', 'maxdepth', 'isprint1', 'njobs']","[None, None, None, None, None, None]","[None, None, None, None, '1', '1']",1546,[],"['clfrf.fit', 'clfrf.predict', 'cm.astype', 'cm.sum', 'print']",5
utilmy/zarchive/py2to3/datanalysis.py:sk_gen_ensemble_weight,sk_gen_ensemble_weight,function,14,29,27,213,7.34,1,0,"['vv', 'acclevel', 'maxlevel']","[None, None, None]","[None, None, '0.88']",1558,[],"['min', 'np.empty', 'range', 'estww.append', 'np.log', 'np.array']",6
utilmy/zarchive/py2to3/datanalysis.py:sk_votingpredict,sk_votingpredict,function,19,33,28,267,8.09,2,1,"['estimators', 'voting', 'ww', 'X_test']","[None, None, None, None]","[None, None, None, None]",1572,[],"['np.zeros', 'enumerate', 'clf.predict_proba', 'range', 'len']",5
utilmy/zarchive/py2to3/datanalysis.py:sk_tree_get_ifthen,sk_tree_get_ifthen,function,26,91,64,774,8.51,2,3,"['tree', 'feature_names', 'target_names', 'spacer_base']","[None, None, None, None]","[None, None, None, '"" ""']",1587,"['    """"""Produce psuedo-code for decision tree.\n', '    tree -- scikit-leant DescisionTree.\n', '    feature_names -- list of feature names.\n', '    target_names -- list of target (output) names.\n', '    spacer_base -- used for spacing code (default: ""    "").\n', '    """"""\n']","['recurse', 'print', 'str', 'zip', 'int']",5
utilmy/zarchive/py2to3/datanalysis.py:sk_model_template1,sk_model_template1,class,39,143,86,1191,8.33,0,3,[],[],[],1473,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:sk_stateRule,sk_stateRule,class,38,138,104,1118,8.1,0,5,[],[],[],1658,[],[],0
utilmy/zarchive/py2to3/datanalysis.py:sk_model_template1:__init__,sk_model_template1:__init__,method,10,13,13,166,12.77,0,0,"['self', 'alpha', 'low_y_cut', 'high_y_cut', 'ww0']","[None, None, None, None, None]","[None, '0.5', '-0.09', '0.09', '0.95']",1474,[],['Ridge'],1
utilmy/zarchive/py2to3/datanalysis.py:sk_model_template1:fit,sk_model_template1:fit,method,17,46,40,465,10.11,0,1,"['self', 'X', 'Y']","[None, None, None]","[None, None, 'None']",1480,[],"['len', 'print', 'np.median']",3
utilmy/zarchive/py2to3/datanalysis.py:sk_model_template1:predict,sk_model_template1:predict,method,10,27,21,166,6.15,0,1,"['self', 'X', 'y', 'ymedian']","[None, None, None, None]","[None, None, 'None', 'None']",1498,[],['Y.clip'],1
utilmy/zarchive/py2to3/datanalysis.py:sk_model_template1:score,sk_model_template1:score,method,14,32,27,216,6.75,0,1,"['self', 'X', 'Ytrue', 'ymedian']","[None, None, None, None]","[None, None, 'None', 'None']",1509,[],"['Y.clip', 'r2_score']",2
utilmy/zarchive/py2to3/datanalysis.py:sk_stateRule:__init__,sk_stateRule:__init__,method,8,44,37,371,8.43,0,1,"['self', 'state', 'trigger', 'colname']","[None, None, None, None]","[None, None, None, '[]']",1661,[],"['np.empty', 'np.shape', 'len', 'xrange', 'util.np_torecarray']",5
utilmy/zarchive/py2to3/datanalysis.py:sk_stateRule:addrule,sk_stateRule:addrule,method,20,47,42,381,8.11,0,2,"['self', 'rulefun', 'name', 'desc']","[None, None, None, None]","[None, None, ""''"", ""''""]",1673,[],"['util.findnone', 'util.find', 'print', 'util.np_addcolumn', 'rulefun', 'copy.deepcopy']",6
utilmy/zarchive/py2to3/datanalysis.py:sk_stateRule:eval,sk_stateRule:eval,method,9,29,24,231,7.97,0,2,"['self', 'idrule', 't', 'ktrig']","[None, None, None, None]","[None, None, None, '0']",1691,[],"['isinstance', 'util.find', 'print']",3
utilmy/zarchive/py2to3/datanalysis.py:sk_stateRule:help,sk_stateRule:help,method,0,0,0,0,0.0,0,0,[],[],[],1699,"[""    '''\n"", 's1= np.arange(5000).reshape((1000, 5))\n', 'trig1= np.ones((1,5))\n', ""state1= sk_stateRule(aa, trig1, ['drawdown','ma100d','ret10d','state_1','state_2'] )\n"", '\n', 'def fun1(s, tr,t):\n', '  return  s.drawdown[t] < tr.drawdown[0] and  s.drawdown[t] < tr.drawdown[0]\n', '\n', 'def fun2(s, tr,t):\n', ' return  s.drawdown[t] > tr.drawdown[0] and  s.drawdown[t] < tr.drawdown[0]\n', '\n', ""state1.addrule(fun1, 'rule6')\n"", ""state1.addrule(fun2, 'rule5')\n"", '\n', 'state1.eval(idrule=0,t=5)\n', '\n', 'state1.eval(idrule=1,t=5)\n', '\n', ""state1.eval(idrule='rule5',t=6)\n"", '\n', ""util.save_obj(state1, 'state1')\n"", '\n', 'np.shape(aa2)\n', '\n', ""aa2= util.np_torecarray(aa,  ['drawdown','a2','a3','a4','a5'])\n"", '\n', 'util.find(5.0, aa2[0])\n', '\n', 'recordarr = np.rec.array([(1,2.,7),(2,3.,5)],\n', ""                   dtype=[('col1', 'f8'),('col2', 'f8'), ('col3', 'f8')])\n"", 'recordarr.col3[0]\n', '\n', 'state1= stateRule(np.ones((100,10)), np.ones((1,10)))\n', '\n', 'col= aa2.a2\n', '\n', ""'''\n""]",[],0
utilmy/zarchive/py2to3/coke_functions.py:date_diffsecond,date_diffsecond,function,4,8,7,71,8.88,0,0,"['str_t1', 'str_t0', ""fmt='YYYY-MM-DD HH""]","[None, None, '']","[None, None, ""'YYYY-MM-DD HH:mm:SS'""]",8,[],"['arrow.get', 'dd.total_seconds']",2
utilmy/zarchive/py2to3/coke_functions.py:date_diffstart,date_diffstart,function,4,9,8,31,3.44,1,0,['t'],[None],[None],13,[],['dd.items'],1
utilmy/zarchive/py2to3/coke_functions.py:date_diffend,date_diffend,function,4,9,8,31,3.44,1,0,['t'],[None],[None],14,[],['dd.items'],1
utilmy/zarchive/py2to3/coke_functions.py:np_dict_tolist,np_dict_tolist,function,4,9,8,31,3.44,1,0,['dd'],[None],[None],17,[],['dd.items'],1
utilmy/zarchive/py2to3/coke_functions.py:np_dict_tostr_val,np_dict_tostr_val,function,1,9,9,46,5.11,0,0,['dd'],[None],[None],20,[],"['str', 'dd.items']",2
utilmy/zarchive/py2to3/coke_functions.py:np_dict_tostr_key,np_dict_tostr_key,function,1,8,8,46,5.75,0,0,['dd'],[None],[None],23,[],"['str', 'dd.items']",2
utilmy/zarchive/py2to3/coke_functions.py:day,day,function,9,16,15,117,7.31,0,0,['s'],[None],[None],31,[],['arrow.get'],1
utilmy/zarchive/py2to3/coke_functions.py:month,month,function,9,16,15,117,7.31,0,0,['s'],[None],[None],32,[],['arrow.get'],1
utilmy/zarchive/py2to3/coke_functions.py:year,year,function,9,16,15,117,7.31,0,0,['s'],[None],[None],33,[],['arrow.get'],1
utilmy/zarchive/py2to3/coke_functions.py:hour,hour,function,9,16,15,117,7.31,0,0,['s'],[None],[None],34,[],['arrow.get'],1
utilmy/zarchive/py2to3/coke_functions.py:weekday,weekday,function,9,16,15,117,7.31,0,0,"['s', 'fmt', 'i0', 'i1']","[None, None, None, None]","[None, ""'YYYY-MM-DD'"", '0', '10']",38,[],['arrow.get'],1
utilmy/zarchive/py2to3/coke_functions.py:season,season,function,4,15,13,47,3.13,0,1,['d'],[None],[None],48,[],['int'],1
utilmy/zarchive/py2to3/coke_functions.py:daytime,daytime,function,7,42,23,123,2.93,0,1,['d'],[None],[None],53,[],['int'],1
utilmy/zarchive/py2to3/coke_functions.py:pd_date_splitall,pd_date_splitall,function,11,22,17,255,11.59,0,0,"['df', 'coldate']","[None, None]","[None, ""'purchased_at'""]",62,[],['copy.deepcopy'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:data_jpsector,data_jpsector,function,3,4,4,47,11.75,0,0,[],[],[],30,[],['util.load_obj'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:date_earningquater,date_earningquater,function,9,103,42,604,5.86,0,5,['t1'],[None],[None],40,[],['datetime.datetime'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:date_is_3rdfriday,date_is_3rdfriday,function,4,14,13,67,4.79,0,0,['s'],[None],[None],59,[],"['datetime.strptime', 'd.weekday']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:date_option_expiry,date_option_expiry,function,10,50,33,354,7.08,0,2,['date'],[None],[None],64,[],['datetime.datetime'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:date_find_kday_fromintradaydate,date_find_kday_fromintradaydate,function,2,3,3,68,22.67,0,0,"['kintraday', 'intradaydate', 'dailydate']","[None, None, None]","[None, None, None]",81,[],['util.np_find'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:date_find_kintraday_fromdate,date_find_kintraday_fromdate,function,4,10,10,88,8.8,0,0,"['d1', 'intradaydate1', 'h1', 'm1']","[None, None, None, None]","[None, None, '9', '30']",85,[],"['datetime.datetime', 'util.np_find']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:date_find_intradateid,date_find_intradateid,function,8,13,13,96,7.38,1,0,"['datetimelist', 'stringdate']","[None, None]","[None, ""['20160420223000']""]",92,[],"['datestring_todatetime', 'util.np_find', 'str']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:datetime_convertzone1_tozone2,datetime_convertzone1_tozone2,function,16,29,25,343,11.83,1,1,"['tt', 'fromzone', 'tozone']","[None, None, None]","[None, ""'Japan'"", ""'US/Eastern'""]",103,[],"['pytz.timezone', 'type', 'tz.localize', 't2.append']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:date_extract_dailyopenclosetime,date_extract_dailyopenclosetime,function,23,51,35,431,8.45,2,3,"['spdateref1', 'market']","[None, None]","[None, ""'us'""]",120,[],"['enumerate', 'spdailyopendate.append', 'np.array', 'spdailyclosedate.append']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:datetime_tostring,datetime_tostring,function,6,34,22,269,7.91,1,3,['tt'],[None],[None],138,[],"['isinstance', 'tt.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:datestring_todatetime,datestring_todatetime,function,6,15,13,125,8.33,1,1,"['datelist1', 'format1']","[None, None]","[None, ' ""%Y%m%d""']",149,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:datetime_todate,datetime_todate,function,6,40,25,319,7.97,1,3,['tt'],[None],[None],158,[],"['isinstance', 'datetime.date', 'pd.to_datetime', 'date2.append']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:datetime_toint,datetime_toint,function,12,37,28,424,11.46,2,2,['datelist1'],[None],[None],169,[],"['isinstance', 'int', 'date2.append', 'datetime_tointhour', 'yy2.append', 'np.array']",6
utilmy/zarchive/py2to3/portfolio_withdate.py:datetime_tointhour,datetime_tointhour,function,7,20,19,234,11.7,1,1,['datelist1'],[None],[None],177,[],"['isinstance', 'int', 'yy2.append', 'np.array']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:dateint_tostring,dateint_tostring,function,7,15,14,183,12.2,1,1,"['datelist1', 'format1']","[None, None]","[None, ""'%b-%y'""]",185,[],"['isinstance', 'dateint_todatetime', 'date2.append', 'np.array']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:dateint_todatetime,dateint_todatetime,function,6,14,12,126,9.0,1,1,['datelist1'],[None],[None],207,[],"['isint', 'parser.parse', 'date2.append']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:datenumpy_todatetime,datenumpy_todatetime,function,9,48,26,381,7.94,0,3,"['tt', 'islocaltime']","[None, None]","[None, 'True']",218,[],['type'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:datetime_tonumpypdate,datetime_tonumpypdate,function,2,2,2,22,11.0,0,0,"['t', 'islocaltime']","[None, None]","[None, 'True']",232,[],['np.datetime64'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:date_diffindays,date_diffindays,function,4,5,5,74,14.8,0,0,"['intdate1', 'intdate2']","[None, None]","[None, None]",237,[],['dateint_todatetime'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:date_finddateid,date_finddateid,function,5,75,20,492,6.56,0,10,"['date1', 'dateref']","[None, None]","[None, None]",242,[],['util.np_findfirst'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:datestring_toint,datestring_toint,function,6,16,14,106,6.62,1,1,['datelist1'],[None],[None],256,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:date_as_float,date_as_float,function,12,32,26,311,9.72,0,2,['dt'],[None],[None],263,[],"['isint', 'dateint_todatetime', 'datetime.datetime', 'isleap', 'timedelta']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:date_todatetime,date_todatetime,function,1,9,9,58,6.44,0,0,['tlist'],[None],[None],277,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:date_removetimezone,date_removetimezone,function,1,6,6,63,10.5,0,0,['datelist'],[None],[None],281,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:datediff_inyear,datediff_inyear,function,2,3,3,53,17.67,0,0,"['startdate', 'enddate']","[None, None]","[None, None]",285,[],['date_as_float'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:date_generatedatetime,date_generatedatetime,function,18,27,27,267,9.89,0,1,"['start', 'nbday', 'end']","[None, None, None]","['""20100101""', '10', '""""']",289,[],"['datestring_todatetime', 'date_add_bdays', 'datetime.timedelta', 'list', 'np.array']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:date_add_bdays,date_add_bdays,function,12,37,30,369,9.97,1,2,"['from_date', 'add_days']","[None, None]","[None, None]",298,[],"['type', 'dateint_todatetime', 'datestring_todatetime', 'datetime.timedelta', 'current_date.weekday']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:date_getspecificdate,date_getspecificdate,function,21,203,58,1312,6.46,8,19,"['datelist', 'datetype1', 'outputype1', 'includelastdate', 'includefirstdate', '']","[None, None, None, None, None, None]","[None, '""yearend""', '""intdate""', 'True', 'False', None]",312,[],"['isint', 'dateint_todatetime', 'enumerate', 'vec2.append', 'np.mod', 'np.array']",6
utilmy/zarchive/py2to3/portfolio_withdate.py:date_alignfromdateref,date_alignfromdateref,function,13,31,27,206,6.65,1,1,"['array1', 'dateref']","[None, None]","[None, None]",373,[],"['range', 'np.float', 'len']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:_date_align,_date_align,function,16,42,33,256,6.1,2,2,"['dateref', 'datei', 'tmax', 'closei']","[None, None, None, None]","[None, None, None, None]",398,[],"['np.zeros', 'range', 'len']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:date_align,date_align,function,13,31,27,206,6.65,1,1,"['array1', 'dateref']","[None, None]","[None, None]",412,"["" ''' #Aligne the price with the same dates date\tyear\tmonth\tday\td\topen\tclose\thigh\tlow\tvolume\taclose '''\n""]","['range', 'np.float', 'len']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:min_withposition,min_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],459,[],['min'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:max_withposition,max_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],463,[],['max'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:_reshape,_reshape,function,2,14,12,68,4.86,0,1,['x'],[None],[None],469,[],"['len', 'np.reshape', 'np.size']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:_notnone,_notnone,function,39,211,114,1769,8.38,1,11,['x'],[None],[None],473,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend']",23
utilmy/zarchive/py2to3/portfolio_withdate.py:plot_price,plot_price,function,69,272,170,2746,10.1,2,11,"['asset', 'y2', 'y3', 'y4', 'y5', 'sym', 'savename1', 'tickperday', 'date1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', 'None', ""''"", '20', 'None', '(10', None]",477,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend', 'plot_priceintraday', 'np.loadtxt', 'np.unique', 'xdays.append', 'np.hstack', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick']",35
utilmy/zarchive/py2to3/portfolio_withdate.py:plot_priceintraday,plot_priceintraday,function,32,59,57,947,16.05,1,0,['data'],[None],[None],556,[],"['np.loadtxt', 'np.unique', 'np.arange', 'xdays.append', 'np.hstack', 'plt.figure', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick', 'plt.show']",14
utilmy/zarchive/py2to3/portfolio_withdate.py:plot_check,plot_check,function,8,19,19,257,13.53,0,0,"['close', 'tt0i', 'tt1i', 'dateref', 'sym', 'tickperday']","[None, None, None, None, None, None]","[None, '20140102', '20160815', '[]', '[]', '120']",598,[],"['util.find', 'getret_fromquotes', 'price_normalize100', 'plot_price']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:plot_pricedate,plot_pricedate,function,18,61,49,534,8.75,1,4,"['date1', 'sym1', 'asset1', 'sym2', 'bsk1', 'verticaldate', 'savename1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None]","[None, None, None, 'None', 'None', 'None', ""''"", '(10', None]",608,[],"['type', 'datestring_todatetime', 'np.row_stack', 'np.concatenate', 'array_todataframe', 'df_asset.plot', 'ax.axvline', 'np.arange', 'int', 'len', 'ax.savefig']",11
utilmy/zarchive/py2to3/portfolio_withdate.py:generate_sepvertical,generate_sepvertical,function,9,13,12,149,11.46,0,1,"['asset1', 'tt', 'tmax', 'start', 'datebar']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",690,[],"['np.zeros', 'datestring_todatetime']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:save_asset_tofile,save_asset_tofile,function,31,129,87,978,7.58,5,9,"['file1', 'asset1', 'asset2', 'asset3', 'date1', 'title1']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",705,[],"['np.shape', 'len', 'np.reshape', 'print', 'np.zeros', 'range', 'int', 'np.row_stack', 'np.savetxt']",9
utilmy/zarchive/py2to3/portfolio_withdate.py:load_asset_fromfile,load_asset_fromfile,function,1,4,4,51,12.75,0,0,['file1'],[None],[None],744,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:array_todataframe,array_todataframe,function,11,41,27,259,6.32,0,2,"['price', 'symbols', 'date1']","[None, None, None]","[None, 'None', 'None']",748,[],"['np.shape', 'len', 'pd.DataFrame']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:pd_dataframe_toarray,pd_dataframe_toarray,function,9,10,10,107,10.7,0,0,['df'],[None],[None],759,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:isfloat,isfloat,function,3,17,13,80,4.71,0,1,['value'],[None],[None],768,[],['float'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:isint,isint,function,32,105,61,1036,9.87,0,8,['x'],[None],[None],776,[],"['Xmat.copy', 'x.mean', 'x.std', 'np.corrcoef', 'MinCovDet', 'OAS', 'correl_inv.copy', 'np.sqrt']",8
utilmy/zarchive/py2to3/portfolio_withdate.py:correlation_mat,correlation_mat,function,32,105,61,1036,9.87,0,8,"['Xmat', 'type1', 'type2']","[None, None, None]","[None, '""robust""', '""correl""']",783,[],"['Xmat.copy', 'x.mean', 'x.std', 'np.corrcoef', 'MinCovDet', 'OAS', 'correl_inv.copy', 'np.sqrt']",8
utilmy/zarchive/py2to3/portfolio_withdate.py:correl_reducebytrigger,correl_reducebytrigger,function,9,22,18,128,5.82,2,1,"['correl2', 'trigger']","[None, None]","[None, None]",831,"[""  ''' Put Zero below trigger  '''\n""]","['correl2.copy', 'range', 'abs']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:sk_cov_fromcorrel,sk_cov_fromcorrel,function,8,14,13,115,8.21,0,0,"['correl', 'ret_close1']","[None, None]","[None, None]",843,[],"['volhisto_fromret', 'np.diag', 'np.dot']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:cointegration,cointegration,function,18,47,37,385,8.19,0,0,"['x', 'y']","[None, None]","[None, None]",849,"['  """""" Output :\n', '    coint_t :t-statistic of unit-root test on residuals\n', ""    pvalue :   MacKinnon's approximate p-value based on MacKinnon (1994)\n"", '    crit_value : Critical values for the test statistic at the 1 %, 5 %, and 10 %  Signifiance levels.\n', '\n', '    The Null hypothesis is that there is no cointegration, the alt hypothesis is that there is cointegrating relationship. \n', '    If the pvalue is small, below a critical size:  Reject Null Hypothesis -->  Cointegration\n', '           pvalue is high --> No Cointegration    \n', '    \n', '    Significance level, also denoted as alpha or , is the probability of rejecting  the null hypothesis when it is true.\n', '    Probability of incorrectly rejecting a true null hypothesis\n', '\n', '    P=0.05  , At least 23% (and typically close to 50%)\n', '    P=0.01  , At least 7% (and typically close to 15%\n', '\n', '    (-2.9607012342275936,\n', '    0.038730981052330332, 0,\n', '    249,\n', ""    {'1%': -3.4568881317725864,\n"", ""       '10%': -2.5729936189738876,\n"", ""     '5%': -2.8732185133016057},\n"", '      601.96849256295991)\n', '        It can be seen that the calculated test statistic of -2.96 is smaller than the 5% critical value of -2.87, \n', ""        which means that we can reject the null hypothesis that  there isn't a cointegrating relationship at the 5% level.\n"", '  """"""\n']",['np.zeros'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:causality_y1_y2,causality_y1_y2,function,10,14,13,153,10.93,0,0,"['price2', 'price1', 'maxlag']","[None, None, None]","[None, None, None]",887,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:reg_slope,reg_slope,function,16,24,24,261,10.88,1,0,"['close', 'dateref', 'tlag', 'type1']","[None, None, None, None]","[None, None, None, ""'elasticv'""]",895,[],"['np.ones', 'xrange', 'regression', 'np.arange']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:rolling_cointegration,rolling_cointegration,function,13,29,26,280,9.66,0,0,"['x', 'y']","[None, None]","[None, None]",910,[],"['rolling_coint', 'pd.expanding_apply']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:regression,regression,function,13,67,32,589,8.79,0,5,"['yreturn', 'xreturn', 'type1']","[None, None, None]","[None, None, '""elasticv""']",929,"["" '''Y = X* reg.coef_  +  reg.intercept_, r2 '''\n""]","['len', 'yreturn.ravel', 'xreturn.reshape', 'reg.fit', 'F', 'reg.score']",6
utilmy/zarchive/py2to3/portfolio_withdate.py:regression_fixedsymbolstock,regression_fixedsymbolstock,function,30,59,54,591,10.02,1,1,"['sym', 'ret_close2', 'tsstart', 'tsample', 'ret_spy', 'spyclose', 'regonly']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, 'True']",966,"["" ''' Price, Weight, R2, Error  '''\n""]","['len', 'range', 'np.reshape', 'np.column_stack', 'regression', 'getpriceret_fromregression', 'np.max']",7
utilmy/zarchive/py2to3/portfolio_withdate.py:regression_getpricefromww,regression_getpricefromww,function,20,38,30,298,7.84,3,0,"['spyclose', 'ww01', 'regasset01', 'ret_close2', 'tstart', 'tlag']","[None, None, None, None, None, None]","[None, None, None, None, None, '1']",989,[],"['np.copy', 'np.zeros', 'range', 'len']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:regression_allstocks_vs_riskfactors,regression_allstocks_vs_riskfactors,function,65,137,113,1351,9.86,2,1,"['symstock', 'pricestock', 'symriskfac', 'priceriskfac', 'nlaglist']","[None, None, None, None, None]","[None, None, None, None, None]",1010,"["" '''Make Regression on all stocks ''' \n""]","['np.shape', 'print', 'np.arange', 'np.empty', 'len', 'enumerate', 'getlogret_fromquotes', 'range', 'np.reshape', 'regression_fixedsymbolstock', 'util.np_sortbycolumn', 'util.pd_array_todataframe', 'util.print_object']",13
utilmy/zarchive/py2to3/portfolio_withdate.py:getdiff_fromquotes,getdiff_fromquotes,function,4,6,6,73,12.17,0,0,"['close', 'timelag']","[None, None]","[None, None]",1107,[],['np.shape'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:getret_fromquotes,getret_fromquotes,function,7,23,17,159,6.91,0,1,"['close', 'timelag']","[None, None]","[None, '1']",1111,[],"['np.shape', 'len']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:getlogret_fromquotes,getlogret_fromquotes,function,4,7,7,81,11.57,0,0,"['close', 'timelag']","[None, None]","[None, '1']",1122,[],"['np.shape', 'np.log']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:getprice_fromret,getprice_fromret,function,12,23,21,190,8.26,2,0,"['ret', 'normprice']","[None, None]","[None, '100']",1127,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:price_normalize100,price_normalize100,function,16,43,36,292,6.79,1,1,"['ret', 'normprice']","[None, None]","[None, '100']",1139,[],"['np.shape', 'len', 'np.reshape', 'np.zeros', 'range', 'np.cumprod']",6
utilmy/zarchive/py2to3/portfolio_withdate.py:price_normalize_1d,price_normalize_1d,function,8,22,19,147,6.68,0,0,"['ret', 'normprice', 'dtype1']","[None, None, None]","[None, '100', ' np.float32']",1159,[],"['np.empty', 'np.cumprod']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:norm_fast,norm_fast,function,7,8,8,85,10.62,0,0,"['y', 'ny']","[None, None]","[None, None]",1168,[],['ne.evaluate'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:correl_fast,correl_fast,function,8,13,13,116,8.92,0,0,"['xn', 'y', 'nx']","[None, None, None]","[None, None, None]",1176,[],['ne.evaluate'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:volhisto_fromret,volhisto_fromret,function,2,14,11,168,12.0,0,1,"['retbsk', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",1184,[],"['len', 'np.std']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:volhisto_fromprice,volhisto_fromprice,function,6,25,17,245,9.8,0,1,"['price', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",1191,[],"['len', 'np.shape', 'np.std']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:volhistorolling_fromprice,volhistorolling_fromprice,function,16,42,30,318,7.57,2,1,"['price', 'volrange']","[None, None]","[None, None]",1200,[],"['np.shape', 'len', 'np.zeros', 'xrange', 'volhisto_fromprice']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:rsk_calc_all_TA,rsk_calc_all_TA,function,51,223,105,1352,6.06,0,10,['df'],[None],"[""'panda_dataframe'""]",1219,"[""  '''Add All TA RMI, RSI To the '''\n""]","['ta.MA', 'ta.distance', 'ta.RET', 'ta.RMI', 'ta.STDDEV', 'ta.nbday_low', 'ta.nbday_high', 'ta.RSI', 'ta.qearning_dist', 'ta.optionexpiry_dist', 'linearreg2', 'np.array', 'np.sum', 'util.np_findlocalmin2', 'len', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",18
utilmy/zarchive/py2to3/portfolio_withdate.py:ta_highbandtrend1,ta_highbandtrend1,function,42,170,75,910,5.35,0,11,"['close2', 'type1']","[None, None]","[None, '0']",1301,[],"['linearreg', 'np.array', 'np.sum', 'util.np_findlocalmax2', 'len', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",8
utilmy/zarchive/py2to3/portfolio_withdate.py:pd_transform_asset,pd_transform_asset,function,11,35,27,409,11.69,0,1,"['q0', 'q1', 'type1']","[None, None, None]","[None, None, '""spread""']",1350,[],"['util.pd_date_intersection', 'util.pd_array_todataframe', 'util.pd_insertcol']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:calcbasket_table,calcbasket_table,function,25,59,53,477,8.08,1,2,"['wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps']","[None, None, None, None, None, None]","[None, None, '""table""', '""constant""', '1', ' 0.000']",1364,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.mod', 'np.sum']",6
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_createvolta_asset,folio_createvolta_asset,function,10,19,19,159,8.37,1,0,"['close', 'vol', 'volrange', 'lev']","[None, None, None, None]","[None, '0.12', '120', '1.0']",1391,[],"['np.zeros_like', 'np.shape', 'xrange', 'folio_volta']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_lowcorrelation,folio_lowcorrelation,function,145,431,300,3894,9.03,8,7,"['sym01', 'nstock', 'periodlist', 'dateref', 'close1', 'kbenchmark', 'badlist', 'costbppa', 'showgraph']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, '0.02', 'True']",1399,[],"['ww_selection', 'np.corrcoef', 'np.nan_to_num', 'np.zeros', 'range', 'util.np_sortbycol', 'int', 'np.std', 'np.isnan', 'util.find', 'np.sum', 'stk_select.append', 'volhisto_fromret', 'sum', '1/len', 'getret_fromquotes', 'len', 'np.empty', 'np.ones', 'enumerate', 'calcbasket_table', 'price_normalize100', 'np.column_stack', 'np.zeros_like', 'np.array', 'np.concatenate', 'print', 'str', 'plot_price', 'sym01[int', 'volhisto_fromprice', 'xrange', 'folio_inverseetf']",33
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_leverageetf,folio_leverageetf,function,8,23,20,156,6.78,1,0,"['price', 'lev', 'costpa']","[None, None, None]","[None, '1.0', '0.0']",1537,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_inverseetf,folio_inverseetf,function,8,18,18,151,8.39,1,0,"['price', 'costpa']","[None, None]","[None, '0.0']",1546,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_longshort_unit,folio_longshort_unit,function,59,240,112,1594,6.64,2,7,"['long1', 'short1', 'ww', '-1]', 'costpa', 'tlag', 'istable', 'wwschedule']","[None, None, None, None, None, None, None, None]","[None, None, '[1', None, '0.0', '1', '1', '[]']",1555,[],"['len', 'np.zeros', 'xrange', 'np.mod', 'folio_longshort_unitfixed']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_longshort_unitfixed,folio_longshort_unitfixed,function,43,111,73,719,6.48,1,3,"['long1', 'short1', 'nn', '-1]', 'costpa', 'tlag', 'istable']","[None, None, None, None, None, None, None]","[None, None, '[1', None, '0.0', '1', '1']",1596,[],"['len', 'np.zeros', 'xrange', 'np.mod']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_longshort_pct,folio_longshort_pct,function,8,21,20,189,9.0,1,0,"['long1', 'short1', 'ww', '-1]', 'costpa']","[None, None, None, None, None]","[None, None, '[1', None, '0.0']",1634,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_histogram,folio_histogram,function,13,75,41,914,12.19,0,0,['close'],[None],[None],1642,[],"['getret_fromquotes', 'plt.subplots', 'np.ones_like', 'ax2.hist', 'ax2.set_title', 'ax2.set_xlabel', 'ax2.set_ylabel', 'ax2.axvline', 'plt.show', 'print']",10
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_voltarget,folio_voltarget,function,2,6,6,61,10.17,0,0,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1670,[],['folio_volta'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_volta,folio_volta,function,54,197,112,1663,8.44,2,8,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1674,[],"['folio_volta', 'len', 'np.shape', 'np.zeros', 'range', 'np.std', 'util.findhigher', 'max', 'min', 'folio_volta2', 'np.abs']",11
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_volta2,folio_volta2,function,29,61,54,514,8.43,1,2,"['bsk', 'riskind', 'par', 'targetvol', 'volrange', 'cap', 'floor', 'costbp']","[None, None, None, None, None, None, None, None]","[None, None, None, '0.11', ' 90', '1.5', '0.0', '0.0005']",1713,[],"['np.shape', 'np.zeros', 'range', 'np.std', 'max', 'min', 'np.abs']",7
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_fixedunitprice,folio_fixedunitprice,function,13,36,33,221,6.14,2,0,"['price', 'fixedww', 'costpa']","[None, None, None]","[None, None, '0.0']",1738,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_fixedweightprice,folio_fixedweightprice,function,13,36,33,225,6.25,2,0,"['price', 'fixedww', 'costpa']","[None, None, None]","[None, None, '0.0']",1752,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_fixedweightret,folio_fixedweightret,function,13,31,28,193,6.23,2,0,"['ret', 'fixedww']","[None, None]","[None, None]",1764,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_cost_turnover,folio_cost_turnover,function,26,88,71,825,9.38,1,1,"['wwall', 'bsk', 'dateref', 'costbp']","[None, None, None, None]","[None, None, None, None]",1777,[],"['len', 'np.zeros', 'xrange', 'np.sum', 'plot_price', 'date1=dateint_tostring']",6
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_riskpa,folio_riskpa,function,20,52,40,422,8.12,2,0,"['ret', 'targetvol', 'volrange', 'cap']","[None, None, None, None]","[None, '0.1', '90', '1.0']",1807,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.sum', 'np.std', 'min', 'list']",8
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_perfreport_schedule,folio_perfreport_schedule,function,14,20,19,189,9.45,1,0,"['sym', 'dateref', 'close', 'wwind', 't0', 'scheduleperiod']","[None, None, None, None, None, None]","[None, None, None, None, None, '""1monthend""']",2121,[],"['np.shape', 'generate_schedule', 'np.zeros', 'enumerate']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:folio_concenfactor2,folio_concenfactor2,function,7,12,12,103,8.58,0,0,"['ww', 'masset']","[None, None]","[None, '12']",2421,[],"['getweight', 'np.sum', 'np.mean']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:calcbasket_objext,calcbasket_objext,function,50,203,96,1094,5.39,1,7,"['RETURN', 'TMAX', 'riskind_i', 'wwmat', 'wwasset0', 'ww0', 'nbrange', 'criteria']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",2439,[],"['xrange', 'np.sum', 'math.sqrt']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:calc_ranktable,calc_ranktable,function,20,44,41,351,7.98,1,0,"['close2', 'symjp1', 'nlag', 'refindex', 'funeval', 'funargs']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",3849,[],"['np.zeros', 'getlogret_fromquotes', 'enumerate', 'funeval', 'util.np_sortbycolumn']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:similarity_correl,similarity_correl,function,7,10,9,88,8.8,0,0,"['ret_close2', 'funargs']","[None, None]","[None, None]",3869,[],['correlation_mat'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:np_similarity,np_similarity,function,6,17,12,138,8.12,0,3,"['x', 'y', 'wwerr', 'type1']","[None, None, None, None]","[None, None, '[]', '0']",3924,[],['np_distance_l1'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:np_distance_l1,np_distance_l1,function,2,3,3,31,10.33,0,0,"['x', 'y', 'wwerr']","[None, None, None]","[None, None, None]",3930,[],"['np.sum', 'np.abs']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:np_similarity,np_similarity,function,6,17,12,138,8.12,0,3,"['x', 'y', 'wwerr', 'type1']","[None, None, None, None]","[None, None, '[]', '0']",3924,[],['np_distance_l1'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:np_distance_l1,np_distance_l1,function,2,3,3,31,10.33,0,0,"['x', 'y', 'wwerr']","[None, None, None]","[None, None, None]",3930,[],"['np.sum', 'np.abs']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:get,get,function,11,44,26,436,9.91,0,1,"['close', 'timelag']","[None, None]","[None, None]",4293,"['    """"""Return dataframe of requested dataset from Quandl.\n', '    :param dataset: str or list, depending on single dataset usage or multiset usage\n', '            Dataset codes are available on the Quandl website\n', '    :param str api_key: Downloads are limited to 50 unless api_key is specified\n', '    :param str start_date, end_date: Optional datefilers, otherwise entire dataset is returned\n', '    :param str collapse: Options are daily, weekly, monthly, quarterly, annual\n', '    :param str transform: options are diff, rdiff, cumul, and normalize\n', '    :param int rows: Number of rows which will be returned\n', '    :param str order: options are asc, desc. Default: `asc`\n', '    :param str returns: specify what format you wish your dataset returned as,\n', '        either `numpy` for a numpy ndarray or `pandas`. Default: `pandas`\n', '    :returns: :class:`pandas.DataFrame` or :class:`numpy.ndarray`\n', '    Any other `kwargs` passed to `get` are sent as field/value params to Quandl\n', '    with no interference.\n', '\n', ""'''\n"", '\n', '\n', '\n', '#--------------------Import data from Yahoo --------------------------------------------------\n', 'def imp_yahoo_getquotes(symbols, start=""20150101"", end=""20160101"", type1=1):\n', ' d1 = datetime.datetime(int(start[0:4]), int(start[4:6]), int(start[6:8]))\n', ' d2 = datetime.datetime(int(end[0:4]), int(end[4:6]), int(end[6:8]))\n', '# symbols = np.array(symbol1)\n', ' quotes= []; errorlist=[]; correctlist=[]\n', ' for i, symbol in enumerate(symbols) :\n', '   # if np.mod(i,400) ==0 and i != 0 :\n', ""     # print('Waiting 30s'); time.sleep(30)\n"", '   try:   \n', '     array1= quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n', '     df= pd.DataFrame.from_records(array1)\n', '     quotes.append(df)\n', '     correctlist.append(symbol)\n', '   except :\n', '       errorlist.append(symbol)\n', ""       print('Err: '+symbol)\n"", ' #quotes = [quotes_historical_yahoo(symbol, d1, d2, asobject=True)  for symbol in symbols]\n', ' #print(errorlist) \n', ' if type1==1 :  return quotes, correctlist\n', ' else: return quotes, correctlist, errorlist  \n', '\n', '              \n', '#Find error in Ticker import\n', 'def imp_yahoo_geterrorticker(symbols, start=""20150101"", end=""20160101""):\n', ' d1 = datetime.datetime(int(start[0:4]), int(start[4:6]), int(start[6:8]))\n', ' d2 = datetime.datetime(int(end[0:4]), int(end[4:6]), int(end[6:8]))\n', ' errorlist= []\n', ' for symbol in symbols :\n', '   try :  \n', '     quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n', '   except :\n', '     errorlist.append(symbol)\n', '     print(symbol)       \n', ' return errorlist\n', ' \n', ""'''\n"", ""symbols0= ['KHC', 'PYPL', 'HPE', 'BXLT', 'SYF', 'WLTW', 'CFG', 'BF-B', 'WRK']\n"", '\n', '#Show list of ticker in error\n', 'import_errorticker(symbols0,""20110101"",""20150601""  )\n', ""'''\n"", '\n', '\n', 'def imp_yahoo_financials_url(ticker_symbol, statement=""is"", quarterly=False):\n', '    if statement == ""is"" or statement == ""bs"" or statement == ""cf"":\n', '        url = ""https://finance.yahoo.com/q/"" + statement + ""?s="" + ticker_symbol\n', '        if not quarterly:\n', '            url += ""&annual""; return BeautifulSoup(requests.get(url).text, ""html.parser"")\n', '    return sys.exit(""Invalid financial statement code \'"" + statement + ""\' passed."")\n', '\n', 'def imp_yahoo_periodic_figure(soup, yahoo_figure):\n', '    values = []; pattern = re.compile(yahoo_figure)\n', '\n', '    title = soup.find(""strong"", text=pattern)    # works for the figures printed in bold\n', '    if title:  row = title.parent.parent\n', '    else:\n', '        title = soup.find(""td"", text=pattern)    # works for any other available figure\n', '        if title:  row = title.parent\n', '        else:      sys.exit(""Invalid figure \'"" + yahoo_figure + ""\' passed."")\n', '\n', '    cells = row.find_all(""td"")[1:]    # exclude the <td> with figure name\n', '    for cell in cells:\n', '        if cell.text.strip() != yahoo_figure:    # needed because some figures are indented\n', '            str_value = cell.text.strip().replace("","", """").replace(""("", ""-"").replace("")"", """")\n', '            if str_value == ""-"": str_value = 0\n', '            value = int(str_value) * 1000\n', '            values.append(value)\n', '\n', '    return values\n', '    \n', '# print(imp_yahoo_periodic(imp_yahoo_financials_url(""AAPL"", ""is""), ""Income Tax Expense""))\n', '\n', ' \n', '\n', '#--------------------Import Quotes Google  ------------------------------------------\n', 'import requests.packages.urllib3\n', 'requests.packages.urllib3.disable_warnings()\n', 'import urllib, os\n', '\n', ""dirstockcsv= 'E:\\_data\\stock\\csv'\n"", '\n', 'def imp_googleIntradayQuote(symbol, freqsec=300, nday=5):\n', ""    ''' Intraday quotes from google. Specify interval seconds and number of days\n"", '    http://www.google.com/finance/getprices?q=7261&i=300&p=3d&f=d,o,h,l,c,v\n', ""    '''\n"", '    url =""http://www.google.com/finance/getprices?q={0}"".format(symbol)\n', '    url +=""&i={0}&p={1}d&f=d,o,h,l,c,v"".format(freqsec, nday)\n', '    try :      \n', '       resp= requests.post(url)\n', '       csv= resp.text\n', '       csv= csv.splitlines()\n', '    except :\n', ""       print('Err Intaday '+symbol)\n"", '\n', '    if len(csv) < 9 :\n', ""      print 'Using Daily',\n"", '\n', ""      url='http://www.google.com/finance/historical?q='+symbol+'&i='+str(freqsec)+'&p=3d&f=d,o,h,l,c,v&output=csv'\n"", '      csv = urllib.urlopen(url).readlines()\n', '      csv.reverse()\n', '      q=[]\n', '      for bar in xrange(0,len(csv)-1):\n', ""        ds,open_,high,low,close,volume = csv[bar].rstrip().split(',')\n"", '        if isfloat(open_) and isfloat(high) :\n', '          open_,high,low,close = [float(x) for x in [open_,high,low,close]]\n', ""          dt = datetime.datetime.strptime(ds,'%d-%b-%y')\n"", '          q.append([dt,open_,high,low,close,volume])\n', ""      df= util.pd_array_todataframe(q, ['date','open','high','low','close','volume'])\n"", '      return df\n', '\n', '\n', '    # print(csv)\n', '    qq=[]\n', '    for bar in xrange(7,len(csv)):\n', ""      if csv[bar].count(',')!=5: continue\n"", ""      offset,close,high,low,open_,volume = csv[bar].split(',')\n"", '      \n', ""      if offset[0]=='a':\n"", '        day = float(offset[1:])\n', '        offset = 0\n', '      else:\n', '        offset = float(offset)\n', '      open_, high, low, close = [float(x) for x in [open_,high,low,close]]\n', '      dt = datetime.datetime.fromtimestamp(day + (freqsec * offset))\n', '      #print (dt,open_,high,low,close,volume)\n', '      qq.append([dt,open_,high,low,close,volume])\n', ""    df= util.pd_array_todataframe(qq, ['date','open','high','low','close','volume'])\n"", '    return df\n', '\n', ""def imp_googleIntradayQuoteSave(symbols=['NKE'], freqsec=300, nday=2000, dircsv='', dbname=''):\n"", "" ''' Save Under various format: csv / db / output in Dataframe '''\n"", ' if isinstance(symbols, str) : symbols= [symbols]\n', ' symout,qlist, sym_error= [], [], []\n', ' for sym in symbols:\n', '   try :\n', '    q = imp_googleIntradayQuote(sym, freqsec, nday)  #interval, timeframe\n', '    print sym,\n', ""    sym= sym.replace(':', '_')\n"", '\n', ""    if dircsv != '' :\n"", '      if not os.path.isdir(dircsv) :  os.makedirs(dircsv)\n', ""      start1= util.datetime_toint(util.datenumpy_todatetime(q['date'].values[0]))\n"", ""      file1= dircsv+ '\\\\' + sym + '_' + str(start1) + '_' + str(freqsec) + '_' + str(nday) + '.csv'\n"", '\n', ""      q= util.pd_addcol(q, 'symbol'); q['symbol']= sym\n"", '      q.to_csv(file1, index=False)\n', '\n', ""    elif dbname != '' :\n"", '      import sqlalchemy as sql\n', '      dbcon= sql.create_engine(dbname)\n', '      if interv_sec== 300 : # 5mins\n', ""        q.to_sql('q5min', dbcon)\n"", '\n', '    else :\n', '      qlist.append(q);   symout.append(sym)\n', '   except:\n', ""      print('Error:'+sym, )\n"", '      sym_error.append(sym)\n', '\n', ' return qlist, symout, sym_error\n', '\n', ""def imp_googleQuote(symbol, start_date='20160101', end_date= '20160101') : # datetime.date.today().isoformat()):\n"", ""    ''' Daily quotes from google. Date format='yyyymmdd' '''\n"", '    symbol = symbol.upper()\n', '    start = datetime.date(int(start_date[0:4]),int(start_date[4:6]),int(start_date[6:8]))\n', '    end =   datetime.date(int(end_date[0:4]),int(end_date[4:6]),int(end_date[6:8]))\n', '    url =""http://www.google.com/finance/historical?q={0}"".format(self.symbol)\n', '    url +=""&startdate={0}&enddate={1}&output=csv"".format( start.strftime(\'%b %d, %Y\'),end.strftime(\'%b %d, %Y\'))\n', '    csv = urllib.urlopen(url).readlines()\n', '    csv.reverse()\n', '    # print(url)\n', '    q=[]\n', '    for bar in xrange(0,len(csv)-1):\n', ""      ds,open_,high,low,close,volume = csv[bar].rstrip().split(',')\n"", '      if isfloat(open_) and isfloat(high) :\n', '        open_,high,low,close = [float(x) for x in [open_,high,low,close]]\n', ""        dt = datetime.datetime.strptime(ds,'%d-%b-%y')\n"", '        q.append([dt,open_,high,low,close,volume])\n', '    return q\n', '\n', 'def imp_googleQuoteSave(symbols, date1, date2, dircsv):\n', ' if isinstance(symbols, str) : symbols= [symbols]\n', ' for sym in symbols:\n', '   try :\n', '      q = imp_googleQuote(sym, date1, date2)   #interval, timeframe\n', ""   except: print('Error:'+symbol)\n"", '\n', ""   sym= sym.replace(':', '_')\n"", '\n', ""   if dircsv != '' :\n"", '     if not os.path.isdir(dircsv) :  os.makedirs(dircsv)\n', ""     start1= util.datetime_toint(util.datenumpy_todatetime(q['date'].values[0]))\n"", ""     file1= dircsv+ '\\\\'+ name1+'_'+ start1 +'_'+ str(date2) +'.csv'\n"", '     q.to_csv(file1)\n', '\n', ""   if dbname != '' :\n"", '     import sqlalchemy as sql\n', '     dbcon= sql.create_engine(dbname)\n', '     if interv_sec== 300 : # 5mins\n', ""       q.to_sql('q5min', dbcon)\n"", '\n', '\n', '\n', ""def imp_csv_dbupdate(indir='E:/_data/stock/intraday/intraday_google_usetf2.h5',\n"", ""                     outdir='E:/_data/stock/intraday/q5min/us/etf/', filelist=[], intype='csv',\n"", ""                     refcols=['date', 'open', 'high', 'low', 'close', 'volume', 'symbol']) :\n"", '\n', '  for file1 in filelist :\n', ""    sym=  file1[:file1.find('_')]  #Clean Up name\n"", '    print sym,\n', ""    outfile= outdir+'/'+sym+'.csv'\n"", '    if os.path.isfile(outfile) :\n', ""      df0= imp_csv_toext(file1=outfile, fromzone='Japan', tozone='UTC', header=0, cols=refcols)\n"", '    else :\n', '      df0= None\n', '\n', ""    if intype== 'hdfs':\n"", '      df1= imp_hdfs_getquote(indir, sym)\n', '      if isinstance(df1, float) : df1= None\n', '\n', ""    if intype=='csv' :\n"", ""      infile= indir+'/'+ file1   #csv\n"", ""      df1= imp_csv_toext(file1=infile, header=0, cols=refcols, fromzone='Japan', tozone='UTC')\n"", '\n', '    if df1 is not None :\n', '       if df0 is not None  : df1= imp_pd_merge(df1[refcols], df0[refcols])\n', '       # return outfile, df1\n', '       df1[refcols].to_csv(outfile, index=False)\n', '\n', '\n', '#####################################################################################\n', 'def imp_numpy_close_fromdb(dbname=\'/aaserialize/store/yahoo.db\', table1=\'\', symlist=[], t0=20010101, t1=20010101, priceid=""close"", batchsize= 400, maxasset=2600, tmax2=2000) :\n', ' print(""Get Numpy Matrix Close from DB SQL"")\n', ' import data.hist_data_storage as yhh\n', ' dstore= yhh.dailyDataStore(dbname)\n', ' close2= np.zeros((maxasset,tmax2),dtype=np.float16)\n', '\n', "" dateref0= dstore.get_histo(['SPY'], table1='daily_us_etf', start1=str(t0), end1=str(t1),  split_df=0).date.values\n"", ' \n', ' k=0; symfull=[];   i=0\n', ' nsym= len(symlist); nbatch= int(nsym / batchsize)+1\n', ' for j in xrange(0, nbatch) : \n', '      qlist, sym= dstore.get_histo(symlist[i:min(nsym,i+batchsize)], table1=table1, start1=str(t0), end1=str(t1),  split_df=1)\n', '\n', '      #print(""batch: ""+str(j), len(qlist), len(qlist[0].index) )\n', '      #for k,q in enumerate(qlist) :print sym[k], len(q.index)\n', '      i= i + batchsize\n', '      symfull= np.concatenate((symfull,sym))\n', '      close,dateref= date_align(qlist, dateref= dateref0, type1=priceid)\n', '      print(""batch: ""+str(j), len(sym),dateref[0],dateref[-1] )\n', '      qlist=None; sym=None\n', '      masset,tmax= np.shape(close)\n', '      if tmax2 < tmax: print(""Error tmax2 < tmax, wrong dates""); break\n', '      close2[k:k+masset,0:tmax] = close\n', '      k=k+ masset\n', '\n', ' maxasset= k\n', ' tmaxx= util.find(0.0, close2[0,:])\n', ' for k in xrange(0,maxasset):\n', '   if close2[k,tmaxx-1] == 0.0 : close2[k,tmaxx-1]=   close2[k,tmaxx-2]\n', ' \n', ' return close2[0:maxasset,0:tmaxx], symfull, dateref\n', '\n', 'def imp_numpy_close_fromhdfs(dbfile, symlist=[], t0=20010101, t1=20010101, priceid=""close"", maxasset=2600, tmax2=2000) :\n', ' print(""Get Numpy Matrix Close from Pandas"")\n', ' close2= np.zeros((maxasset,tmax2),dtype=np.float16)\n', ' store = pd.HDFStore(dbfile)\n', ' \n', "" df= store.select('SPY')\n"", ' df= df[(df.date >= t0) & (df.date <= t1 )]\n', ' dateref0= df.date.values\n', ' \n', ' k=0; symfull=[]; qlist=[]; sym=[]; tmax= 2000\n', ' for j, symbol in enumerate(store.keys()):\n', '  symbol=  symbol[1:]\n', '\n', '  if len(symlist)==0  or  util.find(symbol,symlist) > -1 : \n', '   #print symbol, util.find(symbol,symlist) \n', '   df= store.select(symbol) \n', '   if t0 !=t1 :     df= df[(df.date >= t0) & (df.date <= t1 )]\n', '   \n', '   qlist.append(df);   sym.append(symbol)\n', '   if np.mod(j+1, 401) == 0 :\n', '      print(""batch: ""+str(j))\n', '      close,dateref= date_align(qlist, dateref= dateref0, type1=priceid)   \n', '      print len(sym),dateref[0],dateref[-1] \n', '      symfull= symfull+sym\n', '      qlist=[]; sym=[]\n', ' \n', '      masset,tmax= np.shape(close)\n', '      if tmax2 < tmax: print(""Error tmax2 < tmax, wrong dates""); break\n', '      close2[(k):(k+masset),0:tmax] = close\n', '      k=k+ masset   \n', '     \n', ' if len(qlist) > 0:\n', '  # return qlist, qlist[0] ,  qlist[1] \n', '   close,dateref= date_align(qlist,type1=priceid)   \n', '   print len(sym),dateref[0],dateref[-1] \n', '   symfull= symfull+sym\n', '   masset,tmax= np.shape(close);\n', '   close2[(k):(k+masset),0:tmax] = close\n', ' \n', ' maxasset= k+masset\n', ' tmaxx= util.find(0.0, close2[0,:])\n', ' for k in xrange(0,maxasset):\n', '   if close2[k,tmaxx-1] == 0.0 : close2[k,tmaxx-1]=   close2[k,tmaxx-2]\n', ' \n', ' return close2[0:maxasset,0:tmaxx], symfull, dateref\n', '\n', '\n', ""def imp_sql_getquotes(stocklist01, dbname='sqlite:///aaserialize/store/yahoo.db', start1=20150101, end1=20160616, table1='daily_us_stock'):\n"", '    import data.hist_data_storage as yhh\n', '    dstore = yhh.dailyDataStore(dbname)\n', '    qlist, sym= dstore.get_histo(stocklist01, table1=table1,  start1=start1, end1=end1, split_df=1)\n', '    return qlist, sym\n', '\n', ""def imp_txt_getquotes(stocklist01, filedir='E:/_data/stock/daily/20160610/jp', startdate=20150101, endate=20160616):\n"", ' liststockfile= util.listallfile(filedir, ""*.txt"", dirlevel=5)\n', ' #print liststockfile\n', ' if len(np.shape(liststockfile)) > 1 :\n', '  liststockname=[ x.split(""."")[0] for x in liststockfile[:,0] ]\n', '#  for k in range(0, np.shape(liststockfile)[0]):  liststockname.append( (liststockfile[k,0])[0:4])   \n', '   \n', ' else: \n', '  stk0= liststockfile[0]; #stk2= liststockfile[2]\n', '#  liststockname=[ x[0:4] for x in stk0 ]\n', '  liststockname=[ x for x in stk0 ]\n', '\n', '#  liststockname=[]\n', '#  for k in range(0, len(stk0)):  liststockname.append( (stk0[k])[0:4])\n', ' \n', ' quotes=[]\n', ' print liststockname\n', ' for sym in stocklist01:\n', '   kstock= util.np_findfirst(sym, liststockname)\n', '   if kstock == -1 :\n', ""     print('Not Found '+sym)\n"", '   else :\n', '     df= pd.read_csv(liststockfile[kstock][2])\n', ""     df.columns = ['date', 'open', 'high','low','close', 'volume', 'openint']  #Change Column Name\n"", '     df2= df[(df.date > startdate  )& ( df.date   < endate )] # Filter by date\n', '     quotes.append(df2)\n', '  \n', ' return quotes \n', '\n', '\n', 'def imp_csv_getname(name1, date1, inter, tframe):\n', "" file1= dirstockcsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(inter) +'_'+ str(tframe) +'.csv'\n"", ' return file1\n', '\n', '\n', 'from dateutil import tz\n', ""def imp_csv_toext(file1='SPY.csv', outputfile='.h5', fromzone='Japan', tozone='UTC', header=None,\n"", ""                  cols=['date', 'time','open','high','low','close','volume', 'symbol'], coldate=[0]):\n"", ""  ''' cols: column name,  coldate: position of date column   '''\n"", '  from dateutil import parser\n', '\n', '  if os.path.getsize(file1)> 500 :\n', ""   df = pd.read_csv(file1,sep=',',header=header)  # date_parser=dateparse)  # parse_dates={'date': [] },\n"", '   df.date= [ parser.parse(x) for x in  df.date]\n', '\n', ""   #if util.find('symbol', df.columns.values) < 0 :\n"", ""     #df= util.pd_addcol(df, 'symbol')\n"", ""     #df['symbol']= sym\n"", '\n', ""   type1= outputfile[outputfile.find('.'):]\n"", '\n', ""   if outputfile=='':\n"", '     # df.columns = [  x.lower() for x in df.columns.values ]\n', ""     # df.columns = ['date', 'symbol','open','high','low','close','volume']\n"", '     df.drop(df.columns[dropcol], axis=1, inplace=True)\n', ""     df= util.pd_addcol(df, 'symbol');  df['symbol']= dfname\n"", ""     df.columns = ['date', 'open','high','low','close','volume', 'symbol']\n"", '     return df[cols]\n', '\n', ""   if type1=='csv' :\n"", ""     df[cols].to_csv(outputfile , index=False)  # , compression='gzip' )\n"", '\n', ""   if type1== 'h5' :\n"", ""      # dateparse= lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '      dateparse= lambda x: (parse(x).replace(tzinfo=from_zone).astimezone(tozone))\n', '      # dateparse= lambda x: parse(x, tzinfos=from_zone).astimezone(to_zone)\n', '      # df.date= [pd.to_datetime((str(x)[:-6])) for x in  df.date]\n', '      # df.date= [x.to_datetime() for x in  df.date]\n', '\n', '      df.drop(df.columns[[0]], axis=1, inplace=True)\n', '      print filenameh5\n', ""      if util.find('symbol', df.columns.values) < 0 :\n"", ""         df= util.pd_addcol(df, 'symbol'); df['symbol']= dfname\n"", '      store = pd.HDFStore(outputfile); store.append(dfname, df);  store.close()\n', '\n', '\n', '\n', ""def imp_hdfs_db_updatefromcsv(dircsv, filepd=r'E:\\_data\\stock\\intraday_google.h5', fromtimezone='Japan', tozone='UTC'):\n"", ' lfile= util.os_file_listall(dircsv, pattern=""*.csv"", dirlevel=0)\n', ' for x in lfile:\n', '   name1= (x[0]).split(""_"")[0] #get Ticker name, splitting by _\n', ""   if util.str_isfloat(name1):  name1= 'jp'+name1  #Japanese Stocks case\n"", '   file1 = x[2];  print(name1)\n', '   if os.path.getsize(file1)> 500:\n', ""      filepd2= filepd + '/' +name1 +'.csv'\n"", '      imp_csv_toext(file1, filepd2, name1, fromtimezone, tozone)\n', '\n', ""def imp_hdfs_db_dumpinfo(dbfile='E:\\_data\\stock\\intraday_google.h5'):\n"", '  store = pd.HDFStore(dbfile)\n', '  extract=[]; errsym=[]\n', '  for symbol in store.keys():\n', '     try:\n', '       df= pd.read_hdf(dbfile, symbol)\n', ""       t0= df['date'].values[0]\n"", ""       t1= df['date'].values[-1]\n"", '\n', '       extract.append([symbol[1:], df.shape[1],   df.shape[0], t0, t1 ])\n', '\n', '     except: errsym.append(symbol)\n', '  return np.array(extract), errsym\n', '\n', 'def imp_hdfs_mergedb(filepdfrom, filepdto) :\n', '  store0 = pd.HDFStore(filepdfrom)\n', '  store1 = pd.HDFStore(filepdto)\n', '\n', '  for symbol in store0.keys():\n', '    qq= imp_hdfs_getquote(filepdfrom, symbol)\n', '    store1.append(symbol, qq)\n', '\n', '    qq= imp_hdfs_getquote(filepdto, symbol)\n', ""    qq= qq.drop_duplicates(subset='date', take_last=True)\n"", ""    qq= qq.sort('date', ascending=1)\n"", '    print(symbol)\n', '  store0.close(); store1.close()\n', '\n', ""def imp_hdfs_storecopy(hdfs1='F:/usstock8.h5', hdfs2='F:/usstock8.h5'):\n"", ' store = pd.HDFStore(hdfs1);  store2 = pd.HDFStore(hdfs2)\n', ' for symbol in store.keys() :\n', '   df= imp_hdfs_getquote(hdfs1, symbol)\n', '   symbol= symbol[1:].replace(""-"",""_"")\n', '   store2.append(symbol, df, data_columns=True)\n', ' store.close(); store2.close()\n', '\n', 'def imp_hdfs_getquote(filenameh5, dfname=""data""):\n', '  try: return  pd.read_hdf(filenameh5, dfname)\n', '  except: return -1.0;\n', '\n', ""def imp_hdfs_getListquote(symbols, close1='close', start='12/18/2015 00:00:00+00:00',\n"", ""                          end='3/1/2016 00:00:00+00:00', freq='0d0h10min', filepd= 'E:\\_data\\stock\\intraday_google.h5',\n"", ""                          tozone='Japan', fillna=True, interpo=True):\n"", '\n', ' datefilter= pd.date_range(start=start, end=end, freq= freq,tz=tozone).values\n', '\n', ' errorsym=[]; quotes=[]; correctsym=[]\n', ' for symbol in symbols:  # Issue Not same size\n', '   qq= imp_hdfs_getquote(filepd, symbol)\n', '\n', '   if type(qq)==np.float :  # Error qq=-1\n', '        errorsym.append(symbol)\n', '   else:\n', ""       qq.columns = ['date', 'symbol','open','high','low','close','volume']\n"", ""       qq= qq[qq['date'].isin(datefilter)]   #Only date in the range\n"", ""       #qq= qq.drop_duplicates(cols='date', take_last=True)\n"", ""       #  qq= qq.sort('date', asc=True)\n"", '\n', '       #print qq.date.values[0]\n', ""       if fillna : qq= qq.fillna(method='backfill')\n"", '       if interpo : qq= qq.interpolate()\n', '       quotes.append(qq)\n', '       correctsym.append(symbol)\n', '\n', ' datefilter= datenumpy_todatetime(datefilter)\n', ' return quotes,datefilter, correctsym,   errorsym\n', '\n', '\n', ""def imp_hdfs_removeDuplicate(filepd='E:\\_data\\stock\\intraday_google.h5') :\n"", '  #-------Clean by removing duplicate-------------------------------------------------\n', '  store = pd.HDFStore(filepd)\n', '  for symbol in store.keys():\n', '    #qq= imp_hdfs_getquote(filepd, symbol);\n', '    qq= store.select(symbol[1:])\n', ""    qq= qq.drop_duplicates(subset='date', take_last=True)\n"", ""    qq= qq.sort('date', ascending=1)\n"", '    qq.index = list(np.arange(0,len(qq.index)))\n', '\n', '    store.remove(symbol); store.append(symbol, qq);\n', '  store.close()\n', '\n', '\n', '\n', ""def imp_pd_tohdfs(sym, qqlist, filenameh5, fromzone='Japan', tozone='UTC') :\n"", ""  ''' df list to HDFS '''\n"", '  store = pd.HDFStore(filenameh5);\n', '  for k, df2 in enumerate(qqlist) :\n', '    df= copy.deepcopy(df2)\n', ""    df['date']= datetime_toint(df.date.values)\n"", '\n', ""    symbol= sym[k].replace('-','_')\n"", '    qq= imp_hdfs_getquote(filenameh5, symbol)\n', '    # print symbol, type(qq)\n', '    if type(qq) ==  float or  type(qq) ==  int  :\n', '      store.append(symbol, df)  # , data_columns=True\n', '      # print(symbol + str(df.date.values[-1]))\n', '    else :\n', '      qq= pd.concat([qq, df], ignore_index=True)\n', ""      qq= qq.drop_duplicates(subset='date', keep='last')\n"", ""      qq= qq.sort('date', ascending=1)\n"", '      store.append(symbol, qq)  #Too much space , data_columns=True\n', '      print(symbol + str(qq.date.values[-1]))\n', '\n', '  store.close()\n', '\n', 'def imp_pd_merge(df1, df2) :\n', '  df= pd.concat([df1, df2], axis=0)\n', ""  df= df.drop_duplicates(subset='date', keep='last')\n"", ""  df= df.sort_values(by=['date'], ascending=1)\n"", '  df.index = list(np.arange(0,len(df.index)))\n', '  return df\n', '\n', 'def imp_pd_checkquote(quotes) :\n', ""   for c in quotes: print np.shape(c), c['date'].values[0], c['date'].values[-1]\n"", '\n', 'def imp_pd_getclose(df, datefilter=None):\n', ' if datefilter is  not None :\n', ""    df= df[df['date'].isin(datefilter)]   #Only date in the range\n"", "" df= df.sort('date')\n"", "" close= df.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", ' \n', ' close= np.array(close.values[1:,1:]).astype(np.float)            #Only Price\n', ' close= close[(np.abs(sci.stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close\n', '\n', 'def imp_pd_cashyield(q, duration=10):\n', ' qbond= copy.deepcopy(q)\n', "" for id1 in ['close','open','high','low','aclose'] :\n"", '   try:\n', '     vv= q[id1].values\n', '     pbond=  100 * np.cumprod( (1 + np.ones(len(vv)) * 0.005/365.0 ))\n', '     qbond[id1]= pbond\n', '   except: pass\n', ' return qbond\n', '\n', 'def imp_pd_yield_tobond(q, duration=10):\n', ' qbond= copy.deepcopy(q)\n', "" for id1 in ['close','open','high','low','aclose'] :\n"", '   try:\n', '     vv= q[id1].values\n', '     pbond=  1/ np.power(1+ vv*0.01,duration) *100\n', '     qbond[id1]= pbond\n', ""#     print 'ok'\n"", '   except: pass\n', ' return qbond\n', '\n', 'def imp_pd_errordate(quotes, dateref):\n', "" ''' Show Symbol in Error when importing '''\n"", ' for i, stock in enumerate(quotes) :\n', '   print(i, symbols1[i],  datetime_tostring(stock[0][0]))\n', ' print(""\\n\\n"")\n', '\n', ' for i, stock in enumerate(quotes) :\n', '   date1= datetime_toint(stock[0][0])\n', '   if date1 != dateref : print(i, symbols1[i],  str(date1))\n', '\n', 'def imp_pd_fxtoprice(q):\n', ' dfprice= copy.deepcopy(q)\n', "" for kid in ['close','open','high','low'] :\n"", '   qret= getret_fromquotes(q[kid].values,1)\n', '   qprice= price_normalize100(qret)\n', '   dfprice[kid]= qprice.T\n', ' return dfprice\n', '\n', 'def imp_pd_fxinversetoprice(q):\n', ' dfprice= copy.deepcopy(q)\n', "" for kid in ['close','open','high','low'] :\n"", '   qret= getret_fromquotes(1.0 / q[kid].values,1)\n', '   qprice= price_normalize100(qret)\n', '   dfprice[kid]= qprice.T\n', ' return dfprice\n', '\n', ""def imp_pd_filterbydate(df, dtref=None, start='2016-06-06 00:00:00', end='2016-06-14 00:00:00', freq='0d0h05min', timezone='Japan'):\n"", "" ''' df: DateSeries or TimeSeries of Quotes   '''\n"", '\n', ' if type(df) in {pd.core.frame.DataFrame} :  #Data frame version\n', '   if type(dtref)== str:\n', '      dtref= pd.date_range(start=start, end=end, freq= freq,tz=timezone).values\n', '\n', ""   return df[df['date'].isin(dtref)]\n"", ' else :\n', '   if type(dtref)== str:  #Date version\n', '      dtref= pd.date_range(start=start, end=end, freq= freq,tz=timezone).values\n', '\n', '   return df[df.isin(dtref)].values\n', '\n', 'def imp_pd_cleanquote(q):\n', ' col= q.columns.values\n', "" if isinstance(q['date'].values[0] , str) :\n"", ""   q['date']= datetime_todate(datestring_todatetime(q['date'].values))\n"", '\n', ' for kid in col:\n', ""   if kid not in ['date', 'day','month','year'] :\n"", ""      q[kid]= pd.to_numeric(q[kid], errors='coerce').values  #Put NA on string\n"", '\n', "" q= q.fillna(method='pad')\n"", ' return q\n', '\n', 'def imp_pd_divide(q1, q2, funapply):\n', "" close, dateref= date_align([q1,q2], type1='close')\n"", ' m,tt= np.shape(close)\n', '\n', ' q3= util.pd_createdf( np.zeros((tt,len(q1.columns))) ,q1.columns, dateref)\n', "" q3['date']= dateref\n"", "" q3['close']= (close[0,:] / close[1,:])\n"", '\n', ' for kid in q1.columns :\n', ""   if kid not in ['close', 'volume','date','day','month', 'year'] :\n"", '     close, _= date_align([q1,q2], type1=kid)\n', '     q3[kid]= (close[0,:] / close[1,:])\n', ' return q3\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ""def imp_quote_csvdir_tofile(dircsv='', outdir1='E:/_data/stock/', fromtimezone='Japan', tozone='UTC'):\n"", ' lfile= util.os_file_listall(dircsv, pattern=""*.csv"", dirlevel=0)\n', ' for x in lfile:\n', '   name1= (x[0]).split(""_"")[0] #get Ticker name, splitting by _\n', ""   if util.str_isfloat(name1):  name1= 'jp'+name1  #Japanese Stocks case\n"", '   file1 = x[2];  print(name1)\n', '   if os.path.getsize(file1)> 500:\n', ""      filepd2= outdir + '/' +name1 +'.csv'\n"", '      imp_csv_toext(file1, filepd2,  header=True, fromzone=fromtimezone, tozone=tozone)\n', ""'''\n"", '\n', '\n', ""'''\n"", ""close1='Close' ; start='12/18/2015 00:00:00';\n"", ""end='3/1/2016 00:00:00'; freq='1d0h00min';\n"", ""filepd= 'E:\\_data\\stock\\intraday_google.h5'\n"", '\n', 'datefilter= pd.date_range(start=start, end=end,   freq= freq).values\n', '\n', 'symbols, names = np.array(list(symbol_dict.items())).T\n', 'i=0; vv=[]\n', 'for symbol in symbols:  # Issue Not same size\n', '   qq= quote_frompanda(filepd, symbol); i+=1\n', '   if isfloat(qq) :  vv.append(symbol)\n', '   else:\n', ""     if i==1: close = qq[['Datetime']]  #new dataframe    \n"", '     tmp= pd.DataFrame(data={symbol: qq[close1]});\n', '     if not tmp.isnull().values.all():\n', ""      close = close.join(tmp, how='outer', rsuffix='_1')\n"", ""      close= close.drop_duplicates(cols='Datetime', take_last=True)\n"", '\n', ""close= close[close['Datetime'].isin(datefilter)]   #Only date in the range\n"", ""close= close.sort('Datetime')\n"", ""close= close.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", 'close= np.array(close.values[1:,1:]).astype(np.float)  #Only Price\n', 'close= close[(np.abs(stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close, vv\n', ""'''\n"", '\n', ""'''\n"", 'from datetime import datetime\n', 'from dateutil import tz\n', '\n', ""from_zone = tz.gettz('Japan'); to_zone = tz.gettz('UTC')\n"", '\n', ""date1 = datetime.strptime('2011-01-21 09:00:00', '%Y-%m-%d %H:%M:%S')\n"", 'date1 = date1.replace(tzinfo=from_zone)\n', 'dateutc = date1.astimezone(to_zone)\n', 'date1, dateutc\n', '\n', ""Suppose you have a column 'datetime' with your string, then:\n"", ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '\n', ""df = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n"", ' combine multiple columns into a single datetime column,\n', ""this merges a 'date' and a 'time' column into a single 'datetime' column:\n"", '\n', ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '\n', ""df = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)\n"", ""'''\n"", '\n', '\n', '#date1 = ""20160302""; inter= 60; tframe=80; name1= ""spy""\n', '#googleIntradayQuoteSave(name1, inter, tframe)\n', '\n', '\n', '\n', 'def ta_mar(close,t,m):  return  100*( np.mean(close[(t-m):(t+1)], axis=0) / close[t] )\n', 'def ta_ma(close,t,m):  return   np.mean(close[(t-m):(t+1)], axis=0)\n', '\n', '\n', '############################################################################\n', '#---------------------Calculate the Statistics         --------------------  \n', 'def calc_statestock(close2, dateref, symfull):\n', ' def sort(x,col,asc): return   util.sortcol(x,col,asc)\n', ' def perf(close,t0,t1):  return  100*( close[:,t1] / close[:,t0] -1)\n', ' def and2(tuple1):  return np.logical_and.reduce(tuple1)\n', ' def mar(close,t,m):  return  100*( np.mean(close[:,(t-m):(t+1)], axis=1) / close[:,t] )\n', ' def ma(close,t,m):  return   np.mean(close[:,(t-m):(t+1)], axis=1)\n', '\n', '\n', ' # a= ma(close2, tmax, 5) / ma(close2, tmax, 50)  \n', '\n', '\n', ' def dd(t) :\n', '  x= util.find(t,dateref)  # 607\n', ""  if x==-1 : print 'Error date not found in dateref '+str(t) ; return np.nan\n"", '  else :  return x\n', '\n', ' def gap(close,t0,t1,lag):   \n', '  ret= getret_fromquotes(close[:,t0:t1],lag)\n', '  rmin= 100*np.amin(ret,axis=1)\n', '  return rmin\n', ' \n', ' #--------------------Size----------------------------------------- \n', ' masset= len(symfull)\n', ' ttmax= util.find(0,close2[0,:])\n', ' if ttmax==-1 : ttmax= np.shape(close2)[1]\n', ' close2= close2[0:masset,0:ttmax]\n', ' m, tmax= masset, ttmax-1\n', ' \n', ' #correl\n', ' #close3= np.array(close2[:,0:ttmax],dtype=np.float16)\n', ' #close2ret= getret_fromquotes(close3)\n', ' #close2ret= np.nan_to_num(close2ret)\n', ' #close2ret= util.np_cleanmatrix(close2ret);  del close3\n', '\n', '##################################################################################\n', '#1: 1d,2:5d,3:10d,4:15d,5:20d,11:50d,12:60d,13:90d,15:1y\n', '#16: YTD,17:From Feb,21: 2015Ret,45: Price,50,51: Min,52,53: Max\n', ' print(""Calcul the Stats "" + str(dateref[-1]))\n', ' stat= np.zeros((2600,200),dtype=np.float16)\n', '\n', ' k=0\n', ' stat[k:(k+m),0]= np.arange(k,k+m)\n', ' stat[k:(k+m),1]=  perf(close2,tmax-1,tmax)  \n', ' stat[k:(k+m),2]=  perf(close2,tmax-5,tmax)  \n', ' stat[k:(k+m),3]=  perf(close2,tmax-10,tmax)  \n', ' stat[k:(k+m),4]=  perf(close2,tmax-15,tmax)  \n', ' stat[k:(k+m),5]=  perf(close2,tmax-20,tmax)  \n', '# stat[k:(k+m),6]=  perf(close2,tmax-25,tmax)  \n', ' stat[k:(k+m),7]=  perf(close2,tmax-30,tmax)  \n', '# stat[k:(k+m),8]=  perf(close2,tmax-35,tmax)  \n', ' stat[k:(k+m),9]=  perf(close2,tmax-40,tmax)  \n', '# stat[k:(k+m),10]=  perf(close2,tmax-45,tmax)  \n', ' stat[k:(k+m),11]=  perf(close2,tmax-50,tmax)  \n', ' stat[k:(k+m),12]=  perf(close2,tmax-60,tmax)  \n', ' stat[k:(k+m),13]=  perf(close2,tmax-90,tmax)  \n', ' stat[k:(k+m),14]=  perf(close2,tmax-120,tmax)  \n', ' stat[k:(k+m),15]=  perf(close2,tmax-250,tmax)  \n', '\n', '\n', '# stat[k:(k+m),16]=  perf(close2, dd(20160105),tmax)   #1Jan 2016\n', '# stat[k:(k+m),17]=  perf(close2, dd(20160222),tmax)   # 22 Feb 2016\n', '# stat[k:(k+m),18]=  perf(close2,dd(20160105),dd(20160222))   # Crash DrawDown\n', ' # stat[k:(k+m),19]=  perf(close2,624,tmax)   #  24 June 2016\n', ' # stat[k:(k+m),20]=  perf(close2,623,625)   #  Crash 24 June 2016\n', ' # stat[k:(k+m),21]=  perf(close2,dd(20151230)-250,dd(20151230))    #2015 Return\n', '\n', ' stat[k:(k+m),22]= gap(close2,tmax-80,tmax,1) #Max 1 days Gap\n', ' stat[k:(k+m),23]= gap(close2,tmax-80,tmax,5) #Max 5 days Gap\n', ' stat[k:(k+m),24]= perf(close2,tmax-2,tmax)  \n', ' stat[k:(k+m),25]= perf(close2,tmax-3,tmax)  \n', ' stat[k:(k+m),26]= perf(close2,tmax-4,tmax)  \n', '\n', ' stat[k:(k+m),27]= perf(close2,tmax-2,tmax-1)   \n', ' stat[k:(k+m),28]= perf(close2,tmax-3,tmax-2)   \n', ' stat[k:(k+m),29]= perf(close2,tmax-4,tmax-3)   \n', ' stat[k:(k+m),30]= perf(close2,tmax-5,tmax-4)\n', ' \n', ' stat[k:(k+m),120]= mar(close2, tmax, 20) \n', ' stat[k:(k+m),121]= mar(close2, tmax, 50) \n', ' stat[k:(k+m),122]= 100*ma(close2, tmax, 5) / ma(close2, tmax, 50)  #OverSold /OverBought\n', ' stat[k:(k+m),123]= mar(close2, tmax, 100)  \n', ' stat[k:(k+m),124]= mar(close2, tmax, 10) \n', '# stat[k:(k+m),35]= 0 \n', '# stat[k:(k+m),36]= 0 \n', '# stat[k:(k+m),37]= 0 \n', '# stat[k:(k+m),38]= 0 \n', '# stat[k:(k+m),39]= 0 \n', '\n', ' # stat[k:(k+m),30]= volume / AvgVol3M\n', ' # stat[k:(k+m),30]= Nbday_to_Earnings\n', ' #Volume/3M AvgVolume\n', ' # stat[k:(k+m),26]=  volume(volume2,tmax-120,tmax)  \n', '\n', ' #==============================================================================\n', ' #-------------- Data Fundamental    -------------------------------------------\n', "" df= util.sql_query(sqlr='SELECT ticker,shortratio,sector1_id, sector2_id, marketcap   FROM stockfundamental ', dburl='/aaserialize/store/finviz.db')\n"", ' npdf= df.values; del df\n', ' npdf0= npdf[:,0]  \n', ' for i in range(0,m):  \n', '   try :\n', '     kid= util.find(symfull[i], npdf0)\n', '     if kid != -1 :\n', '      stat[i,150]=  float(npdf[kid, 1])   # short ratio\n', '      stat[i,151]=  float(npdf[kid, 2])   # sector 1      \n', '      stat[i,152]=  float(npdf[kid, 3])   # sector 2     \n', '      stat[i,153]=  float(npdf[kid, 4] / 100000000.0)   # market Cap    \n', '   except : pass\n', ' del npdf, npdf0\n', '\n', '\n', '#==============================================================================\n', ' ################ Volatility\n', ' stat[k:(k+m),40]= volhisto_fromprice(close2,tmax,20,axis=1)* 100\n', ' stat[k:(k+m),41]= volhisto_fromprice(close2,tmax,60,axis=1)* 100\n', ' stat[k:(k+m),42]= volhisto_fromprice(close2,tmax,252,axis=1)* 100\n', '# stat[k:(k+m),43]= stat[k:(k+m),12] / stat[k:(k+m),41] #Sharpe 3Month\n', '# stat[k:(k+m),44]= stat[k:(k+m),15] / stat[k:(k+m),42] #Sharpe 1Year\n', '\n', ' stat[k:(k+m),45]= close2[:,tmax] #Last Close\n', '\n', '# stat[k:(k+m),46]= 0 \n', '# stat[k:(k+m),47]= 0 \n', '# stat[k:(k+m),48]= 0 \n', '# stat[k:(k+m),49]= 0 \n', '\n', '\n', ' ###############  Technical Indicator\n', ' #Min of last 6 months, Max of last 6months\n', ' for i in range(0,m):\n', '  pp0= stat[i,45]\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-120):tmax])\n', '  stat[i,50]= kmin+(tmax-120);  stat[i,51]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-120):tmax])\n', '  stat[i,52]= kmax+(tmax-120);  stat[i,53]= 100*pp0 / pmax\n', '\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-10):tmax])\n', '  stat[i,66]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-10):tmax])\n', '  stat[i,67]= 100*pp0 / pmax\n', '\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-5):tmax])\n', '  stat[i,68]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-5):tmax])\n', '  stat[i,69]= 100*pp0 / pmax\n', '\n', ' del kmax,pmax,i,k  \n', '\n', '\n', ' #Regression from Min Price Time\n', ' for i in range(0,m):\n', '  t0= int(stat[i,50])\n', '  try :\n', '    res= regression(close2[i,t0:tmax ]  ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,54]=  res[2]  #R2\n', '    stat[i,55]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', ' #Regression from Max Price Time\n', ' for i in range(0,m):  \n', '  t0= stat[i,52]\n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,56]=  res[2]  #R2\n', '    stat[i,57]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', ' #Regression from 5 days ago\n', ' t0= tmax-5\n', ' for i in range(0,m): \n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,58]=  res[2]  #R2\n', '    stat[i,59]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', '#Regression from 10 days ago\n', ' t0= tmax-10\n', ' for i in range(0,m):  \n', '   try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,60]=  res[2]  #R2\n', '    stat[i,61]=  res[0][0]  #Slope\n', '   except : pass\n', '\n', "" '''\n"", '#Regression from 1 month\n', ' t0= tmax-20   \n', ' for i in range(0,m):\n', '   try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,62]=  res[2]  #R2\n', '    stat[i,63]=  res[0][0]  #Slope\n', '   except : pass\n', "" '''\n"", '\n', "" '''\n"", '#Regression from 2 months ago\n', ' t0= tmax-40\n', ' for i in range(0,m):  \n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,64]=  res[2]  #R2\n', '    stat[i,65]=  res[0][0]  #Slope\n', '  except : pass\n', "" '''\n"", '\n', '#Nb of Positive days last 5d\n', ' for i in range(0,m):  stat[i,66]=  np_countretsign(close2[i,(tmax-5):tmax])  \n', '\n', '#Nb of Positive days last 10d\n', ' for i in range(0,m):  stat[i,67]=  np_countretsign(close2[i,(tmax-10):tmax])  \n', '\n', '#Nb of Positive days last 20d\n', '# for i in range(0,m):  stat[i,68]=  np_countretsign(close2[i,(tmax-20):tmax])  \n', '\n', '\n', '#Regression from Max Values,200days\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmax[:,1] ,vmax[:,0],type1=""linear"")\n', '    stat[i,70]=  res[2]  #R2\n', '    stat[i,71]=  res[0][0]  #Slope\n', '  except : pass\n', '# del vmax\n', '\n', '\n', '#Regression from Min Values,200days \n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmin= util.np_findlocalmin2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmin[:,1] ,vmin[:,0],type1=""linear"")\n', '    stat[i,72]=  res[2]  #R2\n', '    stat[i,73]=  res[0][0]  #Slope\n', '  except : pass\n', '# del vmin\n', '\n', '\n', '#Regression from Max Values,100days\n', ' t0= tmax-20*5\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmax[:,1] ,vmax[:,0],type1=""linear"")\n', '    stat[i,74]=  res[2]  #R2\n', '    stat[i,75]=  res[0][0]  #Slope\n', '  except : pass\n', ' del vmax\n', '\n', '\n', '#Regression from Min Values,100days \n', ' t0= tmax-20*5\n', ' for i in range(0,m):  \n', '  try :\n', '    vmin= util.np_findlocalmin2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmin[:,1] ,vmin[:,0],type1=""linear"")\n', '    stat[i,76]=  res[2]  #R2\n', '    stat[i,77]=  res[0][0]  #Slope\n', '  except : pass\n', ' del vmin\n', '\n', '\n', '\n', '# Upside Trend with Higher Lows detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '   vmax= util.np_findlocalmin2(close2[i,t0:tmax],4)\n', '   vmax= util.sort(vmax,0,asc=0)\n', '   stat[i,80]=  -1\n', '   if len(vmax) > 1 :\n', '    ss=0\n', '    for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] > vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '    if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,80]=  ss  \n', '        stat[i,81]=  vmax[0,0] + t0\n', '  except: pass\n', '\n', '\n', '# Upside Trend with Higher High detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax],4)\n', '    vmax= util.sort(vmax,0,asc=0)\n', '    stat[i,82]=  -1\n', '    if len(vmax) > 1 :\n', '      ss=0\n', '      for k in range(0,len(vmax)-1):\n', '        if vmax[k,1] > vmax[k+1,1] : ss+=1   #Higher high\n', '        else : break;\n', '      if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,82]=  ss  \n', '        stat[i,83]=  vmax[0,0] + t0\n', '  except: pass\n', '\n', '\n', '# Downside Trend with lower Lows detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  vmax= util.np_findlocalmin2(close2[i,t0:tmax],4)\n', '  vmax= util.sort(vmax,0,asc=0)\n', '  stat[i,84]=  -1\n', '  if len(vmax) > 1 :\n', '   ss=0\n', '   for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] < vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '   if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,84]=  ss  \n', '        stat[i,85]=  vmax[0,0] + t0\n', '\n', '\n', '# Downside Trend  with lower highs detection\n', ' t0= tmax-20*6\n', ' for i in range(0,m):  \n', '  vmax= util.np_findlocalmax2(close2[i,t0:tmax],4)\n', '  vmax= util.sort(vmax,0,asc=0)\n', '  stat[i,80]=  -1\n', '  if len(vmax) > 1 :\n', '   ss=0\n', '   for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] < vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '   if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,86]=  ss    #Nb  of lower high consecutive\n', '        stat[i,87]=  vmax[0,0] + t0\n', '\n', "" print('Higher/Lower Band Support')\n"", '#-----higher band Support / Lower Band Support 120days-------------------------------\n', ' t0= tmax-120\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,90]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,91]= 100.0* stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,92]= 100.0* (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '    # pmin=  (res2.x[0] * (np.arange(t0,tmax)-t0) + res2.x[1]) \n', '    # breachmin= np.sum(np.sign(close2[i, t0:tmax] - pmin*1.03))\n', '  except: pass\n', '\n', '\n', '#-----higher band Support / Lower Band Support 200days-------------------------------\n', ' t0= tmax-200\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,93]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,94]= 100.0 * stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,95]= 100.0 * (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '  except: pass\n', '\n', '\n', '#-----higher band Support / Lower Band Support 300days-------------------------------------\n', ' t0= tmax-300\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,96]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,97]= 100.0* stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,98]= 100.0* (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '  except: pass\n', '\n', '\n', ""  '''\n"", '  # -----higher band Support / Lower Band Support 60days-------------------------------\n', '  t0 = tmax - 60\n', '  for i in range(0, m):\n', '      try:\n', '          res1 = ta_highbandtrend1(close2[i, t0:tmax])\n', '          pmax = (res1.x[0] * (tmax - t0) + res1.x[1])\n', '          stat[i, 99] = 100.0 * stat[i, 45] / pmax  # Higher Band\n', '\n', '          res2 = ta_lowbandtrend1(close2[i, t0:tmax])\n', '          pmin = (res2.x[0] * (tmax - t0) + res2.x[1])\n', '          stat[i, 100] = 100.0 * stat[i, 45] / pmin  # LowerBand\n', '\n', '          stat[i, 101] = 100.0 * (pmax - stat[i, 45]) / (pmax - pmin)  # Price inside Range\n', '\n', '          # pmin=  (res2.x[0] * (np.arange(t0,tmax)-t0) + res2.x[1])\n', '          # breachmin= np.sum(np.sign(close2[i, t0:tmax] - pmin*1.03))\n', '      except:\n', '          pass\n', ""  '''\n"", '\n', '  #-----Technical Indicator------------------------------------------------\n', '# for i in range(0,m): \n', '#  try :\n', '#   res2=0\n', '   # df= pd.read_hdf(dbfile, symfull[i])\n', '   # stat[i,110]= ta.RMI(close2[i,:])\n', '   # stat[i,111]= 100*close2[i,:] / ta.MA(close2[i,:],20)\n', '   # stat[i,112]= 100*close2[i,:] / ta.MA(close2[i,:],50)   \n', '#  except: pass\n', '\n', '\n', ' return np.array(stat, dtype=np.float16) \n', '\n', '\n', ""'''  \n"", 'i=1964  \n', 'stat= s1\n', 'stat[i, 91]  \n', '\n', 'tt= np.arange(t0, tmax)\n', 'plot_price(close2[i,t0:tmax], (res2.x[0] * (tt-t0) + res2.x[1]) )\n', ""'''\n"", '\n', '  \n', '#------------------- Stock Monitoring Tools ------------------------------------------------\n', ""def monitor_addrecommend(string1, dbname='stock_recommend') :\n"", ' ss= string1.replace(""("","" "").replace(\')\',\' \').replace(\':\',\' \')\n', "" ss= ss.replace('\\t', ' ').replace('\\n', ' ')\n"", ' sl1= ss.split("" "")\n', ' print sl1\n', ' aux= [datestring_toint(util.date_now())]\n', ' symfull= copy.deepcopy(a_us_all)\n', ' for x in sl1:\n', '   if len(x) < 5 :\n', '    if util.find(x,symfull) > 0 :   aux.append(x)\n', '\n', ' if len(aux) >  1 :\n', '  stock_recommend= util.load_obj(dbname)\n', '  stock_recommend.append(aux)\n', '  util.save_obj(stock_recommend,dbname)\n', '  print aux\n', ' return stock_recommend\n', '\n', '\n', '\n', '#################### Finviz  ###############################################################\n', 'from bs4 import BeautifulSoup\n', 'import csv,datetime, requests\n', 'def imp_finviz():\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print ""Finviz Overview Start""\n', ' url = ""http://www.finviz.com/screener.ashx?v=111&f=geo_usa""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', "" firstcount = soup.find_all('option')\n"", ' lastnum = len(firstcount) - 1\n', "" lastpagenum = firstcount[lastnum].attrs['value']\n"", ' currentpage = int(lastpagenum)\n', '\n', ' alldata = []\n', ' templist = []\n', ' # Overview = 111, Valuation = 121, Financial = 161, Ownership = 131, Performance = 141\n', ' #pagesarray = [111,121,161,131,141]\n', ' titleslist = soup.find_all(\'td\',{""class"" : ""table-top""})\n', ' titleslisttickerid = soup.find_all(\'td\',{""class"" : ""table-top-s""})\n', ' titleticker = titleslisttickerid[0].text\n', ' titlesarray = []\n', ' for title in titleslist:\n', '    titlesarray.append(title.text)\n', '\n', ' titlesarray.insert(1,titleticker)\n', ' i = 0\n', '\n', ' while(currentpage > 0):\n', '    i += 1\n', '    print str(i) + "" page(s) done""\n', '    secondurl = ""http://www.finviz.com/screener.ashx?v="" + str(111) + ""&f=geo_usa"" + ""&r="" + str(currentpage)\n', '    secondresponse = requests.get(secondurl)\n', '    secondhtml = secondresponse.content\n', '    secondsoup = BeautifulSoup(secondhtml)\n', '    stockdata = secondsoup.find_all(\'a\', {""class"" : ""screener-link""})\n', '    stockticker = secondsoup.find_all(\'a\', {""class"" : ""screener-link-primary""})\n', '    datalength = len(stockdata)\n', '    tickerdatalength = len(stockticker)\n', '\n', '    while(datalength > 0):\n', '        templist = [stockdata[datalength - 10].text,stockticker[tickerdatalength-1].text,stockdata[datalength - 9].text,stockdata[datalength - 8].text,stockdata[datalength - 7].text,stockdata[datalength - 6].text,stockdata[datalength - 5].text,stockdata[datalength - 4].text,stockdata[datalength - 3].text,stockdata[datalength - 2].text,stockdata[datalength - 1].text,]\n', '        alldata.append(templist)\n', '        templist = []\n', '        datalength -= 10\n', '        tickerdatalength -= 1\n', '    currentpage -= 20\n', '\n', "" with open('stockoverview.csv', 'wb') as csvfile:\n"", ""    overview = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=titlesarray)\n"", '    overview.writeheader()\n', '\n', '    for stock in alldata:\n', '        overview.writerow({titlesarray[0] : stock[0], titlesarray[1] : stock[1],titlesarray[2] : stock[2],titlesarray[3] : stock[3],titlesarray[4] : stock[4], titlesarray[5] : stock[5], titlesarray[6] : stock[6], titlesarray[7] : stock[7] , titlesarray[8] : stock[8], titlesarray[9] : stock[9], titlesarray[10] : stock[10] })\n', '\n', ' print ""Finviz Overview Completed""\n', ' return overview\n', ' \n', '\n', 'def imp_finviz_news():\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print ""Finviz Performance Start""\n', ' url = ""http://www.finviz.com/quote.ashx?t=intc""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', ' titleslist = soup.find_all(\'a\',{""class"" : ""tab-link-news""})\n', ' return titleslist\n', '\n', '\n', 'def imp_finviz_financials():\n', ' import datetime\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print datetime.datetime.now()\n', ' print ""Finviz Financial Start""\n', ' url = ""http://www.finviz.com/screener.ashx?v=161&f=geo_usa""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', "" firstcount = soup.find_all('option')\n"", ' lastnum = len(firstcount) - 1\n', "" lastpagenum = firstcount[lastnum].attrs['value']\n"", ' currentpage = int(lastpagenum)\n', '\n', ' alldata = []\n', ' templist = []\n', ' # Overview = 111, Valuation = 121, Financial = 161, Ownership = 131, Performance = 141\n', ' #pagesarray = [111,121,161,131,141]\n', ' titleslist = soup.find_all(\'td\',{""class"" : ""table-top""})\n', ' titleslisttickerid = soup.find_all(\'td\',{""class"" : ""table-top-s""})\n', ' titleticker = titleslisttickerid[0].text\n', ' titlesarray = []\n', ' for title in titleslist:\n', '    titlesarray.append(title.text)\n', '\n', ' titlesarray.insert(1,titleticker)\n', ' i = 0\n', '\n', ' while(currentpage > 0):\n', '    i += 1\n', '    print str(i) + "" page(s) done""\n', '    secondurl = ""http://www.finviz.com/screener.ashx?v="" + str(161) + ""&f=geo_usa"" + ""&r="" + str(currentpage)\n', '    secondresponse = requests.get(secondurl)\n', '    secondhtml = secondresponse.content\n', '    secondsoup = BeautifulSoup(secondhtml)\n', '    stockdata = secondsoup.find_all(\'a\', {""class"" : ""screener-link""})\n', '    stockticker = secondsoup.find_all(\'a\', {""class"" : ""screener-link-primary""})\n', '    datalength = len(stockdata)\n', '    tickerdatalength = len(stockticker)\n', '\n', '    while(datalength > 0):\n', '        templist = [stockdata[datalength - 17].text,stockticker[tickerdatalength-1].text,stockdata[datalength - 16].text,stockdata[datalength - 15].text,stockdata[datalength - 14].text,stockdata[datalength - 13].text,stockdata[datalength - 12].text,stockdata[datalength - 11].text,stockdata[datalength - 10].text,stockdata[datalength - 9].text,stockdata[datalength - 8].text,stockdata[datalength - 7].text,stockdata[datalength - 6].text,stockdata[datalength - 5].text,stockdata[datalength - 4].text,stockdata[datalength - 3].text,stockdata[datalength - 2].text,stockdata[datalength - 1].text,]\n', '        alldata.append(templist)\n', '        templist = []\n', '        datalength -= 17\n', '        tickerdatalength -= 1\n', '    currentpage -= 20\n', '\n', "" with open('stockfinancial.csv', 'wb') as csvfile:\n"", ""    financial = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=titlesarray)\n"", '    financial.writeheader()\n', '\n', '    for stock in alldata:\n', '        financial.writerow({titlesarray[0] : stock[0], titlesarray[1] : stock[1],titlesarray[2] : stock[2],titlesarray[3] : stock[3],titlesarray[4] : stock[4], titlesarray[5] : stock[5], titlesarray[6] : stock[6], titlesarray[7] : stock[7] , titlesarray[8] : stock[8], titlesarray[9] : stock[9], titlesarray[10] : stock[10],titlesarray[11] : stock[11],titlesarray[12] : stock[12],titlesarray[13] : stock[13],titlesarray[14] : stock[14],titlesarray[15] : stock[15],titlesarray[16] : stock[16],titlesarray[17] : stock[17] })\n', '\n', ' print datetime.datetime.now()\n', ' print ""Finviz Financial Completed""\n', '\n', '\n', '\n', 'def get_price2book( symbol ):\n', '   from bs4 import BeautifulSoup as bs\n', '   import urllib as u\n', '   try:\n', ""    \turl = r'http://finviz.com/quote.ashx?t={}'.format(symbol.lower())\n"", '        html = u.request.urlopen(url).read()\n', ""        soup = bs(html, 'lxml')\n"", '        # Change the text below to get a diff metric\n', ""        pb =  soup.find(text = r'P/B')\n"", ""        pb_ = pb.find_next(class_='snapshot-td2').text\n"", ""        print( '{} price to book = {}'.format(symbol, pb_) )\n"", '        return pb_\n', '   except Exception as e:\n', '        print(e)\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ' try:\n', '   q = imp_googleQuote(name1, date1, date2)  #interval, timeframe\n', ""   name1= name1.replace(':','_')\n"", ""   file1= dircsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(date2) +'.csv'\n"", '   q.write_csv(file1)\n', ' except: print\n', ""'''\n"", '\n', '\n', ""'''\n"", 'def imp_googleQuoteList(symbols, freqsec=300, nday=1, intraday1=True) :\n', ' qlist=[]; sym=[]\n', ' if intraday1:\n', '  for symbol in symbols:\n', '     #try :\n', '        q = imp_googleIntradayQuote(symbol, freqsec, nday)  #interval, timeframe\n', '        qlist.append(q)\n', '        sym.append(symbol)\n', ""     #except: print('Error:'+symbol)\n"", '\n', ' else :\n', '  for symbol in symbols:\n', '     try :\n', '       q = imp_googleQuote(name1, freqsec, num_days)  #interval, timeframe\n', '     except: print(symbol)\n', '\n', ' return qlist, sym\n', ""'''\n"", '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''        \n"", ""stock_list = ['XOM','AMZN','AAPL','SWKS']\n"", 'p2b_series = pd.Series( index=stock_list )\n', '\n', 'for sym in stock_list:\tp2b_series[sym] = get_price2book(sym)\n', ""'''\n"", '\n', '\n', '\n', ""'''\n"", ""def imp_csv_toext(file1, filenameh5, dfname='sym1', fromzone='Japan', tozone='UTC'):\n"", "" # dateparse= lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", ' from dateutil.parser import parse\n', ' if os.path.getsize(file1)> 500 :\n', '  from_zone = tz.gettz(fromzone); tozone = tz.gettz(tozone)\n', ""  # dateparse= lambda x: (pd.datetime.strptime(x,'%Y-%m-%d %H:%M:%S').replace(tzinfo=from_zone).astimezone(tozone))\n"", '\n', ""  # dateparse= lambda x: (pd.datetime.strptime(x,'%m-%d-%Y %H:%M:%S').replace(tzinfo=from_zone).astimezone(tozone))\n"", '\n', '  dateparse= lambda x: (parse(x).replace(tzinfo=from_zone).astimezone(tozone))\n', '\n', '  # dateparse= lambda x: parse(x, tzinfos=from_zone).astimezone(to_zone)\n', '  print file1\n', ""  #df = pd.read_csv(file1,sep=',',header=None, parse_dates={'date': [1]}, date_parser=dateparse)\n"", '\n', ""  df = pd.read_csv(file1,sep=',',header=0)\n"", '  #df.date= [pd.to_datetime((str(x)[:-6])) for x in  df.date]\n', '  #df.date= [x.to_datetime() for x in  df.date]\n', '\n', '\n', '  df.drop(df.columns[[0]], axis=1, inplace=True)\n', ""  df= util.pd_addcol(df, 'symbol')\n"", ""  df['symbol']= dfname\n"", '\n', '  print filenameh5\n', ""  df.columns = ['date', 'open','high','low','close','volume', 'symbol' ]\n"", ""  df.to_csv(filenameh5 , index=False)  # , compression='gzip' )\n"", '\n', '  # store = pd.HDFStore(filenameh5);\n', '  # store.append(dfname, df);  store.close()\n', ""'''\n"", '\n', ""'''\n"", ""def imp_csv_toext(file1, filenameh5, dropcol=[], type1='csv', hasheader=True, dfname='sym1', fromzone='Japan', tozone='UTC'):\n"", ' if os.path.getsize(file1)> 500 :\n', '   header= 0 if hasheader else None\n', ""   df = pd.read_csv(file1, sep=',',header=0)\n"", '\n', '   df.drop(df.columns[dropcol], axis=1, inplace=True)\n', ""   df= util.pd_addcol(df, 'symbol');  df['symbol']= dfname\n"", ""   df.columns = ['date', 'open','high','low','close','volume', 'symbol' ]\n"", '\n', ""   if type1=='csv' :\n"", ""     df.to_csv(filenameh5 , index=False)  # , compression='gzip' )\n"", ""'''\n"", '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ""     if i==1: close = qq[['Datetime']]  #new dataframe\n"", '     tmp= pd.DataFrame(data={symbol: qq[close1]});\n', ""     close = close.join(tmp, how='outer', rsuffix='_1')\n"", ""     close= close.drop_duplicates(cols='Datetime', take_last=True)\n"", '\n', "" close= close[close['Datetime'].isin(datefilter)]   #Only date in the range\n"", "" close= close.sort('Datetime')\n"", "" close= close.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", '\n', ' close= np.array(close.values[1:,1:]).astype(np.float)  #Only Price\n', ' close= close[(np.abs(sci.stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close, vv\n', ""'''\n"", '\n', '\n', '\n', ""'''\n"", 'class Quote(object):\n', ""  DATE_FMT = '%Y-%m-%d';  TIME_FMT = '%H:%M:%S'\n"", '\n', '  def __init__(self):\n', ""    self.symbol = '';    self.exchn=''\n"", '    self.date,self.time,self.open_,self.high,self.low,self.close,self.volume = ([] for _ in range(7))\n', '\n', '  def append(self,dt,open_,high,low,close,volume):\n', '    self.date.append(dt.date())\n', '    self.time.append(dt.time())\n', '    self.open_.append(float(open_))\n', '    self.high.append(float(high))\n', '    self.low.append(float(low))\n', '    self.close.append(float(close))\n', '    self.volume.append(int(volume))\n', '\n', '  def to_csv(self):\n', '    return \'\'.join([""{0},{1},{2},{3:.2f},{4:.2f},{5:.2f},{6:.2f},{7}\\n"".format(self.symbol,\n', ""              self.date[bar].strftime('%Y-%m-%d'),self.time[bar].strftime('%H:%M:%S'),\n"", '              self.open_[bar],self.high[bar],self.low[bar],self.close[bar],self.volume[bar])\n', '              for bar in xrange(len(self.close))])\n', '\n', '  def write_csv(self,filename):\n', ""    with open(filename,'w') as f:\n"", '      txt= self.to_csv()\n', '      f.write(txt)\n', '\n', '  def read_csv(self,filename):\n', ""    self.symbol = ''\n"", '    self.date,self.time,self.open_,self.high,self.low,self.close,self.volume = ([] for _ in range(7))\n', ""    for line in open(filename,'r'):\n"", ""      symbol,ds,ts,open_,high,low,close,volume = line.rstrip().split(',')\n"", '      self.symbol = symbol\n', ""      dt = datetime.datetime.strptime(ds+' '+ts,self.DATE_FMT+' '+self.TIME_FMT)\n"", '      self.append(dt,open_,high,low,close,volume)\n', '    return True\n', '\n', '  def __repr__(self):\n', '    return self.to_csv()\n', ""'''\n"", '\n', ' \n', '\n', ""'''\n"", ""def imp_googleIntradayQuoteSave2(name1, date1, inter, tframe, dircsv='', dbname=''):\n"", ' q = imp_googleIntradayQuote(name1, inter, tframe)  #interval, timeframe\n', "" name1= name1.replace(':','_')\n"", '\n', "" if dircsv != '' :\n"", ""    file1= dircsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(inter) +'_'+ str(tframe) +'.csv'\n"", '    q.to_csv(file1)\n', '\n', "" if dbname != '' :\n"", '    import sqlalchemy as sql\n', '    dbcon= sql.create_engine(dbname)\n', '    if inter== 300 : # 5mins\n', ""      q.to_sql('q5min', dbcon)\n"", '\n', '\n', ""def imp_googleQuoteList_save(symbols, date1, date2, inter=23400, tframe=2000, dircsv='', intraday1=True) :\n"", ' if intraday1:\n', '  for symbol in symbols:\n', '     try :\n', '       imp_googleIntradayQuoteSave(symbol, date1, inter, tframe, dircsv)\n', ""     except: print('Error:'+symbol)\n"", '\n', ' else :\n', '  for symbol in symbols:\n', '     try :\n', '       imp_googleQuoteSave(symbol, date1, date2, dircsv)\n', '     except: print(symbol)\n', ""'''\n"", '\n']","['np.shape', 'getret_fromquotes', 'len', 'getlogret_fromquotes', 'np.log', 'getprice_fromret']",6
utilmy/zarchive/py2to3/portfolio_withdate.py:index,index,class,103,300,221,3009,10.03,1,3,[],[],[],1835,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc,folioCalc,class,125,509,332,4736,9.3,2,17,[],[],[],2134,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:folioRiskIndicator,folioRiskIndicator,class,89,306,215,3180,10.39,1,9,[],[],[],2309,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF,folioOptimizationF,class,280,1291,671,10994,8.52,6,39,[],[],[],2487,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity,searchSimilarity,class,173,497,328,4550,9.15,8,17,[],[],[],3934,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:index:__init__,index:__init__,method,11,12,12,116,9.67,0,0,"['self', 'id1', 'sym', 'ww', 'tstart']","[None, None, None, None, None]","[None, None, None, None, None]",1836,[],['date_generatedatetime'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:index:close,index:close,method,0,1,1,4,4.0,0,0,['self'],[None],[None],1841,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:index:updatehisto,index:updatehisto,method,2,12,11,62,5.17,0,0,['self'],[None],[None],1844,[],['print'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:index:help,index:help,method,2,12,11,62,5.17,0,0,['self'],[None],[None],1848,[],['print'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:index:__init__,index:__init__,method,11,12,12,116,9.67,0,0,"['self', 'id1', 'sym', 'ww', 'tstart']","[None, None, None, None, None]","[None, None, None, None, None]",1869,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]",['date_generatedatetime'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:index:calc_baskettable_pct,index:calc_baskettable_pct,method,37,95,77,782,8.23,1,2,"['self', 'type1', 'showdetail']","[None, None, None]","[None, '""table""', '0']",1913,"[""      '''   Calc Basket Values from Input data '''\n""]","['np.zeros', 'np.sum', 'self._udpate_wwindpct', 'range', 'np.mod', 'self._wwpct_rebal']",6
utilmy/zarchive/py2to3/portfolio_withdate.py:index:_wwpct_rebal,index:_wwpct_rebal,method,3,4,3,41,10.25,0,0,"['self', 'wwpct_actual', 't', 'trebal']","[None, None, None, None]","[None, None, None, None]",1940,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:index:_udpate_wwindpct,index:_udpate_wwindpct,method,11,25,17,224,8.96,0,0,"['self', 't', 'bskt', 'hedgecost', 'wwpct_actual', 'wwpct_th']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",1950,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:index:calc_baskettable_unit,index:calc_baskettable_unit,method,0,0,0,0,0.0,0,0,[],[],[],1959,"[""    '''   t           --->  t+1                 ---->   t+2\n"", '            wwactual:    B[t]    -->   B[t+1]   using  w[t+1], B[t] /   S[t]\n', '            wwnew   :    B[t+1]  -->   B[t+2]   using  w[t+2], B[t+1] /   S[t+1]\n', ""    '''\n""]",[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:__init__,folioCalc:__init__,method,21,39,37,356,9.13,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2135,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:set_symclose,folioCalc:set_symclose,method,17,34,33,312,9.18,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2148,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:setcriteria,folioCalc:setcriteria,method,41,76,71,1046,13.76,0,0,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2157,"[""        ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '            wwtype:     constant / regime /regimExtra\n', '            initperiod: NbOfDays_initial_Risk_Monitoring\n', '            riskid:     spprice/spperf/multi\n', ""        '''\n""]","['np.array', 'len']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:_weightcalc_generic,folioCalc:_weightcalc_generic,method,5,35,23,396,11.31,0,4,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2207,[],"['self._weightcalc_constant', 'self._weightcalc_regime', 'print']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:_weightcalc_regime,folioCalc:_weightcalc_regime,method,10,37,32,272,7.35,0,1,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2218,[],"['self._regimecalc', 'np.reshape', 'self._mapping_risk_ww']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:_regimecalc,folioCalc:_regimecalc,method,3,7,6,83,11.86,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2228,[],['self._mapping_calc_risk'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:_weightcalc_constant,folioCalc:_weightcalc_constant,method,3,4,4,28,7.0,0,0,"['self', 'ww2', 't']","[None, None, None]","[None, None, None]",2237,[],['np.sum'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:getweight,folioCalc:getweight,method,5,12,9,126,10.5,0,0,['self'],[None],[None],2241,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:calc_baskettable,folioCalc:calc_baskettable,method,33,111,83,811,7.31,1,4,"['self', 'wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps', 'showdetail']","[None, None, None, None, None, None, None, None]","[None, None, None, '""table""', '""constant""', '1', '0.000', '0']",2248,[],"['np.shape', 'np.zeros', 'range', 'np.mod', 'self._weightcalc_generic', 'np.sum', 'print']",7
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:plot,folioCalc:plot,method,10,29,26,295,10.17,0,2,"['self', 'wwvec', 'show1', 'tickperday']","[None, None, None, None]","[None, 'None', '1', '60']",2276,[],"['self.calc_baskettable', 'plot_price', 'print', 'np.array']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:multiperiod_ww,folioCalc:multiperiod_ww,method,15,36,29,240,6.67,1,1,"['self', 't']","[None, None]","[None, None]",2287,[],['xrange'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioCalc:help,folioCalc:help,method,1,2,2,13,6.5,0,0,['self'],[None],[None],2300,[],['print'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioRiskIndicator:__init__,folioRiskIndicator:__init__,method,23,40,38,356,8.9,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1869,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]","['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:folioRiskIndicator:set_symclose,folioRiskIndicator:set_symclose,method,18,32,32,295,9.22,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2324,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:folioRiskIndicator:setcriteria,folioRiskIndicator:setcriteria,method,34,81,70,1006,12.42,0,0,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2334,"[""    ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '        wwtype:     constant / regime /regimExtra\n', '        initperiod: NbOfDays_initial_Risk_Monitoring\n', '        riskid:     spprice/spperf/multi\n', ""    '''    \n""]","['np.array', 'len', 'np.zeros']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folioRiskIndicator:calcrisk,folioRiskIndicator:calcrisk,method,13,18,17,220,12.22,1,0,"['self', 'wwvec', 'initval']","[None, None, None]","[None, '[]', '1']",2382,[],"['xrange', 'self._weightcalc_generic', 'np.column_stack']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folioRiskIndicator:_weightcalc_generic,folioRiskIndicator:_weightcalc_generic,method,4,34,24,327,9.62,0,3,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2394,[],"['self._weightcalc_regime', 'print']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:folioRiskIndicator:_weightcalc_regime,folioRiskIndicator:_weightcalc_regime,method,3,11,11,68,6.18,0,1,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2403,[],['self._regimecalc'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioRiskIndicator:_regimecalc,folioRiskIndicator:_regimecalc,method,7,18,17,216,12.0,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2408,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioRiskIndicator:calc_optimal_weight,folioRiskIndicator:calc_optimal_weight,method,7,18,17,216,12.0,0,0,"['self', 'maxiter', 'name1', 'isreset', 'popsize']","[None, None, None, None, None]","[None, '1', ""''"", '1', '15']",2414,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:__init__,folioOptimizationF:__init__,method,23,39,37,340,8.72,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1869,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]","['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:set_symclose,folioOptimizationF:set_symclose,method,18,32,32,295,9.22,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2324,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:setcriteria,folioOptimizationF:setcriteria,method,44,128,103,1669,13.04,0,2,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2334,"[""    ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '        wwtype:     constant / regime /regimExtra\n', '        initperiod: NbOfDays_initial_Risk_Monitoring\n', '        riskid:     spprice/spperf/multi\n', ""    '''    \n""]","['np.array', 'len', 'print', 'list']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:calcbasket_obj2,folioOptimizationF:calcbasket_obj2,method,23,76,63,756,9.95,0,1,"['self', 'wwvec']","[None, None]","[None, None]",2572,[],"['np.reshape', 'np.array', 'np.sum', 'round', 'calcbasket_objext', 'float']",6
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:calcbasket_obj,folioOptimizationF:calcbasket_obj,method,55,157,118,1494,9.52,1,4,"['self', 'wwvec']","[None, None]","[None, None]",2598,[],"['np.reshape', 'np.array', 'np.sum', 'round', 'calcbasket_objext', 'float', 'calcbasket_obj', 'concenfactor=folio_concenfactor2', 'np.zeros', 'xrange', 'np.mod', 'self._weightcalc_generic', 'self._loss_obj', 'self._objective_criteria']",14
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:_loss_obj,folioOptimizationF:_loss_obj,method,4,6,6,68,11.33,0,0,"['self', 'ww2', 'wwpenalty']","[None, None, None]","[None, None, None]",2623,[],['np.sum'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:_weightcalc_generic,folioOptimizationF:_weightcalc_generic,method,5,40,26,394,9.85,0,4,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2394,[],"['self._weightcalc_constant', 'self._weightcalc_regime', 'print']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:_weightcalc_regime,folioOptimizationF:_weightcalc_regime,method,8,32,28,239,7.47,0,0,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2403,"['""""""\n', 'if bsk > maxprice :       maxprice= bsk\n', 'if maxprice-bsk > maxdd : maxdd=    maxprice-bsk\n', '\n', '""""""\n']","['self._regimecalc', 'np.reshape', 'self._mapping_risk_ww']",3
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:_regimecalc,folioOptimizationF:_regimecalc,method,4,7,7,83,11.86,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2408,"['""""""\n', 'if bsk > maxprice :       maxprice= bsk\n', 'if maxprice-bsk > maxdd : maxdd=    maxprice-bsk\n', '\n', '""""""\n']",['self._mapping_calc_risk'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:_weightcalc_constant,folioOptimizationF:_weightcalc_constant,method,3,4,4,28,7.0,0,0,"['self', 'ww2', 't']","[None, None, None]","[None, None, None]",2662,[],['np.sum'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:calc_optimal_weight,folioOptimizationF:calc_optimal_weight,method,7,18,17,216,12.0,0,0,"['self', 'maxiter', 'name1', 'isreset', 'popsize']","[None, None, None, None, None]","[None, '1', ""''"", '1', '15']",2414,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:getweight,folioOptimizationF:getweight,method,6,11,9,126,11.45,0,0,['self'],[None],[None],2671,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:calc_baskettable,folioOptimizationF:calc_baskettable,method,36,86,72,827,9.62,1,4,"['self', 'wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps', 'showdetail']","[None, None, None, None, None, None, None, None]","[None, None, None, '""table""', '""constant""', '1', ' 0.000', '0']",2679,[],"['np.shape', 'np.zeros', 'range', 'np.mod', 'self._weightcalc_generic', 'np.sum', 'print']",7
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:plot,folioOptimizationF:plot,method,11,31,30,294,9.48,0,2,"['self', 'wwvec', 'show1', 'tickperday']","[None, None, None, None]","[None, 'None', '1', '60']",2708,[],"['self.calc_baskettable', 'plot_price', 'print', 'np.array']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:_objective_criteria,folioOptimizationF:_objective_criteria,method,31,147,78,1105,7.52,2,7,"['self', 'bsk']","[None, None]","[None, None]",2720,[],"['np.std', 'np.sum', 'range', 'np.arange']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:multiperiod_ww,folioOptimizationF:multiperiod_ww,method,18,29,27,238,8.21,1,1,"['self', 't']","[None, None]","[None, None]",2767,[],['xrange'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:help,folioOptimizationF:help,method,50,152,103,1472,9.68,1,6,['self'],[None],[None],2779,[],"['print', 'util.load_obj', 'int', 'copy.deepcopy', 'xrange', 'next', 'util.save_obj', 'np.mod', 'np.abs', 'np.reshape', 'np.sum']",11
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:mapping_risk_ww,folioOptimizationF:mapping_risk_ww,method,12,56,29,245,4.38,0,1,"['self', 'risk', 'wwmat', 'ww2']","[None, None, None, None]","[None, None, None, 'self.wwasset0']",2838,[],"['np.sum', 'np.array']",2
utilmy/zarchive/py2to3/portfolio_withdate.py:folioOptimizationF:_mapping_calc_risk,folioOptimizationF:_mapping_calc_risk,method,27,201,76,831,4.13,0,3,"['self', 'ss', 'tr', 't', 'risk0']","[None, None, None, None, None]","[None, None, None, None, None]",2850,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:__init__,searchSimilarity:__init__,method,15,18,17,193,10.72,0,0,"['self', ""filejpstock=r'E"", 'sym01', 'symname', 'startdate', 'enddate', 'pricetype']","[None, '', None, None, None, None, None]","[None, ""r'E:/_data/stock/daily/20160616/jp'"", ""['7203']"", ""['Toyota']"", ' 20150101', '20160601', '""close""']",3935,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:load_quotes_fromdb,searchSimilarity:load_quotes_fromdb,method,15,35,33,355,10.14,0,1,"['self', 'picklefile']","[None, None]","[None, ""''""]",3940,[],"['util.load_obj', 'imp_txt_getquotes', 'date_align', 'util.a_cleanmemory']",4
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:__generate_return__,searchSimilarity:__generate_return__,method,4,4,4,64,16.0,0,0,"['self', 'nlag']","[None, None]","[None, None]",3951,[],['getret_fromquotes'],1
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:__overweight__,searchSimilarity:__overweight__,method,17,62,41,412,6.65,2,4,"['self', 'px']","[None, None]","[None, None]",3955,[],"['len', 'np.ones', 'util.np_findlocalmax', 'type', 'np.abs', 'wwerr[max', 'util.np_findlocalmin']",7
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:set_searchcriteria,searchSimilarity:set_searchcriteria,method,34,79,63,793,10.04,0,5,"['self', 'name1', 'date1', 'date2', 'nlag', 'searchperiodstart', 'typesearch', '']","[None, None, None, None, None, None, None, None]","[None, ""'7203'"", '20160301', '20160601', '1', '20120101', '""pattern2""', None]",3975,[],"['getret_fromquotes', 'date_finddateid', 'max', 'util.np_findfirst', 'print', 'price_normalize_1d', 'len', 'self.__overweight__']",8
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:launch_search,searchSimilarity:launch_search,method,53,107,81,908,8.49,3,3,['self'],[None],[None],4004,[],"['len', 'np.zeros', 'range', 'price_normalize_1d', 'np_similarity', 'util.np_sortbycolumn', 'enumerate']",7
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:show_comparison_graph,searchSimilarity:show_comparison_graph,method,33,72,63,682,9.47,1,3,"['self', 'maxresult', 'show_only_different_time', 'fromid', 'fromend', 'filenameout']","[None, None, None, None, None, None]","[None, '20', 'True', '0', ' 0', ""''""]",4038,[],"['np.shape', 'range', 'int', 'print', 'price_normalize_1d', 'str', 'plot_price']",7
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:staticmethod,searchSimilarity:staticmethod,method,0,1,1,4,4.0,0,0,"['self', 'x']","[None, None]","[None, None]",4067,[],[],0
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:get_rankresult,searchSimilarity:get_rankresult,method,24,63,53,496,7.87,2,1,"['self', 'filetosave']","[None, None]","[None, ""''""]",4071,[],"['len', 'np.savetxt', 'np.shape', 'np.array', 'np.empty', 'range', 'int', 'str']",8
utilmy/zarchive/py2to3/portfolio_withdate.py:searchSimilarity:export_results,searchSimilarity:export_results,method,0,0,0,0,0.0,0,0,[],[],[],4093,"[""'''\n"", '#####################################################################################\n', '#---------Run over all the indexes, with time lag -----------------------------------\n', 'def np_similarity(x,y, wwerr=[], type1=0):\n', '  if type1==2 : return np_distance_l1(x,y, wwerr)\n', '  if type1==1: return sci.spatial.distance.correlation(x, y)  \n', '  if type1==0 : return 1-np.corrcoef(x,y)[0,1]\n', '\n', 'def np_distance_l1(x,y, wwerr) :\n', ' return np.sum(wwerr * np.abs(x-y))  \n', '  \n', '#---------------------  Universe Load         ---------------------------------------\n', ""filejpstock= r'E:/_data/stock/daily/20160616/jp'\n"", 'sym01= nk400list\n', '\n', 'quotes= imp_txt_getquotes(sym01, filejpstock, startdate=20090101, endate=20160616)\n', 'open1, dateref= date_align(quotes, type1=""close"")    #Get the data and align dates\n', 'del quotes; util.a_cleanmemory()\n', '\n', ""#util.save_obj(open1, 'close_nk400_2009_2016')\n"", ""#util.save_obj(dateref, 'dateref_nk400_2009_2016')\n"", '\n', ""open1= util.load_obj('close_nk400_2009_2016')\n"", ""dateref= util.load_obj( 'dateref_nk400_2009_2016')\n"", '\n', 'nlag=1\n', 'ret_open1 =  getdailyret_fromquotes(open1,nlag)\n', 'del open1\n', '\n', '#--------------------  Search Asset Input     ---------------------------------------\n', ""name1= '7203' \n"", 'date1= 20160328\n', 'date2= 20160531\n', 'twindow= util.np_findfirst(date2, dateref) - util.np_findfirst(date1, dateref) \n', '\n', '\n', 'tstartx= util.np_findfirst(date1, dateref)          # Date Index\n', 'stockx= util.np_findfirst(name1, sym01)\n', 'retx= ret_open1[stockx, tstartx:(tstartx+twindow-1)]    # Recent Period\n', 'px= price_normalize_1d(retx)\n', 'nx= len(px)\n', '\n', '\n', '#----OverWeight for Local Min, Local Max         -----------------------------------\n', 'wwerr= np.ones(nx, dtype=np.float16)\n', 'vvmax= util.np_findlocalmax(px); xprev=0\n', 'for x in vvmax:\n', '  if x[0] !=0 and np.abs(xprev-x[0]) > 6: \n', '    wwerr[max(0,x[0]-1):min(nx,(x[0]+1))]= 1.5;    xprev= x[0] \n', '\n', 'vvmin= util.np_findlocalmin(px); xprev=0\n', 'for x in vvmin:\n', '  if x[0] !=0 and np.abs(xprev-x[0]) > 6: \n', '    wwerr[max(0,x[0]-1):min(nx,(x[0]+1))]= 1.5;    xprev= x[0] \n', '\n', '\n', '#--------------------  Search the Pattern       -------------------------------------\n', 'tmaxx= len(dateref)-1\n', 'similar_list= np.zeros((nstock*(tmaxx-twindow)+1, 3), np.float16)\n', 'i=0\n', 'for iy in range(0, nstock) :\n', ' for t in range(0, tmaxx-twindow):\n', '   if iy == stockx and t > tstartx : \n', '     pass\n', '   else :\n', '    rety= ret_open1[iy, t:(t+twindow-1)]\n', '    py=   price_normalize_1d(rety)\n', '    ss= np_similarity(px, py, wwerr=wwerr, type1=2)\n', '    i+=1\n', '    similar_list[i,0]= iy\n', '    similar_list[i,1]= t\n', '    similar_list[i,2]= ss\n', '\n', '\n', ' #-------------------Clean the table -------------------------------------------------\n', 'rank= util.np_sortbycolumn(similar_list,2, asc=True)\n', 'for i,x in enumerate(rank) :\n', '  if rank[i,2]==0.0 : rank[i,2]= 1000         # Empty cell\n', '  if rank[i,1]> tstartx-3 : rank[i,2]= 100    # Start Element\n', 'rank= util.np_sortbycolumn(rank,2, asc=True)  \n', '\n', '\n', ' #----- Show Similar Stocks ---------------------------------------------------------\n', 'show_only_different_time= True\n', '\n', 'for i in range(0,10) :\n', ' tstarti= rank[i,1]\n', ' stocki= int(rank[i,0])\n', ' \n', ' if show_only_different_time and tstarti < tstartx-1  :\n', '   py= price_normalize100(ret_open1[stocki, tstarti:(tstarti+twindow-1)]) \n', '\n', '   namei= sym01[stocki]\n', '   namefulli= nk400name[util.np_findfirst(namei, nk400list)]\n', '\n', '   tit= namei+ "" "" +namefulli+ "" "" + str(dateref[tstarti])          \n', ""   plot_price(px-100, py-100, label=('Similar '+tit, 'Time','% Var') )\n"", '\n', '\n', 'kix= util.np_findfirst(name1, statsjp[:,0, ilag])\n', 'stocklist01[390]\n', '\n', '\n', 'stocklist01[157] , nk400name[157] ,  dateref1adj[86]     \n', 'plot_price(price_normalize100(ret_open1[157, 86:86+twindow-1])-100,px-100   )\n', ""'''    \n""]",[],0
utilmy/zarchive/py2to3/utilgeo.py:df_to_geojson,df_to_geojson,function,10,24,21,329,13.71,2,0,"['df', 'col_properties', 'lat', 'lon']","[None, None, None, None]","[None, None, ""'latitude'"", ""'longitude'""]",24,[],['df.iterrows'],1
utilmy/zarchive/py2to3/_HELP.py:os_compileVSsolution,os_compileVSsolution,function,6,32,21,239,7.47,0,4,"['dir1', 'flags1', 'type1', 'compilerdir']","[None, None, None, None]","[None, '""""', '""devenv""', '""""']",744,[],['os.system'],1
utilmy/zarchive/py2to3/_HELP.py:os_VS_build,os_VS_build,function,16,62,54,614,9.9,1,2,"['self', 'lib_to_build']","[None, None]","[None, None]",773,[],"['print', 'subprocess.Popen', 'process.poll', 'process.communicate']",4
utilmy/zarchive/py2to3/_HELP.py:set_rc_version,set_rc_version,function,12,37,34,502,13.57,0,0,"['rcfile', 'target_version']","[None, None]","[None, None]",810,[],"['open', 'f.seek', 'f.write', 'f.truncate']",4
utilmy/zarchive/py2to3/_HELP.py:os_VS_start,os_VS_start,function,18,56,52,881,15.73,0,2,"['self', 'version']","[None, None]","[None, None]",847,[],"['os.getenv', 'Exception', 'default.append', 'libs.append']",4
utilmy/zarchive/py2to3/_HELP.py:fun_cython,fun_cython,function,5,10,9,34,3.4,1,0,['a'],[None],[None],890,[],['xrange'],1
utilmy/zarchive/py2to3/_HELP.py:fun_python,fun_python,function,5,10,9,34,3.4,1,0,['a'],[None],[None],897,[],['xrange'],1
utilmy/zarchive/py2to3/_HELP.py:LookupTable,LookupTable,class,4,7,7,94,13.43,0,0,[],[],[],207,[],[],0
utilmy/zarchive/py2to3/geospatial.py:coordinates_box,coordinates_box,class,1,2,2,16,8.0,0,0,[],[],[],33,[],[],0
utilmy/zarchive/py2to3/allmodule.py:pprint,pprint,function,10,32,23,378,11.81,0,1,"['table1', 'tablefmt']","[None, None]","[None, '""simple""']",34,"[' """"""To print nice column headers, supply the second argument (`headers`):\n', '      - `headers` can be an explicit list of column headers\n', '      - if `headers=""firstrow""`, then the first row of data is used\n', '      - if `headers=""keys""`, then dictionary keys or column indices are used\n', '    print(tabulate([[1, 2.34], [-56, ""8.999""], [""2"", ""10001""]]))\n', '  """"""\n']","['isinstance', 'tabulate', 'pprint2', 'pprint.PrettyPrinter', 'pp.pprint']",5
utilmy/zarchive/py2to3/allmodule.py:pprint2,pprint2,function,5,5,5,59,11.8,0,0,['x'],[None],[None],51,"[""  '''import pprint\n"", ""     stuff = ['spam', 'eggs', 'lumberjack', 'knights', 'ni']\n"", 'stuff.insert(0, stuff[:])\n', 'pp = pprint.PrettyPrinter(indent=4)\n', 'pp.pprint(ALLDB)\n', ""  '''\n""]","['pprint.PrettyPrinter', 'pp.pprint']",2
utilmy/zarchive/py2to3/allmodule.py:str_convert_beforeprint,str_convert_beforeprint,function,2,2,2,23,11.5,0,0,['x'],[None],[None],64,"[""  ''' #Before writing/output to printer put in utf-8 '''\n""]",['x.encode'],1
utilmy/zarchive/py2to3/allmodule.py:str_to_utf8,str_to_utf8,function,2,2,2,23,11.5,0,0,['x'],[None],[None],69,"[""  ''' #Before writing/output to printer put in utf-8 '''\n""]",['x.encode'],1
utilmy/zarchive/py2to3/allmodule.py:str_to_unicode,str_to_unicode,function,4,16,12,98,6.12,0,2,"['x', 'encoding']","[None, None]","[None, ""'utf-8'""]",74,"[""  ''' #After Loading Text from CSV ---> Immediately in unicode '''\n""]","['isinstance', 'unicode']",2
utilmy/zarchive/py2to3/util_min.py:os_wait_cpu,os_wait_cpu,function,12,31,27,286,9.23,1,0,"['priority', 'cpu_min']","[None, None]","['300', '50']",23,[],"['psutil.cpu_percent', 'print', 'arrow.utcnow', 'sleep']",4
utilmy/zarchive/py2to3/util_min.py:isexist,isexist,function,3,11,10,58,5.27,0,0,['a'],[None],[None],48,[],['print'],1
utilmy/zarchive/py2to3/util_min.py:isfloat,isfloat,function,5,15,13,67,4.47,0,1,['x'],[None],[None],54,[],['float'],1
utilmy/zarchive/py2to3/util_min.py:isint,isint,function,6,14,13,80,5.71,0,1,['x'],[None],[None],61,[],['txt.find'],1
utilmy/zarchive/py2to3/util_min.py:a_isanaconda,a_isanaconda,function,6,14,13,80,5.71,0,1,[],[],[],63,[],['txt.find'],1
utilmy/zarchive/py2to3/util_min.py:os_zip_checkintegrity,os_zip_checkintegrity,function,9,33,31,185,5.61,0,1,['filezip1'],[None],[None],72,[],"['zipfile.ZipFile', 'zip_file.testzip', 'print']",3
utilmy/zarchive/py2to3/util_min.py:os_zipfile,os_zipfile,function,23,43,37,397,9.23,2,2,"['folderin', 'folderzipname', 'iscompress']","[None, None, None]","[None, None, 'True']",83,[],"['zipfile.ZipFile', 'os.walk', 'zf.write', 'zf.close', 'os_zip_checkintegrity', 'print']",6
utilmy/zarchive/py2to3/util_min.py:os_zipfolder,os_zipfolder,function,15,35,29,346,9.89,0,3,"['dir_tozip', 'zipname', 'dir_prefix', 'iscompress']","[None, None, None, None]","[""'/zdisks3/output'"", ""'/zdisk3/output.zip'"", 'None', 'True']",99,"["" '''\n"", "" shutil.make_archive('/zdisks3/results/output', 'zip',\n"", ""                     root_dir=/zdisks3/results/',\n"", ""                     base_dir='output')\n"", '\n', "" os_zipfolder('zdisk/test/aapackage', 'zdisk/test/aapackage.zip', 'zdisk/test')'''\n""]","['dir_tozip.split', 'shutil.make_archive', 'os_zip_checkintegrity', 'print']",4
utilmy/zarchive/py2to3/util_min.py:os_zipextractall,os_zipextractall,function,23,59,48,538,9.12,1,3,"['filezip_or_dir', 'tofolderextract', 'isprint']","[None, None, None]","['""folder1/*.zip""', ""'zdisk/test'"", '1']",148,"[""   '''os_zipextractall( 'aapackage.zip','zdisk/test/'      )  '''\n""]","['filezip_or_dir.find', 'os_file_listall', 'os_file_getname', 'zipfile.ZipFile', 'zip_ref.extractall', 'zip_ref.close', 'print']",7
utilmy/zarchive/py2to3/util_min.py:os_folder_copy,os_folder_copy,function,17,36,35,345,9.58,0,1,"['src', 'dst', 'symlinks', 'pattern1', 'fun_file_toignore']","[None, None, None, None, None]","[None, None, 'False', '""*.py""', 'None']",178,"[""   '''\n"", '       callable(src, names) -> ignored_names\n', ""       'src' parameter, which is the directory being visited by copytree(), and\n"", ""       'names' which is the list of `src` contents, as returned by os.listdir():\n"", '\n', '    Since copytree() is called recursively, the callable will be called once for each directory that is copied.\n', '    It returns a  list of names relative to the `src` directory that should not be copied.\n', ""   '''\n""]","['fun_file_toignore', 'fnmatch.filter', 'shutil.copytree', 'shutil.copy']",4
utilmy/zarchive/py2to3/util_min.py:os_folder_create,os_folder_create,function,4,7,7,85,12.14,0,1,['directory'],[None],[None],200,[],"['os.getcwd', 'os.makedirs', 'os.chdir']",3
utilmy/zarchive/py2to3/util_min.py:os_folder_robocopy,os_folder_robocopy,function,3,15,15,166,11.07,0,1,"['from_folder', 'to_folder', ""my_log='H""]","[None, None, '']","[""''"", ""''"", ""'H:/robocopy_log.txt'""]",205,"['    """"""\n', '    Copy files to working directory\n', '    robocopy <Source> <Destination> [<File>[ ...]] [<Options>]\n', '    We want to copy the files to a fast SSD drive\n', '    """"""\n']",['print'],1
utilmy/zarchive/py2to3/util_min.py:os_file_replacestring1,os_file_replacestring1,function,11,25,24,238,9.52,1,0,"['findStr', 'repStr', 'filePath']","[None, None, None]","[None, None, None]",216,[],"['fileinput.FileInput', 'line.replace', 'file1.close', 'print']",4
utilmy/zarchive/py2to3/util_min.py:os_file_replacestring2,os_file_replacestring2,function,6,14,13,161,11.5,1,0,"['findstr', 'replacestr', 'some_dir', 'pattern', 'dirlevel']","[None, None, None, None, None]","[None, None, None, '""*.*""', '1']",227,"['  \'\'\' #fil_replacestring_files(""logo.png"", ""logonew.png"", r""D:/__Alpaca__details/aiportfolio"",    pattern=""*.html"", dirlevel=5  )\n', ""  '''\n""]","['os_file_listall', 'os_file_replacestring1']",2
utilmy/zarchive/py2to3/util_min.py:os_file_getname,os_file_getname,function,8,9,8,75,8.33,0,0,['path'],[None],[None],234,[],"['ntpath.split', 'ntpath.basename']",2
utilmy/zarchive/py2to3/util_min.py:os_file_getpath,os_file_getpath,function,7,7,7,52,7.43,0,0,['path'],[None],[None],239,[],['ntpath.split'],1
utilmy/zarchive/py2to3/util_min.py:os_file_gettext,os_file_gettext,function,4,7,7,56,8.0,0,0,['file1'],[None],[None],244,[],"['open', 'f.read']",2
utilmy/zarchive/py2to3/util_min.py:os_file_listall,os_file_listall,function,28,79,46,902,11.42,4,3,"['dir1', 'pattern', 'dirlevel', 'onlyfolder']","[None, None, None, None]","[None, '""*.*""', '1', '0']",250,"[""  '''\n"", '   # DIRCWD=r""D:\\_devs\\Python01\\project""\n', '   # aa= listallfile(DIRCWD, ""*.*"", 2)\n', '   # aa[0][30];   aa[2][30]\n', '\n', '   :param dir1:\n', '   :param pattern:\n', '   :param dirlevel:\n', '   :param onlyfolder:\n', '   :return:\n', ""  '''\n""]","['dir1.rstrip', 'dir1.count', 'os.walk', 'root.count', 'matches.append', 'fnmatch.filter', 'np.array']",7
utilmy/zarchive/py2to3/util_min.py:os_file_rename,os_file_rename,function,32,56,48,642,11.46,2,1,"['some_dir', 'pattern', 'pattern2', 'dirlevel']","[None, None, None, None]","[None, '""*.*""', '""""', '1']",287,[],"['some_dir.rstrip', 'some_dir.count', 'os.walk', 'root.count', 'matches.append', 'fnmatch.filter', 're.sub', 'os.rename', 'np.array']",9
utilmy/zarchive/py2to3/util_min.py:os_print_tofile,os_print_tofile,function,1,2,2,7,3.5,0,0,"['vv', 'file1', 'mode1']","[None, None, None]","[None, None, ""'a'""]",309,"[""    '''\n"", '    Here is a list of the different modes of opening a file:\n', 'r\n', 'Opens a file for reading only. The file pointer is placed at the beginning of the file. This is the default mode.\n', '\n', 'rb\n', '\n', 'Opens a file for reading only in binary format. The file pointer is placed at the beginning of the file. This is the default mode.\n', 'r+\n', '\n', 'Opens a file for both reading and writing. The file pointer will be at the beginning of the file.\n', 'rb+\n', '\n', 'Opens a file for both reading and writing in binary format. The file pointer will be at the beginning of the file.\n', 'w\n', '\n', 'Opens a file for writing only. Overwrites the file if the file exists. If the file does not exist, creates a new file for writing.\n', 'wb\n', '\n', 'Opens a file for writing only in binary format. Overwrites the file if the file exists. If the file does not exist, creates a new file for writing.\n', 'w+\n', '\n', 'Opens a file for both writing and reading. Overwrites the existing file if the file exists. If the file does not exist, creates a new file for reading and writing.\n', 'wb+\n', '\n', 'Opens a file for both writing and reading in binary format. Overwrites the existing file if the file exists. If the file does not exist, creates a new file for reading and writing.\n', 'a\n', '\n', 'Opens a file for appending. The file pointer is at the end of the file if the file exists. That is, the file is in the append mode. If the file does not exist, it creates a new file for writing.\n', 'ab\n', '\n', 'Opens a file for appending in binary format. The file pointer is at the end of the file if the file exists. That is, the file is in the append mode. If the file does not exist, it creates a new file for writing.\n', 'a+\n', '\n', 'Opens a file for both appending and reading. The file pointer is at the end of the file if the file exists. The file opens in the append mode. If the file does not exist, it creates a new file for reading and writing.\n', 'ab+\n', '\n', 'Opens a file for both appending and reading in binary format. The file pointer is at the end of the file if the file exists. The file opens in the append mode. If the file does not exist, it creates a new file for reading and writing.\n', 'To open a text file, use:\n', 'fh = open(""hello.txt"", ""r"")\n', '\n', 'To read a text file, use:\n', 'print fh.read()\n', '\n', 'To read one line at a time, use:\n', 'print fh.readline()\n', '\n', 'To read a list of lines use:\n', 'print fh.readlines()\n', '\n', 'To write to a file, use:\n', 'fh = open(""hello.txt"", ""w"")\n', 'lines_of_text = [""a line of text"", ""another line of text"", ""a third line""]\n', 'fh.writelines(lines_of_text)\n', 'fh.close()\n', '\n', 'To append to file, use:\n', 'fh = open(""Hello.txt"", ""a"")\n', 'fh.close()\n', '\n', ""    '''\n""]",[],0
utilmy/zarchive/py2to3/util_min.py:a_get_pythonversion,a_get_pythonversion,function,1,2,2,7,3.5,0,0,[],[],[],374,[],[],0
utilmy/zarchive/py2to3/util_min.py:os_path_norm,os_path_norm,function,8,20,18,174,8.7,0,1,['pth'],[None],[None],377,"['    \'\'\' #r""D:\\_devs\\Python01\\project\\03-Connect_Java_CPP_Excel\\PyBindGen\\examples"" \'\'\'\n']","['pth.find', 'b.lstrip']",2
utilmy/zarchive/py2to3/util_min.py:os_path_change,os_path_change,function,4,4,4,34,8.5,0,0,['path1'],[None],[None],389,[],"['open', 'fh.read']",2
utilmy/zarchive/py2to3/util_min.py:os_path_current,os_path_current,function,4,4,4,34,8.5,0,0,[],[],[],391,[],"['open', 'fh.read']",2
utilmy/zarchive/py2to3/util_min.py:os_file_exist,os_file_exist,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],393,[],"['open', 'fh.read']",2
utilmy/zarchive/py2to3/util_min.py:os_file_size,os_file_size,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],395,[],"['open', 'fh.read']",2
utilmy/zarchive/py2to3/util_min.py:os_file_read,os_file_read,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],397,[],"['open', 'fh.read']",2
utilmy/zarchive/py2to3/util_min.py:os_file_mergeall,os_file_mergeall,function,10,20,20,185,9.25,1,0,"['nfile', 'dir1', 'pattern1', 'deepness']","[None, None, None, None]","[None, None, None, '2']",401,[],"['os_file_listall', 'open', 'os_file_gettext', 'nfile1.write', 'nfile1.close']",5
utilmy/zarchive/py2to3/util_min.py:os_path_append,os_path_append,function,4,19,11,121,6.37,0,3,"['p1', 'p2', 'p3', 'p4']","[None, None, None, None]","[None, 'None', 'None', 'None']",412,[],[],0
utilmy/zarchive/py2to3/util_min.py:os_split_dir_file,os_split_dir_file,function,6,15,15,112,7.47,0,1,['dirfile'],[None],[None],421,[],"['dirfile.split', 'len']",2
utilmy/zarchive/py2to3/util_min.py:py_importfromfile,py_importfromfile,function,12,21,19,251,11.95,0,1,"['modulename', 'dir1']","[None, None]","[None, None]",430,[],"['a_get_pythonversion', 'SourceFileLoader', 'foo.MyClass', 'imp.load_source']",4
utilmy/zarchive/py2to3/util_min.py:py_memorysize,py_memorysize,function,16,56,38,320,5.71,0,4,"['o', 'ids', 'hint', 'set(']","[None, None, None, None]","[None, None, '"" deep_getsizeof(df_pd', None]",442,"['    """""" deep_getsizeof(df_pd, set())\n', '    Find the memory footprint of a Python object\n', '    The sys.getsizeof function does a shallow size of only. It counts each\n', '    object inside a container as pointer only regardless of how big it\n', '    """"""\n']","['id', 'getsizeof', 'ids.add', 'isinstance', 'sum', 'd', 'o.iteritems']",7
utilmy/zarchive/py2to3/util_min.py:save,save,function,2,4,4,64,16.0,0,0,"['obj', 'folder', 'isabsolutpath']","[None, None, None]","[None, ""'/folder1/keyname'"", '0']",470,[],['py_save_obj'],1
utilmy/zarchive/py2to3/util_min.py:load,load,function,2,3,3,60,20.0,0,0,"['folder', 'isabsolutpath']","[None, None]","[""'/folder1/keyname'"", '0']",473,[],['py_load_obj'],1
utilmy/zarchive/py2to3/util_min.py:save_test,save_test,function,6,11,11,124,11.27,0,0,"['folder', 'isabsolutpath']","[None, None]","[""'/folder1/keyname'"", '0']",476,[],"['py_load_obj', 'print', 'str', 'gc.collect']",4
utilmy/zarchive/py2to3/util_min.py:py_save_obj,py_save_obj,function,17,36,35,308,8.56,0,1,"['obj', 'folder', 'isabsolutpath']","[None, None, None]","[None, ""'/folder1/keyname'"", '0']",482,[],"['folder.find', 'z_key_splitinto_dir_name', 'os_folder_create', 'open', 'pickle.dump']",5
utilmy/zarchive/py2to3/util_min.py:py_load_obj,py_load_obj,function,16,30,28,265,8.83,0,1,"['folder', 'isabsolutpath', 'encoding1']","[None, None, None]","[""'/folder1/keyname'"", '0', ""'utf-8'""]",495,"[""    '''def load_obj(name, encoding1='utf-8' ):\n"", ""         with open('D:/_devs/Python01/aaserialize/' + name + '.pkl', 'rb') as f:\n"", '            return pickle.load(f, encoding=encoding1)\n', ""    '''\n""]","['folder.find', 'z_key_splitinto_dir_name', 'os_folder_create', 'open', 'pickle.load']",5
utilmy/zarchive/py2to3/util_min.py:z_key_splitinto_dir_name,z_key_splitinto_dir_name,function,6,15,15,112,7.47,0,1,['keyname'],[None],[None],511,[],"['keyname.split', 'len']",2
utilmy/zarchive/py2to3/rstatpy.py:stl,stl,function,32,165,104,1260,7.64,0,11,"['data', 'ns', 'np', 'nt', 'nl', 'isdeg', 'itdeg', 'ildeg', 'nsjump', 'ntjump', 'nljump', 'ni', 'no', 'fulloutput']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', '0', '1', '1', 'None', 'None', 'None', '2', '0', 'False']",9,"['    """"""\n', '    Seasonal-Trend decomposition procedure based on LOESS\n', '    data : pandas.Series\n', '    ns : int\n', '        Length of the seasonal smoother.\n', '        The value of  ns should be an odd integer greater than or equal to 3.\n', '        A value ns>6 is recommended. As ns  increases  the  values  of  the\n', '        seasonal component at a given point in the seasonal cycle (e.g., January\n', '        values of a monthly series with  a  yearly cycle) become smoother.\n', '    np : int\n', '        Period of the seasonal component.\n', '        For example, if  the  time series is monthly with a yearly cycle, then\n', '        np=12.\n', '        If no value is given, then the period will be determined from the\n', '        ``data`` timeseries.\n', '    nt : int\n', '        Length of the trend smoother.\n', '        The  value  of  nt should be an odd integer greater than or equal to 3.\n', '        A value of nt between 1.5*np and 2*np is  recommended. As nt increases,\n', '        the values of the trend component become  smoother.\n', '        If nt is None, it is estimated as the smallest odd integer greater\n', '        or equal to ``(1.5*np)/[1-(1.5/ns)]``\n', '    nl : int\n', '        Length of the low-pass filter.\n', '        The value of nl should  be an odd integer greater than or equal to 3.\n', '        The smallest odd integer greater than or equal to np is used by default.\n', '    isdeg : int\n', '        Degree of locally-fitted polynomial in seasonal smoothing.\n', '        The value is 0 or 1.\n', '    itdeg : int\n', '        Degree of locally-fitted polynomial in trend smoothing.\n', '        The value is 0 or 1.\n', '    ildeg : int\n', '        Degree of locally-fitted polynomial in low-pass smoothing.\n', '        The value is 0 or 1.\n', '    nsjump : int\n', '        Skipping value for seasonal smoothing.\n', '        The seasonal smoother skips ahead nsjump points and then linearly\n', '        interpolates in between.  The value  of nsjump should be a positive\n', '        integer; if nsjump=1, a seasonal smooth is calculated at all n points.\n', '        To make the procedure run faster, a reasonable choice for nsjump is\n', '        10%-20% of ns. By default, nsjump= 0.1*ns.\n', '    ntjump : int\n', '        Skipping value for trend smoothing. If None, ntjump= 0.1*nt\n', '    nljump : int\n', '        Skipping value for low-pass smoothing. If None, nljump= 0.1*nl\n', '    ni :int\n', '        Number of loops for updating the seasonal and trend  components.\n', '        The value of ni should be a positive integer.\n', '        See the next argument for advice on the  choice of ni.\n', '        If ni is None, ni is set to 2 for robust fitting, to 5 otherwise.\n', '    no : int\n', '        Number of iterations of robust fitting. The value of no should\n', '        be a nonnegative integer. If the data are well behaved without\n', '        outliers, then robustness iterations are not needed. In this case\n', '        set no=0, and set ni=2 to 5 depending on how much security\n', '        you want that  the seasonal-trend looping converges.\n', '        If outliers are present then no=3 is a very secure value unless\n', '        the outliers are radical, in which case no=5 or even 10 might\n', '        be better.  If no>0 then set ni to 1 or 2.\n', '        If None, then no is set to 15 for robust fitting, to 0 otherwise.\n', '    fulloutput : bool\n', '        If True, a dictionary holding the full output of the original R routine\n', '        will be returned.\n', '    returns\n', '    data : pandas.DataFrame\n', '        The seasonal, trend, and remainder components\n', '    """"""\n']","['data.copy', '_data.dropna', 'pandas.DateRange', 'pandas.Series', 'ceil', 'robjects.IntVector', 'ts_', 'stl_', 'asarray', 'pandas.DataFrame']",10
utilmy/zarchive/py2to3/linux.py:load_session,load_session,function,4,7,7,86,12.29,0,0,['name'],[None],"[""'test_20160815'""]",100,[],['dill.load_session'],1
utilmy/zarchive/py2to3/linux.py:save_session,save_session,function,6,9,9,107,11.89,0,0,['name'],[None],"[""''""]",106,[],"['date_now', 'dill.dump_session']",2
utilmy/zarchive/py2to3/linux.py:isfloat,isfloat,function,3,17,13,80,4.71,0,1,['value'],[None],[None],121,[],['float'],1
utilmy/zarchive/py2to3/linux.py:isint,isint,function,6,15,14,80,5.33,0,1,['x'],[None],[None],129,[],['txt.find'],1
utilmy/zarchive/py2to3/linux.py:aa_isanaconda,aa_isanaconda,function,6,15,14,80,5.33,0,1,[],[],[],132,[],['txt.find'],1
utilmy/zarchive/py2to3/linux.py:aa_cleanmemory,aa_cleanmemory,function,3,3,3,21,7.0,0,0,[],[],[],147,[],['gc.collect'],1
utilmy/zarchive/py2to3/linux.py:aa_getmodule_doc,aa_getmodule_doc,function,8,13,13,110,8.46,0,0,"['module1', 'fileout']","[None, None]","[None, ""''""]",150,[],"['ca.getmodule_doc', 'np.isnan', 'z.nonzero']",3
utilmy/zarchive/py2to3/linux.py:np_interpolate_nan,np_interpolate_nan,function,7,9,9,78,8.67,0,0,['y'],[None],[None],1176,[],"['__np_nan_helper', 'np.interp', 'x']",3
utilmy/zarchive/py2to3/linux.py:and1,and1,function,8,55,22,356,6.47,0,6,"['x', 'y', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']","[None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None', 'None', 'None']",1182,[],[],0
utilmy/zarchive/py2to3/linux.py:sortcol,sortcol,function,6,8,8,77,9.62,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",1192,"[' """""" df.sort([\'A\', \'B\'], ascending=[1, 0])  """"""\n']","['pd.DataFrame', 'df.sort_values']",2
utilmy/zarchive/py2to3/linux.py:sort,sort,function,7,20,12,181,9.05,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",1198,[],"['pd.DataFrame', 'df.sort_values', 'sort']",3
utilmy/zarchive/py2to3/linux.py:np_ma,np_ma,function,2,3,3,45,15.0,0,0,"['vv', 'n']","[None, None]","[None, None]",1205,"[""  '''Moving average '''\n""]","['np.convolve', 'np.ones']",2
utilmy/zarchive/py2to3/linux.py:np_cleanmatrix,np_cleanmatrix,function,11,22,20,124,5.64,2,1,['m'],[None],[None],1211,[],"['np.nan_to_num', 'np.shape', 'range', 'abs']",4
utilmy/zarchive/py2to3/linux.py:np_sortbycolumn,np_sortbycolumn,function,6,8,8,77,9.62,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",1222,[],"['pd.DataFrame', 'df.sort_values']",2
utilmy/zarchive/py2to3/linux.py:np_sortbycol,np_sortbycol,function,8,30,20,257,8.57,0,1,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",1227,[],"['pd.DataFrame', 'df.sort_values', 'np_sortbycol', 'len', 'np.reshape']",5
utilmy/zarchive/py2to3/linux.py:np_findfirst,np_findfirst,function,5,11,10,55,5.0,1,1,"['item', 'vec']","[None, None]","[None, None]",1238,"['    """"""return the index of the first occurence of item in vec""""""\n']",['xrange'],1
utilmy/zarchive/py2to3/linux.py:np_find,np_find,function,6,24,13,133,5.54,2,2,"['item', 'vec']","[None, None]","[None, None]",1244,[],"['xrange', 'np_find']",2
utilmy/zarchive/py2to3/linux.py:find,find,function,12,42,30,219,5.21,1,3,"['item', 'vec']","[None, None]","[None, None]",1250,"['    """"""return the index of the first occurence of item in vec""""""\n']","['xrange', 'findx', 'type', 'vec.index', 'len', 'finds']",6
utilmy/zarchive/py2to3/linux.py:findx,findx,function,6,27,18,119,4.41,0,2,"['item', 'vec']","[None, None]","[None, None]",1257,"['    """"""return the index of the first occurence of item in vec""""""\n']","['type', 'vec.index', 'len']",3
utilmy/zarchive/py2to3/linux.py:finds,finds,function,12,34,23,156,4.59,2,3,"['itemlist', 'vec']","[None, None]","[None, None]",1268,"['  """"""return the index of the first occurence of item in vec""""""\n']","['xrange', 'idlist.append']",2
utilmy/zarchive/py2to3/linux.py:findhigher,findhigher,function,5,13,12,51,3.92,1,1,"['x', 'vec']","[None, None]","[None, None]",1280,"['    """"""return the index of the first occurence of item in vec""""""\n']",['xrange'],1
utilmy/zarchive/py2to3/linux.py:findlower,findlower,function,5,12,11,51,4.25,1,1,"['x', 'vec']","[None, None]","[None, None]",1287,"['    """"""return the index of the first occurence of item in vec""""""\n']",['xrange'],1
utilmy/zarchive/py2to3/linux.py:np_find_minpos,np_find_minpos,function,4,7,5,95,13.57,0,0,['values'],[None],[None],1297,[],['min'],1
utilmy/zarchive/py2to3/linux.py:np_find_maxpos,np_find_maxpos,function,17,45,34,266,5.91,1,3,['values'],[None],[None],1301,[],"['max', 'np_find_maxpos_2nd', 'float', 'enumerate']",4
utilmy/zarchive/py2to3/linux.py:np_find_maxpos_2nd,np_find_maxpos_2nd,function,13,37,28,138,3.73,1,3,['numbers'],[None],[None],1306,[],"['float', 'enumerate']",2
utilmy/zarchive/py2to3/linux.py:np_findlocalmax2,np_findlocalmax2,function,45,116,77,690,5.95,2,5,"['v', 'trig']","[None, None]","[None, None]",1320,[],"['n=len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'range', 'len', 'findhigher', 'np.abs']",8
utilmy/zarchive/py2to3/linux.py:np_findlocalmin2,np_findlocalmin2,function,46,119,78,703,5.91,2,6,"['v', 'trig']","[None, None]","[None, None]",1355,[],"['n=len', 'np.zeros', 'np_find_minpos', 'enumerate', 'range', 'len', 'findlower', 'np.abs']",8
utilmy/zarchive/py2to3/linux.py:np_findlocalmax,np_findlocalmax,function,45,116,77,690,5.95,2,5,"['v', 'trig']","[None, None]","[None, None]",1392,[],"['n=len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'range', 'len', 'findhigher', 'np.abs']",8
utilmy/zarchive/py2to3/linux.py:np_findlocalmin,np_findlocalmin,function,46,119,78,703,5.91,2,6,"['v', 'trig']","[None, None]","[None, None]",1407,[],"['n=len', 'np.zeros', 'np_find_minpos', 'enumerate', 'range', 'len', 'findlower', 'np.abs']",8
utilmy/zarchive/py2to3/linux.py:np_stack,np_stack,function,12,53,25,323,6.09,0,7,"['v1', 'v2', 'v3', 'v4', 'v5']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",1428,[],"['np.shape', 'np.row_stack', 'np.column_stack']",3
utilmy/zarchive/py2to3/linux.py:np_uniquerows,np_uniquerows,function,6,8,8,148,18.5,0,0,['a'],[None],[None],1443,[],"['np.ascontiguousarray', 'np.unique', 'unique_a.view']",3
utilmy/zarchive/py2to3/linux.py:np_remove_zeros,np_remove_zeros,function,2,4,4,35,8.75,0,0,"['vv', 'axis1']","[None, None]","[None, '1']",1448,[],[],0
utilmy/zarchive/py2to3/linux.py:np_sort,np_sort,function,8,30,20,257,8.57,0,1,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",1451,"['    """"""Produce psuedo-code for decision tree.\n', '    tree -- scikit-leant DescisionTree.\n', '    feature_names -- list of feature names.\n', '    target_names -- list of target (output) names.\n', '    spacer_base -- used for spacing code (default: ""    "").\n', '    """"""\n']","['pd.DataFrame', 'df.sort_values', 'np_sortbycol', 'len', 'np.reshape']",5
utilmy/zarchive/py2to3/linux.py:np_memory_array_adress,np_memory_array_adress,function,2,2,2,38,19.0,0,0,['x'],[None],[None],1454,[],[],0
utilmy/zarchive/py2to3/linux.py:sk_featureimportance,sk_featureimportance,function,9,21,21,228,10.86,1,1,"['clfrf', 'feature_name']","[None, None]","[None, None]",1463,[],"['np.argsort', 'range', 'len', 'str']",4
utilmy/zarchive/py2to3/linux.py:sk_showconfusion,sk_showconfusion,function,11,22,21,244,11.09,0,1,"['clfrf', 'X_train', 'Y_train', 'isprint']","[None, None, None, None]","[None, None, None, 'True']",1471,[],"['clfrf.predict', 'cm.astype', 'cm.sum', 'print']",4
utilmy/zarchive/py2to3/linux.py:sk_tree,sk_tree,function,13,34,32,430,12.65,0,1,"['Xtrain', 'Ytrain', 'nbtree', 'maxdepth', 'print1']","[None, None, None, None, None]","[None, None, None, None, None]",1479,[],"['clfrf.fit', 'clfrf.predict', 'cm.astype', 'cm.sum', 'print']",5
utilmy/zarchive/py2to3/linux.py:sk_gen_ensemble_weight,sk_gen_ensemble_weight,function,14,30,28,213,7.1,1,0,"['vv', 'acclevel', 'maxlevel']","[None, None, None]","[None, None, '0.88']",1492,[],"['min', 'np.empty', 'range', 'estww.append', 'np.log', 'np.array']",6
utilmy/zarchive/py2to3/linux.py:sk_votingpredict,sk_votingpredict,function,19,33,28,267,8.09,2,1,"['estimators', 'voting', 'ww', 'X_test']","[None, None, None, None]","[None, None, None, None]",1506,[],"['np.zeros', 'enumerate', 'clf.predict_proba', 'range', 'len']",5
utilmy/zarchive/py2to3/linux.py:sk_tree_get_ifthen,sk_tree_get_ifthen,function,26,89,62,768,8.63,2,3,"['tree', 'feature_names', 'target_names', 'spacer_base']","[None, None, None, None]","[None, None, None, '"" ""']",1524,"['    """"""Produce psuedo-code for decision tree.\n', '    tree -- scikit-leant DescisionTree.\n', '    feature_names -- list of feature names.\n', '    target_names -- list of target (output) names.\n', '    spacer_base -- used for spacing code (default: ""    "").\n', '    """"""\n']","['recurse', 'print', 'str', 'zip', 'int']",5
utilmy/zarchive/py2to3/linux.py:pd_array_todataframe,pd_array_todataframe,function,13,45,29,277,6.16,0,2,"['price', 'symbols', 'date1', 'dotranspose']","[None, None, None, None]","[None, 'None', 'None', 'False']",1562,[],"['np.shape', 'len', 'pd.DataFrame']",3
utilmy/zarchive/py2to3/linux.py:pd_date_intersection,pd_date_intersection,function,7,18,15,157,8.72,1,0,['qlist'],[None],[None],1574,[],"['set', 'set.intersection', 'list', 'sorted']",4
utilmy/zarchive/py2to3/linux.py:pd_resetindex,pd_resetindex,function,3,4,4,50,12.5,0,0,['df'],[None],[None],1585,[],['list'],1
utilmy/zarchive/py2to3/linux.py:pd_create_colmap_nametoid,pd_create_colmap_nametoid,function,9,12,12,77,6.42,1,0,['df'],[None],[None],1591,"[""  ''' 'close' ---> 5    '''\n""]",['enumerate'],1
utilmy/zarchive/py2to3/linux.py:pd_dataframe_toarray,pd_dataframe_toarray,function,9,10,10,107,10.7,0,0,['df'],[None],[None],1599,[],[],0
utilmy/zarchive/py2to3/linux.py:pd_changeencoding,pd_changeencoding,function,6,8,8,89,11.12,1,0,"['data', 'cols']","[None, None]","[None, None]",1606,[],[],0
utilmy/zarchive/py2to3/linux.py:pd_createdf,pd_createdf,function,2,4,4,53,13.25,0,0,"['val1', 'col1', 'idx1']","[None, None, None]","[None, 'None', 'None']",1613,[],['pd.DataFrame'],1
utilmy/zarchive/py2to3/linux.py:pd_insertcolumn,pd_insertcolumn,function,12,29,27,179,6.17,1,1,"['df', 'colname', 'vec']","[None, None, None]","[None, None, None]",1618,"["" ''' Vec and Colname must be aligned '''\n""]","['len', 'np.shape', 'range', 'df.insert']",4
utilmy/zarchive/py2to3/linux.py:pd_insertrows,pd_insertrows,function,17,31,30,237,7.65,1,1,"['df', 'rowval', 'index1']","[None, None, None]","[None, None, 'None']",1632,[],"['np.shape', 'np.arange', 'pd.DataFrame', 'range', 'df.append']",5
utilmy/zarchive/py2to3/linux.py:pd_replacevalues,pd_replacevalues,function,11,14,14,101,7.21,1,0,"['df', 'matrix']","[None, None]","[None, None]",1645,"["" ''' Matrix replaces df.values  '''\n""]",['np.shape'],1
utilmy/zarchive/py2to3/linux.py:pd_storeadddf,pd_storeadddf,function,6,15,15,113,7.53,0,1,"['df', 'dfname', ""dbfile='F""]","[None, None, '']","[None, None, ""'F:\\temp_pandas.h5'""]",1655,[],"['pd.HDFStore', 'find', 'store.keys', 'store.append', 'store.close']",5
utilmy/zarchive/py2to3/linux.py:pd_storedumpinfo,pd_storedumpinfo,function,13,29,24,298,10.28,1,0,"[""dbfile='E""]",[''],"[""'E:\\_data\\stock\\intraday_google.h5'""]",1663,[],"['pd.HDFStore', 'store.keys', 'pd.read_hdf', 'extract.append', 'datetime_tostring', 'errsym.append', 'np.array']",7
utilmy/zarchive/py2to3/linux.py:pd_remove_row,pd_remove_row,function,2,2,2,29,14.5,0,0,"['df', 'row_list_index', '45]']","[None, None, None]","[None, '[23', None]",1677,[],['df.drop'],1
utilmy/zarchive/py2to3/linux.py:pd_extract_col_idx_val,pd_extract_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],1680,[],[],0
utilmy/zarchive/py2to3/linux.py:pd_split_col_idx_val,pd_split_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],1684,[],[],0
utilmy/zarchive/py2to3/linux.py:pd_addcolumn,pd_addcolumn,function,10,23,19,190,8.26,1,1,"['df1', 'name1']","[None, None]","[None, ""'new'""]",1688,[],"['len', 'type', 'pd.Series']",3
utilmy/zarchive/py2to3/linux.py:pd_removecolumn,pd_removecolumn,function,2,3,3,23,7.67,0,0,"['df1', 'name1']","[None, None]","[None, None]",1697,[],['df1.drop'],1
utilmy/zarchive/py2to3/linux.py:pd_save_vectopanda,pd_save_vectopanda,function,9,22,19,182,8.27,1,0,"['vv', 'filenameh5']","[None, None]","[None, None]",1701,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/py2to3/linux.py:pd_load_panda2vec,pd_load_panda2vec,function,9,22,19,182,8.27,1,0,"['filenameh5', 'store_id']","[None, None]","[None, ""'data'""]",1706,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/py2to3/linux.py:pd_csv_topanda,pd_csv_topanda,function,9,22,19,182,8.27,1,0,"['filein1', 'filename', 'tablen']","[None, None, None]","[None, None, ""'data'""]",1711,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/py2to3/linux.py:pd_getpanda_tonumpy,pd_getpanda_tonumpy,function,6,10,10,92,9.2,0,0,"['filename', 'nsize', 'tablen']","[None, None, None]","[None, None, ""'data'""]",1723,[],['pd.read_hdf'],1
utilmy/zarchive/py2to3/linux.py:pd_getrandom_tonumpy,pd_getrandom_tonumpy,function,6,9,9,101,11.22,0,0,"['filename', 'nbdim', 'nbsample', 'tablen']","[None, None, None, None]","[None, None, None, ""'data'""]",1728,[],['pd.read_hdf'],1
utilmy/zarchive/py2to3/linux.py:sk_cluster_kmeans,sk_cluster_kmeans,function,21,38,37,375,9.87,1,1,"['x', 'nbcluster', 'isplot']","[None, None, None]","[None, '5', 'True']",1861,[],"['np.std', 'kmeans.fit', 'range', 'plt.plot', 'plt.show']",5
utilmy/zarchive/py2to3/linux.py:datetime_tostring,datetime_tostring,function,8,26,21,244,9.38,1,2,['datelist1'],[None],[None],1885,[],"['isinstance', 'datelist1.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zarchive/py2to3/linux.py:date_remove_bdays,date_remove_bdays,function,16,41,28,351,8.56,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",1896,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/py2to3/linux.py:date_add_bdays,date_add_bdays,function,16,40,29,350,8.75,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",1914,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/py2to3/linux.py:datestring_todatetime,datestring_todatetime,function,6,15,13,125,8.33,1,1,"['datelist1', 'format1']","[None, None]","[None, ' ""%Y%m%d""']",1929,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/py2to3/linux.py:datetime_toint,datetime_toint,function,6,15,13,155,10.33,1,1,['datelist1'],[None],[None],1938,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/py2to3/linux.py:dateint_todatetime,dateint_todatetime,function,6,15,13,135,9.0,1,1,['datelist1'],[None],[None],1946,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/py2to3/linux.py:date_diffindays,date_diffindays,function,4,5,5,74,14.8,0,0,"['intdate1', 'intdate2']","[None, None]","[None, None]",1955,[],['dateint_todatetime'],1
utilmy/zarchive/py2to3/linux.py:date_finddateid,date_finddateid,function,5,75,20,492,6.56,0,10,"['date1', 'dateref']","[None, None]","[None, None]",1960,[],['util.np_findfirst'],1
utilmy/zarchive/py2to3/linux.py:datestring_toint,datestring_toint,function,6,16,14,106,6.62,1,1,['datelist1'],[None],[None],1974,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/py2to3/linux.py:date_now,date_now,function,10,18,16,130,7.22,0,1,['i'],[None],['0'],1981,[],"['datetime.now', 'date_add_bdays', 'date_remove_bdays', 'str']",4
utilmy/zarchive/py2to3/linux.py:date_as_float,date_as_float,function,11,28,23,273,9.75,0,1,['dt'],[None],[None],1989,[],"['datetime.datetime', 'isleap', 'timedelta']",3
utilmy/zarchive/py2to3/linux.py:datediff_inyear,datediff_inyear,function,2,3,3,53,17.67,0,0,"['startdate', 'enddate']","[None, None]","[None, None]",2001,[],['date_as_float'],1
utilmy/zarchive/py2to3/linux.py:date_generatedatetime,date_generatedatetime,function,18,27,27,267,9.89,0,1,"['start', 'nbday', 'end']","[None, None, None]","['""20100101""', '10', '""""']",2005,[],"['datestring_todatetime', 'date_add_bdays', 'datetime.timedelta', 'list', 'np.array']",5
utilmy/zarchive/py2to3/linux.py:datetime_tostring,datetime_tostring,function,8,26,21,244,9.38,1,2,['datelist1'],[None],[None],1885,[],"['isinstance', 'datelist1.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zarchive/py2to3/linux.py:datestring_todatetime,datestring_todatetime,function,6,15,13,125,8.33,1,1,"['datelist1', 'format1']","[None, None]","[None, ' ""%Y%m%d""']",1929,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/py2to3/linux.py:datestring_toint,datestring_toint,function,6,16,14,106,6.62,1,1,['datelist1'],[None],[None],1974,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/py2to3/linux.py:numexpr_vect_calc,numexpr_vect_calc,function,13,28,22,214,7.64,0,0,"['filename', 'expr', 'i0', 'imax', ""fileout='E""]","[None, None, None, None, '']","[None, None, '0', '1000', ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",2048,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/py2to3/linux.py:numexpr_topanda,numexpr_topanda,function,10,28,22,248,8.86,0,0,"['filename', 'expr', 'i0', 'imax', ""fileout='E""]","[None, None, None, None, '']","[None, None, '0', '1000', ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",2060,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/py2to3/linux.py:textvect_topanda,textvect_topanda,function,2,8,8,70,8.75,0,0,"['vv', 'fileout']","[None, None]","[None, '""""']",2070,[],"['pd.DataFrame', 'pd.HDFStore', 'st.append']",3
utilmy/zarchive/py2to3/linux.py:comoment,comoment,function,4,6,6,61,10.17,0,0,"['xx', 'yy', 'nsample', 'kx', 'ky']","[None, None, None, None, None]","[None, None, None, None, None]",2093,[],['ne.evaluate'],1
utilmy/zarchive/py2to3/linux.py:acf,acf,function,12,34,31,244,7.18,0,0,['data'],[None],[None],2101,[],"['len', 'np.mean', 'np.sum', 'float', 'r', 'np.arange', 'np.asarray']",7
utilmy/zarchive/py2to3/linux.py:plotsave,plotsave,function,7,16,16,199,12.44,0,0,"['xx', 'yy', 'title1']","[None, None, None]","[None, None, '""""']",2124,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",6
utilmy/zarchive/py2to3/linux.py:plotshow,plotshow,function,4,9,9,101,11.22,0,0,"['xx', 'yy', 'title1']","[None, None, None]","[None, None, '""""']",2135,[],"['plt.scatter', 'plt.autoscale', 'plt.title', 'plt.show']",4
utilmy/zarchive/py2to3/linux.py:compileVSsolution,compileVSsolution,function,5,33,19,239,7.24,0,4,"['dir1', 'flags1', 'type1', 'compilerdir']","[None, None, None, None]","[None, '""""', '""devenv""', '""""']",2568,[],['os.system'],1
utilmy/zarchive/py2to3/linux.py:VS_start,VS_start,function,18,56,52,881,15.73,0,2,"['self', 'version']","[None, None]","[None, None]",2597,[],"['os.getenv', 'Exception', 'default.append', 'libs.append']",4
utilmy/zarchive/py2to3/linux.py:VS_build,VS_build,function,16,62,54,614,9.9,1,2,"['self', 'lib_to_build']","[None, None]","[None, None]",2632,[],"['print', 'subprocess.Popen', 'process.poll', 'process.communicate']",4
utilmy/zarchive/py2to3/linux.py:set_rc_version,set_rc_version,function,12,43,39,500,11.63,0,0,"['rcfile', 'target_version']","[None, None]","[None, None]",2666,[],"['open', 'f.read', 're.compile', 're.sub', 'f.seek', 'f.write', 'f.truncate']",7
utilmy/zarchive/py2to3/kagglegym.py:r_score,r_score,function,4,15,12,146,9.73,0,1,"['y_true', 'y_pred', 'sample_weight', 'multioutput']","[None, None, None, None]","[None, None, 'None', 'None']",77,[],['r2_score'],1
utilmy/zarchive/py2to3/kagglegym.py:make,make,function,2,2,2,19,9.5,0,0,[],[],[],175,[],['Environment'],1
utilmy/zarchive/py2to3/kagglegym.py:Observation,Observation,class,7,11,11,99,9.0,0,0,[],[],[],86,[],[],0
utilmy/zarchive/py2to3/kagglegym.py:Environment,Environment,class,49,148,101,1719,11.61,0,1,[],[],[],93,[],[],0
utilmy/zarchive/py2to3/kagglegym.py:Observation:__init__,Observation:__init__,method,6,6,6,58,9.67,0,0,"['self', 'train', 'target', 'features']","[None, None, None, None]","[None, None, None, None]",87,[],[],0
utilmy/zarchive/py2to3/kagglegym.py:Environment:__init__,Environment:__init__,method,21,37,35,452,12.22,0,0,['self'],[None],[None],94,[],"['pd.HDFStore', 'hfdata.get', 'len', 'int']",4
utilmy/zarchive/py2to3/kagglegym.py:Environment:reset,Environment:reset,method,16,36,34,403,11.19,0,0,['self'],[None],[None],115,[],"['int', 'Observation']",2
utilmy/zarchive/py2to3/kagglegym.py:Environment:step,Environment:step,method,29,64,52,765,11.95,0,1,"['self', 'target']","[None, None]","[None, None]",135,[],"['r_score', 'Observation']",2
utilmy/zarchive/py2to3/kagglegym.py:Environment:__str__,Environment:__str__,method,1,2,2,21,10.5,0,0,['self'],[None],[None],171,[],[],0
utilmy/zarchive/py2to3/portfolio.py:data_jpsector,data_jpsector,function,3,4,4,47,11.75,0,0,[],[],[],37,[],['util.load_obj'],1
utilmy/zarchive/py2to3/portfolio.py:date_earningquater,date_earningquater,function,9,103,42,604,5.86,0,5,['t1'],[None],[None],47,[],['datetime.datetime'],1
utilmy/zarchive/py2to3/portfolio.py:date_is_3rdfriday,date_is_3rdfriday,function,4,14,13,67,4.79,0,0,['s'],[None],[None],66,[],"['datetime.strptime', 'd.weekday']",2
utilmy/zarchive/py2to3/portfolio.py:date_option_expiry,date_option_expiry,function,10,50,33,354,7.08,0,2,['date'],[None],[None],71,[],['datetime.datetime'],1
utilmy/zarchive/py2to3/portfolio.py:date_find_kday_fromintradaydate,date_find_kday_fromintradaydate,function,2,3,3,68,22.67,0,0,"['kintraday', 'intradaydate', 'dailydate']","[None, None, None]","[None, None, None]",88,[],['util.np_find'],1
utilmy/zarchive/py2to3/portfolio.py:date_find_kintraday_fromdate,date_find_kintraday_fromdate,function,4,10,10,88,8.8,0,0,"['d1', 'intradaydate1', 'h1', 'm1']","[None, None, None, None]","[None, None, '9', '30']",92,[],"['datetime.datetime', 'util.np_find']",2
utilmy/zarchive/py2to3/portfolio.py:date_find_intradateid,date_find_intradateid,function,8,13,13,96,7.38,1,0,"['datetimelist', 'stringdate']","[None, None]","[None, ""['20160420223000']""]",99,[],"['datestring_todatetime', 'util.np_find', 'str']",3
utilmy/zarchive/py2to3/portfolio.py:datetime_convertzone1_tozone2,datetime_convertzone1_tozone2,function,16,29,25,343,11.83,1,1,"['tt', 'fromzone', 'tozone']","[None, None, None]","[None, ""'Japan'"", ""'US/Eastern'""]",110,[],"['pytz.timezone', 'type', 'tz.localize', 't2.append']",4
utilmy/zarchive/py2to3/portfolio.py:date_extract_dailyopenclosetime,date_extract_dailyopenclosetime,function,23,51,35,431,8.45,2,3,"['spdateref1', 'market']","[None, None]","[None, ""'us'""]",127,[],"['enumerate', 'spdailyopendate.append', 'np.array', 'spdailyclosedate.append']",4
utilmy/zarchive/py2to3/portfolio.py:date_finddateid,date_finddateid,function,5,75,20,492,6.56,0,10,"['date1', 'dateref']","[None, None]","[None, None]",147,[],['util.np_findfirst'],1
utilmy/zarchive/py2to3/portfolio.py:date_alignfromdateref,date_alignfromdateref,function,13,31,27,206,6.65,1,1,"['array1', 'dateref']","[None, None]","[None, None]",162,[],"['range', 'np.float', 'len']",3
utilmy/zarchive/py2to3/portfolio.py:_date_align,_date_align,function,16,42,33,256,6.1,2,2,"['dateref', 'datei', 'tmax', 'closei']","[None, None, None, None]","[None, None, None, None]",187,[],"['np.zeros', 'range', 'len']",3
utilmy/zarchive/py2to3/portfolio.py:date_align,date_align,function,13,31,27,206,6.65,1,1,"['array1', 'dateref']","[None, None]","[None, None]",201,"["" ''' #Aligne the price with the same dates date\tyear\tmonth\tday\td\topen\tclose\thigh\tlow\tvolume\taclose '''\n""]","['range', 'np.float', 'len']",3
utilmy/zarchive/py2to3/portfolio.py:min_withposition,min_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],249,[],['min'],1
utilmy/zarchive/py2to3/portfolio.py:max_withposition,max_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],253,[],['max'],1
utilmy/zarchive/py2to3/portfolio.py:_reshape,_reshape,function,2,14,12,68,4.86,0,1,['x'],[None],[None],259,[],"['len', 'np.reshape', 'np.size']",3
utilmy/zarchive/py2to3/portfolio.py:_notnone,_notnone,function,39,211,114,1769,8.38,1,11,['x'],[None],[None],263,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend']",23
utilmy/zarchive/py2to3/portfolio.py:plot_price,plot_price,function,69,272,170,2746,10.1,2,11,"['asset', 'y2', 'y3', 'y4', 'y5', 'sym', 'savename1', 'tickperday', 'date1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', 'None', ""''"", '20', 'None', '(10', None]",267,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend', 'plot_priceintraday', 'np.loadtxt', 'np.unique', 'xdays.append', 'np.hstack', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick']",35
utilmy/zarchive/py2to3/portfolio.py:plot_priceintraday,plot_priceintraday,function,32,59,57,947,16.05,1,0,['data'],[None],[None],346,[],"['np.loadtxt', 'np.unique', 'np.arange', 'xdays.append', 'np.hstack', 'plt.figure', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick', 'plt.show']",14
utilmy/zarchive/py2to3/portfolio.py:plot_check,plot_check,function,8,19,19,257,13.53,0,0,"['close', 'tt0i', 'tt1i', 'dateref', 'sym', 'tickperday']","[None, None, None, None, None, None]","[None, '20140102', '20160815', '[]', '[]', '120']",388,[],"['util.find', 'getret_fromquotes', 'price_normalize100', 'plot_price']",4
utilmy/zarchive/py2to3/portfolio.py:plot_pricedate,plot_pricedate,function,18,61,49,534,8.75,1,4,"['date1', 'sym1', 'asset1', 'sym2', 'bsk1', 'verticaldate', 'savename1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None]","[None, None, None, 'None', 'None', 'None', ""''"", '(10', None]",398,[],"['type', 'datestring_todatetime', 'np.row_stack', 'np.concatenate', 'array_todataframe', 'df_asset.plot', 'ax.axvline', 'np.arange', 'int', 'len', 'ax.savefig']",11
utilmy/zarchive/py2to3/portfolio.py:generate_sepvertical,generate_sepvertical,function,9,13,12,149,11.46,0,1,"['asset1', 'tt', 'tmax', 'start', 'datebar']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",480,[],"['np.zeros', 'datestring_todatetime']",2
utilmy/zarchive/py2to3/portfolio.py:save_asset_tofile,save_asset_tofile,function,31,129,87,978,7.58,5,9,"['file1', 'asset1', 'asset2', 'asset3', 'date1', 'title1']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",495,[],"['np.shape', 'len', 'np.reshape', 'print', 'np.zeros', 'range', 'int', 'np.row_stack', 'np.savetxt']",9
utilmy/zarchive/py2to3/portfolio.py:load_asset_fromfile,load_asset_fromfile,function,1,4,4,51,12.75,0,0,['file1'],[None],[None],534,[],[],0
utilmy/zarchive/py2to3/portfolio.py:array_todataframe,array_todataframe,function,11,41,27,259,6.32,0,2,"['price', 'symbols', 'date1']","[None, None, None]","[None, 'None', 'None']",538,[],"['np.shape', 'len', 'pd.DataFrame']",3
utilmy/zarchive/py2to3/portfolio.py:pd_dataframe_toarray,pd_dataframe_toarray,function,9,10,10,107,10.7,0,0,['df'],[None],[None],549,[],[],0
utilmy/zarchive/py2to3/portfolio.py:isfloat,isfloat,function,3,17,13,80,4.71,0,1,['value'],[None],[None],558,[],['float'],1
utilmy/zarchive/py2to3/portfolio.py:isint,isint,function,32,105,61,1036,9.87,0,8,['x'],[None],[None],566,[],"['Xmat.copy', 'x.mean', 'x.std', 'np.corrcoef', 'MinCovDet', 'OAS', 'correl_inv.copy', 'np.sqrt']",8
utilmy/zarchive/py2to3/portfolio.py:correlation_mat,correlation_mat,function,32,105,61,1036,9.87,0,8,"['Xmat', 'type1', 'type2']","[None, None, None]","[None, '""robust""', '""correl""']",573,[],"['Xmat.copy', 'x.mean', 'x.std', 'np.corrcoef', 'MinCovDet', 'OAS', 'correl_inv.copy', 'np.sqrt']",8
utilmy/zarchive/py2to3/portfolio.py:correl_reducebytrigger,correl_reducebytrigger,function,9,22,18,128,5.82,2,1,"['correl2', 'trigger']","[None, None]","[None, None]",621,"[""  ''' Put Zero below trigger  '''\n""]","['correl2.copy', 'range', 'abs']",3
utilmy/zarchive/py2to3/portfolio.py:sk_cov_fromcorrel,sk_cov_fromcorrel,function,8,14,13,115,8.21,0,0,"['correl', 'ret_close1']","[None, None]","[None, None]",633,[],"['volhisto_fromret', 'np.diag', 'np.dot']",3
utilmy/zarchive/py2to3/portfolio.py:cointegration,cointegration,function,18,47,37,385,8.19,0,0,"['x', 'y']","[None, None]","[None, None]",639,"['  """""" Output :\n', '    coint_t :t-statistic of unit-root test on residuals\n', ""    pvalue :   MacKinnon's approximate p-value based on MacKinnon (1994)\n"", '    crit_value : Critical values for the test statistic at the 1 %, 5 %, and 10 %  Signifiance levels.\n', '\n', '    The Null hypothesis is that there is no cointegration, the alt hypothesis is that there is cointegrating relationship. \n', '    If the pvalue is small, below a critical size:  Reject Null Hypothesis -->  Cointegration\n', '           pvalue is high --> No Cointegration    \n', '    \n', '    Significance level, also denoted as alpha or , is the probability of rejecting  the null hypothesis when it is true.\n', '    Probability of incorrectly rejecting a true null hypothesis\n', '\n', '    P=0.05  , At least 23% (and typically close to 50%)\n', '    P=0.01  , At least 7% (and typically close to 15%\n', '\n', '    (-2.9607012342275936,\n', '    0.038730981052330332, 0,\n', '    249,\n', ""    {'1%': -3.4568881317725864,\n"", ""       '10%': -2.5729936189738876,\n"", ""     '5%': -2.8732185133016057},\n"", '      601.96849256295991)\n', '        It can be seen that the calculated test statistic of -2.96 is smaller than the 5% critical value of -2.87, \n', ""        which means that we can reject the null hypothesis that  there isn't a cointegrating relationship at the 5% level.\n"", '  """"""\n']",['np.zeros'],1
utilmy/zarchive/py2to3/portfolio.py:causality_y1_y2,causality_y1_y2,function,10,14,13,153,10.93,0,0,"['price2', 'price1', 'maxlag']","[None, None, None]","[None, None, None]",677,[],[],0
utilmy/zarchive/py2to3/portfolio.py:reg_slope,reg_slope,function,16,24,24,261,10.88,1,0,"['close', 'dateref', 'tlag', 'type1']","[None, None, None, None]","[None, None, None, ""'elasticv'""]",685,[],"['np.ones', 'xrange', 'regression', 'np.arange']",4
utilmy/zarchive/py2to3/portfolio.py:rolling_cointegration,rolling_cointegration,function,13,29,26,280,9.66,0,0,"['x', 'y']","[None, None]","[None, None]",700,[],"['rolling_coint', 'pd.expanding_apply']",2
utilmy/zarchive/py2to3/portfolio.py:regression,regression,function,13,67,32,589,8.79,0,5,"['yreturn', 'xreturn', 'type1']","[None, None, None]","[None, None, '""elasticv""']",719,"["" '''Y = X* reg.coef_  +  reg.intercept_, r2 '''\n""]","['len', 'yreturn.ravel', 'xreturn.reshape', 'reg.fit', 'F', 'reg.score']",6
utilmy/zarchive/py2to3/portfolio.py:regression_fixedsymbolstock,regression_fixedsymbolstock,function,30,59,54,591,10.02,1,1,"['sym', 'ret_close2', 'tsstart', 'tsample', 'ret_spy', 'spyclose', 'regonly']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, 'True']",756,"["" ''' Price, Weight, R2, Error  '''\n""]","['len', 'range', 'np.reshape', 'np.column_stack', 'regression', 'getpriceret_fromregression', 'np.max']",7
utilmy/zarchive/py2to3/portfolio.py:regression_getpricefromww,regression_getpricefromww,function,20,38,30,298,7.84,3,0,"['spyclose', 'ww01', 'regasset01', 'ret_close2', 'tstart', 'tlag']","[None, None, None, None, None, None]","[None, None, None, None, None, '1']",779,[],"['np.copy', 'np.zeros', 'range', 'len']",4
utilmy/zarchive/py2to3/portfolio.py:regression_allstocks_vs_riskfactors,regression_allstocks_vs_riskfactors,function,65,137,113,1351,9.86,2,1,"['symstock', 'pricestock', 'symriskfac', 'priceriskfac', 'nlaglist']","[None, None, None, None, None]","[None, None, None, None, None]",800,"["" '''Make Regression on all stocks ''' \n""]","['np.shape', 'print', 'np.arange', 'np.empty', 'len', 'enumerate', 'getlogret_fromquotes', 'range', 'np.reshape', 'regression_fixedsymbolstock', 'util.np_sortbycolumn', 'util.pd_array_todataframe', 'util.print_object']",13
utilmy/zarchive/py2to3/portfolio.py:getdiff_fromquotes,getdiff_fromquotes,function,4,6,6,73,12.17,0,0,"['close', 'timelag']","[None, None]","[None, None]",897,[],['np.shape'],1
utilmy/zarchive/py2to3/portfolio.py:getret_fromquotes,getret_fromquotes,function,7,23,17,159,6.91,0,1,"['close', 'timelag']","[None, None]","[None, '1']",901,[],"['np.shape', 'len']",2
utilmy/zarchive/py2to3/portfolio.py:getlogret_fromquotes,getlogret_fromquotes,function,4,7,7,81,11.57,0,0,"['close', 'timelag']","[None, None]","[None, '1']",912,[],"['np.shape', 'np.log']",2
utilmy/zarchive/py2to3/portfolio.py:getprice_fromret,getprice_fromret,function,12,23,21,190,8.26,2,0,"['ret', 'normprice']","[None, None]","[None, '100']",917,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/py2to3/portfolio.py:price_normalize100,price_normalize100,function,16,43,36,292,6.79,1,1,"['ret', 'normprice']","[None, None]","[None, '100']",929,[],"['np.shape', 'len', 'np.reshape', 'np.zeros', 'range', 'np.cumprod']",6
utilmy/zarchive/py2to3/portfolio.py:price_normalize_1d,price_normalize_1d,function,8,22,19,147,6.68,0,0,"['ret', 'normprice', 'dtype1']","[None, None, None]","[None, '100', ' np.float32']",949,[],"['np.empty', 'np.cumprod']",2
utilmy/zarchive/py2to3/portfolio.py:norm_fast,norm_fast,function,7,8,8,85,10.62,0,0,"['y', 'ny']","[None, None]","[None, None]",958,[],['ne.evaluate'],1
utilmy/zarchive/py2to3/portfolio.py:correl_fast,correl_fast,function,8,13,13,116,8.92,0,0,"['xn', 'y', 'nx']","[None, None, None]","[None, None, None]",966,[],['ne.evaluate'],1
utilmy/zarchive/py2to3/portfolio.py:volhisto_fromret,volhisto_fromret,function,2,14,11,168,12.0,0,1,"['retbsk', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",974,[],"['len', 'np.std']",2
utilmy/zarchive/py2to3/portfolio.py:volhisto_fromprice,volhisto_fromprice,function,6,25,17,245,9.8,0,1,"['price', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",981,[],"['len', 'np.shape', 'np.std']",3
utilmy/zarchive/py2to3/portfolio.py:volhistorolling_fromprice,volhistorolling_fromprice,function,16,42,30,318,7.57,2,1,"['price', 'volrange']","[None, None]","[None, None]",990,[],"['np.shape', 'len', 'np.zeros', 'xrange', 'volhisto_fromprice']",5
utilmy/zarchive/py2to3/portfolio.py:rsk_calc_all_TA,rsk_calc_all_TA,function,51,223,105,1352,6.06,0,10,['df'],[None],"[""'panda_dataframe'""]",1009,"[""  '''Add All TA RMI, RSI To the '''\n""]","['ta.MA', 'ta.distance', 'ta.RET', 'ta.RMI', 'ta.STDDEV', 'ta.nbday_low', 'ta.nbday_high', 'ta.RSI', 'ta.qearning_dist', 'ta.optionexpiry_dist', 'linearreg2', 'np.array', 'np.sum', 'util.np_findlocalmin2', 'len', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",18
utilmy/zarchive/py2to3/portfolio.py:ta_highbandtrend1,ta_highbandtrend1,function,42,170,75,910,5.35,0,11,"['close2', 'type1']","[None, None]","[None, '0']",1091,[],"['linearreg', 'np.array', 'np.sum', 'util.np_findlocalmax2', 'len', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",8
utilmy/zarchive/py2to3/portfolio.py:pd_transform_asset,pd_transform_asset,function,11,35,27,409,11.69,0,1,"['q0', 'q1', 'type1']","[None, None, None]","[None, None, '""spread""']",1140,[],"['util.pd_date_intersection', 'util.pd_array_todataframe', 'util.pd_insertcol']",3
utilmy/zarchive/py2to3/portfolio.py:calcbasket_table,calcbasket_table,function,25,59,53,477,8.08,1,2,"['wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps']","[None, None, None, None, None, None]","[None, None, '""table""', '""constant""', '1', ' 0.000']",1154,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.mod', 'np.sum']",6
utilmy/zarchive/py2to3/portfolio.py:folio_createvolta_asset,folio_createvolta_asset,function,10,19,19,159,8.37,1,0,"['close', 'vol', 'volrange', 'lev']","[None, None, None, None]","[None, '0.12', '120', '1.0']",1181,[],"['np.zeros_like', 'np.shape', 'xrange', 'folio_volta']",4
utilmy/zarchive/py2to3/portfolio.py:folio_lowcorrelation,folio_lowcorrelation,function,145,431,300,3894,9.03,8,7,"['sym01', 'nstock', 'periodlist', 'dateref', 'close1', 'kbenchmark', 'badlist', 'costbppa', 'showgraph']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, '0.02', 'True']",1189,[],"['ww_selection', 'np.corrcoef', 'np.nan_to_num', 'np.zeros', 'range', 'util.np_sortbycol', 'int', 'np.std', 'np.isnan', 'util.find', 'np.sum', 'stk_select.append', 'volhisto_fromret', 'sum', '1/len', 'getret_fromquotes', 'len', 'np.empty', 'np.ones', 'enumerate', 'calcbasket_table', 'price_normalize100', 'np.column_stack', 'np.zeros_like', 'np.array', 'np.concatenate', 'print', 'str', 'plot_price', 'sym01[int', 'volhisto_fromprice', 'xrange', 'folio_inverseetf']",33
utilmy/zarchive/py2to3/portfolio.py:folio_leverageetf,folio_leverageetf,function,8,23,20,156,6.78,1,0,"['price', 'lev', 'costpa']","[None, None, None]","[None, '1.0', '0.0']",1327,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/py2to3/portfolio.py:folio_inverseetf,folio_inverseetf,function,8,18,18,151,8.39,1,0,"['price', 'costpa']","[None, None]","[None, '0.0']",1336,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/py2to3/portfolio.py:folio_longshort_unit,folio_longshort_unit,function,59,240,112,1594,6.64,2,7,"['long1', 'short1', 'ww', '-1]', 'costpa', 'tlag', 'istable', 'wwschedule']","[None, None, None, None, None, None, None, None]","[None, None, '[1', None, '0.0', '1', '1', '[]']",1345,[],"['len', 'np.zeros', 'xrange', 'np.mod', 'folio_longshort_unitfixed']",5
utilmy/zarchive/py2to3/portfolio.py:folio_longshort_unitfixed,folio_longshort_unitfixed,function,43,111,73,719,6.48,1,3,"['long1', 'short1', 'nn', '-1]', 'costpa', 'tlag', 'istable']","[None, None, None, None, None, None, None]","[None, None, '[1', None, '0.0', '1', '1']",1386,[],"['len', 'np.zeros', 'xrange', 'np.mod']",4
utilmy/zarchive/py2to3/portfolio.py:folio_longshort_pct,folio_longshort_pct,function,8,21,20,189,9.0,1,0,"['long1', 'short1', 'ww', '-1]', 'costpa']","[None, None, None, None, None]","[None, None, '[1', None, '0.0']",1424,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/py2to3/portfolio.py:folio_histogram,folio_histogram,function,13,75,41,914,12.19,0,0,['close'],[None],[None],1432,[],"['getret_fromquotes', 'plt.subplots', 'np.ones_like', 'ax2.hist', 'ax2.set_title', 'ax2.set_xlabel', 'ax2.set_ylabel', 'ax2.axvline', 'plt.show', 'print']",10
utilmy/zarchive/py2to3/portfolio.py:folio_voltarget,folio_voltarget,function,2,6,6,61,10.17,0,0,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1460,[],['folio_volta'],1
utilmy/zarchive/py2to3/portfolio.py:folio_volta,folio_volta,function,54,197,112,1663,8.44,2,8,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1464,[],"['folio_volta', 'len', 'np.shape', 'np.zeros', 'range', 'np.std', 'util.findhigher', 'max', 'min', 'folio_volta2', 'np.abs']",11
utilmy/zarchive/py2to3/portfolio.py:folio_volta2,folio_volta2,function,29,61,54,514,8.43,1,2,"['bsk', 'riskind', 'par', 'targetvol', 'volrange', 'cap', 'floor', 'costbp']","[None, None, None, None, None, None, None, None]","[None, None, None, '0.11', ' 90', '1.5', '0.0', '0.0005']",1503,[],"['np.shape', 'np.zeros', 'range', 'np.std', 'max', 'min', 'np.abs']",7
utilmy/zarchive/py2to3/portfolio.py:folio_fixedunitprice,folio_fixedunitprice,function,13,36,33,221,6.14,2,0,"['price', 'fixedww', 'costpa']","[None, None, None]","[None, None, '0.0']",1528,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/py2to3/portfolio.py:folio_fixedweightprice,folio_fixedweightprice,function,13,36,33,225,6.25,2,0,"['price', 'fixedww', 'costpa']","[None, None, None]","[None, None, '0.0']",1542,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/py2to3/portfolio.py:folio_fixedweightret,folio_fixedweightret,function,13,31,28,193,6.23,2,0,"['ret', 'fixedww']","[None, None]","[None, None]",1554,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/py2to3/portfolio.py:folio_cost_turnover,folio_cost_turnover,function,26,88,71,825,9.38,1,1,"['wwall', 'bsk', 'dateref', 'costbp']","[None, None, None, None]","[None, None, None, None]",1567,[],"['len', 'np.zeros', 'xrange', 'np.sum', 'plot_price', 'date1=dateint_tostring']",6
utilmy/zarchive/py2to3/portfolio.py:folio_riskpa,folio_riskpa,function,20,52,40,422,8.12,2,0,"['ret', 'targetvol', 'volrange', 'cap']","[None, None, None, None]","[None, '0.1', '90', '1.0']",1597,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.sum', 'np.std', 'min', 'list']",8
utilmy/zarchive/py2to3/portfolio.py:folio_perfreport_schedule,folio_perfreport_schedule,function,14,20,19,189,9.45,1,0,"['sym', 'dateref', 'close', 'wwind', 't0', 'scheduleperiod']","[None, None, None, None, None, None]","[None, None, None, None, None, '""1monthend""']",1911,[],"['np.shape', 'generate_schedule', 'np.zeros', 'enumerate']",4
utilmy/zarchive/py2to3/portfolio.py:folio_concenfactor2,folio_concenfactor2,function,7,12,12,103,8.58,0,0,"['ww', 'masset']","[None, None]","[None, '12']",2211,[],"['getweight', 'np.sum', 'np.mean']",3
utilmy/zarchive/py2to3/portfolio.py:calcbasket_objext,calcbasket_objext,function,50,203,96,1094,5.39,1,7,"['RETURN', 'TMAX', 'riskind_i', 'wwmat', 'wwasset0', 'ww0', 'nbrange', 'criteria']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",2229,[],"['xrange', 'np.sum', 'math.sqrt']",3
utilmy/zarchive/py2to3/portfolio.py:calc_ranktable,calc_ranktable,function,20,44,41,351,7.98,1,0,"['close2', 'symjp1', 'nlag', 'refindex', 'funeval', 'funargs']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",3639,[],"['np.zeros', 'getlogret_fromquotes', 'enumerate', 'funeval', 'util.np_sortbycolumn']",5
utilmy/zarchive/py2to3/portfolio.py:similarity_correl,similarity_correl,function,7,10,9,88,8.8,0,0,"['ret_close2', 'funargs']","[None, None]","[None, None]",3659,[],['correlation_mat'],1
utilmy/zarchive/py2to3/portfolio.py:np_similarity,np_similarity,function,6,17,12,138,8.12,0,3,"['x', 'y', 'wwerr', 'type1']","[None, None, None, None]","[None, None, '[]', '0']",3714,[],['np_distance_l1'],1
utilmy/zarchive/py2to3/portfolio.py:np_distance_l1,np_distance_l1,function,2,3,3,31,10.33,0,0,"['x', 'y', 'wwerr']","[None, None, None]","[None, None, None]",3720,[],"['np.sum', 'np.abs']",2
utilmy/zarchive/py2to3/portfolio.py:np_similarity,np_similarity,function,6,17,12,138,8.12,0,3,"['x', 'y', 'wwerr', 'type1']","[None, None, None, None]","[None, None, '[]', '0']",3714,[],['np_distance_l1'],1
utilmy/zarchive/py2to3/portfolio.py:np_distance_l1,np_distance_l1,function,2,3,3,31,10.33,0,0,"['x', 'y', 'wwerr']","[None, None, None]","[None, None, None]",3720,[],"['np.sum', 'np.abs']",2
utilmy/zarchive/py2to3/portfolio.py:get,get,function,11,44,26,436,9.91,0,1,"['close', 'timelag']","[None, None]","[None, None]",4085,"['    """"""Return dataframe of requested dataset from Quandl.\n', '    :param dataset: str or list, depending on single dataset usage or multiset usage\n', '            Dataset codes are available on the Quandl website\n', '    :param str api_key: Downloads are limited to 50 unless api_key is specified\n', '    :param str start_date, end_date: Optional datefilers, otherwise entire dataset is returned\n', '    :param str collapse: Options are daily, weekly, monthly, quarterly, annual\n', '    :param str transform: options are diff, rdiff, cumul, and normalize\n', '    :param int rows: Number of rows which will be returned\n', '    :param str order: options are asc, desc. Default: `asc`\n', '    :param str returns: specify what format you wish your dataset returned as,\n', '        either `numpy` for a numpy ndarray or `pandas`. Default: `pandas`\n', '    :returns: :class:`pandas.DataFrame` or :class:`numpy.ndarray`\n', '    Any other `kwargs` passed to `get` are sent as field/value params to Quandl\n', '    with no interference.\n', '\n', ""'''\n"", '\n', '\n', '\n', '#--------------------Import data from Yahoo --------------------------------------------------\n', 'def imp_yahoo_getquotes(symbols, start=""20150101"", end=""20160101"", type1=1):\n', ' d1 = datetime.datetime(int(start[0:4]), int(start[4:6]), int(start[6:8]))\n', ' d2 = datetime.datetime(int(end[0:4]), int(end[4:6]), int(end[6:8]))\n', '# symbols = np.array(symbol1)\n', ' quotes= []; errorlist=[]; correctlist=[]\n', ' for i, symbol in enumerate(symbols) :\n', '   # if np.mod(i,400) ==0 and i != 0 :\n', ""     # print('Waiting 30s'); time.sleep(30)\n"", '   try:   \n', '     array1= quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n', '     df= pd.DataFrame.from_records(array1)\n', '     quotes.append(df)\n', '     correctlist.append(symbol)\n', '   except :\n', '       errorlist.append(symbol)\n', ""       print('Err: '+symbol)\n"", ' #quotes = [quotes_historical_yahoo(symbol, d1, d2, asobject=True)  for symbol in symbols]\n', ' #print(errorlist) \n', ' if type1==1 :  return quotes, correctlist\n', ' else: return quotes, correctlist, errorlist  \n', '\n', '              \n', '#Find error in Ticker import\n', 'def imp_yahoo_geterrorticker(symbols, start=""20150101"", end=""20160101""):\n', ' d1 = datetime.datetime(int(start[0:4]), int(start[4:6]), int(start[6:8]))\n', ' d2 = datetime.datetime(int(end[0:4]), int(end[4:6]), int(end[6:8]))\n', ' errorlist= []\n', ' for symbol in symbols :\n', '   try :  \n', '     quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n', '   except :\n', '     errorlist.append(symbol)\n', '     print(symbol)       \n', ' return errorlist\n', ' \n', ""'''\n"", ""symbols0= ['KHC', 'PYPL', 'HPE', 'BXLT', 'SYF', 'WLTW', 'CFG', 'BF-B', 'WRK']\n"", '\n', '#Show list of ticker in error\n', 'import_errorticker(symbols0,""20110101"",""20150601""  )\n', ""'''\n"", '\n', '\n', 'def imp_yahoo_financials_url(ticker_symbol, statement=""is"", quarterly=False):\n', '    if statement == ""is"" or statement == ""bs"" or statement == ""cf"":\n', '        url = ""https://finance.yahoo.com/q/"" + statement + ""?s="" + ticker_symbol\n', '        if not quarterly:\n', '            url += ""&annual""; return BeautifulSoup(requests.get(url).text, ""html.parser"")\n', '    return sys.exit(""Invalid financial statement code \'"" + statement + ""\' passed."")\n', '\n', 'def imp_yahoo_periodic_figure(soup, yahoo_figure):\n', '    values = []; pattern = re.compile(yahoo_figure)\n', '\n', '    title = soup.find(""strong"", text=pattern)    # works for the figures printed in bold\n', '    if title:  row = title.parent.parent\n', '    else:\n', '        title = soup.find(""td"", text=pattern)    # works for any other available figure\n', '        if title:  row = title.parent\n', '        else:      sys.exit(""Invalid figure \'"" + yahoo_figure + ""\' passed."")\n', '\n', '    cells = row.find_all(""td"")[1:]    # exclude the <td> with figure name\n', '    for cell in cells:\n', '        if cell.text.strip() != yahoo_figure:    # needed because some figures are indented\n', '            str_value = cell.text.strip().replace("","", """").replace(""("", ""-"").replace("")"", """")\n', '            if str_value == ""-"": str_value = 0\n', '            value = int(str_value) * 1000\n', '            values.append(value)\n', '\n', '    return values\n', '    \n', '# print(imp_yahoo_periodic(imp_yahoo_financials_url(""AAPL"", ""is""), ""Income Tax Expense""))\n', '\n', ' \n', '\n', '#--------------------Import Quotes Google  ------------------------------------------\n', 'import requests.packages.urllib3\n', 'requests.packages.urllib3.disable_warnings()\n', 'import urllib, os\n', '\n', ""dirstockcsv= 'E:\\_data\\stock\\csv'\n"", '\n', 'def imp_googleIntradayQuote(symbol, freqsec=300, nday=5):\n', ""    ''' Intraday quotes from google. Specify interval seconds and number of days\n"", '    http://www.google.com/finance/getprices?q=7261&i=300&p=3d&f=d,o,h,l,c,v\n', ""    '''\n"", '    url =""http://www.google.com/finance/getprices?q={0}"".format(symbol)\n', '    url +=""&i={0}&p={1}d&f=d,o,h,l,c,v"".format(freqsec, nday)\n', '    try :      \n', '       resp= requests.post(url)\n', '       csv= resp.text\n', '       csv= csv.splitlines()\n', '    except :\n', ""       print('Err Intaday '+symbol)\n"", '\n', '    if len(csv) < 9 :\n', ""      print 'Using Daily',\n"", '\n', ""      url='http://www.google.com/finance/historical?q='+symbol+'&i='+str(freqsec)+'&p=3d&f=d,o,h,l,c,v&output=csv'\n"", '      csv = urllib.urlopen(url).readlines()\n', '      csv.reverse()\n', '      q=[]\n', '      for bar in xrange(0,len(csv)-1):\n', ""        ds,open_,high,low,close,volume = csv[bar].rstrip().split(',')\n"", '        if isfloat(open_) and isfloat(high) :\n', '          open_,high,low,close = [float(x) for x in [open_,high,low,close]]\n', ""          dt = datetime.datetime.strptime(ds,'%d-%b-%y')\n"", '          q.append([dt,open_,high,low,close,volume])\n', ""      df= util.pd_array_todataframe(q, ['date','open','high','low','close','volume'])\n"", '      return df\n', '\n', '\n', '    # print(csv)\n', '    qq=[]\n', '    for bar in xrange(7,len(csv)):\n', ""      if csv[bar].count(',')!=5: continue\n"", ""      offset,close,high,low,open_,volume = csv[bar].split(',')\n"", '      \n', ""      if offset[0]=='a':\n"", '        day = float(offset[1:])\n', '        offset = 0\n', '      else:\n', '        offset = float(offset)\n', '      open_, high, low, close = [float(x) for x in [open_,high,low,close]]\n', '      dt = datetime.datetime.fromtimestamp(day + (freqsec * offset))\n', '      #print (dt,open_,high,low,close,volume)\n', '      qq.append([dt,open_,high,low,close,volume])\n', ""    df= util.pd_array_todataframe(qq, ['date','open','high','low','close','volume'])\n"", '    return df\n', '\n', ""def imp_googleIntradayQuoteSave(symbols=['NKE'], freqsec=300, nday=2000, dircsv='', dbname=''):\n"", "" ''' Save Under various format: csv / db / output in Dataframe '''\n"", ' if isinstance(symbols, str) : symbols= [symbols]\n', ' symout,qlist, sym_error= [], [], []\n', ' for sym in symbols:\n', '   try :\n', '    q = imp_googleIntradayQuote(sym, freqsec, nday)  #interval, timeframe\n', '    print sym,\n', ""    sym= sym.replace(':', '_')\n"", '\n', ""    if dircsv != '' :\n"", '      if not os.path.isdir(dircsv) :  os.makedirs(dircsv)\n', ""      start1= util.datetime_toint(util.datenumpy_todatetime(q['date'].values[0]))\n"", ""      file1= dircsv+ '\\\\' + sym + '_' + str(start1) + '_' + str(freqsec) + '_' + str(nday) + '.csv'\n"", '\n', ""      q= util.pd_addcol(q, 'symbol'); q['symbol']= sym\n"", '      q.to_csv(file1, index=False)\n', '\n', ""    elif dbname != '' :\n"", '      import sqlalchemy as sql\n', '      dbcon= sql.create_engine(dbname)\n', '      if interv_sec== 300 : # 5mins\n', ""        q.to_sql('q5min', dbcon)\n"", '\n', '    else :\n', '      qlist.append(q);   symout.append(sym)\n', '   except:\n', ""      print('Error:'+sym, )\n"", '      sym_error.append(sym)\n', '\n', ' return qlist, symout, sym_error\n', '\n', ""def imp_googleQuote(symbol, start_date='20160101', end_date= '20160101') : # datetime.date.today().isoformat()):\n"", ""    ''' Daily quotes from google. Date format='yyyymmdd' '''\n"", '    symbol = symbol.upper()\n', '    start = datetime.date(int(start_date[0:4]),int(start_date[4:6]),int(start_date[6:8]))\n', '    end =   datetime.date(int(end_date[0:4]),int(end_date[4:6]),int(end_date[6:8]))\n', '    url =""http://www.google.com/finance/historical?q={0}"".format(symbol)\n', '    url +=""&startdate={0}&enddate={1}&output=csv"".format( start.strftime(\'%b %d, %Y\'),end.strftime(\'%b %d, %Y\'))\n', '    csv = urllib.urlopen(url).readlines()\n', '    csv.reverse()\n', '    # print(url)\n', '    q=[]\n', '    for bar in xrange(0,len(csv)-1):\n', ""      ds,open_,high,low,close,volume = csv[bar].rstrip().split(',')\n"", '      if isfloat(open_) and isfloat(high) :\n', '        open_,high,low,close = [float(x) for x in [open_, high, low, close]  ]\n', ""        dt = datetime.datetime.strptime(ds,'%d-%b-%y')\n"", '        q.append([dt,open_,high,low,close,volume])\n', '    return  pd.DataFrame(np.array(q), columns= [""date"",""open"",""high"",""low"",""Close"",""volume""])\n', '\n', '\n', 'def imp_googleQuoteSave(symbols, date1, date2, dircsv):\n', ' if isinstance(symbols, str) : symbols= [symbols]\n', ' for sym in symbols:\n', '   try :\n', '      q = imp_googleQuote(sym, date1, date2)   #interval, timeframe\n', ""   except: print('Error:'+symbol)\n"", '\n', ""   sym= sym.replace(':', '_')\n"", '\n', ""   if dircsv != '' :\n"", '     if not os.path.isdir(dircsv) :  os.makedirs(dircsv)\n', ""     start1= util.datetime_toint(util.datenumpy_todatetime(q['date'].values[0]))\n"", ""     file1= dircsv+ '\\\\'+ name1+'_'+ start1 +'_'+ str(date2) +'.csv'\n"", '     q.to_csv(file1)\n', '\n', ""   if dbname != '' :\n"", '     import sqlalchemy as sql\n', '     dbcon= sql.create_engine(dbname)\n', '     if interv_sec== 300 : # 5mins\n', ""       q.to_sql('q5min', dbcon)\n"", '\n', '\n', '\n', ""def imp_csv_dbupdate(indir='E:/_data/stock/intraday/intraday_google_usetf2.h5',\n"", ""                     outdir='E:/_data/stock/intraday/q5min/us/etf/', filelist=[], intype='csv',\n"", ""                     refcols=['date', 'open', 'high', 'low', 'close', 'volume', 'symbol']) :\n"", '\n', '  for file1 in filelist :\n', ""    sym=  file1[:file1.find('_')]  #Clean Up name\n"", '    print sym,\n', ""    outfile= outdir+'/'+sym+'.csv'\n"", '    if os.path.isfile(outfile) :\n', ""      df0= imp_csv_toext(file1=outfile, fromzone='Japan', tozone='UTC', header=0, cols=refcols)\n"", '    else :\n', '      df0= None\n', '\n', ""    if intype== 'hdfs':\n"", '      df1= imp_hdfs_getquote(indir, sym)\n', '      if isinstance(df1, float) : df1= None\n', '\n', ""    if intype=='csv' :\n"", ""      infile= indir+'/'+ file1   #csv\n"", ""      df1= imp_csv_toext(file1=infile, header=0, cols=refcols, fromzone='Japan', tozone='UTC')\n"", '\n', '    if df1 is not None :\n', '       if df0 is not None  : df1= imp_pd_merge(df1[refcols], df0[refcols])\n', '       # return outfile, df1\n', '       df1[refcols].to_csv(outfile, index=False)\n', '\n', '\n', '#####################################################################################\n', 'def imp_numpy_close_fromdb(dbname=\'/aaserialize/store/yahoo.db\', table1=\'\', symlist=[], t0=20010101, t1=20010101, priceid=""close"", batchsize= 400, maxasset=2600, tmax2=2000) :\n', ' print(""Get Numpy Matrix Close from DB SQL"")\n', ' import data.hist_data_storage as yhh\n', ' dstore= yhh.dailyDataStore(dbname)\n', ' close2= np.zeros((maxasset,tmax2),dtype=np.float16)\n', '\n', "" dateref0= dstore.get_histo(['SPY'], table1='daily_us_etf', start1=str(t0), end1=str(t1),  split_df=0).date.values\n"", ' \n', ' k=0; symfull=[];   i=0\n', ' nsym= len(symlist); nbatch= int(nsym / batchsize)+1\n', ' for j in xrange(0, nbatch) : \n', '      qlist, sym= dstore.get_histo(symlist[i:min(nsym,i+batchsize)], table1=table1, start1=str(t0), end1=str(t1),  split_df=1)\n', '\n', '      #print(""batch: ""+str(j), len(qlist), len(qlist[0].index) )\n', '      #for k,q in enumerate(qlist) :print sym[k], len(q.index)\n', '      i= i + batchsize\n', '      symfull= np.concatenate((symfull,sym))\n', '      close,dateref= date_align(qlist, dateref= dateref0, type1=priceid)\n', '      print(""batch: ""+str(j), len(sym),dateref[0],dateref[-1] )\n', '      qlist=None; sym=None\n', '      masset,tmax= np.shape(close)\n', '      if tmax2 < tmax: print(""Error tmax2 < tmax, wrong dates""); break\n', '      close2[k:k+masset,0:tmax] = close\n', '      k=k+ masset\n', '\n', ' maxasset= k\n', ' tmaxx= util.find(0.0, close2[0,:])\n', ' for k in xrange(0,maxasset):\n', '   if close2[k,tmaxx-1] == 0.0 : close2[k,tmaxx-1]=   close2[k,tmaxx-2]\n', ' \n', ' return close2[0:maxasset,0:tmaxx], symfull, dateref\n', '\n', 'def imp_numpy_close_fromhdfs(dbfile, symlist=[], t0=20010101, t1=20010101, priceid=""close"", maxasset=2600, tmax2=2000) :\n', ' print(""Get Numpy Matrix Close from Pandas"")\n', ' close2= np.zeros((maxasset,tmax2),dtype=np.float16)\n', ' store = pd.HDFStore(dbfile)\n', ' \n', "" df= store.select('SPY')\n"", ' df= df[(df.date >= t0) & (df.date <= t1 )]\n', ' dateref0= df.date.values\n', ' \n', ' k=0; symfull=[]; qlist=[]; sym=[]; tmax= 2000\n', ' for j, symbol in enumerate(store.keys()):\n', '  symbol=  symbol[1:]\n', '\n', '  if len(symlist)==0  or  util.find(symbol,symlist) > -1 : \n', '   #print symbol, util.find(symbol,symlist) \n', '   df= store.select(symbol) \n', '   if t0 !=t1 :     df= df[(df.date >= t0) & (df.date <= t1 )]\n', '   \n', '   qlist.append(df);   sym.append(symbol)\n', '   if np.mod(j+1, 401) == 0 :\n', '      print(""batch: ""+str(j))\n', '      close,dateref= date_align(qlist, dateref= dateref0, type1=priceid)   \n', '      print len(sym),dateref[0],dateref[-1] \n', '      symfull= symfull+sym\n', '      qlist=[]; sym=[]\n', ' \n', '      masset,tmax= np.shape(close)\n', '      if tmax2 < tmax: print(""Error tmax2 < tmax, wrong dates""); break\n', '      close2[(k):(k+masset),0:tmax] = close\n', '      k=k+ masset   \n', '     \n', ' if len(qlist) > 0:\n', '  # return qlist, qlist[0] ,  qlist[1] \n', '   close,dateref= date_align(qlist,type1=priceid)   \n', '   print len(sym),dateref[0],dateref[-1] \n', '   symfull= symfull+sym\n', '   masset,tmax= np.shape(close);\n', '   close2[(k):(k+masset),0:tmax] = close\n', ' \n', ' maxasset= k+masset\n', ' tmaxx= util.find(0.0, close2[0,:])\n', ' for k in xrange(0,maxasset):\n', '   if close2[k,tmaxx-1] == 0.0 : close2[k,tmaxx-1]=   close2[k,tmaxx-2]\n', ' \n', ' return close2[0:maxasset,0:tmaxx], symfull, dateref\n', '\n', '\n', ""def imp_sql_getquotes(stocklist01, dbname='sqlite:///aaserialize/store/yahoo.db', start1=20150101, end1=20160616, table1='daily_us_stock'):\n"", '    import data.hist_data_storage as yhh\n', '    dstore = yhh.dailyDataStore(dbname)\n', '    qlist, sym= dstore.get_histo(stocklist01, table1=table1,  start1=start1, end1=end1, split_df=1)\n', '    return qlist, sym\n', '\n', ""def imp_txt_getquotes(stocklist01, filedir='E:/_data/stock/daily/20160610/jp', startdate=20150101, endate=20160616):\n"", ' liststockfile= util.listallfile(filedir, ""*.txt"", dirlevel=5)\n', ' #print liststockfile\n', ' if len(np.shape(liststockfile)) > 1 :\n', '  liststockname=[ x.split(""."")[0] for x in liststockfile[:,0] ]\n', '#  for k in range(0, np.shape(liststockfile)[0]):  liststockname.append( (liststockfile[k,0])[0:4])   \n', '   \n', ' else: \n', '  stk0= liststockfile[0]; #stk2= liststockfile[2]\n', '#  liststockname=[ x[0:4] for x in stk0 ]\n', '  liststockname=[ x for x in stk0 ]\n', '\n', '#  liststockname=[]\n', '#  for k in range(0, len(stk0)):  liststockname.append( (stk0[k])[0:4])\n', ' \n', ' quotes=[]\n', ' print liststockname\n', ' for sym in stocklist01:\n', '   kstock= util.np_findfirst(sym, liststockname)\n', '   if kstock == -1 :\n', ""     print('Not Found '+sym)\n"", '   else :\n', '     df= pd.read_csv(liststockfile[kstock][2])\n', ""     df.columns = ['date', 'open', 'high','low','close', 'volume', 'openint']  #Change Column Name\n"", '     df2= df[(df.date > startdate  )& ( df.date   < endate )] # Filter by date\n', '     quotes.append(df2)\n', '  \n', ' return quotes \n', '\n', '\n', 'def imp_csv_getname(name1, date1, inter, tframe):\n', "" file1= dirstockcsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(inter) +'_'+ str(tframe) +'.csv'\n"", ' return file1\n', '\n', '\n', ""def imp_csv_toext(file1='SPY.csv', outputfile='.h5', fromzone='Japan', tozone='UTC', header=None,\n"", ""                  cols=['date', 'time','open','high','low','close','volume', 'symbol'], coldate=[0]):\n"", ""  ''' cols: column name,  coldate: position of date column   '''\n"", '  from dateutil import parser\n', '\n', '  if os.path.getsize(file1)> 500 :\n', ""   df = pd.read_csv(file1,sep=',',header=header)  # date_parser=dateparse)  # parse_dates={'date': [] },\n"", '   df.date= [ parser.parse(x) for x in  df.date]\n', '\n', ""   #if util.find('symbol', df.columns.values) < 0 :\n"", ""     #df= util.pd_addcol(df, 'symbol')\n"", ""     #df['symbol']= sym\n"", '\n', ""   type1= outputfile[outputfile.find('.'):]\n"", '\n', ""   if outputfile=='':\n"", '     # df.columns = [  x.lower() for x in df.columns.values ]\n', ""     # df.columns = ['date', 'symbol','open','high','low','close','volume']\n"", '     df.drop(df.columns[dropcol], axis=1, inplace=True)\n', ""     df= util.pd_addcol(df, 'symbol');  df['symbol']= dfname\n"", ""     df.columns = ['date', 'open','high','low','close','volume', 'symbol']\n"", '     return df[cols]\n', '\n', ""   if type1=='csv' :\n"", ""     df[cols].to_csv(outputfile , index=False)  # , compression='gzip' )\n"", '\n', ""   if type1== 'h5' :\n"", ""      # dateparse= lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '      dateparse= lambda x: (parse(x).replace(tzinfo=from_zone).astimezone(tozone))\n', '      # dateparse= lambda x: parse(x, tzinfos=from_zone).astimezone(to_zone)\n', '      # df.date= [pd.to_datetime((str(x)[:-6])) for x in  df.date]\n', '      # df.date= [x.to_datetime() for x in  df.date]\n', '\n', '      df.drop(df.columns[[0]], axis=1, inplace=True)\n', '      print filenameh5\n', ""      if util.find('symbol', df.columns.values) < 0 :\n"", ""         df= util.pd_addcol(df, 'symbol'); df['symbol']= dfname\n"", '      store = pd.HDFStore(outputfile); store.append(dfname, df);  store.close()\n', '\n', '\n', '\n', ""def imp_hdfs_db_updatefromcsv(dircsv, filepd=r'E:\\_data\\stock\\intraday_google.h5', fromtimezone='Japan', tozone='UTC'):\n"", ' lfile= util.os_file_listall(dircsv, pattern=""*.csv"", dirlevel=0)\n', ' for x in lfile:\n', '   name1= (x[0]).split(""_"")[0] #get Ticker name, splitting by _\n', ""   if util.str_isfloat(name1):  name1= 'jp'+name1  #Japanese Stocks case\n"", '   file1 = x[2];  print(name1)\n', '   if os.path.getsize(file1)> 500:\n', ""      filepd2= filepd + '/' +name1 +'.csv'\n"", '      imp_csv_toext(file1, filepd2, name1, fromtimezone, tozone)\n', '\n', ""def imp_hdfs_db_dumpinfo(dbfile='E:\\_data\\stock\\intraday_google.h5'):\n"", '  store = pd.HDFStore(dbfile)\n', '  extract=[]; errsym=[]\n', '  for symbol in store.keys():\n', '     try:\n', '       df= pd.read_hdf(dbfile, symbol)\n', ""       t0= df['date'].values[0]\n"", ""       t1= df['date'].values[-1]\n"", '\n', '       extract.append([symbol[1:], df.shape[1],   df.shape[0], t0, t1 ])\n', '\n', '     except: errsym.append(symbol)\n', '  return np.array(extract), errsym\n', '\n', 'def imp_hdfs_mergedb(filepdfrom, filepdto) :\n', '  store0 = pd.HDFStore(filepdfrom)\n', '  store1 = pd.HDFStore(filepdto)\n', '\n', '  for symbol in store0.keys():\n', '    qq= imp_hdfs_getquote(filepdfrom, symbol)\n', '    store1.append(symbol, qq)\n', '\n', '    qq= imp_hdfs_getquote(filepdto, symbol)\n', ""    qq= qq.drop_duplicates(subset='date', take_last=True)\n"", ""    qq= qq.sort('date', ascending=1)\n"", '    print(symbol)\n', '  store0.close(); store1.close()\n', '\n', ""def imp_hdfs_storecopy(hdfs1='F:/usstock8.h5', hdfs2='F:/usstock8.h5'):\n"", ' store = pd.HDFStore(hdfs1);  store2 = pd.HDFStore(hdfs2)\n', ' for symbol in store.keys() :\n', '   df= imp_hdfs_getquote(hdfs1, symbol)\n', '   symbol= symbol[1:].replace(""-"",""_"")\n', '   store2.append(symbol, df, data_columns=True)\n', ' store.close(); store2.close()\n', '\n', 'def imp_hdfs_getquote(filenameh5, dfname=""data""):\n', '  try: return  pd.read_hdf(filenameh5, dfname)\n', '  except: return -1.0;\n', '\n', ""def imp_hdfs_getListquote(symbols, close1='close', start='12/18/2015 00:00:00+00:00',\n"", ""                          end='3/1/2016 00:00:00+00:00', freq='0d0h10min', filepd= 'E:\\_data\\stock\\intraday_google.h5',\n"", ""                          tozone='Japan', fillna=True, interpo=True):\n"", '\n', ' datefilter= pd.date_range(start=start, end=end, freq= freq,tz=tozone).values\n', '\n', ' errorsym=[]; quotes=[]; correctsym=[]\n', ' for symbol in symbols:  # Issue Not same size\n', '   qq= imp_hdfs_getquote(filepd, symbol)\n', '\n', '   if type(qq)==np.float :  # Error qq=-1\n', '        errorsym.append(symbol)\n', '   else:\n', ""       qq.columns = ['date', 'symbol','open','high','low','close','volume']\n"", ""       qq= qq[qq['date'].isin(datefilter)]   #Only date in the range\n"", ""       #qq= qq.drop_duplicates(cols='date', take_last=True)\n"", ""       #  qq= qq.sort('date', asc=True)\n"", '\n', '       #print qq.date.values[0]\n', ""       if fillna : qq= qq.fillna(method='backfill')\n"", '       if interpo : qq= qq.interpolate()\n', '       quotes.append(qq)\n', '       correctsym.append(symbol)\n', '\n', ' datefilter= datenumpy_todatetime(datefilter)\n', ' return quotes,datefilter, correctsym,   errorsym\n', '\n', '\n', ""def imp_hdfs_removeDuplicate(filepd='E:\\_data\\stock\\intraday_google.h5') :\n"", '  #-------Clean by removing duplicate-------------------------------------------------\n', '  store = pd.HDFStore(filepd)\n', '  for symbol in store.keys():\n', '    #qq= imp_hdfs_getquote(filepd, symbol);\n', '    qq= store.select(symbol[1:])\n', ""    qq= qq.drop_duplicates(subset='date', take_last=True)\n"", ""    qq= qq.sort('date', ascending=1)\n"", '    qq.index = list(np.arange(0,len(qq.index)))\n', '\n', '    store.remove(symbol); store.append(symbol, qq);\n', '  store.close()\n', '\n', '\n', '\n', ""def imp_pd_tohdfs(sym, qqlist, filenameh5, fromzone='Japan', tozone='UTC') :\n"", ""  ''' df list to HDFS '''\n"", '  store = pd.HDFStore(filenameh5);\n', '  for k, df2 in enumerate(qqlist) :\n', '    df= copy.deepcopy(df2)\n', ""    df['date']= datetime_toint(df.date.values)\n"", '\n', ""    symbol= sym[k].replace('-','_')\n"", '    qq= imp_hdfs_getquote(filenameh5, symbol)\n', '    # print symbol, type(qq)\n', '    if type(qq) ==  float or  type(qq) ==  int  :\n', '      store.append(symbol, df)  # , data_columns=True\n', '      # print(symbol + str(df.date.values[-1]))\n', '    else :\n', '      qq= pd.concat([qq, df], ignore_index=True)\n', ""      qq= qq.drop_duplicates(subset='date', keep='last')\n"", ""      qq= qq.sort('date', ascending=1)\n"", '      store.append(symbol, qq)  #Too much space , data_columns=True\n', '      print(symbol + str(qq.date.values[-1]))\n', '\n', '  store.close()\n', '\n', 'def imp_pd_merge(df1, df2) :\n', '  df= pd.concat([df1, df2], axis=0)\n', ""  df= df.drop_duplicates(subset='date', keep='last')\n"", ""  df= df.sort_values(by=['date'], ascending=1)\n"", '  df.index = list(np.arange(0,len(df.index)))\n', '  return df\n', '\n', 'def imp_pd_checkquote(quotes) :\n', ""   for c in quotes: print np.shape(c), c['date'].values[0], c['date'].values[-1]\n"", '\n', 'def imp_pd_getclose(df, datefilter=None):\n', ' if datefilter is  not None :\n', ""    df= df[df['date'].isin(datefilter)]   #Only date in the range\n"", "" df= df.sort('date')\n"", "" close= df.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", ' \n', ' close= np.array(close.values[1:,1:]).astype(np.float)            #Only Price\n', ' close= close[(np.abs(sci.stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close\n', '\n', 'def imp_pd_cashyield(q, duration=10):\n', ' qbond= copy.deepcopy(q)\n', "" for id1 in ['close','open','high','low','aclose'] :\n"", '   try:\n', '     vv= q[id1].values\n', '     pbond=  100 * np.cumprod( (1 + np.ones(len(vv)) * 0.005/365.0 ))\n', '     qbond[id1]= pbond\n', '   except: pass\n', ' return qbond\n', '\n', 'def imp_pd_yield_tobond(q, duration=10):\n', ' qbond= copy.deepcopy(q)\n', "" for id1 in ['close','open','high','low','aclose'] :\n"", '   try:\n', '     vv= q[id1].values\n', '     pbond=  1/ np.power(1+ vv*0.01,duration) *100\n', '     qbond[id1]= pbond\n', ""#     print 'ok'\n"", '   except: pass\n', ' return qbond\n', '\n', 'def imp_pd_errordate(quotes, dateref):\n', "" ''' Show Symbol in Error when importing '''\n"", ' for i, stock in enumerate(quotes) :\n', '   print(i, symbols1[i],  datetime_tostring(stock[0][0]))\n', ' print(""\\n\\n"")\n', '\n', ' for i, stock in enumerate(quotes) :\n', '   date1= datetime_toint(stock[0][0])\n', '   if date1 != dateref : print(i, symbols1[i],  str(date1))\n', '\n', 'def imp_pd_fxtoprice(q):\n', ' dfprice= copy.deepcopy(q)\n', "" for kid in ['close','open','high','low'] :\n"", '   qret= getret_fromquotes(q[kid].values,1)\n', '   qprice= price_normalize100(qret)\n', '   dfprice[kid]= qprice.T\n', ' return dfprice\n', '\n', 'def imp_pd_fxinversetoprice(q):\n', ' dfprice= copy.deepcopy(q)\n', "" for kid in ['close','open','high','low'] :\n"", '   qret= getret_fromquotes(1.0 / q[kid].values,1)\n', '   qprice= price_normalize100(qret)\n', '   dfprice[kid]= qprice.T\n', ' return dfprice\n', '\n', ""def imp_pd_filterbydate(df, dtref=None, start='2016-06-06 00:00:00', end='2016-06-14 00:00:00', freq='0d0h05min', timezone='Japan'):\n"", "" ''' df: DateSeries or TimeSeries of Quotes   '''\n"", '\n', ' if type(df) in {pd.core.frame.DataFrame} :  #Data frame version\n', '   if type(dtref)== str:\n', '      dtref= pd.date_range(start=start, end=end, freq= freq,tz=timezone).values\n', '\n', ""   return df[df['date'].isin(dtref)]\n"", ' else :\n', '   if type(dtref)== str:  #Date version\n', '      dtref= pd.date_range(start=start, end=end, freq= freq,tz=timezone).values\n', '\n', '   return df[df.isin(dtref)].values\n', '\n', 'def imp_pd_cleanquote(q):\n', ' col= q.columns.values\n', "" if isinstance(q['date'].values[0] , str) :\n"", ""   q['date']= datetime_todate(datestring_todatetime(q['date'].values))\n"", '\n', ' for kid in col:\n', ""   if kid not in ['date', 'day','month','year'] :\n"", ""      q[kid]= pd.to_numeric(q[kid], errors='coerce').values  #Put NA on string\n"", '\n', "" q= q.fillna(method='pad')\n"", ' return q\n', '\n', 'def imp_pd_divide(q1, q2, funapply):\n', "" close, dateref= date_align([q1,q2], type1='close')\n"", ' m,tt= np.shape(close)\n', '\n', ' q3= util.pd_createdf( np.zeros((tt,len(q1.columns))) ,q1.columns, dateref)\n', "" q3['date']= dateref\n"", "" q3['close']= (close[0,:] / close[1,:])\n"", '\n', ' for kid in q1.columns :\n', ""   if kid not in ['close', 'volume','date','day','month', 'year'] :\n"", '     close, _= date_align([q1,q2], type1=kid)\n', '     q3[kid]= (close[0,:] / close[1,:])\n', ' return q3\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ""def imp_quote_csvdir_tofile(dircsv='', outdir1='E:/_data/stock/', fromtimezone='Japan', tozone='UTC'):\n"", ' lfile= util.os_file_listall(dircsv, pattern=""*.csv"", dirlevel=0)\n', ' for x in lfile:\n', '   name1= (x[0]).split(""_"")[0] #get Ticker name, splitting by _\n', ""   if util.str_isfloat(name1):  name1= 'jp'+name1  #Japanese Stocks case\n"", '   file1 = x[2];  print(name1)\n', '   if os.path.getsize(file1)> 500:\n', ""      filepd2= outdir + '/' +name1 +'.csv'\n"", '      imp_csv_toext(file1, filepd2,  header=True, fromzone=fromtimezone, tozone=tozone)\n', ""'''\n"", '\n', '\n', ""'''\n"", ""close1='Close' ; start='12/18/2015 00:00:00';\n"", ""end='3/1/2016 00:00:00'; freq='1d0h00min';\n"", ""filepd= 'E:\\_data\\stock\\intraday_google.h5'\n"", '\n', 'datefilter= pd.date_range(start=start, end=end,   freq= freq).values\n', '\n', 'symbols, names = np.array(list(symbol_dict.items())).T\n', 'i=0; vv=[]\n', 'for symbol in symbols:  # Issue Not same size\n', '   qq= quote_frompanda(filepd, symbol); i+=1\n', '   if isfloat(qq) :  vv.append(symbol)\n', '   else:\n', ""     if i==1: close = qq[['Datetime']]  #new dataframe    \n"", '     tmp= pd.DataFrame(data={symbol: qq[close1]});\n', '     if not tmp.isnull().values.all():\n', ""      close = close.join(tmp, how='outer', rsuffix='_1')\n"", ""      close= close.drop_duplicates(cols='Datetime', take_last=True)\n"", '\n', ""close= close[close['Datetime'].isin(datefilter)]   #Only date in the range\n"", ""close= close.sort('Datetime')\n"", ""close= close.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", 'close= np.array(close.values[1:,1:]).astype(np.float)  #Only Price\n', 'close= close[(np.abs(stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close, vv\n', ""'''\n"", '\n', ""'''\n"", 'from datetime import datetime\n', 'from dateutil import tz\n', '\n', ""from_zone = tz.gettz('Japan'); to_zone = tz.gettz('UTC')\n"", '\n', ""date1 = datetime.strptime('2011-01-21 09:00:00', '%Y-%m-%d %H:%M:%S')\n"", 'date1 = date1.replace(tzinfo=from_zone)\n', 'dateutc = date1.astimezone(to_zone)\n', 'date1, dateutc\n', '\n', ""Suppose you have a column 'datetime' with your string, then:\n"", ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '\n', ""df = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n"", ' combine multiple columns into a single datetime column,\n', ""this merges a 'date' and a 'time' column into a single 'datetime' column:\n"", '\n', ""dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", '\n', ""df = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)\n"", ""'''\n"", '\n', '\n', '#date1 = ""20160302""; inter= 60; tframe=80; name1= ""spy""\n', '#googleIntradayQuoteSave(name1, inter, tframe)\n', '\n', '\n', '\n', 'def ta_mar(close,t,m):  return  100*( np.mean(close[(t-m):(t+1)], axis=0) / close[t] )\n', 'def ta_ma(close,t,m):  return   np.mean(close[(t-m):(t+1)], axis=0)\n', '\n', '\n', '############################################################################\n', '#---------------------Calculate the Statistics         --------------------  \n', 'def calc_statestock(close2, dateref, symfull):\n', ' def sort(x,col,asc): return   util.sortcol(x,col,asc)\n', ' def perf(close,t0,t1):  return  100*( close[:,t1] / close[:,t0] -1)\n', ' def and2(tuple1):  return np.logical_and.reduce(tuple1)\n', ' def mar(close,t,m):  return  100*( np.mean(close[:,(t-m):(t+1)], axis=1) / close[:,t] )\n', ' def ma(close,t,m):  return   np.mean(close[:,(t-m):(t+1)], axis=1)\n', '\n', '\n', ' # a= ma(close2, tmax, 5) / ma(close2, tmax, 50)  \n', '\n', '\n', ' def dd(t) :\n', '  x= util.find(t,dateref)  # 607\n', ""  if x==-1 : print 'Error date not found in dateref '+str(t) ; return np.nan\n"", '  else :  return x\n', '\n', ' def gap(close,t0,t1,lag):   \n', '  ret= getret_fromquotes(close[:,t0:t1],lag)\n', '  rmin= 100*np.amin(ret,axis=1)\n', '  return rmin\n', ' \n', ' #--------------------Size----------------------------------------- \n', ' masset= len(symfull)\n', ' ttmax= util.find(0,close2[0,:])\n', ' if ttmax==-1 : ttmax= np.shape(close2)[1]\n', ' close2= close2[0:masset,0:ttmax]\n', ' m, tmax= masset, ttmax-1\n', ' \n', ' #correl\n', ' #close3= np.array(close2[:,0:ttmax],dtype=np.float16)\n', ' #close2ret= getret_fromquotes(close3)\n', ' #close2ret= np.nan_to_num(close2ret)\n', ' #close2ret= util.np_cleanmatrix(close2ret);  del close3\n', '\n', '##################################################################################\n', '#1: 1d,2:5d,3:10d,4:15d,5:20d,11:50d,12:60d,13:90d,15:1y\n', '#16: YTD,17:From Feb,21: 2015Ret,45: Price,50,51: Min,52,53: Max\n', ' print(""Calcul the Stats "" + str(dateref[-1]))\n', ' stat= np.zeros((2600,200),dtype=np.float16)\n', '\n', ' k=0\n', ' stat[k:(k+m),0]= np.arange(k,k+m)\n', ' stat[k:(k+m),1]=  perf(close2,tmax-1,tmax)  \n', ' stat[k:(k+m),2]=  perf(close2,tmax-5,tmax)  \n', ' stat[k:(k+m),3]=  perf(close2,tmax-10,tmax)  \n', ' stat[k:(k+m),4]=  perf(close2,tmax-15,tmax)  \n', ' stat[k:(k+m),5]=  perf(close2,tmax-20,tmax)  \n', '# stat[k:(k+m),6]=  perf(close2,tmax-25,tmax)  \n', ' stat[k:(k+m),7]=  perf(close2,tmax-30,tmax)  \n', '# stat[k:(k+m),8]=  perf(close2,tmax-35,tmax)  \n', ' stat[k:(k+m),9]=  perf(close2,tmax-40,tmax)  \n', '# stat[k:(k+m),10]=  perf(close2,tmax-45,tmax)  \n', ' stat[k:(k+m),11]=  perf(close2,tmax-50,tmax)  \n', ' stat[k:(k+m),12]=  perf(close2,tmax-60,tmax)  \n', ' stat[k:(k+m),13]=  perf(close2,tmax-90,tmax)  \n', ' stat[k:(k+m),14]=  perf(close2,tmax-120,tmax)  \n', ' stat[k:(k+m),15]=  perf(close2,tmax-250,tmax)  \n', '\n', '\n', '# stat[k:(k+m),16]=  perf(close2, dd(20160105),tmax)   #1Jan 2016\n', '# stat[k:(k+m),17]=  perf(close2, dd(20160222),tmax)   # 22 Feb 2016\n', '# stat[k:(k+m),18]=  perf(close2,dd(20160105),dd(20160222))   # Crash DrawDown\n', ' # stat[k:(k+m),19]=  perf(close2,624,tmax)   #  24 June 2016\n', ' # stat[k:(k+m),20]=  perf(close2,623,625)   #  Crash 24 June 2016\n', ' # stat[k:(k+m),21]=  perf(close2,dd(20151230)-250,dd(20151230))    #2015 Return\n', '\n', ' stat[k:(k+m),22]= gap(close2,tmax-80,tmax,1) #Max 1 days Gap\n', ' stat[k:(k+m),23]= gap(close2,tmax-80,tmax,5) #Max 5 days Gap\n', ' stat[k:(k+m),24]= perf(close2,tmax-2,tmax)  \n', ' stat[k:(k+m),25]= perf(close2,tmax-3,tmax)  \n', ' stat[k:(k+m),26]= perf(close2,tmax-4,tmax)  \n', '\n', ' stat[k:(k+m),27]= perf(close2,tmax-2,tmax-1)   \n', ' stat[k:(k+m),28]= perf(close2,tmax-3,tmax-2)   \n', ' stat[k:(k+m),29]= perf(close2,tmax-4,tmax-3)   \n', ' stat[k:(k+m),30]= perf(close2,tmax-5,tmax-4)\n', ' \n', ' stat[k:(k+m),120]= mar(close2, tmax, 20) \n', ' stat[k:(k+m),121]= mar(close2, tmax, 50) \n', ' stat[k:(k+m),122]= 100*ma(close2, tmax, 5) / ma(close2, tmax, 50)  #OverSold /OverBought\n', ' stat[k:(k+m),123]= mar(close2, tmax, 100)  \n', ' stat[k:(k+m),124]= mar(close2, tmax, 10) \n', '# stat[k:(k+m),35]= 0 \n', '# stat[k:(k+m),36]= 0 \n', '# stat[k:(k+m),37]= 0 \n', '# stat[k:(k+m),38]= 0 \n', '# stat[k:(k+m),39]= 0 \n', '\n', ' # stat[k:(k+m),30]= volume / AvgVol3M\n', ' # stat[k:(k+m),30]= Nbday_to_Earnings\n', ' #Volume/3M AvgVolume\n', ' # stat[k:(k+m),26]=  volume(volume2,tmax-120,tmax)  \n', '\n', ' #==============================================================================\n', ' #-------------- Data Fundamental    -------------------------------------------\n', "" df= util.sql_query(sqlr='SELECT ticker,shortratio,sector1_id, sector2_id, marketcap   FROM stockfundamental ', dburl='/aaserialize/store/finviz.db')\n"", ' npdf= df.values; del df\n', ' npdf0= npdf[:,0]  \n', ' for i in range(0,m):  \n', '   try :\n', '     kid= util.find(symfull[i], npdf0)\n', '     if kid != -1 :\n', '      stat[i,150]=  float(npdf[kid, 1])   # short ratio\n', '      stat[i,151]=  float(npdf[kid, 2])   # sector 1      \n', '      stat[i,152]=  float(npdf[kid, 3])   # sector 2     \n', '      stat[i,153]=  float(npdf[kid, 4] / 100000000.0)   # market Cap    \n', '   except : pass\n', ' del npdf, npdf0\n', '\n', '\n', '#==============================================================================\n', ' ################ Volatility\n', ' stat[k:(k+m),40]= volhisto_fromprice(close2,tmax,20,axis=1)* 100\n', ' stat[k:(k+m),41]= volhisto_fromprice(close2,tmax,60,axis=1)* 100\n', ' stat[k:(k+m),42]= volhisto_fromprice(close2,tmax,252,axis=1)* 100\n', '# stat[k:(k+m),43]= stat[k:(k+m),12] / stat[k:(k+m),41] #Sharpe 3Month\n', '# stat[k:(k+m),44]= stat[k:(k+m),15] / stat[k:(k+m),42] #Sharpe 1Year\n', '\n', ' stat[k:(k+m),45]= close2[:,tmax] #Last Close\n', '\n', '# stat[k:(k+m),46]= 0 \n', '# stat[k:(k+m),47]= 0 \n', '# stat[k:(k+m),48]= 0 \n', '# stat[k:(k+m),49]= 0 \n', '\n', '\n', ' ###############  Technical Indicator\n', ' #Min of last 6 months, Max of last 6months\n', ' for i in range(0,m):\n', '  pp0= stat[i,45]\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-120):tmax])\n', '  stat[i,50]= kmin+(tmax-120);  stat[i,51]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-120):tmax])\n', '  stat[i,52]= kmax+(tmax-120);  stat[i,53]= 100*pp0 / pmax\n', '\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-10):tmax])\n', '  stat[i,66]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-10):tmax])\n', '  stat[i,67]= 100*pp0 / pmax\n', '\n', '  kmin,pmin= util.np_find_minpos(close2[i,(tmax-5):tmax])\n', '  stat[i,68]= 100*pp0 / pmin\n', '\n', '  kmax,pmax= util.np_find_maxpos(close2[i,(tmax-5):tmax])\n', '  stat[i,69]= 100*pp0 / pmax\n', '\n', ' del kmax,pmax,i,k  \n', '\n', '\n', ' #Regression from Min Price Time\n', ' for i in range(0,m):\n', '  t0= int(stat[i,50])\n', '  try :\n', '    res= regression(close2[i,t0:tmax ]  ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,54]=  res[2]  #R2\n', '    stat[i,55]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', ' #Regression from Max Price Time\n', ' for i in range(0,m):  \n', '  t0= stat[i,52]\n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,56]=  res[2]  #R2\n', '    stat[i,57]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', ' #Regression from 5 days ago\n', ' t0= tmax-5\n', ' for i in range(0,m): \n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,58]=  res[2]  #R2\n', '    stat[i,59]=  res[0][0]  #Slope\n', '  except : pass\n', '\n', '#Regression from 10 days ago\n', ' t0= tmax-10\n', ' for i in range(0,m):  \n', '   try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,60]=  res[2]  #R2\n', '    stat[i,61]=  res[0][0]  #Slope\n', '   except : pass\n', '\n', "" '''\n"", '#Regression from 1 month\n', ' t0= tmax-20   \n', ' for i in range(0,m):\n', '   try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,62]=  res[2]  #R2\n', '    stat[i,63]=  res[0][0]  #Slope\n', '   except : pass\n', "" '''\n"", '\n', "" '''\n"", '#Regression from 2 months ago\n', ' t0= tmax-40\n', ' for i in range(0,m):  \n', '  try :\n', '    res= regression(close2[i,t0:tmax ] ,np.arange(t0,tmax),type1=""linear"")\n', '    stat[i,64]=  res[2]  #R2\n', '    stat[i,65]=  res[0][0]  #Slope\n', '  except : pass\n', "" '''\n"", '\n', '#Nb of Positive days last 5d\n', ' for i in range(0,m):  stat[i,66]=  np_countretsign(close2[i,(tmax-5):tmax])  \n', '\n', '#Nb of Positive days last 10d\n', ' for i in range(0,m):  stat[i,67]=  np_countretsign(close2[i,(tmax-10):tmax])  \n', '\n', '#Nb of Positive days last 20d\n', '# for i in range(0,m):  stat[i,68]=  np_countretsign(close2[i,(tmax-20):tmax])  \n', '\n', '\n', '#Regression from Max Values,200days\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmax[:,1] ,vmax[:,0],type1=""linear"")\n', '    stat[i,70]=  res[2]  #R2\n', '    stat[i,71]=  res[0][0]  #Slope\n', '  except : pass\n', '# del vmax\n', '\n', '\n', '#Regression from Min Values,200days \n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmin= util.np_findlocalmin2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmin[:,1] ,vmin[:,0],type1=""linear"")\n', '    stat[i,72]=  res[2]  #R2\n', '    stat[i,73]=  res[0][0]  #Slope\n', '  except : pass\n', '# del vmin\n', '\n', '\n', '#Regression from Max Values,100days\n', ' t0= tmax-20*5\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmax[:,1] ,vmax[:,0],type1=""linear"")\n', '    stat[i,74]=  res[2]  #R2\n', '    stat[i,75]=  res[0][0]  #Slope\n', '  except : pass\n', ' del vmax\n', '\n', '\n', '#Regression from Min Values,100days \n', ' t0= tmax-20*5\n', ' for i in range(0,m):  \n', '  try :\n', '    vmin= util.np_findlocalmin2(close2[i,t0:tmax ] ,6)\n', '    res= regression(vmin[:,1] ,vmin[:,0],type1=""linear"")\n', '    stat[i,76]=  res[2]  #R2\n', '    stat[i,77]=  res[0][0]  #Slope\n', '  except : pass\n', ' del vmin\n', '\n', '\n', '\n', '# Upside Trend with Higher Lows detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '   vmax= util.np_findlocalmin2(close2[i,t0:tmax],4)\n', '   vmax= util.sort(vmax,0,asc=0)\n', '   stat[i,80]=  -1\n', '   if len(vmax) > 1 :\n', '    ss=0\n', '    for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] > vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '    if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,80]=  ss  \n', '        stat[i,81]=  vmax[0,0] + t0\n', '  except: pass\n', '\n', '\n', '# Upside Trend with Higher High detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  try :\n', '    vmax= util.np_findlocalmax2(close2[i,t0:tmax],4)\n', '    vmax= util.sort(vmax,0,asc=0)\n', '    stat[i,82]=  -1\n', '    if len(vmax) > 1 :\n', '      ss=0\n', '      for k in range(0,len(vmax)-1):\n', '        if vmax[k,1] > vmax[k+1,1] : ss+=1   #Higher high\n', '        else : break;\n', '      if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,82]=  ss  \n', '        stat[i,83]=  vmax[0,0] + t0\n', '  except: pass\n', '\n', '\n', '# Downside Trend with lower Lows detection\n', ' t0= tmax-20*10\n', ' for i in range(0,m):  \n', '  vmax= util.np_findlocalmin2(close2[i,t0:tmax],4)\n', '  vmax= util.sort(vmax,0,asc=0)\n', '  stat[i,84]=  -1\n', '  if len(vmax) > 1 :\n', '   ss=0\n', '   for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] < vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '   if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,84]=  ss  \n', '        stat[i,85]=  vmax[0,0] + t0\n', '\n', '\n', '# Downside Trend  with lower highs detection\n', ' t0= tmax-20*6\n', ' for i in range(0,m):  \n', '  vmax= util.np_findlocalmax2(close2[i,t0:tmax],4)\n', '  vmax= util.sort(vmax,0,asc=0)\n', '  stat[i,80]=  -1\n', '  if len(vmax) > 1 :\n', '   ss=0\n', '   for k in range(0,len(vmax)-1):\n', '     if vmax[k,1] < vmax[k+1,1] : ss+=1   #Higher lows\n', '     else : break;\n', '  \n', '   if tmax - vmax[0,0]-t0 < 30  :  \n', '        stat[i,86]=  ss    #Nb  of lower high consecutive\n', '        stat[i,87]=  vmax[0,0] + t0\n', '\n', "" print('Higher/Lower Band Support')\n"", '#-----higher band Support / Lower Band Support 120days-------------------------------\n', ' t0= tmax-120\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,90]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,91]= 100.0* stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,92]= 100.0* (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '    # pmin=  (res2.x[0] * (np.arange(t0,tmax)-t0) + res2.x[1]) \n', '    # breachmin= np.sum(np.sign(close2[i, t0:tmax] - pmin*1.03))\n', '  except: pass\n', '\n', '\n', '#-----higher band Support / Lower Band Support 200days-------------------------------\n', ' t0= tmax-200\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,93]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,94]= 100.0 * stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,95]= 100.0 * (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '  except: pass\n', '\n', '\n', '#-----higher band Support / Lower Band Support 300days-------------------------------------\n', ' t0= tmax-300\n', ' for i in range(0,m): \n', '  try :\n', '    res1= ta_highbandtrend1(close2[i,t0:tmax])\n', '    pmax= (res1.x[0] * (tmax-t0) + res1.x[1]) \n', '    stat[i,96]= 100.0 * stat[i, 45] / pmax   # Higher Band\n', '\n', '    res2=  ta_lowbandtrend1(close2[i,t0:tmax])\n', '    pmin=  (res2.x[0] * (tmax-t0) + res2.x[1]) \n', '    stat[i,97]= 100.0* stat[i, 45] / pmin    # LowerBand\n', '\n', '    stat[i,98]= 100.0* (pmax - stat[i, 45]) / (pmax - pmin)    # Price inside Range\n', '\n', '  except: pass\n', '\n', '\n', ""  '''\n"", '  # -----higher band Support / Lower Band Support 60days-------------------------------\n', '  t0 = tmax - 60\n', '  for i in range(0, m):\n', '      try:\n', '          res1 = ta_highbandtrend1(close2[i, t0:tmax])\n', '          pmax = (res1.x[0] * (tmax - t0) + res1.x[1])\n', '          stat[i, 99] = 100.0 * stat[i, 45] / pmax  # Higher Band\n', '\n', '          res2 = ta_lowbandtrend1(close2[i, t0:tmax])\n', '          pmin = (res2.x[0] * (tmax - t0) + res2.x[1])\n', '          stat[i, 100] = 100.0 * stat[i, 45] / pmin  # LowerBand\n', '\n', '          stat[i, 101] = 100.0 * (pmax - stat[i, 45]) / (pmax - pmin)  # Price inside Range\n', '\n', '          # pmin=  (res2.x[0] * (np.arange(t0,tmax)-t0) + res2.x[1])\n', '          # breachmin= np.sum(np.sign(close2[i, t0:tmax] - pmin*1.03))\n', '      except:\n', '          pass\n', ""  '''\n"", '\n', '  #-----Technical Indicator------------------------------------------------\n', '# for i in range(0,m): \n', '#  try :\n', '#   res2=0\n', '   # df= pd.read_hdf(dbfile, symfull[i])\n', '   # stat[i,110]= ta.RMI(close2[i,:])\n', '   # stat[i,111]= 100*close2[i,:] / ta.MA(close2[i,:],20)\n', '   # stat[i,112]= 100*close2[i,:] / ta.MA(close2[i,:],50)   \n', '#  except: pass\n', '\n', '\n', ' return np.array(stat, dtype=np.float16) \n', '\n', '\n', ""'''  \n"", 'i=1964  \n', 'stat= s1\n', 'stat[i, 91]  \n', '\n', 'tt= np.arange(t0, tmax)\n', 'plot_price(close2[i,t0:tmax], (res2.x[0] * (tt-t0) + res2.x[1]) )\n', ""'''\n"", '\n', '  \n', '#------------------- Stock Monitoring Tools ------------------------------------------------\n', ""def monitor_addrecommend(string1, dbname='stock_recommend') :\n"", ' ss= string1.replace(""("","" "").replace(\')\',\' \').replace(\':\',\' \')\n', "" ss= ss.replace('\\t', ' ').replace('\\n', ' ')\n"", ' sl1= ss.split("" "")\n', ' print sl1\n', ' aux= [datestring_toint(util.date_now())]\n', ' symfull= copy.deepcopy(a_us_all)\n', ' for x in sl1:\n', '   if len(x) < 5 :\n', '    if util.find(x,symfull) > 0 :   aux.append(x)\n', '\n', ' if len(aux) >  1 :\n', '  stock_recommend= util.load_obj(dbname)\n', '  stock_recommend.append(aux)\n', '  util.save_obj(stock_recommend,dbname)\n', '  print aux\n', ' return stock_recommend\n', '\n', '\n', '\n', '#################### Finviz  ###############################################################\n', 'from bs4 import BeautifulSoup\n', 'import csv,datetime, requests\n', 'def imp_finviz():\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print ""Finviz Overview Start""\n', ' url = ""http://www.finviz.com/screener.ashx?v=111&f=geo_usa""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', "" firstcount = soup.find_all('option')\n"", ' lastnum = len(firstcount) - 1\n', "" lastpagenum = firstcount[lastnum].attrs['value']\n"", ' currentpage = int(lastpagenum)\n', '\n', ' alldata = []\n', ' templist = []\n', ' # Overview = 111, Valuation = 121, Financial = 161, Ownership = 131, Performance = 141\n', ' #pagesarray = [111,121,161,131,141]\n', ' titleslist = soup.find_all(\'td\',{""class"" : ""table-top""})\n', ' titleslisttickerid = soup.find_all(\'td\',{""class"" : ""table-top-s""})\n', ' titleticker = titleslisttickerid[0].text\n', ' titlesarray = []\n', ' for title in titleslist:\n', '    titlesarray.append(title.text)\n', '\n', ' titlesarray.insert(1,titleticker)\n', ' i = 0\n', '\n', ' while(currentpage > 0):\n', '    i += 1\n', '    print str(i) + "" page(s) done""\n', '    secondurl = ""http://www.finviz.com/screener.ashx?v="" + str(111) + ""&f=geo_usa"" + ""&r="" + str(currentpage)\n', '    secondresponse = requests.get(secondurl)\n', '    secondhtml = secondresponse.content\n', '    secondsoup = BeautifulSoup(secondhtml)\n', '    stockdata = secondsoup.find_all(\'a\', {""class"" : ""screener-link""})\n', '    stockticker = secondsoup.find_all(\'a\', {""class"" : ""screener-link-primary""})\n', '    datalength = len(stockdata)\n', '    tickerdatalength = len(stockticker)\n', '\n', '    while(datalength > 0):\n', '        templist = [stockdata[datalength - 10].text,stockticker[tickerdatalength-1].text,stockdata[datalength - 9].text,stockdata[datalength - 8].text,stockdata[datalength - 7].text,stockdata[datalength - 6].text,stockdata[datalength - 5].text,stockdata[datalength - 4].text,stockdata[datalength - 3].text,stockdata[datalength - 2].text,stockdata[datalength - 1].text,]\n', '        alldata.append(templist)\n', '        templist = []\n', '        datalength -= 10\n', '        tickerdatalength -= 1\n', '    currentpage -= 20\n', '\n', "" with open('stockoverview.csv', 'wb') as csvfile:\n"", ""    overview = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=titlesarray)\n"", '    overview.writeheader()\n', '\n', '    for stock in alldata:\n', '        overview.writerow({titlesarray[0] : stock[0], titlesarray[1] : stock[1],titlesarray[2] : stock[2],titlesarray[3] : stock[3],titlesarray[4] : stock[4], titlesarray[5] : stock[5], titlesarray[6] : stock[6], titlesarray[7] : stock[7] , titlesarray[8] : stock[8], titlesarray[9] : stock[9], titlesarray[10] : stock[10] })\n', '\n', ' print ""Finviz Overview Completed""\n', ' return overview\n', ' \n', '\n', 'def imp_finviz_news():\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print ""Finviz Performance Start""\n', ' url = ""http://www.finviz.com/quote.ashx?t=intc""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', ' titleslist = soup.find_all(\'a\',{""class"" : ""tab-link-news""})\n', ' return titleslist\n', '\n', '\n', 'def imp_finviz_financials():\n', ' import datetime\n', ' #pdb.set_trace() - python step by step debugger command\n', ' print datetime.datetime.now()\n', ' print ""Finviz Financial Start""\n', ' url = ""http://www.finviz.com/screener.ashx?v=161&f=geo_usa""\n', ' response = requests.get(url)\n', ' html = response.content\n', ' soup = BeautifulSoup(html)\n', "" firstcount = soup.find_all('option')\n"", ' lastnum = len(firstcount) - 1\n', "" lastpagenum = firstcount[lastnum].attrs['value']\n"", ' currentpage = int(lastpagenum)\n', '\n', ' alldata = []\n', ' templist = []\n', ' # Overview = 111, Valuation = 121, Financial = 161, Ownership = 131, Performance = 141\n', ' #pagesarray = [111,121,161,131,141]\n', ' titleslist = soup.find_all(\'td\',{""class"" : ""table-top""})\n', ' titleslisttickerid = soup.find_all(\'td\',{""class"" : ""table-top-s""})\n', ' titleticker = titleslisttickerid[0].text\n', ' titlesarray = []\n', ' for title in titleslist:\n', '    titlesarray.append(title.text)\n', '\n', ' titlesarray.insert(1,titleticker)\n', ' i = 0\n', '\n', ' while(currentpage > 0):\n', '    i += 1\n', '    print str(i) + "" page(s) done""\n', '    secondurl = ""http://www.finviz.com/screener.ashx?v="" + str(161) + ""&f=geo_usa"" + ""&r="" + str(currentpage)\n', '    secondresponse = requests.get(secondurl)\n', '    secondhtml = secondresponse.content\n', '    secondsoup = BeautifulSoup(secondhtml)\n', '    stockdata = secondsoup.find_all(\'a\', {""class"" : ""screener-link""})\n', '    stockticker = secondsoup.find_all(\'a\', {""class"" : ""screener-link-primary""})\n', '    datalength = len(stockdata)\n', '    tickerdatalength = len(stockticker)\n', '\n', '    while(datalength > 0):\n', '        templist = [stockdata[datalength - 17].text,stockticker[tickerdatalength-1].text,stockdata[datalength - 16].text,stockdata[datalength - 15].text,stockdata[datalength - 14].text,stockdata[datalength - 13].text,stockdata[datalength - 12].text,stockdata[datalength - 11].text,stockdata[datalength - 10].text,stockdata[datalength - 9].text,stockdata[datalength - 8].text,stockdata[datalength - 7].text,stockdata[datalength - 6].text,stockdata[datalength - 5].text,stockdata[datalength - 4].text,stockdata[datalength - 3].text,stockdata[datalength - 2].text,stockdata[datalength - 1].text,]\n', '        alldata.append(templist)\n', '        templist = []\n', '        datalength -= 17\n', '        tickerdatalength -= 1\n', '    currentpage -= 20\n', '\n', "" with open('stockfinancial.csv', 'wb') as csvfile:\n"", ""    financial = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n', fieldnames=titlesarray)\n"", '    financial.writeheader()\n', '\n', '    for stock in alldata:\n', '        financial.writerow({titlesarray[0] : stock[0], titlesarray[1] : stock[1],titlesarray[2] : stock[2],titlesarray[3] : stock[3],titlesarray[4] : stock[4], titlesarray[5] : stock[5], titlesarray[6] : stock[6], titlesarray[7] : stock[7] , titlesarray[8] : stock[8], titlesarray[9] : stock[9], titlesarray[10] : stock[10],titlesarray[11] : stock[11],titlesarray[12] : stock[12],titlesarray[13] : stock[13],titlesarray[14] : stock[14],titlesarray[15] : stock[15],titlesarray[16] : stock[16],titlesarray[17] : stock[17] })\n', '\n', ' print datetime.datetime.now()\n', ' print ""Finviz Financial Completed""\n', '\n', '\n', '\n', 'def get_price2book( symbol ):\n', '   from bs4 import BeautifulSoup as bs\n', '   import urllib as u\n', '   try:\n', ""    \turl = r'http://finviz.com/quote.ashx?t={}'.format(symbol.lower())\n"", '        html = u.request.urlopen(url).read()\n', ""        soup = bs(html, 'lxml')\n"", '        # Change the text below to get a diff metric\n', ""        pb =  soup.find(text = r'P/B')\n"", ""        pb_ = pb.find_next(class_='snapshot-td2').text\n"", ""        print( '{} price to book = {}'.format(symbol, pb_) )\n"", '        return pb_\n', '   except Exception as e:\n', '        print(e)\n', '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ' try:\n', '   q = imp_googleQuote(name1, date1, date2)  #interval, timeframe\n', ""   name1= name1.replace(':','_')\n"", ""   file1= dircsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(date2) +'.csv'\n"", '   q.write_csv(file1)\n', ' except: print\n', ""'''\n"", '\n', '\n', ""'''\n"", 'def imp_googleQuoteList(symbols, freqsec=300, nday=1, intraday1=True) :\n', ' qlist=[]; sym=[]\n', ' if intraday1:\n', '  for symbol in symbols:\n', '     #try :\n', '        q = imp_googleIntradayQuote(symbol, freqsec, nday)  #interval, timeframe\n', '        qlist.append(q)\n', '        sym.append(symbol)\n', ""     #except: print('Error:'+symbol)\n"", '\n', ' else :\n', '  for symbol in symbols:\n', '     try :\n', '       q = imp_googleQuote(name1, freqsec, num_days)  #interval, timeframe\n', '     except: print(symbol)\n', '\n', ' return qlist, sym\n', ""'''\n"", '\n', '\n', '\n', '\n', '\n', '\n', '\n', ""'''        \n"", ""stock_list = ['XOM','AMZN','AAPL','SWKS']\n"", 'p2b_series = pd.Series( index=stock_list )\n', '\n', 'for sym in stock_list:\tp2b_series[sym] = get_price2book(sym)\n', ""'''\n"", '\n', '\n', '\n', ""'''\n"", ""def imp_csv_toext(file1, filenameh5, dfname='sym1', fromzone='Japan', tozone='UTC'):\n"", "" # dateparse= lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n"", ' from dateutil.parser import parse\n', ' if os.path.getsize(file1)> 500 :\n', '  from_zone = tz.gettz(fromzone); tozone = tz.gettz(tozone)\n', ""  # dateparse= lambda x: (pd.datetime.strptime(x,'%Y-%m-%d %H:%M:%S').replace(tzinfo=from_zone).astimezone(tozone))\n"", '\n', ""  # dateparse= lambda x: (pd.datetime.strptime(x,'%m-%d-%Y %H:%M:%S').replace(tzinfo=from_zone).astimezone(tozone))\n"", '\n', '  dateparse= lambda x: (parse(x).replace(tzinfo=from_zone).astimezone(tozone))\n', '\n', '  # dateparse= lambda x: parse(x, tzinfos=from_zone).astimezone(to_zone)\n', '  print file1\n', ""  #df = pd.read_csv(file1,sep=',',header=None, parse_dates={'date': [1]}, date_parser=dateparse)\n"", '\n', ""  df = pd.read_csv(file1,sep=',',header=0)\n"", '  #df.date= [pd.to_datetime((str(x)[:-6])) for x in  df.date]\n', '  #df.date= [x.to_datetime() for x in  df.date]\n', '\n', '\n', '  df.drop(df.columns[[0]], axis=1, inplace=True)\n', ""  df= util.pd_addcol(df, 'symbol')\n"", ""  df['symbol']= dfname\n"", '\n', '  print filenameh5\n', ""  df.columns = ['date', 'open','high','low','close','volume', 'symbol' ]\n"", ""  df.to_csv(filenameh5 , index=False)  # , compression='gzip' )\n"", '\n', '  # store = pd.HDFStore(filenameh5);\n', '  # store.append(dfname, df);  store.close()\n', ""'''\n"", '\n', ""'''\n"", ""def imp_csv_toext(file1, filenameh5, dropcol=[], type1='csv', hasheader=True, dfname='sym1', fromzone='Japan', tozone='UTC'):\n"", ' if os.path.getsize(file1)> 500 :\n', '   header= 0 if hasheader else None\n', ""   df = pd.read_csv(file1, sep=',',header=0)\n"", '\n', '   df.drop(df.columns[dropcol], axis=1, inplace=True)\n', ""   df= util.pd_addcol(df, 'symbol');  df['symbol']= dfname\n"", ""   df.columns = ['date', 'open','high','low','close','volume', 'symbol' ]\n"", '\n', ""   if type1=='csv' :\n"", ""     df.to_csv(filenameh5 , index=False)  # , compression='gzip' )\n"", ""'''\n"", '\n', '\n', '\n', '\n', '\n', '\n', ""'''\n"", ""     if i==1: close = qq[['Datetime']]  #new dataframe\n"", '     tmp= pd.DataFrame(data={symbol: qq[close1]});\n', ""     close = close.join(tmp, how='outer', rsuffix='_1')\n"", ""     close= close.drop_duplicates(cols='Datetime', take_last=True)\n"", '\n', "" close= close[close['Datetime'].isin(datefilter)]   #Only date in the range\n"", "" close= close.sort('Datetime')\n"", "" close= close.interpolate(); close= close.fillna(method='backfill')  #Interpolate\n"", '\n', ' close= np.array(close.values[1:,1:]).astype(np.float)  #Only Price\n', ' close= close[(np.abs(sci.stats.zscore(close)) < 3).all(axis=1)] #Remove Outlier\n', ' return close, vv\n', ""'''\n"", '\n', '\n', '\n', ""'''\n"", 'class Quote(object):\n', ""  DATE_FMT = '%Y-%m-%d';  TIME_FMT = '%H:%M:%S'\n"", '\n', '  def __init__(self):\n', ""    self.symbol = '';    self.exchn=''\n"", '    self.date,self.time,self.open_,self.high,self.low,self.close,self.volume = ([] for _ in range(7))\n', '\n', '  def append(self,dt,open_,high,low,close,volume):\n', '    self.date.append(dt.date())\n', '    self.time.append(dt.time())\n', '    self.open_.append(float(open_))\n', '    self.high.append(float(high))\n', '    self.low.append(float(low))\n', '    self.close.append(float(close))\n', '    self.volume.append(int(volume))\n', '\n', '  def to_csv(self):\n', '    return \'\'.join([""{0},{1},{2},{3:.2f},{4:.2f},{5:.2f},{6:.2f},{7}\\n"".format(self.symbol,\n', ""              self.date[bar].strftime('%Y-%m-%d'),self.time[bar].strftime('%H:%M:%S'),\n"", '              self.open_[bar],self.high[bar],self.low[bar],self.close[bar],self.volume[bar])\n', '              for bar in xrange(len(self.close))])\n', '\n', '  def write_csv(self,filename):\n', ""    with open(filename,'w') as f:\n"", '      txt= self.to_csv()\n', '      f.write(txt)\n', '\n', '  def read_csv(self,filename):\n', ""    self.symbol = ''\n"", '    self.date,self.time,self.open_,self.high,self.low,self.close,self.volume = ([] for _ in range(7))\n', ""    for line in open(filename,'r'):\n"", ""      symbol,ds,ts,open_,high,low,close,volume = line.rstrip().split(',')\n"", '      self.symbol = symbol\n', ""      dt = datetime.datetime.strptime(ds+' '+ts,self.DATE_FMT+' '+self.TIME_FMT)\n"", '      self.append(dt,open_,high,low,close,volume)\n', '    return True\n', '\n', '  def __repr__(self):\n', '    return self.to_csv()\n', ""'''\n"", '\n', ' \n', '\n', ""'''\n"", ""def imp_googleIntradayQuoteSave2(name1, date1, inter, tframe, dircsv='', dbname=''):\n"", ' q = imp_googleIntradayQuote(name1, inter, tframe)  #interval, timeframe\n', "" name1= name1.replace(':','_')\n"", '\n', "" if dircsv != '' :\n"", ""    file1= dircsv+ '\\\\'+ name1+'_'+ date1 +'_'+ str(inter) +'_'+ str(tframe) +'.csv'\n"", '    q.to_csv(file1)\n', '\n', "" if dbname != '' :\n"", '    import sqlalchemy as sql\n', '    dbcon= sql.create_engine(dbname)\n', '    if inter== 300 : # 5mins\n', ""      q.to_sql('q5min', dbcon)\n"", '\n', '\n', ""def imp_googleQuoteList_save(symbols, date1, date2, inter=23400, tframe=2000, dircsv='', intraday1=True) :\n"", ' if intraday1:\n', '  for symbol in symbols:\n', '     try :\n', '       imp_googleIntradayQuoteSave(symbol, date1, inter, tframe, dircsv)\n', ""     except: print('Error:'+symbol)\n"", '\n', ' else :\n', '  for symbol in symbols:\n', '     try :\n', '       imp_googleQuoteSave(symbol, date1, date2, dircsv)\n', '     except: print(symbol)\n', ""'''\n"", '\n']","['np.shape', 'getret_fromquotes', 'len', 'getlogret_fromquotes', 'np.log', 'getprice_fromret']",6
utilmy/zarchive/py2to3/portfolio.py:index,index,class,103,300,221,3009,10.03,1,3,[],[],[],1625,[],[],0
utilmy/zarchive/py2to3/portfolio.py:folioCalc,folioCalc,class,125,509,332,4736,9.3,2,17,[],[],[],1924,[],[],0
utilmy/zarchive/py2to3/portfolio.py:folioRiskIndicator,folioRiskIndicator,class,89,306,215,3180,10.39,1,9,[],[],[],2099,[],[],0
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF,folioOptimizationF,class,280,1291,671,10994,8.52,6,39,[],[],[],2277,[],[],0
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity,searchSimilarity,class,173,497,328,4550,9.15,8,17,[],[],[],3724,[],[],0
utilmy/zarchive/py2to3/portfolio.py:index:__init__,index:__init__,method,11,12,12,116,9.67,0,0,"['self', 'id1', 'sym', 'ww', 'tstart']","[None, None, None, None, None]","[None, None, None, None, None]",1626,[],['date_generatedatetime'],1
utilmy/zarchive/py2to3/portfolio.py:index:close,index:close,method,0,1,1,4,4.0,0,0,['self'],[None],[None],1631,[],[],0
utilmy/zarchive/py2to3/portfolio.py:index:updatehisto,index:updatehisto,method,2,12,11,62,5.17,0,0,['self'],[None],[None],1634,[],['print'],1
utilmy/zarchive/py2to3/portfolio.py:index:help,index:help,method,2,12,11,62,5.17,0,0,['self'],[None],[None],1638,[],['print'],1
utilmy/zarchive/py2to3/portfolio.py:index:__init__,index:__init__,method,11,12,12,116,9.67,0,0,"['self', 'id1', 'sym', 'ww', 'tstart']","[None, None, None, None, None]","[None, None, None, None, None]",1659,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]",['date_generatedatetime'],1
utilmy/zarchive/py2to3/portfolio.py:index:calc_baskettable_pct,index:calc_baskettable_pct,method,37,95,77,782,8.23,1,2,"['self', 'type1', 'showdetail']","[None, None, None]","[None, '""table""', '0']",1703,"[""      '''   Calc Basket Values from Input data '''\n""]","['np.zeros', 'np.sum', 'self._udpate_wwindpct', 'range', 'np.mod', 'self._wwpct_rebal']",6
utilmy/zarchive/py2to3/portfolio.py:index:_wwpct_rebal,index:_wwpct_rebal,method,3,4,3,41,10.25,0,0,"['self', 'wwpct_actual', 't', 'trebal']","[None, None, None, None]","[None, None, None, None]",1730,[],[],0
utilmy/zarchive/py2to3/portfolio.py:index:_udpate_wwindpct,index:_udpate_wwindpct,method,11,25,17,224,8.96,0,0,"['self', 't', 'bskt', 'hedgecost', 'wwpct_actual', 'wwpct_th']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",1740,[],[],0
utilmy/zarchive/py2to3/portfolio.py:index:calc_baskettable_unit,index:calc_baskettable_unit,method,0,0,0,0,0.0,0,0,[],[],[],1749,"[""    '''   t           --->  t+1                 ---->   t+2\n"", '            wwactual:    B[t]    -->   B[t+1]   using  w[t+1], B[t] /   S[t]\n', '            wwnew   :    B[t+1]  -->   B[t+2]   using  w[t+2], B[t+1] /   S[t+1]\n', ""    '''\n""]",[],0
utilmy/zarchive/py2to3/portfolio.py:folioCalc:__init__,folioCalc:__init__,method,21,39,37,356,9.13,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1925,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio.py:folioCalc:set_symclose,folioCalc:set_symclose,method,17,34,33,312,9.18,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1938,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio.py:folioCalc:setcriteria,folioCalc:setcriteria,method,41,76,71,1046,13.76,0,0,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",1947,"[""        ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '            wwtype:     constant / regime /regimExtra\n', '            initperiod: NbOfDays_initial_Risk_Monitoring\n', '            riskid:     spprice/spperf/multi\n', ""        '''\n""]","['np.array', 'len']",2
utilmy/zarchive/py2to3/portfolio.py:folioCalc:_weightcalc_generic,folioCalc:_weightcalc_generic,method,5,35,23,396,11.31,0,4,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",1997,[],"['self._weightcalc_constant', 'self._weightcalc_regime', 'print']",3
utilmy/zarchive/py2to3/portfolio.py:folioCalc:_weightcalc_regime,folioCalc:_weightcalc_regime,method,10,37,32,272,7.35,0,1,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2008,[],"['self._regimecalc', 'np.reshape', 'self._mapping_risk_ww']",3
utilmy/zarchive/py2to3/portfolio.py:folioCalc:_regimecalc,folioCalc:_regimecalc,method,3,7,6,83,11.86,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2018,[],['self._mapping_calc_risk'],1
utilmy/zarchive/py2to3/portfolio.py:folioCalc:_weightcalc_constant,folioCalc:_weightcalc_constant,method,3,4,4,28,7.0,0,0,"['self', 'ww2', 't']","[None, None, None]","[None, None, None]",2027,[],['np.sum'],1
utilmy/zarchive/py2to3/portfolio.py:folioCalc:getweight,folioCalc:getweight,method,5,12,9,126,10.5,0,0,['self'],[None],[None],2031,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/py2to3/portfolio.py:folioCalc:calc_baskettable,folioCalc:calc_baskettable,method,33,111,83,811,7.31,1,4,"['self', 'wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps', 'showdetail']","[None, None, None, None, None, None, None, None]","[None, None, None, '""table""', '""constant""', '1', '0.000', '0']",2038,[],"['np.shape', 'np.zeros', 'range', 'np.mod', 'self._weightcalc_generic', 'np.sum', 'print']",7
utilmy/zarchive/py2to3/portfolio.py:folioCalc:plot,folioCalc:plot,method,10,29,26,295,10.17,0,2,"['self', 'wwvec', 'show1', 'tickperday']","[None, None, None, None]","[None, 'None', '1', '60']",2066,[],"['self.calc_baskettable', 'plot_price', 'print', 'np.array']",4
utilmy/zarchive/py2to3/portfolio.py:folioCalc:multiperiod_ww,folioCalc:multiperiod_ww,method,15,36,29,240,6.67,1,1,"['self', 't']","[None, None]","[None, None]",2077,[],['xrange'],1
utilmy/zarchive/py2to3/portfolio.py:folioCalc:help,folioCalc:help,method,1,2,2,13,6.5,0,0,['self'],[None],[None],2090,[],['print'],1
utilmy/zarchive/py2to3/portfolio.py:folioRiskIndicator:__init__,folioRiskIndicator:__init__,method,23,40,38,356,8.9,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1659,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]","['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio.py:folioRiskIndicator:set_symclose,folioRiskIndicator:set_symclose,method,18,32,32,295,9.22,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2114,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio.py:folioRiskIndicator:setcriteria,folioRiskIndicator:setcriteria,method,34,81,70,1006,12.42,0,0,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2124,"[""    ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '        wwtype:     constant / regime /regimExtra\n', '        initperiod: NbOfDays_initial_Risk_Monitoring\n', '        riskid:     spprice/spperf/multi\n', ""    '''    \n""]","['np.array', 'len', 'np.zeros']",3
utilmy/zarchive/py2to3/portfolio.py:folioRiskIndicator:calcrisk,folioRiskIndicator:calcrisk,method,13,18,17,220,12.22,1,0,"['self', 'wwvec', 'initval']","[None, None, None]","[None, '[]', '1']",2172,[],"['xrange', 'self._weightcalc_generic', 'np.column_stack']",3
utilmy/zarchive/py2to3/portfolio.py:folioRiskIndicator:_weightcalc_generic,folioRiskIndicator:_weightcalc_generic,method,4,34,24,327,9.62,0,3,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2184,[],"['self._weightcalc_regime', 'print']",2
utilmy/zarchive/py2to3/portfolio.py:folioRiskIndicator:_weightcalc_regime,folioRiskIndicator:_weightcalc_regime,method,3,11,11,68,6.18,0,1,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2193,[],['self._regimecalc'],1
utilmy/zarchive/py2to3/portfolio.py:folioRiskIndicator:_regimecalc,folioRiskIndicator:_regimecalc,method,7,18,17,216,12.0,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2198,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/py2to3/portfolio.py:folioRiskIndicator:calc_optimal_weight,folioRiskIndicator:calc_optimal_weight,method,7,18,17,216,12.0,0,0,"['self', 'maxiter', 'name1', 'isreset', 'popsize']","[None, None, None, None, None]","[None, '1', ""''"", '1', '15']",2204,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:__init__,folioOptimizationF:__init__,method,23,39,37,340,8.72,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",1659,"[""    ''' initperiod: NbOfDays_initial_Risk_Monitoring   '''\n""]","['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:set_symclose,folioOptimizationF:set_symclose,method,18,32,32,295,9.22,0,1,"['self', 'sym', 'close', 'dateref']","[None, None, None, None]","[None, None, None, None]",2114,[],"['getret_fromquotes', 'np.shape', 'np.ones', 'len', 'print']",5
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:setcriteria,folioOptimizationF:setcriteria,method,44,128,103,1669,13.04,0,2,"['self', 'lweight', 'lbounds', 'statedata', 'name', 'optimcrit', 'wwtype', 'nbregime', 'initperiod', 'riskid', 'lfun']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None, '""spprice""', 'None']",2124,"[""    ''' optimcrit:  perf / perfyearly / drawdownyearly / sharpe/ volatility/ linear\n"", '        wwtype:     constant / regime /regimExtra\n', '        initperiod: NbOfDays_initial_Risk_Monitoring\n', '        riskid:     spprice/spperf/multi\n', ""    '''    \n""]","['np.array', 'len', 'print', 'list']",4
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:calcbasket_obj2,folioOptimizationF:calcbasket_obj2,method,23,76,63,756,9.95,0,1,"['self', 'wwvec']","[None, None]","[None, None]",2362,[],"['np.reshape', 'np.array', 'np.sum', 'round', 'calcbasket_objext', 'float']",6
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:calcbasket_obj,folioOptimizationF:calcbasket_obj,method,55,157,118,1494,9.52,1,4,"['self', 'wwvec']","[None, None]","[None, None]",2388,[],"['np.reshape', 'np.array', 'np.sum', 'round', 'calcbasket_objext', 'float', 'calcbasket_obj', 'concenfactor=folio_concenfactor2', 'np.zeros', 'xrange', 'np.mod', 'self._weightcalc_generic', 'self._loss_obj', 'self._objective_criteria']",14
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:_loss_obj,folioOptimizationF:_loss_obj,method,4,6,6,68,11.33,0,0,"['self', 'ww2', 'wwpenalty']","[None, None, None]","[None, None, None]",2413,[],['np.sum'],1
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:_weightcalc_generic,folioOptimizationF:_weightcalc_generic,method,5,40,26,394,9.85,0,4,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",2184,[],"['self._weightcalc_constant', 'self._weightcalc_regime', 'print']",3
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:_weightcalc_regime,folioOptimizationF:_weightcalc_regime,method,8,32,28,239,7.47,0,0,"['self', 'wwvec', 'wwextra', 't']","[None, None, None, None]","[None, None, None, None]",2193,"['""""""\n', 'if bsk > maxprice :       maxprice= bsk\n', 'if maxprice-bsk > maxdd : maxdd=    maxprice-bsk\n', '\n', '""""""\n']","['self._regimecalc', 'np.reshape', 'self._mapping_risk_ww']",3
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:_regimecalc,folioOptimizationF:_regimecalc,method,4,7,7,83,11.86,0,0,"['self', 't', 'wwextra']","[None, None, None]","[None, None, None]",2198,"['""""""\n', 'if bsk > maxprice :       maxprice= bsk\n', 'if maxprice-bsk > maxdd : maxdd=    maxprice-bsk\n', '\n', '""""""\n']",['self._mapping_calc_risk'],1
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:_weightcalc_constant,folioOptimizationF:_weightcalc_constant,method,3,4,4,28,7.0,0,0,"['self', 'ww2', 't']","[None, None, None]","[None, None, None]",2452,[],['np.sum'],1
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:calc_optimal_weight,folioOptimizationF:calc_optimal_weight,method,7,18,17,216,12.0,0,0,"['self', 'maxiter', 'name1', 'isreset', 'popsize']","[None, None, None, None, None]","[None, '1', ""''"", '1', '15']",2204,[],['datanalysis.sk_optim_de'],1
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:getweight,folioOptimizationF:getweight,method,6,11,9,126,11.45,0,0,['self'],[None],[None],2461,[],"['np.reshape', 'np.sum']",2
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:calc_baskettable,folioOptimizationF:calc_baskettable,method,36,86,72,827,9.62,1,4,"['self', 'wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps', 'showdetail']","[None, None, None, None, None, None, None, None]","[None, None, None, '""table""', '""constant""', '1', ' 0.000', '0']",2469,[],"['np.shape', 'np.zeros', 'range', 'np.mod', 'self._weightcalc_generic', 'np.sum', 'print']",7
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:plot,folioOptimizationF:plot,method,11,31,30,294,9.48,0,2,"['self', 'wwvec', 'show1', 'tickperday']","[None, None, None, None]","[None, 'None', '1', '60']",2498,[],"['self.calc_baskettable', 'plot_price', 'print', 'np.array']",4
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:_objective_criteria,folioOptimizationF:_objective_criteria,method,31,147,78,1105,7.52,2,7,"['self', 'bsk']","[None, None]","[None, None]",2510,[],"['np.std', 'np.sum', 'range', 'np.arange']",4
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:multiperiod_ww,folioOptimizationF:multiperiod_ww,method,18,29,27,238,8.21,1,1,"['self', 't']","[None, None]","[None, None]",2557,[],['xrange'],1
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:help,folioOptimizationF:help,method,50,152,103,1472,9.68,1,6,['self'],[None],[None],2569,[],"['print', 'util.load_obj', 'int', 'copy.deepcopy', 'xrange', 'next', 'util.save_obj', 'np.mod', 'np.abs', 'np.reshape', 'np.sum']",11
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:mapping_risk_ww,folioOptimizationF:mapping_risk_ww,method,12,56,29,245,4.38,0,1,"['self', 'risk', 'wwmat', 'ww2']","[None, None, None, None]","[None, None, None, 'self.wwasset0']",2628,[],"['np.sum', 'np.array']",2
utilmy/zarchive/py2to3/portfolio.py:folioOptimizationF:_mapping_calc_risk,folioOptimizationF:_mapping_calc_risk,method,27,201,76,831,4.13,0,3,"['self', 'ss', 'tr', 't', 'risk0']","[None, None, None, None, None]","[None, None, None, None, None]",2640,[],[],0
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:__init__,searchSimilarity:__init__,method,15,18,17,193,10.72,0,0,"['self', ""filejpstock=r'E"", 'sym01', 'symname', 'startdate', 'enddate', 'pricetype']","[None, '', None, None, None, None, None]","[None, ""r'E:/_data/stock/daily/20160616/jp'"", ""['7203']"", ""['Toyota']"", ' 20150101', '20160601', '""close""']",3725,[],[],0
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:load_quotes_fromdb,searchSimilarity:load_quotes_fromdb,method,15,35,33,355,10.14,0,1,"['self', 'picklefile']","[None, None]","[None, ""''""]",3730,[],"['util.load_obj', 'imp_txt_getquotes', 'date_align', 'util.a_cleanmemory']",4
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:__generate_return__,searchSimilarity:__generate_return__,method,4,4,4,64,16.0,0,0,"['self', 'nlag']","[None, None]","[None, None]",3741,[],['getret_fromquotes'],1
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:__overweight__,searchSimilarity:__overweight__,method,17,62,41,412,6.65,2,4,"['self', 'px']","[None, None]","[None, None]",3745,[],"['len', 'np.ones', 'util.np_findlocalmax', 'type', 'np.abs', 'wwerr[max', 'util.np_findlocalmin']",7
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:set_searchcriteria,searchSimilarity:set_searchcriteria,method,34,79,63,793,10.04,0,5,"['self', 'name1', 'date1', 'date2', 'nlag', 'searchperiodstart', 'typesearch', '']","[None, None, None, None, None, None, None, None]","[None, ""'7203'"", '20160301', '20160601', '1', '20120101', '""pattern2""', None]",3765,[],"['getret_fromquotes', 'date_finddateid', 'max', 'util.np_findfirst', 'print', 'price_normalize_1d', 'len', 'self.__overweight__']",8
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:launch_search,searchSimilarity:launch_search,method,53,107,81,908,8.49,3,3,['self'],[None],[None],3794,[],"['len', 'np.zeros', 'range', 'price_normalize_1d', 'np_similarity', 'util.np_sortbycolumn', 'enumerate']",7
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:show_comparison_graph,searchSimilarity:show_comparison_graph,method,33,72,63,682,9.47,1,3,"['self', 'maxresult', 'show_only_different_time', 'fromid', 'fromend', 'filenameout']","[None, None, None, None, None, None]","[None, '20', 'True', '0', ' 0', ""''""]",3828,[],"['np.shape', 'range', 'int', 'print', 'price_normalize_1d', 'str', 'plot_price']",7
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:staticmethod,searchSimilarity:staticmethod,method,0,1,1,4,4.0,0,0,"['self', 'x']","[None, None]","[None, None]",3857,[],[],0
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:get_rankresult,searchSimilarity:get_rankresult,method,24,63,53,496,7.87,2,1,"['self', 'filetosave']","[None, None]","[None, ""''""]",3861,[],"['len', 'np.savetxt', 'np.shape', 'np.array', 'np.empty', 'range', 'int', 'str']",8
utilmy/zarchive/py2to3/portfolio.py:searchSimilarity:export_results,searchSimilarity:export_results,method,0,0,0,0,0.0,0,0,[],[],[],3883,"[""'''\n"", '#####################################################################################\n', '#---------Run over all the indexes, with time lag -----------------------------------\n', 'def np_similarity(x,y, wwerr=[], type1=0):\n', '  if type1==2 : return np_distance_l1(x,y, wwerr)\n', '  if type1==1: return sci.spatial.distance.correlation(x, y)  \n', '  if type1==0 : return 1-np.corrcoef(x,y)[0,1]\n', '\n', 'def np_distance_l1(x,y, wwerr) :\n', ' return np.sum(wwerr * np.abs(x-y))  \n', '  \n', '#---------------------  Universe Load         ---------------------------------------\n', ""filejpstock= r'E:/_data/stock/daily/20160616/jp'\n"", 'sym01= nk400list\n', '\n', 'quotes= imp_txt_getquotes(sym01, filejpstock, startdate=20090101, endate=20160616)\n', 'open1, dateref= date_align(quotes, type1=""close"")    #Get the data and align dates\n', 'del quotes; util.a_cleanmemory()\n', '\n', ""#util.save_obj(open1, 'close_nk400_2009_2016')\n"", ""#util.save_obj(dateref, 'dateref_nk400_2009_2016')\n"", '\n', ""open1= util.load_obj('close_nk400_2009_2016')\n"", ""dateref= util.load_obj( 'dateref_nk400_2009_2016')\n"", '\n', 'nlag=1\n', 'ret_open1 =  getdailyret_fromquotes(open1,nlag)\n', 'del open1\n', '\n', '#--------------------  Search Asset Input     ---------------------------------------\n', ""name1= '7203' \n"", 'date1= 20160328\n', 'date2= 20160531\n', 'twindow= util.np_findfirst(date2, dateref) - util.np_findfirst(date1, dateref) \n', '\n', '\n', 'tstartx= util.np_findfirst(date1, dateref)          # Date Index\n', 'stockx= util.np_findfirst(name1, sym01)\n', 'retx= ret_open1[stockx, tstartx:(tstartx+twindow-1)]    # Recent Period\n', 'px= price_normalize_1d(retx)\n', 'nx= len(px)\n', '\n', '\n', '#----OverWeight for Local Min, Local Max         -----------------------------------\n', 'wwerr= np.ones(nx, dtype=np.float16)\n', 'vvmax= util.np_findlocalmax(px); xprev=0\n', 'for x in vvmax:\n', '  if x[0] !=0 and np.abs(xprev-x[0]) > 6: \n', '    wwerr[max(0,x[0]-1):min(nx,(x[0]+1))]= 1.5;    xprev= x[0] \n', '\n', 'vvmin= util.np_findlocalmin(px); xprev=0\n', 'for x in vvmin:\n', '  if x[0] !=0 and np.abs(xprev-x[0]) > 6: \n', '    wwerr[max(0,x[0]-1):min(nx,(x[0]+1))]= 1.5;    xprev= x[0] \n', '\n', '\n', '#--------------------  Search the Pattern       -------------------------------------\n', 'tmaxx= len(dateref)-1\n', 'similar_list= np.zeros((nstock*(tmaxx-twindow)+1, 3), np.float16)\n', 'i=0\n', 'for iy in range(0, nstock) :\n', ' for t in range(0, tmaxx-twindow):\n', '   if iy == stockx and t > tstartx : \n', '     pass\n', '   else :\n', '    rety= ret_open1[iy, t:(t+twindow-1)]\n', '    py=   price_normalize_1d(rety)\n', '    ss= np_similarity(px, py, wwerr=wwerr, type1=2)\n', '    i+=1\n', '    similar_list[i,0]= iy\n', '    similar_list[i,1]= t\n', '    similar_list[i,2]= ss\n', '\n', '\n', ' #-------------------Clean the table -------------------------------------------------\n', 'rank= util.np_sortbycolumn(similar_list,2, asc=True)\n', 'for i,x in enumerate(rank) :\n', '  if rank[i,2]==0.0 : rank[i,2]= 1000         # Empty cell\n', '  if rank[i,1]> tstartx-3 : rank[i,2]= 100    # Start Element\n', 'rank= util.np_sortbycolumn(rank,2, asc=True)  \n', '\n', '\n', ' #----- Show Similar Stocks ---------------------------------------------------------\n', 'show_only_different_time= True\n', '\n', 'for i in range(0,10) :\n', ' tstarti= rank[i,1]\n', ' stocki= int(rank[i,0])\n', ' \n', ' if show_only_different_time and tstarti < tstartx-1  :\n', '   py= price_normalize100(ret_open1[stocki, tstarti:(tstarti+twindow-1)]) \n', '\n', '   namei= sym01[stocki]\n', '   namefulli= nk400name[util.np_findfirst(namei, nk400list)]\n', '\n', '   tit= namei+ "" "" +namefulli+ "" "" + str(dateref[tstarti])          \n', ""   plot_price(px-100, py-100, label=('Similar '+tit, 'Time','% Var') )\n"", '\n', '\n', 'kix= util.np_findfirst(name1, statsjp[:,0, ilag])\n', 'stocklist01[390]\n', '\n', '\n', 'stocklist01[157] , nk400name[157] ,  dateref1adj[86]     \n', 'plot_price(price_normalize100(ret_open1[157, 86:86+twindow-1])-100,px-100   )\n', ""'''    \n""]",[],0
utilmy/zarchive/py2to3/fast.py:day,day,function,9,15,14,125,8.33,0,0,['s'],[None],[None],130,[],['arrow.get'],1
utilmy/zarchive/py2to3/fast.py:month,month,function,9,15,14,125,8.33,0,0,['s'],[None],[None],131,[],['arrow.get'],1
utilmy/zarchive/py2to3/fast.py:year,year,function,9,15,14,125,8.33,0,0,['s'],[None],[None],132,[],['arrow.get'],1
utilmy/zarchive/py2to3/fast.py:hour,hour,function,9,15,14,125,8.33,0,0,['s'],[None],[None],133,[],['arrow.get'],1
utilmy/zarchive/py2to3/fast.py:weekday,weekday,function,9,15,14,125,8.33,0,0,['s'],[None],[None],139,[],['arrow.get'],1
utilmy/zarchive/py2to3/fast.py:season,season,function,4,15,13,47,3.13,0,1,['d'],[None],[None],148,[],['int'],1
utilmy/zarchive/py2to3/fast.py:daytime,daytime,function,5,40,23,123,3.08,0,1,['d'],[None],[None],154,[],['int'],1
utilmy/zarchive/py2to3/fast.py:fastStrptime,fastStrptime,function,8,46,35,288,6.26,0,2,"['val', 'format']","[None, None]","[None, None]",164,[],"['len', 'int', 'datetime.datetime']",3
utilmy/zarchive/py2to3/fast.py:drawdown_calc_fast,drawdown_calc_fast,function,36,80,63,618,7.72,1,3,['price'],[None],[None],190,[],"['len', 'np.zeros', 'range']",3
utilmy/zarchive/py2to3/fast.py:std,std,function,6,12,12,61,5.08,0,0,['x'],[None],[None],225,"['    """"""Std Deviation 1D array""""""\n']","['x.sum', 'sqrt']",2
utilmy/zarchive/py2to3/fast.py:mean,mean,function,2,3,3,24,8.0,0,0,['x'],[None],[None],236,"['    """"""Mean  """"""\n']",['x.sum'],1
utilmy/zarchive/py2to3/fast.py:_compute_overlaps,_compute_overlaps,function,13,27,20,108,4.0,1,0,"['u', 'v']","[None, None]","[None, None]",249,[],['xrange'],1
utilmy/zarchive/py2to3/fast.py:distance_jaccard2,distance_jaccard2,function,5,11,11,55,5.0,0,0,"['u', 'v']","[None, None]","[None, None]",260,[],"['_compute_overlaps', 'float']",2
utilmy/zarchive/py2to3/fast.py:distance_jaccard,distance_jaccard,function,5,11,11,55,5.0,0,0,"['u', 'v']","[None, None]","[None, None]",266,[],"['_compute_overlaps', 'float']",2
utilmy/zarchive/py2to3/fast.py:distance_jaccard_X,distance_jaccard_X,function,13,21,19,163,7.76,2,0,['X'],[None],[None],277,[],"['np.zeros', 'xrange', 'distance_jaccard']",3
utilmy/zarchive/py2to3/fast.py:cosine,cosine,function,11,43,26,262,6.09,0,2,"['u', 'v']","[None, None]","[None, None]",292,[],['np.sqrt'],1
utilmy/zarchive/py2to3/fast.py:rmse,rmse,function,1,7,6,31,4.43,0,0,"['y', 'yhat']","[None, None]","[None, None]",321,"['    """""" Calculate and return Root Mean Squared Error (RMSE)\n', '    Returns: float: Root Mean Squared Error\n', '    """"""\n']",[],0
utilmy/zarchive/py2to3/fast.py:cross,cross,function,17,31,25,211,6.81,0,0,"['vec1', 'vec2']","[None, None]","[None, None]",329,"['    """""" Calculate the dot product of two 3d vectors. """"""\n']","['double', 'np.zeros']",2
utilmy/zarchive/py2to3/fast.py:norm,norm,function,4,8,8,61,7.62,1,0,['vec'],[None],[None],341,"['    """""" Calculate the norm of a 3d vector. """"""\n']","['sqrt', 'range']",2
utilmy/zarchive/py2to3/fast.py:log_exp_sum2,log_exp_sum2,function,0,1,1,5,5.0,0,0,"['a', 'b']","[None, None]","[None, None]",242,[],[],0
utilmy/zarchive/py2to3/report.py:map_show,map_show,function,0,1,1,4,4.0,0,0,[],[],[],36,[],[],0
utilmy/zarchive/py2to3/report.py:xl_create_pivot,xl_create_pivot,function,4,9,9,134,14.89,0,0,"['infile', 'index_list', '""Rep""', '""Product""]', 'value_list', '""Quantity""]']","[None, None, None, None, None, None]","[None, '[""Manager""', None, None, '[""Price""', None]",65,"[""   ''' Read in the Excel file, create a pivot table and return it as a DataFrame '''\n""]",[],0
utilmy/zarchive/py2to3/report.py:xl_save_report,xl_save_report,function,6,10,10,167,16.7,1,0,"['report', 'outfile']","[None, None]","[None, None]",72,"[""   '''  Take a report and save it to a single Excel file\n"", '       sales_report = create_pivot(args.infile.name)\n', '       save_report(sales_report, args.outfile.name)\n', ""   '''\n""]","['temp_df.to_excel', 'writer.save']",2
utilmy/zarchive/py2to3/report.py:xl_create_pdf,xl_create_pdf,function,23,44,39,743,16.89,1,0,[],[],[],97,[],"['sales_report=create_pivot', 'manager_df.append', 'sales_report.xs', 'env=Environment', 'get_summary_stats', 'sales_report.to_html', 'HTML']",7
utilmy/zarchive/storage/theano_imdb.py:prepare_data,prepare_data,function,30,71,54,552,7.77,2,3,"['seqs', 'labels', 'maxlen']","[None, None, None]","[None, None, 'None']",12,"['    """"""Create the matrices from the datasets.\n', '\n', '    This pad each sequence to the same lenght: the lenght of the\n', '    longuest sequence or maxlen.\n', '\n', '    if maxlen is set, we will cut all sequence to this maximum\n', '    lenght.\n', '\n', '    This swap the axis!\n', '    """"""\n']","['zip', 'new_seqs.append', 'new_labels.append', 'new_lengths.append', 'len', 'numpy.max', 'numpy.zeros', 'enumerate']",8
utilmy/zarchive/storage/theano_imdb.py:get_dataset_file,get_dataset_file,function,16,43,35,416,9.67,0,3,"['dataset', 'default_dataset', 'origin']","[None, None, None]","[None, None, None]",54,"[""    '''Look for it as if it was a full path, if not, try local file,\n"", '    if not try in the data directory.\n', '\n', '    Download dataset if it is not present\n', '\n', ""    '''\n""]",['print'],1
utilmy/zarchive/storage/theano_imdb.py:load_data,load_data,function,54,180,103,1753,9.74,13,5,"['path', 'n_words', 'valid_portion', 'maxlen', 'sort_by_len']","[None, None, None, None, None]","['""imdb.pkl""', '100000', '0.1', 'None', 'True']",82,"[""    '''Loads the dataset\n"", '\n', '    :type path: String\n', '    :param path: The path to the dataset (here IMDB)\n', '    :type n_words: int\n', '    :param n_words: The number of word to keep in the vocabulary.\n', '        All extra words are set to unknow (1).\n', '    :type valid_portion: float\n', '    :param valid_portion: The proportion of the full train set used for\n', '        the validation set.\n', '    :type maxlen: None or positive int\n', '    :param maxlen: the max sequence length we use in the train/valid set.\n', '    :type sort_by_len: bool\n', '    :name sort_by_len: Sort by the sequence lenght for the train,\n', '        valid and test set. This allow faster execution as it cause\n', '        less padding per minibatch. Another mechanism must be used to\n', '        shuffle the train set at each epoch.\n', '\n', ""    '''\n""]","['get_dataset_file', 'path.endswith', 'gzip.open', 'open', 'pickle.load', 'f.close', 'zip', 'len', 'new_train_set_x.append', 'new_train_set_y.append', 'int', 'remove_unk', 'len_argsort', 'sorted']",14
utilmy/zarchive/storage/excel.py:get_workbook_name,get_workbook_name,function,3,5,5,48,9.6,0,0,[],[],[],9,"['    """"""Writes the name of the Workbook into Range(""D3"") of Sheet 1""""""\n']","['Workbook.caller', 'Range']",2
utilmy/zarchive/storage/excel.py:double_sum,double_sum,function,1,4,4,13,3.25,0,0,"['x', 'y']","[None, None]","[None, None]",16,"['    """"""Returns twice the sum of the two arguments""""""\n']",[],0
utilmy/zarchive/storage/excel.py:add_one,add_one,function,5,11,9,40,3.64,2,0,['data'],[None],[None],23,"['    """"""Adds 1 to every cell in Range""""""\n']",[],0
utilmy/zarchive/storage/excel.py:matrix_mult,matrix_mult,function,2,2,2,14,7.0,0,0,"['x', 'y']","[None, None]","[None, None]",31,"['    """"""Alternative implementation of Excel\'s MMULT, requires NumPy""""""\n']",['x.dot'],1
utilmy/zarchive/storage/excel.py:npdot,npdot,function,0,0,0,0,0.0,0,0,[],[],[],43,[],[],0
utilmy/zarchive/storage/multiprocessfunc.py:multigbm_paralell_func,multigbm_paralell_func,function,34,117,81,913,7.8,1,4,"['nbsimul', 'ww', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'price', 'type1', 'strike', 'cp']","[None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, '0', '0', '1']",20,[],"['np.zeros', 'range', 'np.dot', 'np.multiply', 'np.exp', 'np.cumprod', 'np.maximum', 'sum', 'sum1/float']",9
utilmy/zarchive/storage/multiprocessfunc.py:func,func,function,6,10,10,56,5.6,1,0,"['val', 'lock']","[None, None]","[None, None]",73,[],"['range', 'time.sleep']",2
utilmy/zarchive/storage/multiprocessfunc.py:multigbm_processfast7,multigbm_processfast7,function,24,48,44,419,8.73,3,0,"['nbsimul', 's0', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'price']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",150,[],"['range', 'np.dot', 'bm_generator', 'np.exp', 'sum', 'sum1/float']",6
utilmy/zarchive/storage/multiprocessfunc.py:bm_generator,bm_generator,function,5,9,8,43,4.78,0,1,"['bm', 'dt', 'n', 'type1']","[None, None, None, None]","[None, None, None, None]",177,[],[],0
utilmy/zarchive/storage/multiprocessfunc.py:merge,merge,function,6,20,17,106,5.3,1,1,['d2'],[None],[None],211,[],"['time.sleep', 'd2.keys']",2
utilmy/zarchive/storage/multiprocessfunc.py:integratenp2,integratenp2,function,10,29,25,184,6.34,1,0,"['its', 'nchunk']","[None, None]","[None, None]",222,[],"['range', 'ne.evaluate']",2
utilmy/zarchive/storage/multiprocessfunc.py:integratenp,integratenp,function,27,77,47,454,5.9,2,0,"['its', 'nchunk']","[None, None]","[None, None]",234,[],"['range', 'ne.evaluate', 'integratenp', 'np.sum', 'float']",5
utilmy/zarchive/storage/multiprocessfunc.py:integratene,integratene,function,12,44,38,275,6.25,1,0,['its'],[None],[None],247,[],"['range', 'ne.evaluate', 'float']",3
utilmy/zarchive/storage/multiprocessfunc.py:parzen_estimation,parzen_estimation,function,7,30,26,185,6.17,2,1,"['x_samples', 'point_x', 'h']","[None, None, None]","[None, None, None]",268,[],"['np.abs', 'len']",2
utilmy/zarchive/storage/multiprocessfunc.py:init2,init2,function,2,4,4,13,3.25,0,0,['d'],[None],[None],282,[],[],0
utilmy/zarchive/storage/multiprocessfunc.py:init_global1,init_global1,function,2,2,2,37,18.5,0,0,"['l', 'r']","[None, None]","[None, None]",286,[],[],0
utilmy/zarchive/storage/multiprocessfunc.py:np_sin,np_sin,function,2,2,2,19,9.5,0,0,['value'],[None],[None],291,[],['np.sin'],1
utilmy/zarchive/storage/multiprocessfunc.py:ne_sin,ne_sin,function,2,2,2,27,13.5,0,0,['x'],[None],[None],294,[],['ne.evaluate'],1
utilmy/zarchive/storage/multiprocessfunc.py:res_shared2,res_shared2,function,4,16,14,94,5.88,0,0,[],[],[],297,[],['float'],1
utilmy/zarchive/storage/multiprocessfunc.py:list_append,list_append,function,3,4,4,54,13.5,1,0,"['count', 'id', 'out_list']","[None, None, None]","[None, None, None]",261,"['\t""""""Creates an empty list and then appends a random number to the list \'count\' number of times. A CPU-heavy operation!""""""\n']",['range'],1
utilmy/zarchive/storage/technical_indicator.py:np_find,np_find,function,5,11,10,55,5.0,1,1,"['item', 'vec']","[None, None]","[None, None]",5,"['    """"""return the index of the first occurence of item in vec""""""\n']",['xrange'],1
utilmy/zarchive/storage/technical_indicator.py:np_find_minpos,np_find_minpos,function,4,7,5,95,13.57,0,0,['values'],[None],[None],12,[],['min'],1
utilmy/zarchive/storage/technical_indicator.py:np_find_maxpos,np_find_maxpos,function,4,7,5,95,13.57,0,0,['values'],[None],[None],16,[],['max'],1
utilmy/zarchive/storage/technical_indicator.py:date_earningquater,date_earningquater,function,4,15,13,96,6.4,0,1,['t1'],[None],[None],20,[],['datetime'],1
utilmy/zarchive/storage/technical_indicator.py:date_option_expiry,date_option_expiry,function,10,60,33,387,6.45,0,2,['date'],[None],[None],39,[],['datetime'],1
utilmy/zarchive/storage/technical_indicator.py:linearreg,linearreg,function,11,11,11,57,5.18,0,0,"['a', '*args']","[None, None]","[None, None]",57,[],['np.sum'],1
utilmy/zarchive/storage/technical_indicator.py:np_sortbycolumn,np_sortbycolumn,function,6,8,8,70,8.75,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",62,[],"['pd.DataFrame', 'df.sort']",2
utilmy/zarchive/storage/technical_indicator.py:np_findlocalmax,np_findlocalmax,function,22,47,34,233,4.96,1,3,['v'],[None],[None],68,[],"['n=len', 'np.zeros', 'enumerate', 'np_sortbycolumn', 'np_find_maxpos']",5
utilmy/zarchive/storage/technical_indicator.py:findhigher,findhigher,function,5,12,11,54,4.5,1,1,"['item', 'vec']","[None, None]","[None, None]",82,[],['xrange'],1
utilmy/zarchive/storage/technical_indicator.py:findlower,findlower,function,5,12,11,54,4.5,1,1,"['item', 'vec']","[None, None]","[None, None]",87,[],['xrange'],1
utilmy/zarchive/storage/technical_indicator.py:np_findlocalmin,np_findlocalmin,function,24,50,37,250,5.0,1,3,['v'],[None],[None],93,[],"['n=len', 'np.zeros', 'enumerate', 'np_sortbycolumn', 'np_find_minpos']",5
utilmy/zarchive/storage/technical_indicator.py:supportmaxmin1,supportmaxmin1,function,115,500,206,3613,7.23,2,11,['df1'],[None],[None],113,[],"['np_findlocalmax', 'len', 'np_find_maxpos', 'range', 'findhigher', 'np.abs', 'np.shape', 'np.arange', 'min', 'np.zeros', 'max', 'np_findlocalmin', 'np_find_minpos', 'findlower', 'pd.Series', 'df1.join']",16
utilmy/zarchive/storage/technical_indicator.py:RET,RET,function,10,21,16,123,5.86,0,0,"['df', 'n']","[None, None]","[None, None]",265,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:qearning_dist,qearning_dist,function,19,38,32,317,8.34,1,0,['df'],[None],[None],274,[],"['np.zeros', 'enumerate', 'date_earningquater', 'pd.Series', 'df.join']",5
utilmy/zarchive/storage/technical_indicator.py:optionexpiry_dist,optionexpiry_dist,function,15,26,23,212,8.15,1,0,['df'],[None],[None],287,[],"['np.zeros', 'enumerate', 'date_option_expiry', 'pd.Series', 'df.join']",5
utilmy/zarchive/storage/technical_indicator.py:nbtime_reachtop,nbtime_reachtop,function,17,34,31,284,8.35,1,0,"['df', 'n', 'trigger']","[None, None, None]","[None, None, '0.005']",298,"[""  '''nb of days from 1 year low '''\n""]","['np.zeros', 'range', 'len', 'np_find_maxpos', 'np.abs', 'np.sign', 'pd.Series', 'str', 'df.join']",9
utilmy/zarchive/storage/technical_indicator.py:nbday_high,nbday_high,function,20,39,32,333,8.54,1,0,"['df', 'n']","[None, None]","[None, None]",312,"[""  '''nb of days from 1 year low '''\n""]","['np.zeros', 'range', 'len', 'np_find_maxpos', 'pd.Series', 'str', 'df.join']",7
utilmy/zarchive/storage/technical_indicator.py:distance_day,distance_day,function,14,28,24,192,6.86,1,0,"['df', 'tk', 'tkname']","[None, None, None]","[None, None, None]",328,[],"['datetime.date', 'np.zeros', 'range', 'len', 'pd.Series', 'df.join']",6
utilmy/zarchive/storage/technical_indicator.py:distance,distance,function,16,44,36,298,6.77,1,0,"['df', 'tk', 'tkname']","[None, None, None]","[None, None, None]",339,[],"['datetime.date', 'np.zeros', 'range', 'len', 'pd.Series', 'df.join', 'distance']",7
utilmy/zarchive/storage/technical_indicator.py:MA,MA,function,5,12,10,86,7.17,0,0,"['df', 'n']","[None, None]","[None, None]",346,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:EMA,EMA,function,5,16,14,102,6.38,0,0,"['df', 'n']","[None, None]","[None, None]",352,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:MOM,MOM,function,5,11,9,79,7.18,0,0,"['df', 'n']","[None, None]","[None, None]",358,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:ROC,ROC,function,9,20,15,111,5.55,0,0,"['df', 'n']","[None, None]","[None, None]",364,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:ATR,ATR,function,12,47,32,280,5.96,1,0,"['df', 'n']","[None, None]","[None, None]",372,[],"['max', 'df.get_value', 'min', 'TR_l.append', 'pd.Series', 'str', 'df.join']",7
utilmy/zarchive/storage/technical_indicator.py:BBANDS,BBANDS,function,10,41,25,261,6.37,0,0,"['df', 'n']","[None, None]","[None, None]",385,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:PPSR,PPSR,function,14,55,37,378,6.87,0,0,['df'],[None],[None],398,[],"['pd.Series', 'pd.DataFrame', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:STOK,STOK,function,5,13,11,98,7.54,0,0,['df'],[None],[None],412,[],"['pd.Series', 'df.join']",2
utilmy/zarchive/storage/technical_indicator.py:STO,STO,function,7,40,24,282,7.05,0,0,['df'],[None],[None],418,[],"['pd.Series', 'df.join', 'STO', 'str']",4
utilmy/zarchive/storage/technical_indicator.py:TRIX,TRIX,function,14,58,33,286,4.93,1,0,"['df', 'n']","[None, None]","[None, None]",425,[],"['pd.ewma', 'ROC_l.append', 'pd.Series', 'str', 'df.join']",5
utilmy/zarchive/storage/technical_indicator.py:ADX,ADX,function,25,152,68,821,5.4,2,2,"['df', 'n', 'n_ADX']","[None, None, None]","[None, None, None]",440,[],"['df.get_value', 'UpI.append', 'DoI.append', 'max', 'min', 'TR_l.append', 'pd.Series', 'str', 'df.join']",9
utilmy/zarchive/storage/technical_indicator.py:MACD,MACD,function,9,57,32,465,8.16,0,0,"['df', 'n_fast', 'n_slow']","[None, None, None]","[None, None, None]",473,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:MassI,MassI,function,11,34,22,202,5.94,0,0,['df'],[None],[None],485,[],"['pd.ewma', 'pd.Series', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:Vortex,Vortex,function,14,72,36,456,6.33,2,0,"['df', 'n']","[None, None]","[None, None]",495,[],"['max', 'df.get_value', 'min', 'TR.append', 'abs', 'VM.append', 'pd.Series', 'pd.rolling_sum', 'str', 'df.join']",10
utilmy/zarchive/storage/technical_indicator.py:KST,KST,function,13,83,42,485,5.84,0,0,"['df', 'r1', 'r2', 'r3', 'r4', 'n1', 'n2', 'n3', 'n4']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, None]",513,[],"['pd.Series', 'pd.rolling_sum', 'str', 'df.join']",4
utilmy/zarchive/storage/technical_indicator.py:RSI,RSI,function,19,103,46,512,4.97,1,2,"['df', 'n']","[None, None]","[None, '14']",531,[],"['df.get_value', 'UpI.append', 'DoI.append', 'pd.Series', 'str', 'df.join']",6
utilmy/zarchive/storage/technical_indicator.py:RMI,RMI,function,20,102,50,567,5.56,1,2,"['df', 'n', 'm']","[None, None, None]","[None, '14', '10']",558,[],"['list', 'df.get_value', 'UpI.append', 'DoI.append', 'pd.Series', 'str', 'df.join']",7
utilmy/zarchive/storage/technical_indicator.py:TSI,TSI,function,12,52,31,333,6.4,0,0,"['df', 'r', 's']","[None, None, None]","[None, None, None]",588,[],"['pd.Series', 'abs', 'str', 'df.join']",4
utilmy/zarchive/storage/technical_indicator.py:ACCDIST,ACCDIST,function,11,32,22,184,5.75,0,0,"['df', 'n']","[None, None]","[None, None]",600,[],"['ad.diff', 'ad.shift', 'pd.Series', 'str', 'df.join']",5
utilmy/zarchive/storage/technical_indicator.py:Chaikin,Chaikin,function,6,28,22,205,7.32,0,0,['df'],[None],[None],610,[],"['pd.Series', 'pd.ewma', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:MFI,MFI,function,15,60,40,322,5.37,1,1,"['df', 'n']","[None, None]","[None, None]",617,[],"['PosMF.append', 'df.get_value', 'pd.Series', 'str', 'df.join']",5
utilmy/zarchive/storage/technical_indicator.py:OBV,OBV,function,10,66,30,404,6.12,1,3,"['df', 'n']","[None, None]","[None, None]",635,[],"['df.get_value', 'OBV.append', 'pd.Series', 'str', 'df.join']",5
utilmy/zarchive/storage/technical_indicator.py:FORCE,FORCE,function,5,12,10,97,8.08,0,0,"['df', 'n']","[None, None]","[None, None]",652,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:EOM,EOM,function,6,20,17,170,8.5,0,0,"['df', 'n']","[None, None]","[None, None]",658,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:CCI,CCI,function,6,21,18,146,6.95,0,0,"['df', 'n']","[None, None]","[None, None]",665,[],"['pd.Series', 'pd.rolling_mean', 'pd.rolling_std', 'str', 'df.join']",5
utilmy/zarchive/storage/technical_indicator.py:COPP,COPP,function,11,48,24,261,5.44,0,0,"['df', 'n']","[None, None]","[None, None]",672,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:KELCH,KELCH,function,7,45,24,362,8.04,0,0,"['df', 'n']","[None, None]","[None, None]",684,[],"['pd.Series', 'str', 'df.join']",3
utilmy/zarchive/storage/technical_indicator.py:ULTOSC,ULTOSC,function,15,67,40,552,8.24,1,0,['df'],[None],[None],694,[],"['max', 'df.get_value', 'min', 'TR_l.append', 'BP_l.append', 'pd.Series', 'pd.rolling_sum', 'df.join']",8
utilmy/zarchive/storage/technical_indicator.py:DONCH,DONCH,function,13,57,27,249,4.37,2,0,"['df', 'n']","[None, None]","[None, None]",709,[],"['DC_l.append', 'max', 'min', 'pd.Series', 'str', 'DonCh.shift', 'df.join']",7
utilmy/zarchive/storage/technical_indicator.py:STDDEV,STDDEV,function,3,9,8,80,8.89,0,0,"['df', 'n']","[None, None]","[None, None]",726,[],"['df.join', 'str']",2
utilmy/zarchive/storage/technical_indicator.py:RWI,RWI,function,1,2,2,7,3.5,0,0,"['df', 'nn', 'nATR']","[None, None, None]","[None, None, None]",731,[],[],0
utilmy/zarchive/storage/technical_indicator.py:nbday_low,nbday_low,function,20,39,32,331,8.49,1,0,"['df', 'n']","[None, None]","[None, None]",745,"[""  '''nb of days from 1 year low '''\n""]","['np.zeros', 'range', 'len', 'np_find_minpos', 'pd.Series', 'str', 'df.join']",7
utilmy/zarchive/storage/technical_indicator.py:nbday_high,nbday_high,function,20,39,32,333,8.54,1,0,"['df', 'n']","[None, None]","[None, None]",312,"[""  '''nb of days from 1 year low '''\n""]","['np.zeros', 'range', 'len', 'np_find_maxpos', 'pd.Series', 'str', 'df.join']",7
utilmy/zarchive/storage/benchmarktest.py:payoff1,payoff1,function,4,5,5,61,12.2,0,0,['pricepath'],[None],[None],130,[],"['pricepath[len', 'np.maximum']",2
utilmy/zarchive/storage/benchmarktest.py:payoffeuro1,payoffeuro1,function,2,3,3,30,10.0,0,0,['st'],[None],[None],145,[],['np.maximum'],1
utilmy/zarchive/storage/benchmarktest.py:payoff2,payoff2,function,6,13,13,127,9.77,0,0,['pricepath'],[None],[None],181,[],"['np.shape', 'np.sum', 'np.maximum']",3
utilmy/zarchive/storage/benchmarktest.py:payoffeuro1,payoffeuro1,function,2,3,3,30,10.0,0,0,['st'],[None],[None],145,[],['np.maximum'],1
utilmy/zarchive/storage/dl_utils.py:save_weights,save_weights,function,1,5,5,42,8.4,0,0,"['file', 'tuple_weights']","[None, None]","[None, None]",21,[],"['pickle.dump', 'open']",2
utilmy/zarchive/storage/dl_utils.py:save_prediction,save_prediction,function,1,5,5,39,7.8,0,0,"['file', 'prediction']","[None, None]","[None, None]",24,[],"['pickle.dump', 'open']",2
utilmy/zarchive/storage/dl_utils.py:log,log,function,4,6,6,91,15.17,0,0,"['msg', 'file']","[None, None]","[None, '""""']",27,[],"['open', 'logfile']",2
utilmy/zarchive/storage/dl_utils.py:logfile,logfile,function,5,8,8,76,9.5,0,0,"['msg', 'file']","[None, None]","[None, None]",30,[],"['open', 'myfile.write']",2
utilmy/zarchive/storage/dl_utils.py:log_p,log_p,function,3,2,2,23,11.5,0,0,"['msg', 'file']","[None, None]","[None, '""""']",34,[],['log'],1
utilmy/zarchive/storage/dl_utils.py:init_weight,init_weight,function,1,4,4,52,13.0,0,0,"['hidden1', 'hidden2', 'acti_type']","[None, None, None]","[None, None, None]",38,[],[],0
utilmy/zarchive/storage/dl_utils.py:get_all_data,get_all_data,function,21,26,25,476,18.31,1,2,['file'],[None],[None],52,[],"['str', 'open', 'line.strip', 'x.split']",4
utilmy/zarchive/storage/dl_utils.py:get_batch_data,get_batch_data,function,7,7,7,195,27.86,0,1,"['file', 'index', 'size']","[None, None, None]","[None, None, None]",75,[],"['line.strip', 'x.split']",2
utilmy/zarchive/storage/dl_utils.py:get_xy,get_xy,function,5,6,6,107,17.83,0,0,['line'],[None],[None],95,[],"['y=int', 'x.split']",2
utilmy/zarchive/storage/dl_utils.py:file_len,file_len,function,7,10,10,62,6.2,1,0,['fname'],[None],[None],103,[],"['open', 'enumerate']",2
utilmy/zarchive/storage/dl_utils.py:feats_len,feats_len,function,4,5,5,65,13.0,0,0,['fname'],[None],[None],111,[],['open'],1
utilmy/zarchive/storage/panda_util.py:excel_topandas,excel_topandas,function,14,26,26,215,8.27,1,0,"['filein', 'fileout']","[None, None]","[None, None]",72,[],"['pd.ExcelFile', 'xls_file.parse', 'df.to_hdf', 'pd.HDFStore']",4
utilmy/zarchive/storage/panda_util.py:panda_toexcel,panda_toexcel,function,0,0,0,0,0.0,0,0,[],[],[],84,"[""'''\n"", '\n', '\n', '\n', '\n', 'from scipy.interpolate import interp1d\n', '\n', ""f2 = interp1d(qq['Datetime'], qq['Open'],bounds_error=False)\n"", 'open =    np.column_stack((open, f2(dater)))\n', '\n', 'f2(qq.Datetime)\n', '\n', '\n', ""open = qq[['Datetime','Close']]  #new dataframe\n"", '\n', ""open = open.join(pd.DataFrame(data={'Close1':qq['Close']}), how='outer', rsuffix='_1')\n"", '\n', '\n', '\n', '\n', ""f2= pd.DataFrame( qq['Close'], index=qq['Datetime'])\n"", '\n', 'pd.concat([df1,df2,df3])\n', '\n', ""qq['Datetime','Close']\n"", '\n', '\n', '\n', '\n', '#------Generate Sequence of date-------------------\n', '# Every 5 hours 10 minutes\n', '\n', 'dater= dater.values\n', ""dater= pd.date_range(start='12/24/2015', end='1/1/2016',   freq='0h05min').values\n"", ""Indexing DatetimeIndex objects. (To get all data from December 2012 through the end of May 2013 data you could do df.ix['December 2012':May 2013'])\n"", '\n', '\n', '\n', 'qq.ix[dater]\n', '\n', '\n', '\n', 'df.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]\n', '\n', '\n', '#  Get values where Index is not in  \n', ""rpt[~rpt['STK_ID'].isin(stk_list)]\n"", '\n', ""qq['Close'].where( qq['Datetime']= dater)\n"", '\n', '\n', ""qq['Datetime' = dater]\n"", ""qq.resample('5min')\n"", '\n', '\n', '\n', '\n', '\n', '# evenly spaced times\n', 't1 = np.array([0,0.5,1.0,1.5,2.0])\n', 'y1 = t1\n', '\n', '# unevenly spaced times\n', 't2 = np.array([0,0.34,1.01,1.4,1.6,1.7,2.01])\n', 'y2 = 3*t2\n', '\n', ""df1 = pd.DataFrame(data={'y1':y1,'t':t1})\n"", ""df2 = pd.DataFrame(data={'y2':y2,'t':t2})\n"", '\n', 'f2 = interp1d(t2,y2,bounds_error=False)\n', ""df1['y2'] = f2(df1.t)\n"", '\n', ""'''\n""]",[],0
utilmy/zarchive/storage/panda_util.py:panda_todabatase,panda_todabatase,function,0,0,0,0,0.0,0,0,[],[],[],88,"[""'''\n"", '\n', '\n', '\n', '\n', 'from scipy.interpolate import interp1d\n', '\n', ""f2 = interp1d(qq['Datetime'], qq['Open'],bounds_error=False)\n"", 'open =    np.column_stack((open, f2(dater)))\n', '\n', 'f2(qq.Datetime)\n', '\n', '\n', ""open = qq[['Datetime','Close']]  #new dataframe\n"", '\n', ""open = open.join(pd.DataFrame(data={'Close1':qq['Close']}), how='outer', rsuffix='_1')\n"", '\n', '\n', '\n', '\n', ""f2= pd.DataFrame( qq['Close'], index=qq['Datetime'])\n"", '\n', 'pd.concat([df1,df2,df3])\n', '\n', ""qq['Datetime','Close']\n"", '\n', '\n', '\n', '\n', '#------Generate Sequence of date-------------------\n', '# Every 5 hours 10 minutes\n', '\n', 'dater= dater.values\n', ""dater= pd.date_range(start='12/24/2015', end='1/1/2016',   freq='0h05min').values\n"", ""Indexing DatetimeIndex objects. (To get all data from December 2012 through the end of May 2013 data you could do df.ix['December 2012':May 2013'])\n"", '\n', '\n', '\n', 'qq.ix[dater]\n', '\n', '\n', '\n', 'df.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]\n', '\n', '\n', '#  Get values where Index is not in  \n', ""rpt[~rpt['STK_ID'].isin(stk_list)]\n"", '\n', ""qq['Close'].where( qq['Datetime']= dater)\n"", '\n', '\n', ""qq['Datetime' = dater]\n"", ""qq.resample('5min')\n"", '\n', '\n', '\n', '\n', '\n', '# evenly spaced times\n', 't1 = np.array([0,0.5,1.0,1.5,2.0])\n', 'y1 = t1\n', '\n', '# unevenly spaced times\n', 't2 = np.array([0,0.34,1.01,1.4,1.6,1.7,2.01])\n', 'y2 = 3*t2\n', '\n', ""df1 = pd.DataFrame(data={'y1':y1,'t':t1})\n"", ""df2 = pd.DataFrame(data={'y2':y2,'t':t2})\n"", '\n', 'f2 = interp1d(t2,y2,bounds_error=False)\n', ""df1['y2'] = f2(df1.t)\n"", '\n', ""'''\n""]",[],0
utilmy/zarchive/storage/panda_util.py:database_topanda,database_topanda,function,0,0,0,0,0.0,0,0,[],[],[],92,"[""'''\n"", '\n', '\n', '\n', '\n', 'from scipy.interpolate import interp1d\n', '\n', ""f2 = interp1d(qq['Datetime'], qq['Open'],bounds_error=False)\n"", 'open =    np.column_stack((open, f2(dater)))\n', '\n', 'f2(qq.Datetime)\n', '\n', '\n', ""open = qq[['Datetime','Close']]  #new dataframe\n"", '\n', ""open = open.join(pd.DataFrame(data={'Close1':qq['Close']}), how='outer', rsuffix='_1')\n"", '\n', '\n', '\n', '\n', ""f2= pd.DataFrame( qq['Close'], index=qq['Datetime'])\n"", '\n', 'pd.concat([df1,df2,df3])\n', '\n', ""qq['Datetime','Close']\n"", '\n', '\n', '\n', '\n', '#------Generate Sequence of date-------------------\n', '# Every 5 hours 10 minutes\n', '\n', 'dater= dater.values\n', ""dater= pd.date_range(start='12/24/2015', end='1/1/2016',   freq='0h05min').values\n"", ""Indexing DatetimeIndex objects. (To get all data from December 2012 through the end of May 2013 data you could do df.ix['December 2012':May 2013'])\n"", '\n', '\n', '\n', 'qq.ix[dater]\n', '\n', '\n', '\n', 'df.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]\n', '\n', '\n', '#  Get values where Index is not in  \n', ""rpt[~rpt['STK_ID'].isin(stk_list)]\n"", '\n', ""qq['Close'].where( qq['Datetime']= dater)\n"", '\n', '\n', ""qq['Datetime' = dater]\n"", ""qq.resample('5min')\n"", '\n', '\n', '\n', '\n', '\n', '# evenly spaced times\n', 't1 = np.array([0,0.5,1.0,1.5,2.0])\n', 'y1 = t1\n', '\n', '# unevenly spaced times\n', 't2 = np.array([0,0.34,1.01,1.4,1.6,1.7,2.01])\n', 'y2 = 3*t2\n', '\n', ""df1 = pd.DataFrame(data={'y1':y1,'t':t1})\n"", ""df2 = pd.DataFrame(data={'y2':y2,'t':t2})\n"", '\n', 'f2 = interp1d(t2,y2,bounds_error=False)\n', ""df1['y2'] = f2(df1.t)\n"", '\n', ""'''\n""]",[],0
utilmy/zarchive/storage/panda_util.py:sqlquery_topanda,sqlquery_topanda,function,0,0,0,0,0.0,0,0,[],[],[],96,"[""'''\n"", '\n', '\n', '\n', '\n', 'from scipy.interpolate import interp1d\n', '\n', ""f2 = interp1d(qq['Datetime'], qq['Open'],bounds_error=False)\n"", 'open =    np.column_stack((open, f2(dater)))\n', '\n', 'f2(qq.Datetime)\n', '\n', '\n', ""open = qq[['Datetime','Close']]  #new dataframe\n"", '\n', ""open = open.join(pd.DataFrame(data={'Close1':qq['Close']}), how='outer', rsuffix='_1')\n"", '\n', '\n', '\n', '\n', ""f2= pd.DataFrame( qq['Close'], index=qq['Datetime'])\n"", '\n', 'pd.concat([df1,df2,df3])\n', '\n', ""qq['Datetime','Close']\n"", '\n', '\n', '\n', '\n', '#------Generate Sequence of date-------------------\n', '# Every 5 hours 10 minutes\n', '\n', 'dater= dater.values\n', ""dater= pd.date_range(start='12/24/2015', end='1/1/2016',   freq='0h05min').values\n"", ""Indexing DatetimeIndex objects. (To get all data from December 2012 through the end of May 2013 data you could do df.ix['December 2012':May 2013'])\n"", '\n', '\n', '\n', 'qq.ix[dater]\n', '\n', '\n', '\n', 'df.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]\n', '\n', '\n', '#  Get values where Index is not in  \n', ""rpt[~rpt['STK_ID'].isin(stk_list)]\n"", '\n', ""qq['Close'].where( qq['Datetime']= dater)\n"", '\n', '\n', ""qq['Datetime' = dater]\n"", ""qq.resample('5min')\n"", '\n', '\n', '\n', '\n', '\n', '# evenly spaced times\n', 't1 = np.array([0,0.5,1.0,1.5,2.0])\n', 'y1 = t1\n', '\n', '# unevenly spaced times\n', 't2 = np.array([0,0.34,1.01,1.4,1.6,1.7,2.01])\n', 'y2 = 3*t2\n', '\n', ""df1 = pd.DataFrame(data={'y1':y1,'t':t1})\n"", ""df2 = pd.DataFrame(data={'y2':y2,'t':t2})\n"", '\n', 'f2 = interp1d(t2,y2,bounds_error=False)\n', ""df1['y2'] = f2(df1.t)\n"", '\n', ""'''\n""]",[],0
utilmy/zarchive/storage/panda_util.py:folder_topanda,folder_topanda,function,0,0,0,0,0.0,0,0,[],[],[],100,"[""'''\n"", '\n', '\n', '\n', '\n', 'from scipy.interpolate import interp1d\n', '\n', ""f2 = interp1d(qq['Datetime'], qq['Open'],bounds_error=False)\n"", 'open =    np.column_stack((open, f2(dater)))\n', '\n', 'f2(qq.Datetime)\n', '\n', '\n', ""open = qq[['Datetime','Close']]  #new dataframe\n"", '\n', ""open = open.join(pd.DataFrame(data={'Close1':qq['Close']}), how='outer', rsuffix='_1')\n"", '\n', '\n', '\n', '\n', ""f2= pd.DataFrame( qq['Close'], index=qq['Datetime'])\n"", '\n', 'pd.concat([df1,df2,df3])\n', '\n', ""qq['Datetime','Close']\n"", '\n', '\n', '\n', '\n', '#------Generate Sequence of date-------------------\n', '# Every 5 hours 10 minutes\n', '\n', 'dater= dater.values\n', ""dater= pd.date_range(start='12/24/2015', end='1/1/2016',   freq='0h05min').values\n"", ""Indexing DatetimeIndex objects. (To get all data from December 2012 through the end of May 2013 data you could do df.ix['December 2012':May 2013'])\n"", '\n', '\n', '\n', 'qq.ix[dater]\n', '\n', '\n', '\n', 'df.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]\n', '\n', '\n', '#  Get values where Index is not in  \n', ""rpt[~rpt['STK_ID'].isin(stk_list)]\n"", '\n', ""qq['Close'].where( qq['Datetime']= dater)\n"", '\n', '\n', ""qq['Datetime' = dater]\n"", ""qq.resample('5min')\n"", '\n', '\n', '\n', '\n', '\n', '# evenly spaced times\n', 't1 = np.array([0,0.5,1.0,1.5,2.0])\n', 'y1 = t1\n', '\n', '# unevenly spaced times\n', 't2 = np.array([0,0.34,1.01,1.4,1.6,1.7,2.01])\n', 'y2 = 3*t2\n', '\n', ""df1 = pd.DataFrame(data={'y1':y1,'t':t1})\n"", ""df2 = pd.DataFrame(data={'y2':y2,'t':t2})\n"", '\n', 'f2 = interp1d(t2,y2,bounds_error=False)\n', ""df1['y2'] = f2(df1.t)\n"", '\n', ""'''\n""]",[],0
utilmy/zarchive/storage/panda_util.py:panda_tofolder,panda_tofolder,function,0,0,0,0,0.0,0,0,[],[],[],104,"[""'''\n"", '\n', '\n', '\n', '\n', 'from scipy.interpolate import interp1d\n', '\n', ""f2 = interp1d(qq['Datetime'], qq['Open'],bounds_error=False)\n"", 'open =    np.column_stack((open, f2(dater)))\n', '\n', 'f2(qq.Datetime)\n', '\n', '\n', ""open = qq[['Datetime','Close']]  #new dataframe\n"", '\n', ""open = open.join(pd.DataFrame(data={'Close1':qq['Close']}), how='outer', rsuffix='_1')\n"", '\n', '\n', '\n', '\n', ""f2= pd.DataFrame( qq['Close'], index=qq['Datetime'])\n"", '\n', 'pd.concat([df1,df2,df3])\n', '\n', ""qq['Datetime','Close']\n"", '\n', '\n', '\n', '\n', '#------Generate Sequence of date-------------------\n', '# Every 5 hours 10 minutes\n', '\n', 'dater= dater.values\n', ""dater= pd.date_range(start='12/24/2015', end='1/1/2016',   freq='0h05min').values\n"", ""Indexing DatetimeIndex objects. (To get all data from December 2012 through the end of May 2013 data you could do df.ix['December 2012':May 2013'])\n"", '\n', '\n', '\n', 'qq.ix[dater]\n', '\n', '\n', '\n', 'df.ix[datetime.date(year=2014,month=1,day=1):datetime.date(year=2014,month=2,day=1)]\n', '\n', '\n', '#  Get values where Index is not in  \n', ""rpt[~rpt['STK_ID'].isin(stk_list)]\n"", '\n', ""qq['Close'].where( qq['Datetime']= dater)\n"", '\n', '\n', ""qq['Datetime' = dater]\n"", ""qq.resample('5min')\n"", '\n', '\n', '\n', '\n', '\n', '# evenly spaced times\n', 't1 = np.array([0,0.5,1.0,1.5,2.0])\n', 'y1 = t1\n', '\n', '# unevenly spaced times\n', 't2 = np.array([0,0.34,1.01,1.4,1.6,1.7,2.01])\n', 'y2 = 3*t2\n', '\n', ""df1 = pd.DataFrame(data={'y1':y1,'t':t1})\n"", ""df2 = pd.DataFrame(data={'y2':y2,'t':t2})\n"", '\n', 'f2 = interp1d(t2,y2,bounds_error=False)\n', ""df1['y2'] = f2(df1.t)\n"", '\n', ""'''\n""]",[],0
utilmy/zarchive/storage/panda_util.py:numpy_topanda,numpy_topanda,function,2,6,6,65,10.83,0,0,"['vv', 'fileout', 'colname']","[None, None, None]","[None, '""""', '""data""']",354,[],"['pd.DataFrame', 'pd.HDFStore', 'st.append']",3
utilmy/zarchive/storage/panda_util.py:panda_tonumpy,panda_tonumpy,function,6,10,10,92,9.2,0,0,"['filename', 'nsize', 'tablen']","[None, None, None]","[None, None, ""'data'""]",357,[],['pd.read_hdf'],1
utilmy/zarchive/storage/panda_util.py:df_topanda,df_topanda,function,2,5,5,68,13.6,0,0,"['vv', 'filenameh5', 'colname']","[None, None, None]","[None, None, ""'data'""]",361,[],"['pd.HDFStore', 'store.append', 'store.close']",3
utilmy/zarchive/storage/panda_util.py:load_frompanda,load_frompanda,function,2,5,5,68,13.6,0,0,"['filenameh5', 'colname']","[None, None]","[None, '""data""']",364,[],"['pd.HDFStore', 'store.append', 'store.close']",3
utilmy/zarchive/storage/panda_util.py:csv_topanda,csv_topanda,function,2,5,5,68,13.6,0,0,"['filein1, filename, tablen']",[None],"['\'data\', lineterminator="",""']",368,[],"['pd.HDFStore', 'store.append', 'store.close']",3
utilmy/zarchive/storage/panda_util.py:getrandom_tonumpy,getrandom_tonumpy,function,6,9,9,101,11.22,0,0,"['filename', 'nbdim', 'nbsample', 'tablen']","[None, None, None, None]","[None, None, None, ""'data'""]",377,[],['pd.read_hdf'],1
utilmy/zarchive/storage/panda_util.py:numexpr_topanda,numexpr_topanda,function,10,28,22,248,8.86,0,0,"['filename', 'i0', 'imax', 'expr', ""fileout='E""]","[None, None, None, None, '']","[None, '0', '1000', None, ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",385,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/storage/panda_util.py:excel_topanda,excel_topanda,function,14,26,26,215,8.27,1,0,"['filein', 'fileout']","[None, None]","[None, None]",397,"[""'''\n"", '\n', "">>> %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')\n"", '1000 loops, best of 3: 1.54 ms per loop\n', '\n', '>>> %timeit df3.groupby(df3.index).first()\n', '1000 loops, best of 3: 580 s per loop\n', '\n', ""'''\n""]","['pd.ExcelFile', 'xls_file.parse', 'df.to_hdf', 'pd.HDFStore']",4
utilmy/zarchive/storage/panda_util.py:array_toexcel,array_toexcel,function,4,8,8,148,18.5,0,0,"['vv', 'wk', 'r1)subset', 'take_last=True)level=0))a)']","[None, None, None, '']","[None, None, ""'rownum'"", 'True)level=0))a):']",402,"[""'''\n"", '\n', "">>> %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')\n"", '1000 loops, best of 3: 1.54 ms per loop\n', '\n', '>>> %timeit df3.groupby(df3.index).first()\n', '1000 loops, best of 3: 580 s per loop\n', '\n', ""'''\n""]","['np.ascontiguousarray', 'np.unique', 'unique_a.view']",3
utilmy/zarchive/storage/panda_util.py:unique_rows,unique_rows,function,4,8,8,148,18.5,0,0,['a'],[None],[None],432,[],"['np.ascontiguousarray', 'np.unique', 'unique_a.view']",3
utilmy/zarchive/storage/panda_util.py:remove_zeros,remove_zeros,function,0,0,0,0,0.0,0,0,[],[],[],436,[],[],0
utilmy/zarchive/storage/panda_util.py:sort_array,sort_array,function,0,0,0,0,0.0,0,0,[],[],[],438,[],[],0
utilmy/zarchive/storage/allmodule.py:aa_isanaconda,aa_isanaconda,function,6,15,14,80,5.33,0,1,[],[],[],4,[],['txt.find'],1
utilmy/zarchive/storage/theano_lstm.py:numpy_floatX,numpy_floatX,function,2,3,3,45,15.0,0,0,['data'],[None],[None],26,[],['numpy.asarray'],1
utilmy/zarchive/storage/theano_lstm.py:get_minibatches_idx,get_minibatches_idx,function,11,29,26,373,12.86,1,1,"['n', 'minibatch_size', 'shuffle']","[None, None, None]","[None, None, 'False']",30,"['    """"""\n', '    Used to shuffle the dataset at each iteration.\n', '    """"""\n']","['numpy.arange', 'range', 'minibatches.append', 'zip']",4
utilmy/zarchive/storage/theano_lstm.py:get_dataset,get_dataset,function,3,3,3,41,13.67,0,0,['name'],[None],[None],54,[],[],0
utilmy/zarchive/storage/theano_lstm.py:zipp,zipp,function,4,6,6,51,8.5,1,0,"['params', 'tparams']","[None, None]","[None, None]",58,"['    """"""\n', '    When we reload the model. Needed for the GPU stuff.\n', '    """"""\n']",['params.items'],1
utilmy/zarchive/storage/theano_lstm.py:unzip,unzip,function,8,11,10,97,8.82,1,0,['zipped'],[None],[None],66,"['    """"""\n', '    When we pickle the model. Needed for the GPU stuff.\n', '    """"""\n']","['OrderedDict', 'zipped.items', 'vv.get_value']",3
utilmy/zarchive/storage/theano_lstm.py:dropout_layer,dropout_layer,function,4,12,11,146,12.17,0,0,"['state_before', 'use_noise', 'trng']","[None, None, None]","[None, None, None]",76,[],"['tensor.switch', 'trng.binomial']",2
utilmy/zarchive/storage/theano_lstm.py:_p,_p,function,1,5,5,23,4.6,0,0,"['pp', 'name']","[None, None]","[None, None]",86,[],[],0
utilmy/zarchive/storage/theano_lstm.py:init_params,init_params,function,13,20,18,394,19.7,0,0,['options'],[None],[None],90,"['    """"""\n', '    Global (not LSTM) parameter. For the embeding and the classifier.\n', '    """"""\n']","['OrderedDict', 'get_layer', 'numpy.zeros']",3
utilmy/zarchive/storage/theano_lstm.py:load_params,load_params,function,11,25,22,128,5.12,1,1,"['path', 'params']","[None, None]","[None, None]",110,[],"['numpy.load', 'params.items', 'Warning']",3
utilmy/zarchive/storage/theano_lstm.py:init_tparams,init_tparams,function,8,12,11,107,8.92,1,0,['params'],[None],[None],120,[],"['OrderedDict', 'params.items', 'theano.shared']",3
utilmy/zarchive/storage/theano_lstm.py:get_layer,get_layer,function,3,4,3,26,6.5,0,0,['name'],[None],[None],127,[],[],0
utilmy/zarchive/storage/theano_lstm.py:ortho_weight,ortho_weight,function,8,9,9,87,9.67,0,0,['ndim'],[None],[None],132,[],['u.astype'],1
utilmy/zarchive/storage/theano_lstm.py:param_init_lstm,param_init_lstm,function,9,26,18,487,18.73,0,0,"['options', 'params', 'prefix']","[None, None, None]","[None, None, ""'lstm'""]",138,"['    """"""\n', '    Init the LSTM parameter:\n', '\n', '    :see: init_params\n', '    """"""\n']","['numpy.concatenate', 'ortho_weight', 'params[_p', 'numpy.zeros', 'b.astype']",5
utilmy/zarchive/storage/theano_lstm.py:lstm_layer,lstm_layer,function,41,115,79,988,8.59,0,2,"['tparams', 'state_below', 'options', 'prefix', 'mask']","[None, None, None, None, None]","[None, None, None, ""'lstm'"", 'None']",160,[],"['_slice', '_step', 'tensor.dot', 'tparams[_p', 'tensor.tanh', 'theano.scan', 'tensor.alloc', 'name=_p']",8
utilmy/zarchive/storage/theano_lstm.py:sgd,sgd,function,8,46,38,359,7.8,0,0,"['lr', 'tparams', 'grads', 'x', 'mask', 'y', 'cost']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",213,"['    """""" Stochastic Gradient Descent\n', '\n', '    :note: A more complicated version of sgd then needed.  This is\n', '        done like that for adadelta and rmsprop.\n', '\n', '    """"""\n']","['tparams.items', 'zip', 'theano.function']",3
utilmy/zarchive/storage/theano_lstm.py:adadelta,adadelta,function,12,112,70,907,8.1,0,0,"['lr', 'tparams', 'grads', 'x', 'mask', 'y', 'cost']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",241,"['    """"""\n', '    An adaptive learning rate optimizer\n', '\n', '    Parameters\n', '    ----------\n', '    lr : Theano SharedVariable\n', '        Initial learning rate\n', '    tpramas: Theano SharedVariable\n', '        Model parameters\n', '    grads: Theano variable\n', '        Gradients of cost w.r.t to parameres\n', '    x: Theano variable\n', '        Model inputs\n', '    mask: Theano variable\n', '        Sequence mask\n', '    y: Theano variable\n', '        Targets\n', '    cost: Theano variable\n', '        Objective fucntion to minimize\n', '\n', '    Notes\n', '    -----\n', '    For more information, see [ADADELTA]_.\n', '\n', '    .. [ADADELTA] Matthew D. Zeiler, *ADADELTA: An Adaptive Learning\n', '       Rate Method*, arXiv:1212.5701.\n', '    """"""\n']","['numpy_floatX', 'tparams.items', 'zip', 'theano.function', 'tensor.sqrt']",5
utilmy/zarchive/storage/theano_lstm.py:rmsprop,rmsprop,function,16,129,77,1028,7.97,1,0,"['lr', 'tparams', 'grads', 'x', 'mask', 'y', 'cost']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",302,"['    """"""\n', '    A variant of  SGD that scales the step size by running average of the\n', '    recent step norms.\n', '\n', '    Parameters\n', '    ----------\n', '    lr : Theano SharedVariable\n', '        Initial learning rate\n', '    tpramas: Theano SharedVariable\n', '        Model parameters\n', '    grads: Theano variable\n', '        Gradients of cost w.r.t to parameres\n', '    x: Theano variable\n', '        Model inputs\n', '    mask: Theano variable\n', '        Sequence mask\n', '    y: Theano variable\n', '        Targets\n', '    cost: Theano variable\n', '        Objective fucntion to minimize\n', '\n', '    Notes\n', '    -----\n', '    For more information, see [Hint2014]_.\n', '\n', '    .. [Hint2014] Geoff Hinton, *Neural Networks for Machine Learning*,\n', '       lecture 6a,\n', '       http://cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n', '    """"""\n']","['numpy_floatX', 'tparams.items', 'zip', 'theano.function', 'tensor.sqrt']",5
utilmy/zarchive/storage/theano_lstm.py:build_model,build_model,function,39,78,66,934,11.97,0,3,"['tparams', 'options']","[None, None]","[None, None]",367,[],"['RandomStreams', 'theano.shared', 'tensor.matrix', 'tensor.vector', 'get_layer', 'mask.sum', 'dropout_layer', 'theano.function', 'pred.argmax']",9
utilmy/zarchive/storage/theano_lstm.py:pred_probs,pred_probs,function,18,41,37,377,9.2,1,1,"['f_pred_prob', 'prepare_data', 'data', 'iterator', 'verbose']","[None, None, None, None, None]","[None, None, None, None, 'False']",406,"['    """""" If you want to use a trained model, this is useful to compute\n', '    the probabilities of new examples.\n', '    """"""\n']","['len', 'numpy.zeros', 'prepare_data', 'numpy.array', 'f_pred_prob', 'print']",6
utilmy/zarchive/storage/theano_lstm.py:pred_error,pred_error,function,15,32,27,303,9.47,1,0,"['f_pred', 'prepare_data', 'data', 'iterator', 'verbose']","[None, None, None, None, None]","[None, None, None, None, 'False']",429,"['    """"""\n', '    Just compute the error\n', '    f_pred: Theano fct computing the prediction\n', '    prepare_data: usual prepare_data for that dataset.\n', '    """"""\n']","['prepare_data', 'numpy.array', 'f_pred', 'numpy_floatX', 'len']",5
utilmy/zarchive/storage/theano_lstm.py:train_lstm,train_lstm,function,95,417,264,3684,8.83,3,16,"['dim_proj', '# word embeding dimension and LSTM number of hidden units.patience', '# Number of epoch to wait before early stop if no progressmax_epochs', '# The maximum number of epoch to rundispFreq', '# Display to stdout the training progress every N updatesdecay_c', '# Weight decay for the classifier applied to the U weights.not used for adadelta and rmsprop)n_words', '# Vocabulary sizeprobably need momentum and decaying learning rate).encoder', '# TODO', '# The best model will be saved therevalidFreq', '# Compute the validation error after this number of update.saveFreq', '# Save the parameters after every saveFreq updatesmaxlen', '# Sequence longer then this get ignoredbatch_size', '# The batch size during training.valid_batch_size', '# The batch size used for validation/test set.dataset', 'noise_std', 'use_dropout', '# if False slightly faster', 'but worst test errorreload_model', '# Path to a saved model we want to start from.test_size', '# If >0', 'we keep only this number of test example.']","[None, None, None, None, None, None, None, ' can be removed must be lstm.saveto', None, None, None, None, None, None, None, None, None, None, None, None, None]","['128', '10', '5000', '10', '0.', '10000', ""'lstm'"", ""'lstm_model.npz'"", '370', '1110', '100', '16', '64', ""'imdb'"", '0.', 'True', None, 'None', '-1', None, None]",448,[],"['locals', 'print', 'get_dataset', 'load_data', 'numpy.arange', 'numpy.max', 'init_params', 'load_params', 'init_tparams', 'build_model', 'theano.shared', 'theano.function', 'tensor.grad', 'wrt=list', 'tensor.scalar', 'optimizer', 'get_minibatches_idx', 'len', 'time.time', 'range', 'use_noise.set_value', 'prepare_data', 'f_grad_shared', 'f_update', 'numpy.isnan', 'numpy.isinf', 'numpy.mod', 'unzip', 'numpy.savez', 'pickle.dump', 'open', 'pred_error', 'history_errs.append', 'numpy.array', 'zipp']",35
utilmy/zarchive/storage/java.py:importJAR,importJAR,function,7,26,16,261,10.04,0,3,"['path1', 'path2', 'path3', 'path4']","[None, None, None, None]","['""""', '""""', '""""', '""""']",24,[],['jp.startJVM'],1
utilmy/zarchive/storage/java.py:listallfile,listallfile,function,20,36,30,353,9.81,2,1,"['some_dir', 'pattern', 'dirlevel']","[None, None, None]","[None, '""*.*""', '1']",36,[],"['some_dir.rstrip', 'some_dir.count', 'os.walk', 'root.count', 'fnmatch.filter', 'matches.append']",6
utilmy/zarchive/storage/java.py:importFolderJAR,importFolderJAR,function,7,18,17,182,10.11,1,0,"['dir1', 'dirlevel']","[None, None]","['""""', '1']",52,[],"['listallfile', 'jp.startJVM']",2
utilmy/zarchive/storage/java.py:importFromMaven,importFromMaven,function,1,2,2,7,3.5,0,0,[],[],[],62,[],[],0
utilmy/zarchive/storage/java.py:showLoadedClass,showLoadedClass,function,5,10,10,93,9.3,0,0,[],[],[],68,[],"['zipfile.ZipFile', 'archive.namelist']",2
utilmy/zarchive/storage/java.py:inspectJAR,inspectJAR,function,5,10,10,93,9.3,0,0,['dir1'],[None],[None],77,[],"['zipfile.ZipFile', 'archive.namelist']",2
utilmy/zarchive/storage/java.py:loadSingleton,loadSingleton,function,6,7,7,102,14.57,0,0,['class1'],[None],[None],86,[],"['os.getenv', 'subprocess.check_call']",2
utilmy/zarchive/storage/java.py:java_print,java_print,function,6,7,7,102,14.57,0,0,['x'],[None],[None],89,[],"['os.getenv', 'subprocess.check_call']",2
utilmy/zarchive/storage/java.py:compileJAVA,compileJAVA,function,6,7,7,102,14.57,0,0,['javafile'],[None],[None],92,[],"['os.getenv', 'subprocess.check_call']",2
utilmy/zarchive/storage/java.py:writeText,writeText,function,2,5,5,68,13.6,0,0,"['text', 'filename']","[None, None]","[None, None]",98,[],"['open', 'text_file.write', 'text_file.close']",3
utilmy/zarchive/storage/java.py:compileJAVAtext,compileJAVAtext,function,8,15,14,161,10.73,0,1,"['classname', 'javatxt', 'path1']","[None, None, None]","[None, None, '""""']",102,[],"['os.getcwd', 'text_file=open', 'text_file.write', 'text_file.close', 'compileJAVA']",5
utilmy/zarchive/storage/java.py:execute_javamain,execute_javamain,function,10,16,14,171,10.69,0,0,['java_file'],[None],[None],124,[],"['os.getenv', 'print']",2
utilmy/zarchive/storage/java.py:javaerror,javaerror,function,1,8,8,97,12.12,0,0,['jpJavaException'],[None],[None],136,[],"['print', 'jpJavaException.message']",2
utilmy/zarchive/storage/java.py:launchPDFbox,launchPDFbox,function,7,14,13,213,15.21,0,0,[],[],[],169,[],"['jp.startJVM', 'showLoadedClass']",2
utilmy/zarchive/storage/java.py:getfpdffulltext,getfpdffulltext,function,8,13,12,211,16.23,0,0,['pdfile1'],[None],[None],177,[],"['jp.JClass', 'pd.load', 'Text1', 'text.getText', 'document.close']",5
utilmy/zarchive/storage/java.py:launchTIKA,launchTIKA,function,3,9,8,116,12.89,0,0,[],[],[],195,[],"['importJAR', 'showLoadedClass']",2
utilmy/zarchive/storage/java.py:getfulltext,getfulltext,function,20,45,40,537,11.93,1,1,"['file1', 'withMeta']","[None, None]","[None, '0']",201,[],"['jp.JClass', 'parser.parse', 'handler.toString', 'metadata.names', 'metadata.get']",5
utilmy/zarchive/storage/java.py:directorygetalltext,directorygetalltext,function,22,84,47,574,6.83,2,2,"['dir1', 'filetype1', 'withMeta', 'fileout']","[None, None, None, None]","[None, '""*.*""', '0', '""""']",220,[],"['glob.glob', 'getfulltext', 'vv.append', 'pd.DataFrame', 'pd.HDFStore', 'st.append', 'directorygetalltext2']",7
utilmy/zarchive/storage/java.py:directorygetalltext2,directorygetalltext2,function,20,42,39,263,6.26,1,1,"['dir1', 'filetype1', 'type1', 'fileout']","[None, None, None, None]","[None, '""*.*""', '0', '""""']",238,[],"['glob.glob', 'getfulltext', 'pd.DataFrame', 'pd.HDFStore', 'st.append']",5
utilmy/zarchive/storage/stateprocessor.py:sort,sort,function,5,8,8,84,10.5,0,0,"['x', 'col', 'asc']","[None, None, None]","[None, None, None]",8,[],['pf.getret_fromquotes'],1
utilmy/zarchive/storage/stateprocessor.py:perf,perf,function,5,8,8,84,10.5,0,0,"['close', 't0', 't1']","[None, None, None]","[None, None, None]",9,[],['pf.getret_fromquotes'],1
utilmy/zarchive/storage/stateprocessor.py:and2,and2,function,5,8,8,84,10.5,0,0,['tuple1'],[None],[None],10,[],['pf.getret_fromquotes'],1
utilmy/zarchive/storage/stateprocessor.py:ff,ff,function,5,8,8,84,10.5,0,0,"['x', 'symfull']","[None, None]","[None, 'symfull']",12,[],['pf.getret_fromquotes'],1
utilmy/zarchive/storage/stateprocessor.py:gap,gap,function,5,8,8,84,10.5,0,0,"['close', 't0', 't1', 'lag']","[None, None, None, None]","[None, None, None, None]",15,[],['pf.getret_fromquotes'],1
utilmy/zarchive/storage/stateprocessor.py:process_stock,process_stock,function,15,41,30,261,6.37,2,2,"['stkstr', 'show1']","[None, None]","[None, '1']",20,[],"['stkstr.split', 'enumerate', 'x.strip', 'list', 'v.sort']",5
utilmy/zarchive/storage/stateprocessor.py:printn,printn,function,19,44,39,364,8.27,1,0,"['ss', 'symfull', 's1']","[None, None, None]","[None, 'symfull', 's1']",33,[],"['util.sortcol', 'range', 'len', 'int', 'aux2.append', 'round']",6
utilmy/zarchive/storage/stateprocessor.py:show,show,function,10,144,88,862,5.99,1,2,"['ll', 's1']","[None, None]","[None, 's1']",50,[],['type'],1
utilmy/zarchive/storage/stateprocessor.py:get_treeselect,get_treeselect,function,48,110,86,745,6.77,2,3,"['stk', 's1', 'xnewdata', 'newsample', 'show1', 'nbtree', 'depthtree']","[None, None, None, None, None, None, None]","[None, 's1', 'None', '5', '1', '5', '10']",82,[],"['process_stock', 'util.find', 'np.array', 'range', 'np.shape', 'np.row_stack', 'np.ones', 'np.concatenate', 'np.max', 'np.min', 'util.sk_tree', 'np.sum', 'clfrf.predict', 'print']",14
utilmy/zarchive/storage/stateprocessor.py:store_patternstate,store_patternstate,function,14,25,25,236,9.44,1,0,"['tree', 'sym1', 'theme', 'symfull']","[None, None, None, None]","[None, None, None, 'symfull']",111,[],"['util.find', 'lstate.append', 'np.array', 'str', 'util.save_obj']",5
utilmy/zarchive/storage/stateprocessor.py:load_patternstate,load_patternstate,function,7,10,7,70,7.0,0,0,['name1'],[None],[None],125,[],['util.load_obj'],1
utilmy/zarchive/storage/stateprocessor.py:get_stocklist,get_stocklist,function,19,40,35,338,8.45,1,2,"['clf', 's11', 'initial', 'show1']","[None, None, None, None]","[None, None, None, '1']",130,[],"['process_stock', 'clf.predict', 'enumerate', 'str', 'laux.append', 'list', 'aux2.sort']",7
utilmy/zarchive/storage/testmulti.py:mc01,mc01,function,0,0,0,0,0.0,0,0,[],[],[],18,"[""'''\n"", '\n', 'from __future__ import print_function\n', '\n', 'import multiprocessing\n', 'import ctypes\n', 'import numpy as np\n', '\n', 'def shared_array(shape):\n', '    """"""\n', '    Form a shared memory numpy array.\n', '    \n', '    http://stackoverflow.com/questions/5549190/is-shared-readonly-data-copied-to-different-processes-for-python-multiprocessing \n', '    """"""\n', '    \n', '    shared_array_base = multiprocessing.Array(ctypes.c_double, shape[0]*shape[1])\n', '    shared_array = np.ctypeslib.as_array(shared_array_base.get_obj())\n', '    shared_array = shared_array.reshape(*shape)\n', '    return shared_array\n', '\n', '\n', '# Form a shared array and a lock, to protect access to shared memory.\n', 'array = shared_array((1000, 1000))\n', 'lock = multiprocessing.Lock()\n', '\n', '\n', 'def parallel_function(i, def_param=(lock, array)):\n', '    """"""\n', '    Function that operates on shared memory.\n', '    """"""\n', '    \n', '    # Make sure your not modifying data when someone else is.\n', '    lock.acquire()    \n', '    \n', '    array[i, :] = i\n', '    \n', '    # Always release the lock!\n', '    lock.release()\n', '\n', ""if __name__ == '__main__':\n"", '    """"""\n', '    The processing pool needs to be instantiated in the main \n', '    thread of execution. \n', '    """"""\n', '        \n', '    pool = multiprocessing.Pool(processes=4)\n', '        \n', '    # Call the parallel function with different inputs.\n', '    args = [(0), \n', '            (1), \n', '            (2)]\n', '    \n', '    # Use map - blocks until all processes are done.\n', '    pool.map(parallel_function, args )\n', '    \n', '    print(array)\n', '    \n', '    \n', ""    '''\n""]",[],0
utilmy/zarchive/storage/testmulti.py:mc02,mc02,function,0,0,0,0,0.0,0,0,[],[],[],28,"[""'''\n"", '\n', 'from __future__ import print_function\n', '\n', 'import multiprocessing\n', 'import ctypes\n', 'import numpy as np\n', '\n', 'def shared_array(shape):\n', '    """"""\n', '    Form a shared memory numpy array.\n', '    \n', '    http://stackoverflow.com/questions/5549190/is-shared-readonly-data-copied-to-different-processes-for-python-multiprocessing \n', '    """"""\n', '    \n', '    shared_array_base = multiprocessing.Array(ctypes.c_double, shape[0]*shape[1])\n', '    shared_array = np.ctypeslib.as_array(shared_array_base.get_obj())\n', '    shared_array = shared_array.reshape(*shape)\n', '    return shared_array\n', '\n', '\n', '# Form a shared array and a lock, to protect access to shared memory.\n', 'array = shared_array((1000, 1000))\n', 'lock = multiprocessing.Lock()\n', '\n', '\n', 'def parallel_function(i, def_param=(lock, array)):\n', '    """"""\n', '    Function that operates on shared memory.\n', '    """"""\n', '    \n', '    # Make sure your not modifying data when someone else is.\n', '    lock.acquire()    \n', '    \n', '    array[i, :] = i\n', '    \n', '    # Always release the lock!\n', '    lock.release()\n', '\n', ""if __name__ == '__main__':\n"", '    """"""\n', '    The processing pool needs to be instantiated in the main \n', '    thread of execution. \n', '    """"""\n', '        \n', '    pool = multiprocessing.Pool(processes=4)\n', '        \n', '    # Call the parallel function with different inputs.\n', '    args = [(0), \n', '            (1), \n', '            (2)]\n', '    \n', '    # Use map - blocks until all processes are done.\n', '    pool.map(parallel_function, args )\n', '    \n', '    print(array)\n', '    \n', '    \n', ""    '''\n""]",[],0
utilmy/zarchive/storage/testmulti.py:serial,serial,function,1,8,8,54,6.75,0,0,"['samples', 'x', 'widths']","[None, None, None]","[None, None, None]",149,[],[],0
utilmy/zarchive/storage/testmulti.py:multiprocess,multiprocess,function,5,27,22,231,8.56,0,0,"['processes', 'samples', 'x', 'widths']","[None, None, None, None]","[None, None, None, None]",152,[],"['mp.Pool', 'pool.terminate', 'pool.join', 'print']",4
utilmy/zarchive/storage/testmulti.py:test01,test01,function,28,54,39,407,7.54,3,0,[],[],[],209,[],"['range', 'list', 'mp.Process']",3
utilmy/zarchive/storage/testmulti.py:random_tree,random_tree,function,3,3,3,42,14.0,0,0,['Data'],[None],[None],251,[],"['calculation', 'forest.append']",2
utilmy/zarchive/storage/testmulti.py:random_tree,random_tree,function,3,3,3,42,14.0,0,0,['Data'],[None],[None],251,[],"['calculation', 'forest.append']",2
utilmy/zarchive/storage/testmulti.py:test01,test01,function,28,54,39,407,7.54,3,0,[],[],[],209,[],"['range', 'list', 'mp.Process']",3
utilmy/zarchive/storage/derivatives.py:loadbrownian,loadbrownian,function,4,4,4,125,31.25,0,0,"['nbasset', 'step', 'nbsimul']","[None, None, None]","[None, None, None]",35,[],['np.load'],1
utilmy/zarchive/storage/derivatives.py:dN,dN,function,1,13,12,63,4.85,0,0,['d'],[None],[None],47,[],['np.sqrt'],1
utilmy/zarchive/storage/derivatives.py:dN2d,dN2d,function,1,13,12,63,4.85,0,0,"['x', 'y']","[None, None]","[None, None]",49,[],['np.sqrt'],1
utilmy/zarchive/storage/derivatives.py:N,N,function,1,13,12,63,4.85,0,0,['d'],[None],[None],54,[],['np.sqrt'],1
utilmy/zarchive/storage/derivatives.py:d1f,d1f,function,1,13,12,63,4.85,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",56,[],['np.sqrt'],1
utilmy/zarchive/storage/derivatives.py:d2f,d2f,function,1,13,12,63,4.85,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",59,[],['np.sqrt'],1
utilmy/zarchive/storage/derivatives.py:bsbinarycall,bsbinarycall,function,4,11,11,64,5.82,0,0,"['S0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",63,[],"['d2f', 'np.exp']",2
utilmy/zarchive/storage/derivatives.py:bscall,bscall,function,6,15,14,118,7.87,0,0,"['S0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",68,[],['d1f'],1
utilmy/zarchive/storage/derivatives.py:bsput,bsput,function,5,15,14,121,8.07,0,0,"['S0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",74,[],['d1f'],1
utilmy/zarchive/storage/derivatives.py:bs,bs,function,30,342,119,2051,6.0,0,2,"['S0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",80,[],"['d2f', 'np.exp', 'bscall', 'd1f', 'bsput', 'bs', 'bsdelta', 'N', 'bsgamma', 'np.sqrt', 'bsstrikedelta', 'bsstrikegamma', 'bstheta', 'bsrho', 'bsvega', 'dN', 'bsdvd', 'bsvanna', 'bsvolga', 'bsgammaspot']",20
utilmy/zarchive/storage/derivatives.py:bsdelta,bsdelta,function,4,19,19,102,5.37,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",86,[],"['d1f', 'np.exp', 'np.sqrt']",3
utilmy/zarchive/storage/derivatives.py:bsgamma,bsgamma,function,4,19,19,102,5.37,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",93,[],"['d1f', 'np.exp', 'np.sqrt']",3
utilmy/zarchive/storage/derivatives.py:bsstrikedelta,bsstrikedelta,function,4,19,19,102,5.37,0,0,"['s0', 'K', 't', 'T', 'r', 'd', 'vol', 'cp1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",98,[],"['d1f', 'np.exp', 'np.sqrt']",3
utilmy/zarchive/storage/derivatives.py:bsstrikegamma,bsstrikegamma,function,4,19,19,102,5.37,0,0,"['s0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",105,[],"['d1f', 'np.exp', 'np.sqrt']",3
utilmy/zarchive/storage/derivatives.py:bstheta,bstheta,function,3,20,19,104,5.2,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",110,[],"['d1f', 'np.sqrt', 'N']",3
utilmy/zarchive/storage/derivatives.py:bsrho,bsrho,function,4,16,15,72,4.5,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",115,[],"['d2f', 'np.exp', 'N']",3
utilmy/zarchive/storage/derivatives.py:bsvega,bsvega,function,4,13,13,75,5.77,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",120,[],"['d1f', 'np.sqrt', 'dN']",3
utilmy/zarchive/storage/derivatives.py:bsdvd,bsdvd,function,3,22,20,97,4.41,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",125,[],"['d1f', 'np.sqrt', 'np.exp', 'N']",4
utilmy/zarchive/storage/derivatives.py:bsvanna,bsvanna,function,3,16,15,85,5.31,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",130,[],"['d1f', 'np.sqrt', 'dN']",3
utilmy/zarchive/storage/derivatives.py:bsvolga,bsvolga,function,4,18,16,103,5.72,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",135,[],"['d1f', 'np.sqrt', 'dN']",3
utilmy/zarchive/storage/derivatives.py:bsgammaspot,bsgammaspot,function,4,18,18,124,6.89,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",140,[],"['d1f', 'np.exp']",2
utilmy/zarchive/storage/derivatives.py:gdelta,gdelta,function,1,15,10,63,4.2,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'pv']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",149,[],['pv'],1
utilmy/zarchive/storage/derivatives.py:ggamma,ggamma,function,1,21,11,88,4.19,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'pv']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",152,[],"['pv', '2*pv']",2
utilmy/zarchive/storage/derivatives.py:gvega,gvega,function,1,15,11,58,3.87,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'pv']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",155,[],['pv'],1
utilmy/zarchive/storage/derivatives.py:gtheta,gtheta,function,1,15,11,66,4.4,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'pv']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",158,[],['pv'],1
utilmy/zarchive/storage/derivatives.py:genmatrix,genmatrix,function,9,14,12,83,5.93,2,0,"['ni', 'nj', 'gg']","[None, None, None]","[None, None, None]",166,[],"['np.zeros', 'range', 'gg']",3
utilmy/zarchive/storage/derivatives.py:gensymmatrix,gensymmatrix,function,9,19,15,89,4.68,2,1,"['ni', 'nj', 'pp']","[None, None, None]","[None, None, None]",175,[],"['np.zeros', 'range']",2
utilmy/zarchive/storage/derivatives.py:timegrid,timegrid,function,6,13,13,125,9.62,1,0,"['timestep', 'maturityyears']","[None, None]","[None, None]",191,[],"['np.int', 'np.zeros', 'range']",3
utilmy/zarchive/storage/derivatives.py:generateall_multigbm1,generateall_multigbm1,function,42,138,118,1319,9.56,1,3,"['process', 'ww', 's0', 'mu', 'vol', 'corrmatrix', 'timegrid', 'nbsimul', 'nproc', 'type1', 'strike', 'cp']","[None, None, None, None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None, '-1', '-1', '0.0', '1']",207,[],"['len', 'np.asmatrix', 'np.tile', 'np.multiply', 'np.zeros', 'range', 'vol*sqrt', 'mpf.multigbm_paralell_func', 'int', 'mp.Pool', 'pool.map', 'sum', 'float']",13
utilmy/zarchive/storage/derivatives.py:logret_to_ret,logret_to_ret,function,19,29,25,164,5.66,1,0,['log_returns'],[None],[None],315,[],"['len', 'np.exp', 'np.zeros', 'range', 'np.array']",5
utilmy/zarchive/storage/derivatives.py:logret_to_price,logret_to_price,function,19,29,25,164,5.66,1,0,"['s0', 'log_ret']","[None, None]","[None, None]",317,[],"['len', 'np.exp', 'np.zeros', 'range', 'np.array']",5
utilmy/zarchive/storage/derivatives.py:brownian_logret,brownian_logret,function,7,14,14,100,7.14,0,0,"['mu', 'vol', 'timegrid']","[None, None, None]","[None, None, None]",327,[],"['len', 'np.diff', 'nx.evaluate', 'sqrt']",4
utilmy/zarchive/storage/derivatives.py:brownian_process,brownian_process,function,2,4,4,55,13.75,0,0,"['s0', 'vol', 'timegrid']","[None, None, None]","[None, None, None]",334,[],"['logret_to_price', 'brownian_logret']",2
utilmy/zarchive/storage/derivatives.py:gbm_logret,gbm_logret,function,8,26,21,154,5.92,0,0,"['mu', 'vol', 'timegrid']","[None, None, None]","[None, None, None]",338,[],"['len', 'np.diff', 'nx.evaluate', 'sqrt']",4
utilmy/zarchive/storage/derivatives.py:gbm_process,gbm_process,function,28,95,60,637,6.71,1,0,"['s0', 'mu', 'vol', 'timegrid']","[None, None, None, None]","[None, None, None, None]",355,[],"['len', 'np.diff', 'nx.evaluate', 'sqrt', 'np.zeros', 'range', 'np.array', 'gbm_process_euro', 'np.cumprod', 'gbm_process2', 'logret_to_price', 'gbm_logret']",12
utilmy/zarchive/storage/derivatives.py:gbm_process_euro,gbm_process_euro,function,2,5,5,53,10.6,0,0,"['s0', 'mu', 'vol', 'timegrid']","[None, None, None, None]","[None, None, None, None]",369,[],"['logret_to_price', 'gbm_logret']",2
utilmy/zarchive/storage/derivatives.py:gbm_process2,gbm_process2,function,2,5,5,53,10.6,0,0,"['s0', 'mu', 'vol', 'timegrid']","[None, None, None, None]","[None, None, None, None]",384,[],"['logret_to_price', 'gbm_logret']",2
utilmy/zarchive/storage/derivatives.py:generateallprocess,generateallprocess,function,22,64,57,493,7.7,1,0,"['process', 'params01', 'timegrid1', 'nbsimul']","[None, None, None, None]","[None, None, None, None]",390,[],"['np.zeros', 'len', 'range', 'process', 'generateallprocess_gbmeuro', 's0*exp', 'vol*sqrt', 'nx.evaluate']",8
utilmy/zarchive/storage/derivatives.py:generateallprocess_gbmeuro,generateallprocess_gbmeuro,function,32,63,48,436,6.92,1,1,"['process', 'params01', 'timegrid1', 'nbsimul']","[None, None, None, None]","[None, None, None, None]",399,[],"['np.shape', 'np.zeros', 'payoff', 'range', 'np.mean', 'np.std']",6
utilmy/zarchive/storage/derivatives.py:getpv,getpv,function,32,63,48,436,6.92,1,1,"['discount', 'payoff', 'allpriceprocess']","[None, None, None]","[None, None, None]",413,[],"['np.shape', 'np.zeros', 'payoff', 'range', 'np.mean', 'np.std']",6
utilmy/zarchive/storage/derivatives.py:multigbm_processfast,multigbm_processfast,function,15,50,44,386,7.72,1,0,"['s0', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'kk']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",448,[],"['np.asmatrix', 'np.dot', 'np.multiply', 'np.exp', 'range']",5
utilmy/zarchive/storage/derivatives.py:getbrowniandata,getbrowniandata,function,3,4,4,43,10.75,0,0,"['nbasset', 'step', 'simulk']","[None, None, None]","[None, None, None]",468,[],[],0
utilmy/zarchive/storage/derivatives.py:multigbm_processfast2,multigbm_processfast2,function,15,51,45,382,7.49,1,0,"['s0', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'kk']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",471,[],"['np.asmatrix', 'getbrowniandata', 'np.dot', 'np.multiply', 'np.exp', 'range']",6
utilmy/zarchive/storage/derivatives.py:generateallmultigbmfast,generateallmultigbmfast,function,24,109,87,917,8.41,3,1,"['process', 's0', 'mu', 'vol', 'corrmatrix', 'timegrid', 'nbsimul', 'type1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",494,[],"['len', 'np.zeros', 'np.asmatrix', 'np.dot', 'np.multiply', 'range', 'vol*sqrt', 'multigbm_processfast', 'multigbm_processfast2']",9
utilmy/zarchive/storage/derivatives.py:multigbm_processfast3,multigbm_processfast3,function,18,53,47,375,7.08,1,0,"['s0', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'kk']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",525,[],"['np.dot', 'np.multiply', 'np.exp', 'np.zeros', 'range']",5
utilmy/zarchive/storage/derivatives.py:generateallmultigbmfast2,generateallmultigbmfast2,function,32,109,86,833,7.64,3,1,"['process', 's0', 'mu', 'vol', 'corrmatrix', 'timegrid', 'nbsimul', 'type1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",557,[],"['len', 'np.zeros', 'np.asmatrix', 'np.diff', 'np.dot', 'np.multiply', 'range', 'np.sqrt', 'vol*sqrt', 'multigbm_processfast', 'multigbm_processfast2']",11
utilmy/zarchive/storage/derivatives.py:multibrownian_logret,multibrownian_logret,function,19,63,59,609,9.67,1,0,"['mu', 'vol', 'corrmatrix', 'timegrid']","[None, None, None, None]","[None, None, None, None]",652,[],"['np.asmatrix', 'np.shape', 'len', 'np.zeros', 'range', 'np.multiply', 'vol*sqrt', 'np.dot']",8
utilmy/zarchive/storage/derivatives.py:multigbm_logret,multigbm_logret,function,12,42,38,413,9.83,0,0,"['mu', 'vol', 'corrmatrix', 'timegrid']","[None, None, None, None]","[None, None, None, None]",673,[],"['np.asmatrix', 'np.shape', 'np.dot', 'np.array', 'np.multiply']",5
utilmy/zarchive/storage/derivatives.py:multilogret_to_price,multilogret_to_price,function,13,36,32,260,7.22,1,0,"['s0', 'log_ret']","[None, None]","[None, None]",688,[],"['np.shape', 'np.exp', 'np.asmatrix', 'range', 'np.multiply', 'np.array']",6
utilmy/zarchive/storage/derivatives.py:multigbm_process,multigbm_process,function,15,50,44,386,7.72,1,0,"['s0', 'voldt', 'drift', 'upper_cholesky', 'nbasset', 'n', 'kk']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",698,"['    """""":param param: the model parameters object :return: jump sizes for each point in time (mostly zeroes if jumps are infrequent) """"""\n']","['np.asmatrix', 'np.dot', 'np.multiply', 'np.exp', 'range']",5
utilmy/zarchive/storage/derivatives.py:generateallmultiprocess,generateallmultiprocess,function,9,31,28,221,7.13,1,0,"['process', 's0', 'mu', 'vol', 'corrmatrix', 'timegrid', 'nbsimul']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",705,[],"['len', 'np.zeros', 'range', 'multigbm_process']",4
utilmy/zarchive/storage/derivatives.py:jump_process,jump_process,function,15,40,37,309,7.72,2,0,"['lamda', 'jumps_mu', 'jumps_vol', 'timegrid']","[None, None, None, None]","[None, None, None, None]",722,"['    """""":param param: the model parameters object :return: jump sizes for each point in time (mostly zeroes if jumps are infrequent) """"""\n']","['np.diff', 'len', 'np.zeros', 'range', 'Sum', 'Nt']",6
utilmy/zarchive/storage/derivatives.py:gbmjump_logret,gbmjump_logret,function,6,11,10,169,15.36,0,0,"['s0', 'mu', 'vol', 'lamda', 'jump_mu', 'jump_vol', 'timegrid']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",739,"['    """"""returns a GBM process with jumps in it""""""\n']","['jump_process', 'gbm_logret', 'np.add']",3
utilmy/zarchive/storage/derivatives.py:gbmjump_process,gbmjump_process,function,8,18,18,95,5.28,1,0,"['s0', 'mu', 'vol', 'lamda', 'jump_mu', 'jump_vol', 'timegrid']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",746,[],"['len', 'range']",2
utilmy/zarchive/storage/derivatives.py:lgnormalmoment1,lgnormalmoment1,function,8,18,18,95,5.28,1,0,"['ww', 'fft', 'vol', 'corr', 'tt']","[None, None, None, None, None]","[None, None, None, None, None]",759,[],"['len', 'range']",2
utilmy/zarchive/storage/derivatives.py:lgnormalmoment2,lgnormalmoment2,function,8,18,18,160,8.89,1,0,"['ww', 'fft', 'vol', 'corr', 'tt']","[None, None, None, None, None]","[None, None, None, None, None]",767,[],"['len', 'np.ndindex']",2
utilmy/zarchive/storage/derivatives.py:lgnormalmoment3,lgnormalmoment3,function,9,22,22,232,10.55,1,0,"['ww', 'fft', 'vol', 'corr', 'tt']","[None, None, None, None, None]","[None, None, None, None, None]",774,[],"['len', 'np.ndindex', 'exp']",3
utilmy/zarchive/storage/derivatives.py:lgnormalmoment4,lgnormalmoment4,function,16,32,27,340,10.62,1,0,"['ww', 'fft', 'vol', 'corr', 'tt']","[None, None, None, None, None]","[None, None, None, None, None]",782,[],"['len', 'np.ndindex', 'exp']",3
utilmy/zarchive/storage/derivatives.py:solve_momentmatch3,solve_momentmatch3,function,27,57,48,443,7.77,0,0,"['ww', 'b0', 'fft', 'vol', 'corr', 'tt']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",798,[],"['lgnormalmoment1', 'lgnormalmoment2', 'lgnormalmoment3', 'sy.symbols', 'sy.solve', 'exp', 'sy.sqrt', 'sy.log', 'np.array']",9
utilmy/zarchive/storage/derivatives.py:savebrownian,savebrownian,function,1,8,8,58,7.25,0,0,"['nbasset', 'step', 'nbsimul']","[None, None, None]","[None, None, None]",30,[],[],0
utilmy/zarchive/storage/derivatives.py:plot_greeks,plot_greeks,function,39,78,68,528,6.77,2,0,"['function', 'greek']","[None, None]","[None, None]",1036,[],"['np.linspace', 'np.zeros', 'len', 'range', 'function', 'np.meshgrid', 'plt.figure', 'p3.Axes3D', 'plot.plot_wireframe', 'plot.set_xlabel', 'plot.set_ylabel', 'plot.set_zlabel']",12
utilmy/zarchive/storage/derivatives.py:plot_greeks,plot_greeks,function,39,78,68,528,6.77,2,0,"['function', 'greek']","[None, None]","[None, None]",1036,[],"['np.linspace', 'np.zeros', 'len', 'range', 'function', 'np.meshgrid', 'plt.figure', 'p3.Axes3D', 'plot.plot_wireframe', 'plot.set_xlabel', 'plot.set_ylabel', 'plot.set_zlabel']",12
utilmy/zarchive/storage/derivatives.py:plot_values,plot_values,function,32,125,76,888,7.1,0,0,['function'],[None],[None],1097,[],"['plt.figure', 'plt.subplot', 'np.linspace', 'plt.plot', 'plt.grid', 'plt.xlabel', 'plt.ylabel', 'plt.axis', 'plt.tight_layout']",9
utilmy/zarchive/storage/derivatives.py:CRR_option_value,CRR_option_value,function,32,121,84,518,4.28,1,1,"['S0', 'K', 'T', 'r', 'vol', 'otype', 'M']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, '4']",1153,"[""    ''' Cox-Ross-Rubinstein European option valuation.\n"", ""    otype : string  either 'call' or 'put'\n"", '    M : int  number of time intervals\n', ""    '''\n""]","['np.exp', 'np.sqrt', 'np.arange', 'np.resize', 'np.transpose', 'np.maximum', 'range']",7
utilmy/zarchive/storage/codeanalysis.py:wi,wi,function,6,11,9,88,8.0,0,0,['*args'],[None],[None],13,[],"['str', 'dx.replace']",2
utilmy/zarchive/storage/codeanalysis.py:printinfile,printinfile,function,5,12,10,53,4.42,1,1,"['vv', 'file2']","[None, None]","[None, None]",24,[],[],0
utilmy/zarchive/storage/codeanalysis.py:wi2,wi2,function,5,12,10,53,4.42,1,1,['*args'],[None],[None],28,[],[],0
utilmy/zarchive/storage/codeanalysis.py:indent,indent,function,17,43,34,344,8.0,0,4,[],[],[],33,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find']",4
utilmy/zarchive/storage/codeanalysis.py:dedent,dedent,function,17,43,34,344,8.0,0,4,[],[],[],34,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find']",4
utilmy/zarchive/storage/codeanalysis.py:describe_builtin,describe_builtin,function,17,43,34,344,8.0,0,4,['obj'],[None],[None],38,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find']",4
utilmy/zarchive/storage/codeanalysis.py:describe_func,describe_func,function,18,59,46,443,7.51,0,5,"['obj', 'method']","[None, None]","[None, 'False']",58,[],"['inspect.getargspec', 'wi', 'str', 'len']",4
utilmy/zarchive/storage/codeanalysis.py:describe_klass,describe_klass,function,12,33,30,238,7.21,1,2,['obj'],[None],[None],81,[],"['wi', 'indent', 'getattr', 'inspect.ismethod', '1;describe_func', 'dedent']",6
utilmy/zarchive/storage/codeanalysis.py:describe,describe,function,61,353,164,2895,8.2,4,21,['obj'],[None],[None],97,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find', 'describe_func', 'inspect.getargspec', 'str', 'len', 'describe_klass', 'indent', 'getattr', 'inspect.ismethod', '1;describe_func', 'dedent', 'describe', 'dir', 'inspect.isclass', 'inspect.isfunction', 'inspect.isbuiltin', 'describe_builtin', 'describe_builtin2', 'describe_func2', 'describe_func3', 'aux.replace', 'aux.rstrip', 'describe_klass2', 'describe2']",27
utilmy/zarchive/storage/codeanalysis.py:describe_builtin2,describe_builtin2,function,16,33,26,272,8.24,0,3,"['obj', 'name1']","[None, None]","[None, None]",118,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find']",4
utilmy/zarchive/storage/codeanalysis.py:describe_func2,describe_func2,function,9,21,21,143,6.81,0,1,"['obj', 'method', 'name1']","[None, None, None]","[None, 'False', ""''""]",135,[],"['inspect.getargspec', 'wi', 'str']",3
utilmy/zarchive/storage/codeanalysis.py:describe_func3,describe_func3,function,14,34,31,236,6.94,0,1,"['obj', 'method', 'name1']","[None, None, None]","[None, 'False', ""''""]",145,[],"['inspect.getargspec', 'str', 'aux.replace', 'aux.rstrip', 'wi']",5
utilmy/zarchive/storage/codeanalysis.py:describe_klass2,describe_klass2,function,8,18,18,151,8.39,1,1,"['obj', 'name1']","[None, None]","[None, ""''""]",158,[],"['getattr', 'inspect.ismethod', 'describe_func2']",3
utilmy/zarchive/storage/codeanalysis.py:describe2,describe2,function,13,43,33,377,8.77,1,2,"['module', 'type1']","[None, None]","[None, '0']",168,[],"['wi', 'dir', 'getattr', 'inspect.isclass', 'describe_klass2', 'inspect.isfunction', 'describe_func2', 'describe_func3', 'inspect.isbuiltin', 'describe_builtin2']",10
utilmy/zarchive/storage/codeanalysis.py:getmodule_doc,getmodule_doc,function,32,102,69,872,8.55,5,1,"['module1', 'file2']","[None, None]","[None, ""''""]",192,[],"['importlib.import_module', 'pkgutil.walk_packages', 'vv.append', 'wi', 'describe2', 'sys.exc_info', 'describe', 'print']",8
utilmy/zarchive/storage/rec_metrics.py:predict,predict,function,3,5,4,131,26.2,0,0,"['model', 'uid', 'pids']","[None, None, None]","[None, None, None]",6,[],[],0
utilmy/zarchive/storage/rec_metrics.py:precision_at_k,precision_at_k,function,21,39,38,538,13.79,1,1,"['model', 'ground_truth', 'k', 'user_features', 'item_features']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",14,"['    """"""\n', '    Measure precision at k for model and ground truth.\n', '\n', '    Arguments:\n', '    - lightFM instance model\n', '    - sparse matrix ground_truth (no_users, no_items)\n', '    - int k\n', '\n', '    Returns:\n', '    - float precision@k\n', '    """"""\n']","['ground_truth.tocsr', 'np.arange', 'enumerate', 'np.empty', 'uid_array.fill', 'model.predict', 'set', 'precisions.append', 'float', 'sum', 'len']",11
utilmy/zarchive/storage/rec_metrics.py:full_auc,full_auc,function,21,34,32,398,11.71,1,1,"['model', 'ground_truth']","[None, None]","[None, None]",52,"['    """"""\n', '    Measure AUC for model and ground truth on all items.\n', '\n', '    Returns:\n', '    - float AUC\n', '    """"""\n']","['ground_truth.tocsr', 'np.arange', 'enumerate', 'predict', 'np.zeros', 'len', 'scores.append', 'sum']",8
utilmy/zarchive/storage/portfolio.py:data_jpsector,data_jpsector,function,3,4,4,47,11.75,0,0,[],[],[],49,[],['util.load_obj'],1
utilmy/zarchive/storage/portfolio.py:date_earningquater,date_earningquater,function,9,103,42,604,5.86,0,5,['t1'],[None],[None],62,[],['datetime.datetime'],1
utilmy/zarchive/storage/portfolio.py:date_is_3rdfriday,date_is_3rdfriday,function,4,14,13,67,4.79,0,0,['s'],[None],[None],81,[],"['datetime.strptime', 'd.weekday']",2
utilmy/zarchive/storage/portfolio.py:date_option_expiry,date_option_expiry,function,10,52,33,360,6.92,0,2,['date'],[None],[None],86,[],['datetime.datetime'],1
utilmy/zarchive/storage/portfolio.py:date_find_kday_fromintradaydate,date_find_kday_fromintradaydate,function,2,3,3,68,22.67,0,0,"['kintraday', 'intradaydate', 'dailydate']","[None, None, None]","[None, None, None]",103,[],['util.np_find'],1
utilmy/zarchive/storage/portfolio.py:date_find_kintraday_fromdate,date_find_kintraday_fromdate,function,4,10,10,88,8.8,0,0,"['d1', 'intradaydate1', 'h1', 'm1']","[None, None, None, None]","[None, None, '9', '30']",107,[],"['datetime.datetime', 'util.np_find']",2
utilmy/zarchive/storage/portfolio.py:date_find_intradateid,date_find_intradateid,function,8,13,13,101,7.77,1,0,"['datetimelist', 'stringdate']","[None, None]","[None, ""['20160420223000']""]",114,[],"['util.datestring_todatetime', 'util.np_find', 'str']",3
utilmy/zarchive/storage/portfolio.py:datetime_convertzone1_tozone2,datetime_convertzone1_tozone2,function,16,29,25,343,11.83,1,1,"['tt', 'fromzone', 'tozone']","[None, None, None]","[None, ""'Japan'"", ""'US/Eastern'""]",125,[],"['pytz.timezone', 'type', 'tz.localize', 't2.append']",4
utilmy/zarchive/storage/portfolio.py:date_extract_dailyopenclosetime,date_extract_dailyopenclosetime,function,23,51,35,431,8.45,2,3,"['spdateref1', 'market']","[None, None]","[None, ""'us'""]",143,[],"['enumerate', 'spdailyopendate.append', 'np.array', 'spdailyclosedate.append']",4
utilmy/zarchive/storage/portfolio.py:datetime_tostring,datetime_tostring,function,8,26,21,244,9.38,1,2,['datelist1'],[None],[None],161,[],"['isinstance', 'datelist1.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zarchive/storage/portfolio.py:datestring_todatetime,datestring_todatetime,function,6,15,13,125,8.33,1,1,"['datelist1', 'format1']","[None, None]","[None, ' ""%Y%m%d""']",172,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/storage/portfolio.py:datetime_todate,datetime_todate,function,6,20,18,160,8.0,1,1,['tt'],[None],[None],181,[],"['isinstance', 'datetime.date', 'date2.append']",3
utilmy/zarchive/storage/portfolio.py:datetime_toint,datetime_toint,function,12,37,28,424,11.46,2,2,['datelist1'],[None],[None],190,[],"['isinstance', 'int', 'date2.append', 'datetime_tointhour', 'yy2.append', 'np.array']",6
utilmy/zarchive/storage/portfolio.py:datetime_tointhour,datetime_tointhour,function,7,20,19,234,11.7,1,1,['datelist1'],[None],[None],198,[],"['isinstance', 'int', 'yy2.append', 'np.array']",4
utilmy/zarchive/storage/portfolio.py:dateint_tostring,dateint_tostring,function,7,15,14,183,12.2,1,1,"['datelist1', 'format1']","[None, None]","[None, ""'%b-%y'""]",206,[],"['isinstance', 'dateint_todatetime', 'date2.append', 'np.array']",4
utilmy/zarchive/storage/portfolio.py:dateint_todatetime,dateint_todatetime,function,6,14,12,126,9.0,1,1,['datelist1'],[None],[None],228,[],"['isint', 'parser.parse', 'date2.append']",3
utilmy/zarchive/storage/portfolio.py:datenumpy_todatetime,datenumpy_todatetime,function,9,48,26,381,7.94,0,3,"['tt', 'islocaltime']","[None, None]","[None, 'True']",237,[],['type'],1
utilmy/zarchive/storage/portfolio.py:datetime_tonumpypdate,datetime_tonumpypdate,function,2,2,2,22,11.0,0,0,"['t', 'islocaltime']","[None, None]","[None, 'True']",251,[],['np.datetime64'],1
utilmy/zarchive/storage/portfolio.py:date_diffindays,date_diffindays,function,4,5,5,74,14.8,0,0,"['intdate1', 'intdate2']","[None, None]","[None, None]",257,[],['dateint_todatetime'],1
utilmy/zarchive/storage/portfolio.py:date_finddateid,date_finddateid,function,5,75,20,492,6.56,0,10,"['date1', 'dateref']","[None, None]","[None, None]",262,[],['util.np_findfirst'],1
utilmy/zarchive/storage/portfolio.py:datestring_toint,datestring_toint,function,6,16,14,106,6.62,1,1,['datelist1'],[None],[None],276,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/storage/portfolio.py:date_as_float,date_as_float,function,12,32,26,311,9.72,0,2,['dt'],[None],[None],283,[],"['isint', 'dateint_todatetime', 'datetime.datetime', 'isleap', 'timedelta']",5
utilmy/zarchive/storage/portfolio.py:datediff_inyear,datediff_inyear,function,2,3,3,53,17.67,0,0,"['startdate', 'enddate']","[None, None]","[None, None]",296,[],['date_as_float'],1
utilmy/zarchive/storage/portfolio.py:date_generatedatetime,date_generatedatetime,function,18,27,27,267,9.89,0,1,"['start', 'nbday', 'end']","[None, None, None]","['""20100101""', '10', '""""']",300,[],"['datestring_todatetime', 'date_add_bdays', 'datetime.timedelta', 'list', 'np.array']",5
utilmy/zarchive/storage/portfolio.py:date_add_bdays,date_add_bdays,function,12,37,30,369,9.97,1,2,"['from_date', 'add_days']","[None, None]","[None, None]",311,[],"['type', 'dateint_todatetime', 'datestring_todatetime', 'datetime.timedelta', 'current_date.weekday']",5
utilmy/zarchive/storage/portfolio.py:date_getspecificdate,date_getspecificdate,function,21,203,58,1312,6.46,8,19,"['datelist', 'datetype1', 'outputype1', 'includelastdate', 'includefirstdate', '']","[None, None, None, None, None, None]","[None, '""yearend""', '""intdate""', 'True', 'False', None]",325,[],"['isint', 'dateint_todatetime', 'enumerate', 'vec2.append', 'np.mod', 'np.array']",6
utilmy/zarchive/storage/portfolio.py:date_alignfromdateref,date_alignfromdateref,function,9,18,14,152,8.44,0,0,"['array1', 'dateref']","[None, None]","[None, None]",386,[],['np.float'],1
utilmy/zarchive/storage/portfolio.py:_date_align,_date_align,function,15,35,27,203,5.8,2,2,"['dateref', 'datei', 'tmax', 'closei']","[None, None, None, None]","[None, None, None, None]",411,[],"['np.zeros', 'range', 'len']",3
utilmy/zarchive/storage/portfolio.py:date_align,date_align,function,9,18,14,152,8.44,0,0,"['array1', 'dateref']","[None, None]","[None, None]",429,"["" ''' #Aligne the price with the same dates\n"", "" date\tyear\tmonth\tday\td\topen\tclose\thigh\tlow\tvolume\taclose '''\n""]",['np.float'],1
utilmy/zarchive/storage/portfolio.py:min_withposition,min_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],478,[],['min'],1
utilmy/zarchive/storage/portfolio.py:max_withposition,max_withposition,function,4,7,5,95,13.57,0,0,['values'],[None],[None],482,[],['max'],1
utilmy/zarchive/storage/portfolio.py:_reshape,_reshape,function,2,14,12,68,4.86,0,1,['x'],[None],[None],488,[],"['len', 'np.reshape', 'np.size']",3
utilmy/zarchive/storage/portfolio.py:_notnone,_notnone,function,39,213,113,1775,8.33,1,11,['x'],[None],[None],492,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend']",23
utilmy/zarchive/storage/portfolio.py:plot_price,plot_price,function,86,347,207,3422,9.86,3,15,"['asset', 'y2', 'y3', 'y4', 'y5', 'sym', 'savename1', 'tickperday', 'date1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', 'None', ""''"", '20', 'None', '(10', None]",496,[],"['_reshape', '_notnone', 'np.concatenate', 'np.shape', 'np.arange', 'np.zeros', 'ta_lowbandtrend1', 'ta_highbandtrend1', 'plt.figure', 'plt.plot', 'plt.grid', 'plt.title', 'plt.xlabel', 'plt.ylabel', 'int', 'plt.xticks', 'len', 'plt.savefig', 'plt.show', 'range', 'min', 'fig.savefig', 'plt.legend', 'plot_priceintraday', 'np.loadtxt', 'np.unique', 'xdays.append', 'np.hstack', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick', 'plot_pricedate', 'type', 'datestring_todatetime', 'np.row_stack', 'array_todataframe', 'df_asset.plot', 'ax.axvline', 'ax.savefig']",43
utilmy/zarchive/storage/portfolio.py:plot_priceintraday,plot_priceintraday,function,32,59,57,947,16.05,1,0,['data'],[None],[None],575,[],"['np.loadtxt', 'np.unique', 'np.arange', 'xdays.append', 'np.hstack', 'plt.figure', 'fig.add_axes', 'ax.tick_params', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_ylabel', 'ax.set_ylim', 'candlestick', 'plt.show']",14
utilmy/zarchive/storage/portfolio.py:plot_pricedate,plot_pricedate,function,19,63,51,550,8.73,1,4,"['date1', 'sym1', 'asset1', 'sym2', 'bsk1', 'verticaldate', 'savename1', 'graphsize', '5']","[None, None, None, None, None, None, None, None, None]","[None, None, None, 'None', 'None', 'None', ""''"", '(10', None]",618,[],"['type', 'datestring_todatetime', 'np.row_stack', 'np.concatenate', 'array_todataframe', 'df_asset.plot', 'ax.axvline', 'len', 'np.arange', 'int', 'ax.savefig']",11
utilmy/zarchive/storage/portfolio.py:generate_sepvertical,generate_sepvertical,function,9,13,12,159,12.23,0,1,"['asset1', 'tt', 'tmax', 'start', 'datebar']","[None, None, None, None, None]","[None, None, None, 'None', 'None']",701,[],"['np.zeros', 'util.datestring_todatetime']",2
utilmy/zarchive/storage/portfolio.py:save_asset_tofile,save_asset_tofile,function,31,129,87,978,7.58,5,9,"['file1', 'asset1', 'asset2', 'asset3', 'date1', 'title1']","[None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None']",716,[],"['np.shape', 'len', 'np.reshape', 'print', 'np.zeros', 'range', 'int', 'np.row_stack', 'np.savetxt']",9
utilmy/zarchive/storage/portfolio.py:load_asset_fromfile,load_asset_fromfile,function,1,4,4,51,12.75,0,0,['file1'],[None],[None],755,[],[],0
utilmy/zarchive/storage/portfolio.py:array_todataframe,array_todataframe,function,11,41,27,259,6.32,0,2,"['price', 'symbols', 'date1']","[None, None, None]","[None, 'None', 'None']",759,[],"['np.shape', 'len', 'pd.DataFrame']",3
utilmy/zarchive/storage/portfolio.py:dataframe_toarray,dataframe_toarray,function,9,10,10,107,10.7,0,0,['df'],[None],[None],770,[],[],0
utilmy/zarchive/storage/portfolio.py:isfloat,isfloat,function,3,17,13,80,4.71,0,1,['value'],[None],[None],779,[],['float'],1
utilmy/zarchive/storage/portfolio.py:isint,isint,function,29,98,56,994,10.14,0,8,['x'],[None],[None],787,[],"['matx.copy', 'x.mean', 'x.std', 'np.corrcoef', 'correl_inv.copy', 'np.sqrt']",6
utilmy/zarchive/storage/portfolio.py:correlation_mat,correlation_mat,function,29,98,56,994,10.14,0,8,"['matx', 'type1', 'type2']","[None, None, None]","[None, '""robust""', '""correl""']",794,[],"['matx.copy', 'x.mean', 'x.std', 'np.corrcoef', 'correl_inv.copy', 'np.sqrt']",6
utilmy/zarchive/storage/portfolio.py:correl_reducebytrigger,correl_reducebytrigger,function,9,22,18,128,5.82,2,1,"['correl2', 'trigger']","[None, None]","[None, None]",839,"[""  ''' Put Zero below trigger  '''\n""]","['correl2.copy', 'range', 'abs']",3
utilmy/zarchive/storage/portfolio.py:sk_cov_fromcorrel,sk_cov_fromcorrel,function,8,14,13,118,8.43,0,0,"['correl', 'ret_close1']","[None, None]","[None, None]",851,[],"['pf.volhisto_fromret', 'np.diag', 'np.dot']",3
utilmy/zarchive/storage/portfolio.py:cointegration,cointegration,function,18,47,37,385,8.19,0,0,"['x', 'y']","[None, None]","[None, None]",857,"['  """""" Output :\n', '    coint_t :t-statistic of unit-root test on residuals\n', ""    pvalue :   MacKinnon's approximate p-value based on MacKinnon (1994)\n"", '    crit_value : Critical values for the test statistic at the 1 %, 5 %, and 10 %  Signifiance levels.\n', '\n', '    The Null hypothesis is that there is no cointegration, the alt hypothesis is that there is cointegrating relationship. \n', '    If the pvalue is small, below a critical size:  Reject Null Hypothesis -->  Cointegration\n', '           pvalue is high --> No Cointegration    \n', '    \n', '    Significance level, also denoted as alpha or , is the probability of rejecting  the null hypothesis when it is true.\n', '    Probability of incorrectly rejecting a true null hypothesis\n', '\n', '    P=0.05  , At least 23% (and typically close to 50%)\n', '    P=0.01  , At least 7% (and typically close to 15%\n', '\n', '    (-2.9607012342275936,\n', '    0.038730981052330332, 0,\n', '    249,\n', ""    {'1%': -3.4568881317725864,\n"", ""       '10%': -2.5729936189738876,\n"", ""     '5%': -2.8732185133016057},\n"", '      601.96849256295991)\n', '        It can be seen that the calculated test statistic of -2.96 is smaller than the 5% critical value of -2.87, \n', ""        which means that we can reject the null hypothesis that  there isn't a cointegrating relationship at the 5% level.\n"", '  """"""\n']",['np.zeros'],1
utilmy/zarchive/storage/portfolio.py:causality_y1_y2,causality_y1_y2,function,10,14,13,153,10.93,0,0,"['price2', 'price1', 'maxlag']","[None, None, None]","[None, None, None]",895,[],[],0
utilmy/zarchive/storage/portfolio.py:rolling_cointegration,rolling_cointegration,function,13,29,26,280,9.66,0,0,"['x', 'y']","[None, None]","[None, None]",905,[],"['rolling_coint', 'pd.expanding_apply']",2
utilmy/zarchive/storage/portfolio.py:regression,regression,function,117,328,220,3116,9.5,6,7,"['yreturn', 'xreturn', 'type1']","[None, None, None]","[None, None, '""elasticv""']",924,"["" '''Y = X* reg.coef_  +  reg.intercept_, r2 '''\n""]","['len', 'yreturn.ravel', 'xreturn.reshape', 'reg.fit', 'F', 'reg.score', 'regression_fixedsymbolstock', 'range', 'np.reshape', 'np.column_stack', 'regression', 'getpriceret_fromregression', 'np.max', 'regression_getpricefromww', 'np.copy', 'np.zeros', 'regression_allstocks_vs_riskfactors', 'np.shape', 'print', 'np.arange', 'np.empty', 'enumerate', 'pf.getlogret_fromquotes', 'pf.regression_fixedsymbolstock', 'util.np_sortbycolumn', 'util.pd_array_todataframe', 'util.print_object']",27
utilmy/zarchive/storage/portfolio.py:regression_fixedsymbolstock,regression_fixedsymbolstock,function,30,61,55,597,9.79,1,1,"['sym', 'ret_close2', 'tsstart', 'tsample', 'ret_spy', 'spyclose', 'regonly']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, 'True']",961,"["" ''' Price, Weight, R2, Error  '''\n""]","['len', 'range', 'np.reshape', 'np.column_stack', 'regression', 'getpriceret_fromregression', 'np.max']",7
utilmy/zarchive/storage/portfolio.py:regression_getpricefromww,regression_getpricefromww,function,20,38,30,298,7.84,3,0,"['spyclose', 'ww01', 'regasset01', 'ret_close2', 'tstart', 'tlag']","[None, None, None, None, None, None]","[None, None, None, None, None, '1']",984,[],"['np.copy', 'np.zeros', 'range', 'len']",4
utilmy/zarchive/storage/portfolio.py:regression_allstocks_vs_riskfactors,regression_allstocks_vs_riskfactors,function,65,137,113,1360,9.93,2,1,"['symstock', 'pricestock', 'symriskfac', 'priceriskfac', 'nlaglist']","[None, None, None, None, None]","[None, None, None, None, None]",1005,"["" '''Make Regression on all stocks ''' \n""]","['np.shape', 'print', 'np.arange', 'np.empty', 'len', 'enumerate', 'pf.getlogret_fromquotes', 'range', 'np.reshape', 'pf.regression_fixedsymbolstock', 'util.np_sortbycolumn', 'util.pd_array_todataframe', 'util.print_object']",13
utilmy/zarchive/storage/portfolio.py:getdiff_fromquotes,getdiff_fromquotes,function,4,6,6,73,12.17,0,0,"['close', 'timelag']","[None, None]","[None, None]",1102,[],['np.shape'],1
utilmy/zarchive/storage/portfolio.py:getret_fromquotes,getret_fromquotes,function,7,23,17,159,6.91,0,1,"['close', 'timelag']","[None, None]","[None, '1']",1106,[],"['np.shape', 'len']",2
utilmy/zarchive/storage/portfolio.py:getlogret_fromquotes,getlogret_fromquotes,function,4,7,7,81,11.57,0,0,"['close', 'timelag']","[None, None]","[None, '1']",1117,[],"['np.shape', 'np.log']",2
utilmy/zarchive/storage/portfolio.py:getprice_fromret,getprice_fromret,function,12,23,21,190,8.26,2,0,"['ret', 'normprice']","[None, None]","[None, '100']",1122,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/storage/portfolio.py:price_normalize100,price_normalize100,function,16,43,36,292,6.79,1,1,"['ret', 'normprice']","[None, None]","[None, '100']",1134,[],"['np.shape', 'len', 'np.reshape', 'np.zeros', 'range', 'np.cumprod']",6
utilmy/zarchive/storage/portfolio.py:price_normalize_1d,price_normalize_1d,function,8,22,19,147,6.68,0,0,"['ret', 'normprice', 'dtype1']","[None, None, None]","[None, '100', ' np.float16']",1154,[],"['np.empty', 'np.cumprod']",2
utilmy/zarchive/storage/portfolio.py:norm_fast,norm_fast,function,7,8,8,85,10.62,0,0,"['y', 'ny']","[None, None]","[None, None]",1163,[],['ne.evaluate'],1
utilmy/zarchive/storage/portfolio.py:correl_fast,correl_fast,function,8,13,13,116,8.92,0,0,"['xn', 'y', 'nx']","[None, None, None]","[None, None, None]",1171,[],['ne.evaluate'],1
utilmy/zarchive/storage/portfolio.py:volhisto_fromret,volhisto_fromret,function,2,16,12,174,10.88,0,1,"['retbsk', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",1179,[],"['len', 'np.std']",2
utilmy/zarchive/storage/portfolio.py:volhisto_fromprice,volhisto_fromprice,function,6,27,18,251,9.3,0,1,"['price', 't', 'volrange', 'axis']","[None, None, None, None]","[None, None, None, '0']",1186,[],"['len', 'np.shape', 'np.std']",3
utilmy/zarchive/storage/portfolio.py:rsk_calc_all_TA,rsk_calc_all_TA,function,11,76,33,593,7.8,0,0,['df'],[None],"[""'panda_dataframe'""]",1198,"[""  '''Add All TA RMI, RSI To the '''\n""]",[],0
utilmy/zarchive/storage/portfolio.py:ta_lowbandtrend1,ta_lowbandtrend1,function,43,151,76,871,5.77,0,10,"['close2', 'type1']","[None, None]","[None, '0']",1238,"[""  '''Get lower band trend '''\n""]","['linearreg2', 'np.array', 'np.sum', 'util.np_findlocalmin2', 'len', 'util.sort', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",9
utilmy/zarchive/storage/portfolio.py:ta_highbandtrend1,ta_highbandtrend1,function,43,172,76,939,5.46,0,11,"['close2', 'type1']","[None, None]","[None, '0']",1280,[],"['linearreg', 'np.array', 'np.sum', 'util.np_findlocalmax2', 'len', 'util.sort', 'util.np_find_minpos', 'util.np_find_maxpos', 'np.abs']",9
utilmy/zarchive/storage/portfolio.py:pd_transform_asset,pd_transform_asset,function,11,35,27,409,11.69,0,1,"['q0', 'q1', 'type1']","[None, None, None]","[None, None, '""spread""']",1329,[],"['util.pd_date_intersection', 'util.pd_array_todataframe', 'util.pd_insertcol']",3
utilmy/zarchive/storage/portfolio.py:calcbasket_table,calcbasket_table,function,25,59,53,477,8.08,1,2,"['wwvec', 'ret', 'type1', 'wwtype', 'rebfreq', 'costbps']","[None, None, None, None, None, None]","[None, None, '""table""', '""constant""', '1', ' 0.000']",1343,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.mod', 'np.sum']",6
utilmy/zarchive/storage/portfolio.py:folio_lowcorrelation,folio_lowcorrelation,function,145,431,300,3927,9.11,8,7,"['sym01', 'nstock', 'periodlist', 'dateref', 'close1', 'kbenchmark', 'badlist', 'costbppa', 'showgraph']","[None, None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, '0.02', 'True']",1370,[],"['ww_selection', 'np.corrcoef', 'np.nan_to_num', 'np.zeros', 'range', 'util.np_sortbycol', 'int', 'np.std', 'np.isnan', 'util.find', 'np.sum', 'stk_select.append', 'pf.volhisto_fromret', 'sum', '1/len', 'pf.getret_fromquotes', 'len', 'np.empty', 'np.ones', 'enumerate', 'pf.calcbasket_table', 'pf.price_normalize100', 'np.column_stack', 'np.zeros_like', 'np.array', 'np.concatenate', 'print', 'str', 'pf.plot_price', 'sym01[int', 'pf.volhisto_fromprice', 'xrange', 'folio_inverseetf']",33
utilmy/zarchive/storage/portfolio.py:folio_inverseetf,folio_inverseetf,function,8,18,18,151,8.39,1,0,"['price', 'costpa']","[None, None]","[None, '0.0']",1510,[],"['np.zeros', 'xrange', 'len']",3
utilmy/zarchive/storage/portfolio.py:folio_voltarget,folio_voltarget,function,2,6,6,61,10.17,0,0,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1519,[],['folio_volta'],1
utilmy/zarchive/storage/portfolio.py:folio_volta,folio_volta,function,17,46,37,443,9.63,2,0,"['bsk', 'targetvol', 'volrange', 'expocap']","[None, None, None, None]","[None, '0.11', ' 90', '1.5']",1523,[],"['folio_volta', 'np.shape', 'np.zeros', 'range', 'np.std', 'min']",6
utilmy/zarchive/storage/portfolio.py:folio_fixedweightprice,folio_fixedweightprice,function,13,36,33,225,6.25,2,0,"['price', 'fixedww', 'costpa']","[None, None, None]","[None, None, '0.0']",1538,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/storage/portfolio.py:folio_fixedweightret,folio_fixedweightret,function,13,31,28,193,6.23,2,0,"['ret', 'fixedww']","[None, None]","[None, None]",1550,[],"['np.shape', 'np.zeros', 'range']",3
utilmy/zarchive/storage/portfolio.py:folio_cost_turnover,folio_cost_turnover,function,17,38,32,278,7.32,1,1,"['wwall', 'bsk', 'dateref']","[None, None, None]","[None, None, None]",1563,[],"['len', 'xrange', 'np.sum']",3
utilmy/zarchive/storage/portfolio.py:folio_riskpa,folio_riskpa,function,21,55,47,437,7.95,2,1,"['ret', 'targetvol', 'volrange']","[None, None, None]","[None, '0.1', '90']",1577,[],"['np.shape', 'np.zeros', 'np.ones', 'range', 'np.sum', 'np.std', 'isfloat']",7
utilmy/zarchive/storage/portfolio.py:objective_criteria,objective_criteria,function,21,82,54,585,7.13,1,6,"['bsk', 'criteria', 'date1']","[None, None, None]","[None, None, 'None']",2009,[],"['np.sum', 'range', 'np.std', 'np.var']",4
utilmy/zarchive/storage/portfolio.py:calcbasket_obj,calcbasket_obj,function,31,76,62,510,6.71,2,3,"['wwvec', '*data']","[None, None]","[None, None]",2037,[],"['np.zeros', 'range', 'np.mod', 'weightcalc_generic', 'np.sum', 'objective_criteria']",6
utilmy/zarchive/storage/portfolio.py:calc_optimal_weight,calc_optimal_weight,function,1,6,6,40,6.67,0,0,"['args', 'bounds', 'maxiter']","[None, None, None]","[None, None, '1']",2062,[],[],0
utilmy/zarchive/storage/portfolio.py:fitness,fitness,function,6,12,12,75,6.25,0,0,['p'],[None],[None],2104,[],['10*cos'],1
utilmy/zarchive/storage/portfolio.py:np_countretsign,np_countretsign,function,6,18,18,80,4.44,1,1,['x'],[None],[None],2860,[],"['range', 'len', 'np.sign']",3
utilmy/zarchive/storage/portfolio.py:np_trendtest,np_trendtest,function,32,96,68,535,5.57,3,3,"['x', 'alpha ']","[None, None]","[None, ' 0.05']",2867,"['    """"""\n', '    This function is derived from code originally posted by Sat Kumar Tomer (satkumartomer@gmail.com)\n', '    See also: http://vsp.pnnl.gov/help/Vsample/Design_Trend_Mann_Kendall.htm\n', '    The purpose of the Mann-Kendall (MK) test (Mann 1945, Kendall 1975, Gilbert 1987) is to statistically assess if there is a monotonic upward or downward trend of the variable of interest over time. A monotonic upward (downward) trend means that the variable consistently increases (decreases) through time, but the trend may or may not be linear. The MK test can be used in place of a parametric linear regression analysis, which can be used to test if the slope of the estimated linear regression line is different from zero. The regression analysis requires that the residuals from the fitted regression line be normally distributed; an assumption not required by the MK test, that is, the MK test is a non-parametric (distribution-free) test.\n', '    Hirsch, Slack and Smith (1982, page 107) indicate that the MK test is best viewed as an exploratory analysis and is most appropriately used to identify stations where changes are significant or of large magnitude and to quantify these findings.\n', '    Input:\n', '        x:   a vector of data\n', '        alpha: significance level (0.05 default)\n', '    \n', '    Output:\n', '        trend: tells the trend (increasing, decreasing or no trend)\n', '        h: True (if trend is present) or False (if trend is absence)\n', '        p: p value of the significance test\n', '        z: normalized test statistics \n', '        \n', '    Examples x = np.random.rand(100) trend,h,p,z = mk_test(x,0.05) \n', '    """"""\n']","['len', 'range', 'np.sign', 'np.unique', 'np.zeros', 'sum', 'np.sum', 'abs', 'norm.ppf']",9
utilmy/zarchive/storage/portfolio.py:correl_rankbystock,correl_rankbystock,function,13,32,28,258,8.06,1,0,"['stkid', '5', '6]', 'correl', '0]', '[0', '1]]']","[None, None, None, None, None, None, None]","['[2', None, None, '[[1', None, None, None]",2922,"[' """""" Ranking of stocks by correlation """"""\n']","['np.zeros', 'enumerate', 'np.sum', 'util.sortcol']",4
utilmy/zarchive/storage/portfolio.py:calc_print_correlrank,calc_print_correlrank,function,25,164,88,1540,9.39,2,2,"['close2', 'symjp1', 'nlag', 'refindexname', 'toprank2', 'customnameid', 'customnameid2']","[None, None, None, None, None, None, None]","[None, None, None, None, '5', '[]', '[]']",2936,"["" ''' Most correlated/Un-correlated from One Risk Factor'''\n""]","['util.np_findfirst', 'pf.calc_ranktable', 'np.shape', 'print', 'util.np_find', 'enumerate', 'int', 'util.np_mergelist', 'pf.getret_fromquotes', 'pf.price_normalize100', 'pf.plot_price']",11
utilmy/zarchive/storage/portfolio.py:calc_ranktable,calc_ranktable,function,20,44,41,354,8.05,1,0,"['close2', 'symjp1', 'nlag', 'refindex', 'funeval', 'funargs']","[None, None, None, None, None, None]","[None, None, None, None, None, None]",2987,[],"['np.zeros', 'pf.getlogret_fromquotes', 'enumerate', 'funeval', 'util.np_sortbycolumn']",5
utilmy/zarchive/storage/portfolio.py:similarity_correl,similarity_correl,function,7,10,9,91,9.1,0,0,"['ret_close2', 'funargs']","[None, None]","[None, None]",3007,[],['pf.correlation_mat'],1
utilmy/zarchive/storage/portfolio.py:np_similarity,np_similarity,function,6,17,12,138,8.12,0,3,"['x', 'y', 'wwerr', 'type1']","[None, None, None, None]","[None, None, '[]', '0']",3062,[],['np_distance_l1'],1
utilmy/zarchive/storage/portfolio.py:np_distance_l1,np_distance_l1,function,2,6,6,37,6.17,0,0,"['x', 'y', 'wwerr']","[None, None, None]","[None, None, None]",3068,[],"['np.sum', 'np.abs']",2
utilmy/zarchive/storage/portfolio.py:imp_findticker,imp_findticker,function,10,16,14,113,7.06,1,0,"['tickerlist', 'sym01', 'symname']","[None, None, None]","[None, None, None]",3354,[],['v.append'],1
utilmy/zarchive/storage/portfolio.py:imp_close_dateref,imp_close_dateref,function,27,72,65,631,8.76,2,1,"['sym01', 'sdate', 'edate', 'datasource', 'typeprice']","[None, None, None, None, None]","[None, '20100101', '20160628', ""''"", '""close""']",3363,[],"['imp_yahooticker', 'start=str', 'str', 'util.listallfile', 'range', 'len', 'liststockname.append', 'pf.imp_txt_getquotes', 'enumerate', 'print', 'pf.date_align', 'util.a_cleanmemory']",12
utilmy/zarchive/storage/portfolio.py:imp_yahooticker,imp_yahooticker,function,23,45,42,485,10.78,1,1,"['symbols', 'start', 'end', 'type1']","[None, None, None, None]","[None, '""20150101""', '""20160101""', '1']",3391,[],"['datetime.datetime', 'int', 'enumerate', 'quotes_historical_yahoo_ochl', 'quotes.append', 'correctlist.append', 'errorlist.append', 'print']",8
utilmy/zarchive/storage/portfolio.py:imp_errorticker,imp_errorticker,function,11,29,27,291,10.03,1,0,"['symbols', 'start', 'end']","[None, None, None]","[None, '""20150101""', '""20160101""']",3414,[],"['datetime.datetime', 'int', 'quotes_historical_yahoo_ochl', 'errorlist.append', 'print']",5
utilmy/zarchive/storage/portfolio.py:imp_yahoo_financials_url,imp_yahoo_financials_url,function,7,32,23,276,8.62,0,2,"['ticker_symbol', 'statement', 'quarterly']","[None, None, None]","[None, '""is""', 'False']",3434,[],"['BeautifulSoup', 'sys.exit']",2
utilmy/zarchive/storage/portfolio.py:imp_yahoo_periodic_figure,imp_yahoo_periodic_figure,function,17,80,60,610,7.62,1,4,"['soup', 'yahoo_figure']","[None, None]","[None, None]",3442,[],"['re.compile', 'soup.find', 'sys.exit', 'row.find_all', 'int', 'values.append']",6
utilmy/zarchive/storage/portfolio.py:imp_googleIntradayQuoteSave,imp_googleIntradayQuoteSave,function,10,19,18,180,9.47,0,0,"['name1', 'date1', 'inter', 'tframe', 'dircsv']","[None, None, None, None, None]","[None, None, None, None, None]",3578,[],"['googleIntradayQuote', 'name1.replace', 'str', 'q.write_csv']",4
utilmy/zarchive/storage/portfolio.py:imp_googleQuoteSave,imp_googleQuoteSave,function,11,21,21,173,8.24,0,0,"['name1', 'date1', 'date2', 'dircsv']","[None, None, None, None]","[None, None, None, None]",3585,[],"['googleQuote', 'name1.replace', 'str', 'q.write_csv']",4
utilmy/zarchive/storage/portfolio.py:imp_googleQuoteList,imp_googleQuoteList,function,6,34,21,228,6.71,2,1,"['symbols', 'date1', 'date2', 'inter', 'tframe', 'dircsv', 'intraday1']","[None, None, None, None, None, None, None]","[None, None, None, '23400', '2000', ""''"", 'True']",3594,[],"['imp_googleIntradayQuoteSave', 'print', 'imp_googleQuoteSave']",3
utilmy/zarchive/storage/portfolio.py:pd_filterbydate,pd_filterbydate,function,6,35,21,318,9.09,0,3,"['df', 'dtref', ""start='2016-06-06 00"", ""end='2016-06-14 00"", 'freq', 'timezone']","[None, None, '', '', None, None]","[None, 'None', ""'2016-06-06 00:00:00'"", ""'2016-06-14 00:00:00'"", ""'0d0h05min'"", ""'Japan'""]",3609,"["" ''' df: DateSeries or TimeSeries of Quotes   '''\n""]","['type', 'pd.date_range']",2
utilmy/zarchive/storage/portfolio.py:imp_panda_db_dumpinfo,imp_panda_db_dumpinfo,function,15,32,28,274,8.56,1,0,"[""dbfile='E""]",[''],"[""'E:\\_data\\stock\\intraday_google.h5'""]",3624,[],"['pd.HDFStore', 'store.keys', 'pd.read_hdf', 'extract.append', 'errsym.append', 'np.array']",6
utilmy/zarchive/storage/portfolio.py:imp_numpyclose_frompandas,imp_numpyclose_frompandas,function,47,129,93,1105,8.57,2,6,"['dbfile', 'symlist', 't0', 't1', 'priceid', 'maxasset', 'tmax2']","[None, None, None, None, None, None, None]","[None, '[]', '20010101', '20010101', '""close""', '2500', '2000']",3639,[],"['print', 'np.zeros', 'pd.HDFStore', 'store.select', 'enumerate', 'len', 'util.find', 'qlist.append', 'sym.append', 'np.mod', 'date_align', 'np.shape', 'pf.date_align', 'xrange']",14
utilmy/zarchive/storage/portfolio.py:imp_quotes_fromtxt,imp_quotes_fromtxt,function,26,76,65,583,7.67,2,2,"['stocklist01', ""filedir='E"", 'startdate', 'endate']","[None, '', None, None]","[None, ""'E:/_data/stock/daily/20160610/jp'"", '20150101', '20160616']",3686,[],"['util.listallfile', 'len', 'x.split', 'util.np_findfirst', 'print', 'pd.read_csv', 'quotes.append']",7
utilmy/zarchive/storage/portfolio.py:imp_quotes_errordate,imp_quotes_errordate,function,8,25,17,222,8.88,2,1,"['quotes', 'dateref']","[None, None]","[None, None]",3716,"["" ''' Show Symbol in Error when importing '''\n""]","['enumerate', 'print', 'util.datetime_tostring', 'util.datetime_toint', 'str']",5
utilmy/zarchive/storage/portfolio.py:imp_getcsvname,imp_getcsvname,function,7,12,11,88,7.33,0,0,"['name1', 'date1', 'inter', 'tframe']","[None, None, None, None]","[None, None, None, None]",3728,[],['str'],1
utilmy/zarchive/storage/portfolio.py:imp_quote_tohdfs,imp_quote_tohdfs,function,20,55,44,510,9.27,1,1,"['sym', 'qqlist', 'filenameh5', 'fromzone', 'tozone']","[None, None, None, None, None]","[None, None, None, ""'Japan'"", ""'UTC'""]",3763,[],"['pd.HDFStore', 'enumerate', 'copy.deepcopy', 'datetime_toint', 'pf.imp_hdfs_getquote', 'type', 'store.append', 'pd.concat', 'qq.drop_duplicates', 'qq.sort', 'print', 'str', 'store.close']",13
utilmy/zarchive/storage/portfolio.py:date_todatetime,date_todatetime,function,1,9,9,58,6.44,0,0,['tlist'],[None],[None],3785,[],[],0
utilmy/zarchive/storage/portfolio.py:date_removetimezone,date_removetimezone,function,1,6,6,63,10.5,0,0,['datelist'],[None],[None],3791,[],[],0
utilmy/zarchive/storage/portfolio.py:imp_csvquote_topanda,imp_csvquote_topanda,function,13,41,35,508,12.39,0,1,"['file1', 'filenameh5', 'dfname', 'fromzone', 'tozone']","[None, None, None, None, None]","[None, None, ""'sym1'"", ""'Japan'"", ""'UTC'""]",3796,[],"['tz.gettz', 'pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",5
utilmy/zarchive/storage/portfolio.py:imp_panda_insertfoldercsv,imp_panda_insertfoldercsv,function,10,34,32,300,8.82,1,2,"['dircsv', ""filepd= r'E"", 'fromtimezone', 'tozone']","[None, '', None, None]","[None, "" r'E:\\_data\\stock\\intraday_google.h5'"", ""'Japan'"", ""'UTC'""]",3813,[],"['util.listallfile', 'util.str_isfloat', 'print', 'imp_csvquote_topanda']",4
utilmy/zarchive/storage/portfolio.py:imp_panda_checkquote,imp_panda_checkquote,function,4,8,8,70,8.75,1,0,['quotes'],[None],[None],3822,[],['np.shape'],1
utilmy/zarchive/storage/portfolio.py:imp_panda_getquote,imp_panda_getquote,function,2,7,6,59,8.43,0,0,"['filenameh5', 'dfname']","[None, None]","[None, '""data""']",3826,[],['pd.read_hdf'],1
utilmy/zarchive/storage/portfolio.py:imp_pd_merge_database,imp_pd_merge_database,function,12,27,22,321,11.89,1,0,"['filepdfrom', 'filepdto']","[None, None]","[None, None]",3831,[],"['pd.HDFStore', 'store0.keys', 'pf.imp_hdfs_getquote', 'store1.append', 'qq.drop_duplicates', 'qq.sort', 'print', 'store0.close', 'store1.close']",9
utilmy/zarchive/storage/portfolio.py:imp_panda_getListquote,imp_panda_getListquote,function,28,58,46,567,9.78,1,3,"['symbols', 'close1', ""start='12/18/2015 00"", ""end='3/1/2016 00"", 'freq', ""filepd= 'E"", 'tozone', 'fillna', 'interpo']","[None, None, '', '', None, '', None, None, None]","[None, ""'close'"", ""'12/18/2015 00:00:00+00:00'"", ""'3/1/2016 00:00:00+00:00'"", ""'0d0h10min'"", "" 'E:\\_data\\stock\\intraday_google.h5'"", ""'Japan'"", 'True', 'True']",3846,[],"['pd.date_range', 'imp_panda_getquote', 'type', 'errorsym.append', 'qq.fillna', 'qq.interpolate', 'quotes.append', 'correctsym.append', 'util.datenumpy_todatetime']",9
utilmy/zarchive/storage/portfolio.py:imp_panda_cleanquotes,imp_panda_cleanquotes,function,9,26,21,295,11.35,0,0,"['df', 'datefilter']","[None, None]","[None, None]",3891,[],"['df.sort', 'df.interpolate', 'close.fillna', 'np.array']",4
utilmy/zarchive/storage/portfolio.py:imp_panda_storecopy,imp_panda_storecopy,function,12,20,20,253,12.65,1,0,[],[],[],3901,[],"['pd.HDFStore', 'store.keys', 'pf.imp_hdfs_getquote', 'store2.append', 'store.close', 'store2.close']",6
utilmy/zarchive/storage/portfolio.py:imp_panda_removeDuplicate,imp_panda_removeDuplicate,function,11,26,20,269,10.35,1,0,"[""filepd=  'E""]",[''],"[""  'E:\\_data\\stock\\intraday_google.h5'""]",3920,[],"['pd.HDFStore', 'store.keys', 'store.select', 'qq.drop_duplicates', 'qq.sort', 'list', 'store.remove', 'store.append', 'store.close']",9
utilmy/zarchive/storage/portfolio.py:calc_statestock,calc_statestock,function,137,805,298,6715,8.34,22,14,"['close2', 'dateref', 'symfull']","[None, None, None]","[None, None, None]",3983,[],"['sort', 'util.sortcol', 'perf', 'and2', 'mar', 'np.mean', 'ma', 'dd', 'util.find', 'gap', 'pf.getret_fromquotes', 'len', 'np.shape', 'print', 'str', 'np.zeros', 'np.arange', '100*ma', 'pf.volhisto_fromprice', 'util.np_find_minpos', 'util.np_find_maxpos', 'range', 'int', 'pf.regression', 'pf.np_countretsign', 'util.np_findlocalmax2', 'util.np_findlocalmin2', 'util.sort', 'ta_highbandtrend1', 'ta_lowbandtrend1', 'pf.ta_highbandtrend1', 'pf.ta_lowbandtrend1', 'np.array']",33
utilmy/zarchive/storage/portfolio.py:imp_screening_addrecommend,imp_screening_addrecommend,function,19,48,40,412,8.58,1,3,"['string1', 'dbname']","[None, None]","[None, ""'stock_recommend'""]",4384,[],"['string1.replace', 'ss.replace', 'ss.split', 'copy.deepcopy', 'len', 'util.find', 'aux.append', 'util.load_obj', 'stock_recommend.append', 'util.save_obj']",10
utilmy/zarchive/storage/portfolio.py:imp_finviz,imp_finviz,function,53,369,159,4708,12.76,8,0,[],[],[],4411,[],"['requests.get', 'BeautifulSoup', 'soup.find_all', 'len', 'int', 'titlesarray.append', 'titlesarray.insert', 'while', 'str', 'page', 'secondsoup.find_all', 'alldata.append', 'open', 'csv.DictWriter', 'overview.writeheader', 'overview.writerow', 'imp_finviz_news', 'imp_finviz_financials', 'financial.writeheader', 'financial.writerow']",20
utilmy/zarchive/storage/portfolio.py:imp_finviz_news,imp_finviz_news,function,11,18,17,222,12.33,0,0,[],[],[],4468,[],"['requests.get', 'BeautifulSoup', 'soup.find_all']",3
utilmy/zarchive/storage/portfolio.py:imp_finviz_financials,imp_finviz_financials,function,47,187,139,2445,13.07,4,0,[],[],[],4479,[],"['requests.get', 'BeautifulSoup', 'soup.find_all', 'len', 'int', 'titlesarray.append', 'titlesarray.insert', 'while', 'str', 'page', 'secondsoup.find_all', 'alldata.append', 'open', 'csv.DictWriter', 'financial.writeheader', 'financial.writerow']",16
utilmy/zarchive/storage/portfolio.py:get_price2book,get_price2book,function,18,38,35,327,8.61,0,0,['symbol'],[None],[None],4539,[],"['bs', 'soup.find', 'pb.find_next', 'print']",4
utilmy/zarchive/storage/portfolio.py:index,index,class,145,447,285,3718,8.32,3,16,[],[],[],1605,[],[],0
utilmy/zarchive/storage/portfolio.py:searchSimilarity,searchSimilarity,class,178,505,335,4637,9.18,8,17,[],[],[],3072,[],[],0
utilmy/zarchive/storage/portfolio.py:Quote,Quote,class,33,73,53,1228,16.82,2,0,[],[],[],3479,[],[],0
utilmy/zarchive/storage/portfolio.py:googleIntradayQuote,googleIntradayQuote,class,23,51,42,757,14.84,1,2,[],[],[],3521,[],[],0
utilmy/zarchive/storage/portfolio.py:googleQuote,googleQuote,class,21,43,39,840,19.53,1,1,[],[],[],3556,[],[],0
utilmy/zarchive/storage/portfolio.py:index:__init__,index:__init__,method,11,12,12,121,10.08,0,0,"['self', 'id1', 'sym', 'ww', 'tstart']","[None, None, None, None, None]","[None, None, None, None, None]",1606,[],['util.date_generatedatetime'],1
utilmy/zarchive/storage/portfolio.py:index:close,index:close,method,0,1,1,4,4.0,0,0,[],[],[],1611,[],[],0
utilmy/zarchive/storage/portfolio.py:index:updatehisto,index:updatehisto,method,2,12,11,62,5.17,0,0,[],[],[],1614,[],['print'],1
utilmy/zarchive/storage/portfolio.py:index:help,index:help,method,2,12,11,62,5.17,0,0,[],[],[],1618,[],['print'],1
utilmy/zarchive/storage/portfolio.py:index:_statecalc,index:_statecalc,method,31,148,79,1115,7.53,2,7,['self'],[None],[None],1672,[],"['np.sum', 'range', 'np.std', 'np.arange']",4
utilmy/zarchive/storage/portfolio.py:index:_objective_criteria,index:_objective_criteria,method,31,148,79,1115,7.53,2,7,"['self', 'bsk']","[None, None]","[None, None]",1685,[],"['np.sum', 'range', 'np.std', 'np.arange']",4
utilmy/zarchive/storage/portfolio.py:index:calcbasket_obj,index:calcbasket_obj,method,31,65,58,547,8.42,1,3,"['self', 'wwvec']","[None, None]","[None, None]",1730,[],"['np.zeros', 'xrange', 'np.mod', 'self._weightcalc_generic', 'np.sum', 'self._objective_criteria']",6
utilmy/zarchive/storage/portfolio.py:index:calc_optimal_weight,index:calc_optimal_weight,method,14,23,22,356,15.48,0,1,"['self', 'maxiter']","[None, None]","[None, '1']",1769,[],"['np.reshape', 'np.sum', 'print']",3
utilmy/zarchive/storage/portfolio.py:index:_weightcalc_generic,index:_weightcalc_generic,method,6,26,17,235,9.04,0,3,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",1781,[],"['self._weightcalc_constant', 'self._weightcalc_regime', 'self._weightcalc_regime2', 'print']",4
utilmy/zarchive/storage/portfolio.py:index:_weightcalc_constant,index:_weightcalc_constant,method,3,4,4,28,7.0,0,0,"['self', 'ww2', 't']","[None, None, None]","[None, None, None]",1787,[],['np.sum'],1
utilmy/zarchive/storage/portfolio.py:index:_weightcalc_regime2,index:_weightcalc_regime2,method,2,2,2,14,7.0,0,0,"['self', 'wwvec', 't']","[None, None, None]","[None, None, None]",1791,[],[],0
utilmy/zarchive/storage/portfolio.py:searchSimilarity:__init__,searchSimilarity:__init__,method,15,18,17,193,10.72,0,0,"['self', ""filejpstock=r'E"", 'sym01', 'symname', 'startdate', 'enddate', 'pricetype']","[None, '', None, None, None, None, None]","[None, ""r'E:/_data/stock/daily/20160616/jp'"", ""['7203']"", ""['Toyota']"", ' 20150101', '20160601', '""close""']",3073,[],[],0
utilmy/zarchive/storage/portfolio.py:searchSimilarity:load_quotes_fromdb,searchSimilarity:load_quotes_fromdb,method,15,35,33,361,10.31,0,1,"['self', 'picklefile']","[None, None]","[None, ""''""]",3078,[],"['util.load_obj', 'pf.imp_txt_getquotes', 'pf.date_align', 'util.a_cleanmemory']",4
utilmy/zarchive/storage/portfolio.py:searchSimilarity:__generate_return__,searchSimilarity:__generate_return__,method,4,4,4,67,16.75,0,0,"['self', 'nlag']","[None, None]","[None, None]",3089,[],['pf.getret_fromquotes'],1
utilmy/zarchive/storage/portfolio.py:searchSimilarity:__overweight__,searchSimilarity:__overweight__,method,17,62,41,412,6.65,2,4,"['self', 'px']","[None, None]","[None, None]",3093,[],"['len', 'np.ones', 'util.np_findlocalmax', 'type', 'np.abs', 'wwerr[max', 'util.np_findlocalmin']",7
utilmy/zarchive/storage/portfolio.py:searchSimilarity:set_searchcriteria,searchSimilarity:set_searchcriteria,method,34,79,63,799,10.11,0,5,"['self', 'name1', 'date1', 'date2', 'nlag', 'searchperiodstart', 'typesearch', '']","[None, None, None, None, None, None, None, None]","[None, ""'7203'"", '20160301', '20160601', '1', '20120101', '""pattern2""', None]",3113,[],"['pf.getret_fromquotes', 'date_finddateid', 'max', 'util.np_findfirst', 'print', 'pf.price_normalize_1d', 'len', 'self.__overweight__']",8
utilmy/zarchive/storage/portfolio.py:searchSimilarity:launch_search,searchSimilarity:launch_search,method,53,108,82,916,8.48,3,3,['self'],[None],[None],3142,[],"['len', 'np.zeros', 'range', 'pf.price_normalize_1d', 'np_similarity', 'util.np_sortbycolumn', 'enumerate']",7
utilmy/zarchive/storage/portfolio.py:searchSimilarity:show_comparison_graph,searchSimilarity:show_comparison_graph,method,33,72,63,688,9.56,1,3,"['self', 'maxresult', 'show_only_different_time', 'fromid', 'fromend', 'filenameout']","[None, None, None, None, None, None]","[None, '20', 'True', '0', ' 0', ""''""]",3176,[],"['np.shape', 'range', 'int', 'print', 'pf.price_normalize_1d', 'str', 'pf.plot_price']",7
utilmy/zarchive/storage/portfolio.py:searchSimilarity:staticmethod,searchSimilarity:staticmethod,method,0,1,1,4,4.0,0,0,"['self', 'x']","[None, None]","[None, None]",3205,[],[],0
utilmy/zarchive/storage/portfolio.py:searchSimilarity:get_rankresult,searchSimilarity:get_rankresult,method,24,63,53,496,7.87,2,1,"['self', 'filetosave']","[None, None]","[None, ""''""]",3209,[],"['len', 'np.savetxt', 'np.shape', 'np.array', 'np.empty', 'range', 'int', 'str']",8
utilmy/zarchive/storage/portfolio.py:searchSimilarity:export_results,searchSimilarity:export_results,method,5,7,7,57,8.14,0,0,"['self', 'filename']","[None, None]","[None, None]",3231,[],"['open', 'self.to_csv', 'f.write']",3
utilmy/zarchive/storage/portfolio.py:Quote:__init__,Quote:__init__,method,3,9,9,120,13.33,0,0,['self'],[None],[None],3483,[],['range'],1
utilmy/zarchive/storage/portfolio.py:Quote:append,Quote:append,method,7,7,7,209,29.86,0,0,"['self', 'dt', 'open_', 'high', 'low', 'close', 'volume']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",3488,[],[],0
utilmy/zarchive/storage/portfolio.py:Quote:to_csv,Quote:to_csv,method,3,9,9,272,30.22,1,0,['self'],[None],[None],3497,[],['xrange'],1
utilmy/zarchive/storage/portfolio.py:Quote:write_csv,Quote:write_csv,method,5,7,7,57,8.14,0,0,"['self', 'filename']","[None, None]","[None, None]",3503,[],"['open', 'self.to_csv', 'f.write']",3
utilmy/zarchive/storage/portfolio.py:Quote:read_csv,Quote:read_csv,method,11,23,20,346,15.04,1,0,"['self', 'filename']","[None, None]","[None, None]",3508,[],"['range', 'open', 'line.rstrip', 'self.append']",4
utilmy/zarchive/storage/portfolio.py:Quote:__repr__,Quote:__repr__,method,2,2,2,19,9.5,0,0,['self'],[None],[None],3518,[],['self.to_csv'],1
utilmy/zarchive/storage/portfolio.py:googleIntradayQuote:__init__,googleIntradayQuote:__init__,method,22,49,40,699,14.27,1,2,"['self', 'symbol', 'interval_seconds', 'num_days']","[None, None, None, None]","[None, None, '300', '5']",3525,[],"['super', 'symbol.upper', 'requests.post', 'print', 'csv.splitlines', 'xrange', 'float', 'self.append']",8
utilmy/zarchive/storage/portfolio.py:googleQuote:__init__,googleQuote:__init__,method,20,41,37,760,18.54,1,1,"['self', 'symbol', 'start_date', 'end_date']","[None, None, None, None]","[None, None, None, 'datetime.date.today(']",3559,[],"['super', 'symbol.upper', 'datetime.date', 'start.strftime', 'urllib.urlopen', 'csv.reverse', 'print', 'xrange', 'isfloat', 'self.append']",10
utilmy/zarchive/storage/symbolicmath.py:spp,spp,function,19,61,27,221,3.62,0,8,[],[],[],66,[],['spp'],1
utilmy/zarchive/storage/symbolicmath.py:print2,print2,function,19,61,27,221,3.62,0,8,"['a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8']","[None, None, None, None, None, None, None, None, None]","[None, ""''"", ""''"", ""''"", ""''"", ""''"", ""''"", ""''"", ""''""]",68,[],['spp'],1
utilmy/zarchive/storage/symbolicmath.py:factorpoly,factorpoly,function,7,10,9,75,7.5,1,0,['pp'],[None],[None],87,[],"['rr=roots', 'rr.iteritems']",2
utilmy/zarchive/storage/symbolicmath.py:EEvarbrownian,EEvarbrownian,function,12,114,46,895,7.85,0,0,['ff1d'],[None],[None],104,[],"['symbols', 'factor', 'exp', 'Integral', 'ee.doit', 'simplify', '1/sqrt', 'ee.subs', 'vv.doit', 'vv.subs', 'EEvarbrownian2d']",11
utilmy/zarchive/storage/symbolicmath.py:EEvarbrownian2d,EEvarbrownian2d,function,11,60,36,482,8.03,0,0,['ff'],[None],[None],126,[],"['symbols', 'factor', 'exp', 'Integral', 'ee.doit', 'simplify', 'ee.subs', 'vv.doit', 'vv.subs']",9
utilmy/zarchive/storage/symbolicmath.py:lagrangian2d,lagrangian2d,function,12,50,45,592,11.84,1,0,['ll'],[None],[None],168,[],"['simplify', 'print2', 'solvers.solve', 'range', 'res.__len__', 'str']",6
utilmy/zarchive/storage/symbolicmath.py:decomposecorrel,decomposecorrel,function,16,47,39,402,8.55,0,0,['m1'],[None],[None],195,[],"['factor', 'print2', 'm1.eigenvals', 'm1.eigenvects', 'simplify', 'm1.LDLdecomposition']",6
utilmy/zarchive/storage/symbolicmath.py:nn,nn,function,3,26,21,196,7.54,0,1,['x'],[None],[None],233,[],"['1/sqrt', 'nn2', 'abs', 'Integral']",4
utilmy/zarchive/storage/symbolicmath.py:nn2,nn2,function,2,20,18,129,6.45,0,1,"['x', 'y', 'p']","[None, None, None]","[None, None, None]",236,[],"['abs', 'Integral']",2
utilmy/zarchive/storage/symbolicmath.py:dnn2,dnn2,function,1,3,3,54,18.0,0,0,"['x', 'y', 'p']","[None, None, None]","[None, None, None]",241,[],['exp'],1
utilmy/zarchive/storage/symbolicmath.py:dnn,dnn,function,2,7,6,95,13.57,0,0,"['x', 'y', 'p']","[None, None, None]","[None, None, None]",245,[],"['exp', 'dnn', '1/sqrt']",3
utilmy/zarchive/storage/symbolicmath.py:taylor2,taylor2,function,11,24,22,197,8.21,1,0,"['ff', 'x0', 'n']","[None, None, None]","[None, None, None]",249,[],"['simplify', 'range', 'Derivative', 'dffk.doit']",4
utilmy/zarchive/storage/symbolicmath.py:diffn,diffn,function,7,11,11,111,10.09,0,0,"['ff', 'x0', 'kk']","[None, None, None]","[None, None, None]",260,[],"['Derivative', 'simplify', 'dffk.doit']",3
utilmy/zarchive/storage/symbolicmath.py:dN,dN,function,1,2,2,29,14.5,0,0,['x'],[None],[None],270,[],['1/sqrt'],1
utilmy/zarchive/storage/symbolicmath.py:N,N,function,1,5,5,51,10.2,0,0,['x'],[None],[None],273,[],['1/sqrt'],1
utilmy/zarchive/storage/symbolicmath.py:d1f,d1f,function,2,14,12,68,4.86,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",276,[],"['log', 'sqrt']",2
utilmy/zarchive/storage/symbolicmath.py:d2f,d2f,function,2,14,12,68,4.86,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",281,[],"['log', 'sqrt']",2
utilmy/zarchive/storage/symbolicmath.py:d1xf,d1xf,function,2,8,8,28,3.5,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",286,[],['sqrt'],1
utilmy/zarchive/storage/symbolicmath.py:d2xf,d2xf,function,2,8,8,28,3.5,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",289,[],['sqrt'],1
utilmy/zarchive/storage/symbolicmath.py:bsbinarycall,bsbinarycall,function,6,11,11,61,5.55,0,0,"['s0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",293,[],"['d2f', 'exp']",2
utilmy/zarchive/storage/symbolicmath.py:bscall,bscall,function,8,15,14,109,7.27,0,0,"['s0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",299,[],"['d1f', 'vol*sqrt', 's0*exp', 'K*exp']",4
utilmy/zarchive/storage/symbolicmath.py:bsput,bsput,function,7,15,14,112,7.47,0,0,"['s0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",306,[],"['d1f', 'vol*sqrt', 'K*exp']",3
utilmy/zarchive/storage/symbolicmath.py:bs,bs,function,38,337,112,1970,5.85,1,2,"['s0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",313,[],"['d2f', 'exp', 'bscall', 'd1f', 'vol*sqrt', 's0*exp', 'K*exp', 'bsput', 'bs', 'bsdelta', 'N', 'bsstrikedelta', 'bsstrikegamma', 'sqrt', 'bsgamma', 'bstheta', 'St*exp', 'bsrho', 'bsvega', 'dN', 'bsdvd', 'bsvanna', 'bsvolga', 'bsgammaspot']",24
utilmy/zarchive/storage/symbolicmath.py:bsdelta,bsdelta,function,11,27,25,132,4.89,1,1,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",320,[],"['d1f', 'exp', 'N']",3
utilmy/zarchive/storage/symbolicmath.py:bsstrikedelta,bsstrikedelta,function,7,28,22,158,5.64,0,1,"['s0', 'K', 't', 'T', 'r', 'd', 'vol', 'cp1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",328,[],"['d2f', 'exp']",2
utilmy/zarchive/storage/symbolicmath.py:bsstrikegamma,bsstrikegamma,function,4,19,19,96,5.05,0,0,"['s0', 'K', 't', 'T', 'r', 'd', 'vol']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",337,[],"['d1f', 'exp', 'sqrt']",3
utilmy/zarchive/storage/symbolicmath.py:bsgamma,bsgamma,function,4,19,19,96,5.05,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",341,[],"['d1f', 'exp', 'sqrt']",3
utilmy/zarchive/storage/symbolicmath.py:bstheta,bstheta,function,5,20,19,98,4.9,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",345,[],"['d1f', 'St*exp', 'sqrt', 'N']",4
utilmy/zarchive/storage/symbolicmath.py:bsrho,bsrho,function,6,14,13,65,4.64,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",351,[],"['sqrt', 'exp', 'N']",3
utilmy/zarchive/storage/symbolicmath.py:bsvega,bsvega,function,4,13,13,70,5.38,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",356,[],"['d1f', 'St*exp', 'sqrt', 'dN']",4
utilmy/zarchive/storage/symbolicmath.py:bsdvd,bsdvd,function,3,16,15,72,4.5,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",361,[],"['d1f', 'exp', 'N']",3
utilmy/zarchive/storage/symbolicmath.py:bsvanna,bsvanna,function,6,16,15,79,4.94,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",366,[],"['d1f', 'sqrt', 'dN']",3
utilmy/zarchive/storage/symbolicmath.py:bsvolga,bsvolga,function,7,18,16,94,5.22,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",372,[],"['d1f', 'sqrt', 'St*exp', 'dN']",4
utilmy/zarchive/storage/symbolicmath.py:bsgammaspot,bsgammaspot,function,4,18,18,118,6.56,0,0,"['St', 'K', 't', 'T', 'r', 'd', 'vol', 'cp']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",378,[],"['d1f', 'exp']",2
utilmy/zarchive/storage/sobol.py:convert_csv2hd5f,convert_csv2hd5f,function,10,21,19,181,8.62,1,0,"['filein1', 'filename']","[None, None]","[None, None]",64,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/storage/sobol.py:getrandom_tonumpy,getrandom_tonumpy,function,6,9,9,101,11.22,0,0,"['filename', 'nbdim', 'nbsample']","[None, None, None]","[None, None, None]",77,[],['pd.read_hdf'],1
utilmy/zarchive/storage/sobol.py:comoment,comoment,function,4,6,6,61,10.17,0,0,"['xx', 'yy', 'nsample', 'kx', 'ky']","[None, None, None, None, None]","[None, None, None, None, None]",93,[],['ne.evaluate'],1
utilmy/zarchive/storage/sobol.py:acf,acf,function,12,34,31,244,7.18,0,0,['data'],[None],[None],101,[],"['len', 'np.mean', 'np.sum', 'float', 'r', 'np.arange', 'np.asarray']",7
utilmy/zarchive/storage/sobol.py:getdvector,getdvector,function,7,21,20,150,7.14,1,0,"['dimmax', 'istart', 'idimstart']","[None, None, None]","[None, None, None]",130,[],['range'],1
utilmy/zarchive/storage/sobol.py:pathScheme_std,pathScheme_std,function,3,3,3,28,9.33,0,0,"['T', 'n', 'zz']","[None, None, None]","[None, None, None]",140,[],[],0
utilmy/zarchive/storage/sobol.py:pathScheme_bb,pathScheme_bb,function,18,32,30,200,6.25,1,0,"['T', 'n', 'zz']","[None, None, None]","[None, None, None]",148,[],"['int', 'range', 'np.sqrt']",3
utilmy/zarchive/storage/sobol.py:pathScheme_,pathScheme_,function,3,3,3,28,9.33,0,0,"['T', 'n', 'zz']","[None, None, None]","[None, None, None]",167,"[""'''\n"", 'ttdim=1024  # dim= 2^k\n', '[[500, testdensity(500, ttdim, 400,-1)],\n', '[1000, testdensity(1000, ttdim, 400,-1)],\n', '[2000, testdensity(2000, ttdim, 400,-1)], \n', '[3000, testdensity(3000, ttdim, 400,-1)],\n', '[4000, testdensity(4000, ttdim, 400,-1)]]\n', ""'''\n""]",[],0
utilmy/zarchive/storage/sobol.py:testdensity,testdensity,function,27,59,56,415,7.03,3,0,"['nsample', 'totdim', 'bin01', 'Ti']","[None, None, None, None]","[None, None, None, '-1']",177,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_', 'np.arange', 'np.histogram', 'dx.dN', 'np.sum']",8
utilmy/zarchive/storage/sobol.py:plotdensity,plotdensity,function,29,65,63,531,8.17,1,0,"['nsample', 'totdim', 'bin01', 'tit0', 'Ti']","[None, None, None, None, None]","[None, None, None, None, '-1']",195,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_', 'np.arange', 'np.histogram', 'dx.dN', 'plt.plot', 'plt.axis', 'plt.title']",10
utilmy/zarchive/storage/sobol.py:testdensity2d,testdensity2d,function,39,85,74,702,8.26,2,0,"['nsample', 'totdim', 'bin01', 'nbasset']","[None, None, None, None]","[None, None, None, None]",240,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'np.arange', 'np.histogram2d', 'dx.dN2d', 'np.meshgrid', 'plt.imshow', 'np.sum']",10
utilmy/zarchive/storage/sobol.py:lognormal_process2d,lognormal_process2d,function,2,5,4,49,9.8,0,0,"['a1', 'z1', 'a2', 'z2', 'k']","[None, None, None, None, None]","[None, None, None, None, None]",281,[],"['np.maximum', 'np.exp']",2
utilmy/zarchive/storage/sobol.py:testdensity2d2,testdensity2d2,function,40,91,80,750,8.24,2,0,"['nsample', 'totdim', 'bin01', 'nbasset', 'process01', 'a1', 'a2', 'kk']","[None, None, None, None, None, None, None, None]","[None, None, None, None, 'lognormal_process2d', '0.25', '0.25', '1']",287,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'np.arange', 'np.histogram2d', 'dx.dN2d', 'process01', 'np.meshgrid', 'plt.imshow', 'np.sum']",11
utilmy/zarchive/storage/sobol.py:call_process,call_process,function,15,30,28,245,8.17,1,0,"['a', 'z', 'k']","[None, None, None]","[None, None, None]",335,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'process01', 'np.var']",6
utilmy/zarchive/storage/sobol.py:binary_process,binary_process,function,15,30,28,245,8.17,1,0,"['a', 'z', 'k']","[None, None, None]","[None, None, None]",339,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'process01', 'np.var']",6
utilmy/zarchive/storage/sobol.py:pricing01,pricing01,function,15,30,28,245,8.17,1,0,"['totdim', 'nsample', 'a', 'strike', 'process01', 'aa', 'itmax', 'tt']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, '0.25', '-1', '10']",345,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'process01', 'np.var']",6
utilmy/zarchive/storage/sobol.py:plotdensity2,plotdensity2,function,35,78,74,590,7.56,2,0,"['nsample', 'totdim', 'bin01', 'tit0', 'process01', 'vol', 'tt', 'Ti']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, '0.25', '5', '-1']",369,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_', 'np.arange', 'np.histogram', 'process01', 'call_process', 'plt.plot', 'plt.axis', 'plt.title']",11
utilmy/zarchive/storage/sobol.py:Plot2D_random_show,Plot2D_random_show,function,46,106,80,855,8.07,0,0,"['dir1', 'title1', 'dimxmax', 'dimymax', 'dimstep', 'samplejump', 'nsamplegraph']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",421,[],"['pd.read_hdf', 'plt.scatter', 'plt.axis', 'title1+str', 'plt.title', 'plt.show', 'np.mean', 'np.var', 'comoment']",9
utilmy/zarchive/storage/sobol.py:Plot2D_random_save,Plot2D_random_save,function,51,106,77,723,6.82,2,0,"['dir1', 'title1', 'dimxmax', 'dimymax', 'dimstep', 'samplejump', 'nsamplegraph', '']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",468,[],"['range', 'pd.read_hdf', 'plt.scatter', 'plt.axis', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",8
utilmy/zarchive/storage/sobol.py:getoutlier_fromrandom,getoutlier_fromrandom,function,57,168,111,1242,7.39,2,2,"['filename', 'jmax1', 'imax1', 'isamplejum', 'nsample', ""fileoutlier=   'E""]","[None, None, None, None, None, '']","[None, None, None, None, None, ""   'E:\\_data\\_QUASI_SOBOL_gaussian_outlier.h5'""]",518,[],"['range', 'pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'unique_rows', 'store.append', 'store.close']",7
utilmy/zarchive/storage/sobol.py:numexpr_vect_calc,numexpr_vect_calc,function,13,28,22,214,7.64,0,0,"['filename', 'i0', 'imax', 'expr', ""fileout='E""]","[None, None, None, None, '']","[None, None, None, None, ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",586,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/storage/sobol.py:getoutlier_fromrandom_fast,getoutlier_fromrandom_fast,function,71,183,130,1320,7.21,2,2,"['filename', 'jmax1', 'imax1', 'isamplejum', 'nsample', 'trigger1', ""fileoutlier=   'E""]","[None, None, None, None, None, None, '']","[None, None, None, None, None, '0.28', ""   'E:\\_data\\_QUASI_SOBOL_gaussian_outlier.h5'""]",606,[],"['np.zeros', 'range', 'ne.evaluate', 'pd.HDFStore', 'store.append', 'store.close']",6
utilmy/zarchive/storage/sobol.py:outlier_clean,outlier_clean,function,7,16,15,98,6.12,1,1,['vv2'],[None],[None],674,[],['range'],1
utilmy/zarchive/storage/sobol.py:overwrite_data,overwrite_data,function,6,11,9,112,10.18,0,0,"['fileoutlier', 'vv2']","[None, None]","[None, None]",682,[],"['pd.HDFStore', 'store.remove', 'store.append', 'store.close']",4
utilmy/zarchive/storage/sobol.py:doublecheck_outlier,doublecheck_outlier,function,69,163,121,1260,7.73,1,1,"['fileoutlier', 'ijump', 'nsample', 'trigger1', '']","[None, None, None, None, None]","[None, None, '4000', '0.1', None]",702,[],"['pd.read_hdf', 'np.zeros', 'np.shape', 'range', 'ne.evaluate', 'pd.HDFStore', 'store.append', 'store.close']",8
utilmy/zarchive/storage/sobol.py:plot_outlier,plot_outlier,function,19,32,32,287,8.97,0,0,"['fileoutlier', 'kk']","[None, None]","[None, None]",762,[],"['pd.read_hdf', 'plt.scatter', 'plt.axis', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",7
utilmy/zarchive/storage/sobol.py:permute,permute,function,24,64,53,631,9.86,2,0,"['yy', 'kmax']","[None, None]","[None, None]",845,[],"['int', 'np.copy', 'range', 'permute2', 'plt.scatter', 'plt.axis', 'title1+str', 'plt.title', 'comoment']",9
utilmy/zarchive/storage/sobol.py:permute2,permute2,function,18,42,41,461,10.98,1,0,"['xx', 'yy', 'kmax']","[None, None, None]","[None, None, None]",858,[],"['int', 'np.copy', 'range', 'plt.scatter', 'plt.axis', 'title1+str', 'plt.title', 'comoment']",8
utilmy/zarchive/storage/rec_data.py:_get_movielens_path,_get_movielens_path,function,2,3,3,79,26.33,0,0,[],[],[],14,"['    """"""\n', '    Get path to the movielens dataset file.\n', '    """"""\n']",[],0
utilmy/zarchive/storage/rec_data.py:_download_movielens,_download_movielens,function,9,18,18,207,11.5,1,0,['dest_path'],[None],[None],23,"['    """"""\n', '    Download the dataset.\n', '    """"""\n']","['requests.get', 'print', 'open', 'req.iter_content', 'fd.write']",5
utilmy/zarchive/storage/rec_data.py:_get_raw_movielens_data,_get_raw_movielens_data,function,8,13,13,233,17.92,0,1,[],[],[],38,"['    """"""\n', '    Return the raw lines of the train and test files.\n', '    """"""\n']","['_get_movielens_path', '_download_movielens', 'zipfile.ZipFile', 'datafile.read']",4
utilmy/zarchive/storage/rec_data.py:_parse,_parse,function,7,22,16,120,5.45,1,1,['data'],[None],[None],53,"['    """"""\n', '    Parse movielens dataset lines.\n', '    """"""\n']",['line.split'],1
utilmy/zarchive/storage/rec_data.py:_build_interaction_matrix,_build_interaction_matrix,function,12,19,19,130,6.84,1,1,"['rows', 'cols', 'data']","[None, None, None]","[None, None, None]",68,[],"['sp.lil_matrix', 'mat.tocoo']",2
utilmy/zarchive/storage/rec_data.py:_get_movie_raw_metadata,_get_movie_raw_metadata,function,8,12,12,190,15.83,0,1,[],[],[],80,"['    """"""\n', '    Get raw lines of the genre file.\n', '    """"""\n']","['_get_movielens_path', '_download_movielens', 'zipfile.ZipFile', 'datafile.read']",4
utilmy/zarchive/storage/rec_data.py:get_movielens_item_metadata,get_movielens_item_metadata,function,25,57,46,474,8.32,5,3,['use_item_ids'],[None],[None],94,"['    """"""\n', '    Build a matrix of genre features (no_items, no_features).\n', '\n', '    If use_item_ids is True, per-item feeatures will also be used.\n', '    """"""\n']","['set', '_get_movie_raw_metadata', 'line.split', 'int', 'zip', 'genres.append', 'genre_set.add', 'sp.lil_matrix', 'len', 'features.items']",10
utilmy/zarchive/storage/rec_data.py:get_dense_triplets,get_dense_triplets,function,7,8,8,139,17.38,0,0,"['uids', 'pids', 'nids', 'num_users', 'num_items']","[None, None, None, None, None]","[None, None, None, None, None]",136,[],['np.identity'],1
utilmy/zarchive/storage/rec_data.py:get_triplets,get_triplets,function,4,5,5,71,14.2,0,0,['mat'],[None],[None],144,[],['size=len'],1
utilmy/zarchive/storage/rec_data.py:get_movielens_data,get_movielens_data,function,18,30,27,335,11.17,1,0,[],[],[],149,"['    """"""\n', '    Return (train_interactions, test_interactions).\n', '    """"""\n']","['_get_raw_movielens_data', 'set', 'itertools.chain', '_parse', 'uids.add', 'iids.add', 'max', '_build_interaction_matrix']",8
utilmy/zarchive/py3/util.py:session_load_function,session_load_function,function,6,9,9,88,9.78,0,0,['name'],[None],"[""'test_20160815'""]",162,[],"['dill.load_session', 'print']",2
utilmy/zarchive/py3/util.py:session_save_function,session_save_function,function,9,12,12,128,10.67,0,0,['name'],[None],"[""'test'""]",174,[],"['date_now', 'dill.dump_session', 'print']",3
utilmy/zarchive/py3/util.py:py_save_obj_dill,py_save_obj_dill,function,28,58,54,552,9.52,0,1,"['obj1', 'keyname']","[None, None]","[None, None]",183,[],"['z_key_splitinto_dir_name', 'os_folder_create', 'open', 'dill.dumps', 'print']",5
utilmy/zarchive/py3/util.py:session_spyder_showall,session_spyder_showall,function,7,12,12,122,10.17,1,0,[],[],[],202,[],"['os_file_listall', 'print']",2
utilmy/zarchive/py3/util.py:session_guispyder_save,session_guispyder_save,function,4,5,5,83,16.6,0,0,['filename'],[None],[None],208,[],['save_session'],1
utilmy/zarchive/py3/util.py:session_guispyder_load,session_guispyder_load,function,4,5,5,76,15.2,0,0,['filename'],[None],[None],212,[],['load_session'],1
utilmy/zarchive/py3/util.py:session_load,session_load,function,6,9,9,88,9.78,0,0,['name'],[None],"[""'test_20160815'""]",234,"[""  ''' .spydata file,  dict1: already provided Dict,  towhere= main, function, dict '''\n""]","['dill.load_session', 'print']",2
utilmy/zarchive/py3/util.py:session_save,session_save,function,9,12,12,128,10.67,0,0,['name'],[None],"[""'test'""]",260,"[""    '''Need to pass globals() Cannot Get Save data to .spydata file\n"", '\n', '   BIG issue with Import, Impor FULL MODULE ----> BIG ISSUE\n', '     BIG ISSUE with DICT, USE LIST INSTEAD\n', '        If you try to put this code in a module and import the function then you will have to pass globals() to the function explicitly as the globals() in the function is not the IPython global namespace. However, you can put the above code inside your ~/.ipython/profile_PROFILE/startup/startup.ipy file and it will work as expected.\n', '       PROFILE is the name of the profile that you plan to start IPython with.\n', ""    '''\n""]","['date_now', 'dill.dump_session', 'print']",3
utilmy/zarchive/py3/util.py:aa_unicode_ascii_utf8_issue,aa_unicode_ascii_utf8_issue,function,0,0,0,0,0.0,0,0,[],[],[],338,"[""   '''Take All csv in a folder and provide Table, Column Schema, type\n"", '\n', ' METHOD FOR Unicode / ASCII issue\n', '1. Decode early\n', ""Decode to <type 'unicode'> ASAP\n"", ""df['PREF_NAME']=       df['PREF_NAME'].apply(to_unicode)\n"", '\n', '2. Unicode everywhere\n', '\n', '\n', '3. Encode late\n', "">>> f = open('/tmp/ivan_out.txt','w')\n"", "">>> f.write(ivan_uni.encode('utf-8'))\n"", '\n', 'Important methods\n', ""s.decode(encoding)  <type 'str'> to <type 'unicode'>\n"", ""u.encode(encoding)  <type 'unicode'> to <type 'str'>\n"", '\n', 'http://farmdev.com/talks/unicode/\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/py3/util.py:isexist,isexist,function,3,9,8,46,5.11,0,0,['a'],[None],[None],362,[],[],0
utilmy/zarchive/py3/util.py:isfloat,isfloat,function,5,16,14,67,4.19,0,1,['x'],[None],[None],368,[],['float'],1
utilmy/zarchive/py3/util.py:isint,isint,function,6,15,14,80,5.33,0,1,['x'],[None],[None],375,[],['txt.find'],1
utilmy/zarchive/py3/util.py:a_isanaconda,a_isanaconda,function,6,15,14,80,5.33,0,1,[],[],[],377,[],['txt.find'],1
utilmy/zarchive/py3/util.py:a_run_ipython,a_run_ipython,function,1,1,1,33,33.0,0,0,['cmd1'],[None],[None],387,"["" ''' Execute Ipython Command in python code\n"", '     run -i :  run including current interprete variable\n', "" '''\n""]",['IPython.get_ipython'],1
utilmy/zarchive/py3/util.py:a_autoreload,a_autoreload,function,1,4,4,64,16.0,0,0,[],[],[],393,[],['a_run_ipython'],1
utilmy/zarchive/py3/util.py:a_get_platform,a_get_platform,function,1,2,2,13,6.5,0,0,[],[],[],396,[],[],0
utilmy/zarchive/py3/util.py:a_start_log,a_start_log,function,1,14,12,98,7.0,0,0,"['id1', 'folder']","[None, None]","[""''"", ""'aaserialize/log/'""]",400,[],"['a_run_ipython', 'str', 'a_get_platform', 'date_now']",4
utilmy/zarchive/py3/util.py:a_cleanmemory,a_cleanmemory,function,3,3,3,21,7.0,0,0,[],[],[],403,[],['gc.collect'],1
utilmy/zarchive/py3/util.py:a_module_codesample,a_module_codesample,function,6,9,9,130,14.44,0,0,['module_str'],[None],"[""'pandas'""]",406,[],"['os_file_read', 'os_gui_popup_show']",2
utilmy/zarchive/py3/util.py:a_module_doc,a_module_doc,function,6,9,9,123,13.67,0,0,['module_str'],[None],"[""'pandas'""]",412,[],"['os_file_read', 'os_gui_popup_show']",2
utilmy/zarchive/py3/util.py:a_module_generatedoc,a_module_generatedoc,function,9,16,16,179,11.19,0,1,"['module_str', 'fileout']","[None, None]","['""pandas""', ""''""]",418,"['  \'\'\' #  getmodule_doc(""jedi"", r""D:\\_devs\\Python01\\aapackage\\doc.txt"")\'\'\'\n']","['os.makedirs', 'ca.getmodule_doc']",2
utilmy/zarchive/py3/util.py:a_info_conda_jupyter,a_info_conda_jupyter,function,61,131,110,1065,8.13,1,3,[],[],[],426,[],"['printProgress', 'shutil.make_archive', 'os_zipfolder', 'dir_tozip.split', 'os_zip_checkintegrity', 'print']",6
utilmy/zarchive/py3/util.py:os_zipextractall,os_zipextractall,function,23,59,48,538,9.12,1,3,"['filezip_or_dir', 'tofolderextract', 'isprint']","[None, None, None]","['""folder1/*.zip""', ""'zdisk/test'"", '1']",697,"[""   '''os_zipextractall( 'aapackage.zip','zdisk/test/'      )  '''\n""]","['filezip_or_dir.find', 'os_file_listall', 'os_file_getname', 'zipfile.ZipFile', 'zip_ref.extractall', 'zip_ref.close', 'print']",7
utilmy/zarchive/py3/util.py:os_folder_copy,os_folder_copy,function,17,35,34,340,9.71,0,1,"['src', 'dst', 'symlinks', 'pattern1', 'fun_file_toignore']","[None, None, None, None, None]","[None, None, 'False', '""*.py""', 'None']",727,"[""   '''\n"", '       callable(src, names) -> ignored_names\n', ""       'src' parameter, which is the directory being visited by copytree(), and\n"", ""       'names' which is the list of `src` contents, as returned by os.listdir():\n"", '\n', '    Since copytree() is called recursively, the callable will be called once for each directory that is copied.\n', '    It returns a  list of names relative to the `src` directory that should not be copied.\n', '   :param fun_ignore:\n', ""   '''\n""]","['fun_file_toignore', 'fnmatch.filter', 'shutil.copytree', 'shutil.copy']",4
utilmy/zarchive/py3/util.py:os_folder_create,os_folder_create,function,4,7,7,85,12.14,0,1,['directory'],[None],[None],750,[],"['os.getcwd', 'os.makedirs', 'os.chdir']",3
utilmy/zarchive/py3/util.py:os_folder_robocopy,os_folder_robocopy,function,3,14,14,157,11.21,0,1,"['from_folder', 'to_folder', ""my_log='H""]","[None, None, '']","[""''"", ""''"", ""'H:/robocopy_log.txt'""]",755,"['    """"""\n', '    Copy files to working directory\n', '    robocopy <Source> <Destination> [<File>[ ...]] [<Options>]\n', '    We want to copy the files to a fast SSD drive\n', '    """"""\n']","['subprocess.call', 'print']",2
utilmy/zarchive/py3/util.py:os_file_replacestring1,os_file_replacestring1,function,11,26,25,238,9.15,1,0,"['findStr', 'repStr', 'filePath']","[None, None, None]","[None, None, None]",766,[],"['fileinput.FileInput', 'line.replace', 'file1.close', 'print']",4
utilmy/zarchive/py3/util.py:os_file_replacestring2,os_file_replacestring2,function,6,14,13,160,11.43,1,0,"['findstr', 'replacestr', 'some_dir', 'pattern', 'dirlevel']","[None, None, None, None, None]","[None, None, None, '""*.*""', '1']",777,"['  \'\'\' #fil_replacestring_files(""logo.png"", ""logonew.png"", r""D:/__Alpaca__details/aiportfolio"",    pattern=""*.html"", dirlevel=5  )\n', ""  '''\n""]","['listallfile', 'fil_replacestring_onefile']",2
utilmy/zarchive/py3/util.py:os_file_getname,os_file_getname,function,8,9,8,75,8.33,0,0,['path'],[None],[None],784,[],"['ntpath.split', 'ntpath.basename']",2
utilmy/zarchive/py3/util.py:os_file_getpath,os_file_getpath,function,7,7,7,52,7.43,0,0,['path'],[None],[None],789,[],['ntpath.split'],1
utilmy/zarchive/py3/util.py:os_file_gettext,os_file_gettext,function,4,8,8,56,7.0,0,0,['file1'],[None],[None],794,[],"['open', 'f.read']",2
utilmy/zarchive/py3/util.py:os_file_listall,os_file_listall,function,28,82,46,904,11.02,4,3,"['dir1', 'pattern', 'dirlevel', 'onlyfolder']","[None, None, None, None]","[None, '""*.*""', '1', '0']",800,"[""  '''\n"", '   # DIRCWD=r""D:\\_devs\\Python01\\project""\n', '   # aa= listallfile(DIRCWD, ""*.*"", 2)\n', '   # aa[0][30];   aa[2][30]\n', '\n', '   :param dir1:\n', '   :param pattern:\n', '   :param dirlevel:\n', '   :param onlyfolder:\n', '   :return:\n', ""  '''\n""]","['dir1.rstrip', 'dir1.count', 'os.walk', 'root.count', 'matches.append', 'fnmatch.filter', 'np.array']",7
utilmy/zarchive/py3/util.py:os_file_rename,os_file_rename,function,32,60,48,643,10.72,2,1,"['some_dir', 'pattern', 'pattern2', 'dirlevel']","[None, None, None, None]","[None, '""*.*""', '""""', '1']",837,[],"['some_dir.rstrip', 'some_dir.count', 'os.walk', 'root.count', 'matches.append', 'fnmatch.filter', 're.sub', 'os.rename', 'np.array']",9
utilmy/zarchive/py3/util.py:os_gui_popup_show,os_gui_popup_show,function,25,38,37,361,9.5,0,0,['txt'],[None],[None],858,[],"['os_gui_popup_show2', 'Tk', 'Scrollbar', 'Text', 'S.pack', 'T.pack', 'S.config', 'T.config', 'T.insert', 'root.attributes', 'mainloop']",11
utilmy/zarchive/py3/util.py:os_print_tofile,os_print_tofile,function,8,21,19,174,8.29,0,1,"['vv', 'file1', 'mode1']","[None, None, None]","[None, None, ""'a'""]",875,"[""    '''\n"", '    Here is a list of the different modes of opening a file:\n', 'r\n', 'Opens a file for reading only. The file pointer is placed at the beginning of the file. This is the default mode.\n', '\n', 'rb\n', '\n', 'Opens a file for reading only in binary format. The file pointer is placed at the beginning of the file. This is the default mode.\n', 'r+\n', '\n', 'Opens a file for both reading and writing. The file pointer will be at the beginning of the file.\n', 'rb+\n', '\n', 'Opens a file for both reading and writing in binary format. The file pointer will be at the beginning of the file.\n', 'w\n', '\n', 'Opens a file for writing only. Overwrites the file if the file exists. If the file does not exist, creates a new file for writing.\n', 'wb\n', '\n', 'Opens a file for writing only in binary format. Overwrites the file if the file exists. If the file does not exist, creates a new file for writing.\n', 'w+\n', '\n', 'Opens a file for both writing and reading. Overwrites the existing file if the file exists. If the file does not exist, creates a new file for reading and writing.\n', 'wb+\n', '\n', 'Opens a file for both writing and reading in binary format. Overwrites the existing file if the file exists. If the file does not exist, creates a new file for reading and writing.\n', 'a\n', '\n', 'Opens a file for appending. The file pointer is at the end of the file if the file exists. That is, the file is in the append mode. If the file does not exist, it creates a new file for writing.\n', 'ab\n', '\n', 'Opens a file for appending in binary format. The file pointer is at the end of the file if the file exists. That is, the file is in the append mode. If the file does not exist, it creates a new file for writing.\n', 'a+\n', '\n', 'Opens a file for both appending and reading. The file pointer is at the end of the file if the file exists. The file opens in the append mode. If the file does not exist, it creates a new file for reading and writing.\n', 'ab+\n', '\n', 'Opens a file for both appending and reading in binary format. The file pointer is at the end of the file if the file exists. The file opens in the append mode. If the file does not exist, it creates a new file for reading and writing.\n', 'To open a text file, use:\n', 'fh = open(""hello.txt"", ""r"")\n', '\n', 'To read a text file, use:\n', 'print fh.read()\n', '\n', 'To read one line at a time, use:\n', 'print fh.readline()\n', '\n', 'To read a list of lines use:\n', 'print fh.readlines()\n', '\n', 'To write to a file, use:\n', 'fh = open(""hello.txt"", ""w"")\n', 'lines_of_text = [""a line of text"", ""another line of text"", ""a third line""]\n', 'fh.writelines(lines_of_text)\n', 'fh.close()\n', '\n', 'To append to file, use:\n', 'fh = open(""Hello.txt"", ""a"")\n', 'fh.close()\n', '\n', ""    '''\n""]","['pth.find', 'b.lstrip']",2
utilmy/zarchive/py3/util.py:os_path_norm,os_path_norm,function,8,21,19,174,8.29,0,1,['pth'],[None],[None],939,"['    \'\'\' #r""D:\\_devs\\Python01\\project\\03-Connect_Java_CPP_Excel\\PyBindGen\\examples"" \'\'\'\n']","['pth.find', 'b.lstrip']",2
utilmy/zarchive/py3/util.py:os_path_change,os_path_change,function,4,4,4,34,8.5,0,0,['path1'],[None],[None],951,[],"['open', 'fh.read']",2
utilmy/zarchive/py3/util.py:os_path_current,os_path_current,function,4,4,4,34,8.5,0,0,[],[],[],953,[],"['open', 'fh.read']",2
utilmy/zarchive/py3/util.py:os_file_exist,os_file_exist,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],955,[],"['open', 'fh.read']",2
utilmy/zarchive/py3/util.py:os_file_size,os_file_size,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],957,[],"['open', 'fh.read']",2
utilmy/zarchive/py3/util.py:os_file_read,os_file_read,function,4,4,4,34,8.5,0,0,['file1'],[None],[None],959,[],"['open', 'fh.read']",2
utilmy/zarchive/py3/util.py:os_file_mergeall,os_file_mergeall,function,10,20,20,182,9.1,1,0,"['nfile', 'dir1', 'pattern1', 'deepness']","[None, None, None, None]","[None, None, None, '2']",963,[],"['listallfile', 'open', 'gettext_fromfile', 'nfile1.write', 'nfile1.close']",5
utilmy/zarchive/py3/util.py:os_extracttext_allfile,os_extracttext_allfile,function,15,33,31,282,8.55,1,0,"['nfile', 'dir1', 'pattern1', 'htmltag', 'deepness']","[None, None, None, None, None]","[None, None, '""*.html""', ""'p'"", '2']",972,"["" ''' Extract text from html '''\n""]","['listallfile', 'open', 'gettext_fromfile', 'BeautifulSoup', 'soup.find_all', 'txt2.strip', 'newfile1.write', 'nfile1.close']",8
utilmy/zarchive/py3/util.py:os_path_append,os_path_append,function,4,19,11,121,6.37,0,3,"['p1', 'p2', 'p3', 'p4']","[None, None, None, None]","[None, 'None', 'None', 'None']",986,[],[],0
utilmy/zarchive/py3/util.py:os_split_dir_file,os_split_dir_file,function,6,15,15,112,7.47,0,1,['dirfile'],[None],[None],996,[],"['dirfile.split', 'len']",2
utilmy/zarchive/py3/util.py:os_process_run,os_process_run,function,13,31,31,317,10.23,0,1,"['cmd_list', ""'arg1'"", ""'arg2']"", 'capture_output']","[None, None, None, None]","[""['program'"", None, None, 'False']",1002,[],"['subprocess.Popen', 'proc.communicate', 'print', 'str']",4
utilmy/zarchive/py3/util.py:os_process_2,os_process_2,function,0,1,1,4,4.0,0,0,[],[],[],1018,[],[],0
utilmy/zarchive/py3/util.py:py_importfromfile,py_importfromfile,function,12,21,19,251,11.95,0,1,"['modulename', 'dir1']","[None, None]","[None, None]",1055,[],"['a_get_pythonversion', 'SourceFileLoader', 'foo.MyClass', 'imp.load_source']",4
utilmy/zarchive/py3/util.py:py_memorysize,py_memorysize,function,16,56,38,318,5.68,0,4,"['o', 'ids', 'hint', 'set(']","[None, None, None, None]","[None, None, '"" deep_getsizeof(df_pd', None]",1067,"['    """""" deep_getsizeof(df_pd, set())\n', '    Find the memory footprint of a Python object\n', '    The sys.getsizeof function does a shallow size of only. It counts each\n', '    object inside a container as pointer only regardless of how big it\n', '    """"""\n']","['id', 'getsizeof', 'ids.add', 'isinstance', 'sum', 'd', 'list']",7
utilmy/zarchive/py3/util.py:save,save,function,2,4,4,64,16.0,0,0,"['obj', 'folder', 'isabsolutpath']","[None, None, None]","[None, ""'/folder1/keyname'"", '0']",1095,[],['py_save_obj'],1
utilmy/zarchive/py3/util.py:load,load,function,2,3,3,60,20.0,0,0,"['folder', 'isabsolutpath']","[None, None]","[""'/folder1/keyname'"", '0']",1098,[],['py_load_obj'],1
utilmy/zarchive/py3/util.py:save_test,save_test,function,6,11,11,124,11.27,0,0,"['folder', 'isabsolutpath']","[None, None]","[""'/folder1/keyname'"", '0']",1101,[],"['py_load_obj', 'print', 'str', 'gc.collect']",4
utilmy/zarchive/py3/util.py:py_save_obj,py_save_obj,function,28,58,54,552,9.52,0,1,"['obj1', 'keyname']","[None, None]","[None, None]",1107,"[""    '''def load_obj(name, encoding1='utf-8' ):\n"", ""         with open('D:/_devs/Python01/aaserialize/' + name + '.pkl', 'rb') as f:\n"", '            return pickle.load(f, encoding=encoding1)\n', ""    '''\n""]","['z_key_splitinto_dir_name', 'os_folder_create', 'open', 'dill.dumps', 'print']",5
utilmy/zarchive/py3/util.py:py_load_obj,py_load_obj,function,16,30,28,265,8.83,0,1,"['folder', 'isabsolutpath', 'encoding1']","[None, None, None]","[""'/folder1/keyname'"", '0', ""'utf-8'""]",1120,"[""    '''def load_obj(name, encoding1='utf-8' ):\n"", ""         with open('D:/_devs/Python01/aaserialize/' + name + '.pkl', 'rb') as f:\n"", '            return pickle.load(f, encoding=encoding1)\n', ""    '''\n""]","['folder.find', 'z_key_splitinto_dir_name', 'os_folder_create', 'open', 'pickle.load']",5
utilmy/zarchive/py3/util.py:z_key_splitinto_dir_name,z_key_splitinto_dir_name,function,6,15,15,112,7.47,0,1,['keyname'],[None],[None],1136,[],"['keyname.split', 'len']",2
utilmy/zarchive/py3/util.py:sql_getdate,sql_getdate,function,0,1,1,4,4.0,0,0,[],[],[],1146,[],[],0
utilmy/zarchive/py3/util.py:obj_getclass_of_method,obj_getclass_of_method,function,7,15,13,114,7.6,1,1,['meth'],[None],[None],1218,[],['inspect.getmro'],1
utilmy/zarchive/py3/util.py:obj_getclass_property,obj_getclass_property,function,3,9,9,69,7.67,1,0,['pfi'],[None],[None],1226,[],"['list', 'print']",2
utilmy/zarchive/py3/util.py:print_topdf,print_topdf,function,27,114,95,913,8.01,0,0,[],[],[],1243,[],"['PdfPages', 'plt.figure', 'plt.plot', 'plt.title', 'pdf.savefig', 'plt.close', 'plt.rc', 'np.arange', 'np.sin', 'pdf.infodict', 'datetime.datetime']",11
utilmy/zarchive/py3/util.py:os_config_setfile,os_config_setfile,function,8,39,26,235,6.03,1,1,"['dict_params', 'outfile', 'mode1']","[None, None, None]","[None, None, ""'w+'""]",1290,[],"['open', 'list', 'isinstance', 'f1.write', 'str', 'f1.close', 'print']",7
utilmy/zarchive/py3/util.py:os_config_getfile,os_config_getfile,function,7,16,15,73,4.56,1,0,['file1'],[None],[None],1302,[],"['open', 'f1.readlines', 'print']",3
utilmy/zarchive/py3/util.py:os_csv_process,os_csv_process,function,1,2,2,9,4.5,0,0,['file1'],[None],[None],1310,[],['print'],1
utilmy/zarchive/py3/util.py:read_funding_data,read_funding_data,function,5,13,12,79,6.08,1,0,['path'],[None],[None],1336,[],"['open', 'csv.DictReader']",2
utilmy/zarchive/py3/util.py:read_funding_data,read_funding_data,function,5,13,12,79,6.08,1,0,['path'],[None],[None],1336,[],"['open', 'csv.DictReader']",2
utilmy/zarchive/py3/util.py:read_funding_data,read_funding_data,function,5,13,12,79,6.08,1,0,['path'],[None],[None],1336,[],"['open', 'csv.DictReader']",2
utilmy/zarchive/py3/util.py:find_fuzzy,find_fuzzy,function,4,12,11,49,4.08,1,1,"['xstring', 'list_string']","[None, None]","[None, None]",1504,"["" ''' if xstring matches partially, add to the list   '''\n""]",['xi.find'],1
utilmy/zarchive/py3/util.py:str_match_fuzzy,str_match_fuzzy,function,4,12,11,64,5.33,1,1,"['xstring', 'list_string']","[None, None]","[None, None]",1509,"["" ''' if any of list_strinf elt matches partially xstring '''\n""]",['xstring.find'],1
utilmy/zarchive/py3/util.py:str_parse_stringcalendar,str_parse_stringcalendar,function,14,39,29,278,7.13,3,1,['cal'],[None],[None],1516,"[""    '''----------Parse Calendar  --------'''\n""]","['cal.split', 'x.find', 'cal4.append', 'np.array', 'print']",5
utilmy/zarchive/py3/util.py:str_make_unicode,str_make_unicode,function,7,16,13,102,6.38,0,1,"['input', 'errors']","[None, None]","[None, ""'replace'""]",1530,[],"['type', 'input.decode']",2
utilmy/zarchive/py3/util.py:str_empty_string_array,str_empty_string_array,function,9,30,23,184,6.13,2,1,"['x', 'y']","[None, None]","[None, '1']",1536,[],"['range', 'str_empty_string_array_numpy', 'np.empty']",3
utilmy/zarchive/py3/util.py:str_empty_string_array_numpy,str_empty_string_array_numpy,function,4,9,8,56,6.22,0,0,"['nx', 'ny']","[None, None]","[None, '1']",1540,[],['np.empty'],1
utilmy/zarchive/py3/util.py:str_isfloat,str_isfloat,function,1,8,7,46,5.75,0,0,['value'],[None],[None],1545,[],['float'],1
utilmy/zarchive/py3/util.py:str_is_azchar,str_is_azchar,function,1,8,7,42,5.25,0,0,['x'],[None],[None],1549,[],['float'],1
utilmy/zarchive/py3/util.py:str_is_az09char,str_is_az09char,function,1,8,7,42,5.25,0,0,['x'],[None],[None],1553,[],['float'],1
utilmy/zarchive/py3/util.py:str_reindent,str_reindent,function,3,8,7,52,6.5,0,0,"['s', 'numSpaces']","[None, None]","[None, None]",1557,"[""    '''\n"", '   if args:\n', ""       aux= name1+'.'+obj.__name__ +'('+ str(args) +')  \\n' + str(inspect.getdoc(obj))\n"", ""       aux= aux.replace('\\n', '\\n       ')\n"", '       aux= aux.rstrip()\n', ""       aux= aux + ' \\n'\n"", '       wi( aux)\n', ""    '''\n""]",['x.decode'],1
utilmy/zarchive/py3/util.py:str_split2,str_split2,function,3,8,7,52,6.5,0,0,"['delimiters', 'string', 'maxsplit']","[None, None, None]","[None, None, '0']",1571,[],['x.decode'],1
utilmy/zarchive/py3/util.py:str_split_pattern,str_split_pattern,function,3,8,7,52,6.5,0,0,"['sep2', 'll', 'maxsplit']","[None, None, None]","[None, None, '0']",1576,[],['x.decode'],1
utilmy/zarchive/py3/util.py:pd_str_isascii,pd_str_isascii,function,3,8,7,52,6.5,0,0,['x'],[None],[None],1584,[],['x.decode'],1
utilmy/zarchive/py3/util.py:str_to_utf8,str_to_utf8,function,2,2,2,23,11.5,0,0,['x'],[None],[None],1590,"['  """""" Do it before saving/output to external printer """"""\n']",['x.encode'],1
utilmy/zarchive/py3/util.py:str_to_unicode,str_to_unicode,function,3,14,11,85,6.07,0,2,"['x', 'encoding']","[None, None]","[None, ""'utf-8'""]",1595,"[""  ''' Do it First after Loading some text '''\n""]","['isinstance', 'str']",2
utilmy/zarchive/py3/util.py:web_restapi_toresp,web_restapi_toresp,function,12,19,18,168,8.84,0,1,['apiurl1'],[None],[None],1696,[],"['requests.get', 'ApiError']",2
utilmy/zarchive/py3/util.py:web_getrawhtml,web_getrawhtml,function,13,20,20,176,8.8,0,1,['url1'],[None],[None],1703,[],"['requests.get', 'ApiError']",2
utilmy/zarchive/py3/util.py:web_importio_todataframe,web_importio_todataframe,function,41,78,61,641,8.22,4,4,"['apiurl1', 'isurl']","[None, None]","[None, '1']",1711,[],"['requests.get', 'ApiError', 'resp.json', 'list', 'colname.append', 'str', 'np.array', 'colmax=len', 'np.empty', 'pd_createdf', 'np.arange', 'len']",12
utilmy/zarchive/py3/util.py:web_getjson_fromurl,web_getjson_fromurl,function,10,12,11,138,11.5,0,0,['url'],[None],[None],1738,[],"['urllib3.connection_from_url', 'http.urlopen', 'print', 'json.loads']",4
utilmy/zarchive/py3/util.py:web_gettext_fromurl,web_gettext_fromurl,function,9,19,18,203,10.68,0,0,"['url', 'htmltag']","[None, None]","[None, ""'p'""]",1753,[],"['urllib3.connection_from_url', 'http.urlopen', 'BeautifulSoup', 'soup.find_all']",4
utilmy/zarchive/py3/util.py:web_gettext_fromhtml,web_gettext_fromhtml,function,8,21,20,176,8.38,0,0,"['file1', 'htmltag']","[None, None]","[None, ""'p'""]",1761,[],"['open', 'BeautifulSoup', 'soup.find_all']",3
utilmy/zarchive/py3/util.py:web_getlink_fromurl,web_getlink_fromurl,function,14,22,21,248,11.27,1,0,['url'],[None],[None],1820,[],"['urllib3.connection_from_url', 'http.urlopen', 'BeautifulSoup', 'soup.prettify', 'soup.findAll', 'links.append', 'set']",7
utilmy/zarchive/py3/util.py:web_send_email,web_send_email,function,33,127,78,1266,9.97,0,0,"['FROM', 'recipient', 'subject', 'body', 'login1', 'pss1', 'server1', 'port1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, '""mizenjapan@gmail.com""', '""sophieelise237""', '""smtp.gmail.com""', '465']",1832,"['    \'\'\'  # send_email(""Kevin"", ""brookm291@gmail.com"", ""JapaneseText:"" , ""txt"") \'\'\'\n']","['MIMEMultipart', 'msg.set_charset', 'MIMEText', 'msg.attach', 'smtplib.SMTP_SSL', 'server_ssl.ehlo', 'login', 'server_ssl.login', 'server_ssl.sendmail', 'msg.as_string', 'server_ssl.close', 'print', 'web_send_email_tls', 'smtplib.SMTP', 'mailserver.ehlo', 'mailserver.starttls', 'mailserver.login', 'mailserver.sendmail', 'mailserver.quit']",19
utilmy/zarchive/py3/util.py:web_send_email_tls,web_send_email_tls,function,25,56,51,568,10.14,0,0,"['FROM', 'recipient', 'subject', 'body', 'login1', 'pss1', 'server1', 'port1']","[None, None, None, None, None, None, None, None]","[None, None, None, None, '""mizenjapan@gmail.com""', '""sophieelise237""', '""smtp.gmail.com""', '465']",1857,[],"['MIMEMultipart', 'msg.set_charset', 'MIMEText', 'msg.attach', 'smtplib.SMTP', 'mailserver.ehlo', 'mailserver.starttls', 'mailserver.login', 'mailserver.sendmail', 'msg.as_string', 'mailserver.quit', 'print']",12
utilmy/zarchive/py3/util.py:web_sendurl,web_sendurl,function,3,11,11,95,8.64,0,0,['url1'],[None],[None],1893,[],"['web_gettext_fromurl', 'send_email']",2
utilmy/zarchive/py3/util.py:np_minimize,np_minimize,function,12,39,35,358,9.18,1,0,"['fun_obj', 'x0', 'argext', '0']","[None, None, None, None]","[None, '[0.0]', '(0', None]",1901,[],"['penalty', 'enumerate', 'max', 'loss', 'fun_obj', 'np_minimizeDE']",6
utilmy/zarchive/py3/util.py:np_minimizeDE,np_minimizeDE,function,16,51,41,422,8.27,1,2,"['fun_obj', 'bounds', 'name1', 'solver']","[None, None, None, None]","[None, None, None, 'None']",1914,[],"['range', 'next', 'print', 'save_obj', 'name1+date_now', 'np.mod', 'np.abs']",7
utilmy/zarchive/py3/util.py:np_remove_NA_INF_2d,np_remove_NA_INF_2d,function,9,22,19,116,5.27,2,1,['X'],[None],[None],1931,[],"['np.shape', 'range', 'np.isnan', 'np.isinf']",4
utilmy/zarchive/py3/util.py:np_addcolumn,np_addcolumn,function,8,11,11,86,7.82,0,0,"['arr', 'nbcol']","[None, None]","[None, None]",1938,[],"['np.shape', 'np.zeros', 'np.column_stack']",3
utilmy/zarchive/py3/util.py:np_addrow,np_addrow,function,9,20,19,139,6.95,0,1,"['arr', 'nbrow']","[None, None]","[None, None]",1944,[],"['np.shape', 'len', 'np.zeros', 'np.row_stack', 'np.append']",5
utilmy/zarchive/py3/util.py:np_int_tostr,np_int_tostr,function,3,17,12,69,4.06,0,1,['i'],[None],[None],1953,[],['str'],1
utilmy/zarchive/py3/util.py:np_dictordered_create,np_dictordered_create,function,5,6,6,52,8.67,0,0,[],[],[],1959,[],['OrderedDict'],1
utilmy/zarchive/py3/util.py:np_list_unique,np_list_unique,function,5,8,8,46,5.75,0,0,['seq'],[None],[None],1963,[],"['Set', 'list']",2
utilmy/zarchive/py3/util.py:np_list_tofreqdict,np_list_tofreqdict,function,10,39,23,162,4.15,2,1,"['l1', 'wweight']","[None, None]","[None, '[]']",1969,[],"['dict', 'len', 'enumerate']",3
utilmy/zarchive/py3/util.py:np_list_flatten,np_list_flatten,function,11,25,19,114,4.56,2,1,['seq'],[None],[None],1986,[],"['type', 'flatten', 'l.append']",3
utilmy/zarchive/py3/util.py:np_dict_tolist,np_dict_tolist,function,5,22,15,100,4.55,2,1,"['dd', 'withkey']","[None, None]","[None, '0']",1997,[],['list'],1
utilmy/zarchive/py3/util.py:np_dict_tostr_val,np_dict_tostr_val,function,1,9,9,52,5.78,0,0,['dd'],[None],[None],2003,[],"['str', 'list']",2
utilmy/zarchive/py3/util.py:np_dict_tostr_key,np_dict_tostr_key,function,1,8,8,52,6.5,0,0,['dd'],[None],[None],2006,[],"['str', 'list']",2
utilmy/zarchive/py3/util.py:np_removelist,np_removelist,function,6,15,15,71,4.73,1,1,"['x0', 'xremove']","[None, None]","[None, '[]']",2011,[],"['np_findfirst', 'xnew.append']",2
utilmy/zarchive/py3/util.py:np_transform2d_int_1d,np_transform2d_int_1d,function,19,31,28,227,7.32,2,0,"['m2d', 'onlyhalf']","[None, None]","[None, 'False']",2017,[],"['np.shape', 'np.zeros', 'range', 'np.abs', 'np_sortbycol']",5
utilmy/zarchive/py3/util.py:np_mergelist,np_mergelist,function,5,10,10,55,5.5,1,0,"['x0', 'x1']","[None, None]","[None, None]",2029,[],"['list', 'xnew.append']",2
utilmy/zarchive/py3/util.py:np_enumerate2,np_enumerate2,function,6,14,14,83,5.93,1,0,['vec_1d'],[None],[None],2035,[],"['np.empty', 'enumerate']",2
utilmy/zarchive/py3/util.py:np_pivottable_count,np_pivottable_count,function,12,24,23,170,7.08,1,0,['mylist'],[None],[None],2041,[],"['np_dict_tolist', 'sorted', 'int']",3
utilmy/zarchive/py3/util.py:np_nan_helper,np_nan_helper,function,2,5,5,40,8.0,0,0,['y'],[None],[None],2049,"['    """""" Input:  - y, 1d numpy array with possible NaNs\n', '        Output - nans, logical indices of NaNs - index, a function, with signature indices= index(logical_indices),\n', ""              to convert logical indices of NaNs to 'equivalent' indices\n"", '    """"""\n']","['np.isnan', 'z.nonzero']",2
utilmy/zarchive/py3/util.py:np_interpolate_nan,np_interpolate_nan,function,7,9,9,76,8.44,0,0,['y'],[None],[None],2056,[],"['np_nan_helper', 'np.interp', 'x']",3
utilmy/zarchive/py3/util.py:np_and1,np_and1,function,8,55,22,356,6.47,0,6,"['x', 'y', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']","[None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', 'None', 'None', 'None', 'None']",2061,[],[],0
utilmy/zarchive/py3/util.py:np_sortcol,np_sortcol,function,6,8,8,77,9.62,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",2069,"[' """""" df.sort([\'A\', \'B\'], ascending=[1, 0])  """"""\n']","['pd.DataFrame', 'df.sort_values']",2
utilmy/zarchive/py3/util.py:np_sort,np_sort,function,7,19,12,184,9.68,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",2075,[],"['pd.DataFrame', 'df.sort_values', 'np_sort']",3
utilmy/zarchive/py3/util.py:np_ma,np_ma,function,2,3,3,54,18.0,0,0,"['vv', 'n']","[None, None]","[None, None]",2081,"[""  '''Moving average '''\n""]","['np.convolve', 'old_div']",2
utilmy/zarchive/py3/util.py:np_cleanmatrix,np_cleanmatrix,function,11,22,20,124,5.64,2,1,['m'],[None],[None],2087,[],"['np.nan_to_num', 'np.shape', 'range', 'abs']",4
utilmy/zarchive/py3/util.py:np_torecarray,np_torecarray,function,2,6,6,105,17.5,0,0,"['arr', 'colname']","[None, None]","[None, None]",2095,[],['np.shape'],1
utilmy/zarchive/py3/util.py:np_sortbycolumn,np_sortbycolumn,function,6,8,8,77,9.62,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",2098,[],"['pd.DataFrame', 'df.sort_values']",2
utilmy/zarchive/py3/util.py:np_sortbycol,np_sortbycol,function,8,28,19,251,8.96,0,1,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, 'True']",2103,[],"['pd.DataFrame', 'df.sort_values', 'np_sortbycol', 'len', 'np.reshape']",5
utilmy/zarchive/py3/util.py:min_kpos,min_kpos,function,2,3,3,36,12.0,0,0,"['arr', 'kth']","[None, None]","[None, None]",2114,"[""   ''' return kth mininimun '''\n""]",['np.partition'],1
utilmy/zarchive/py3/util.py:max_kpos,max_kpos,function,3,5,5,53,10.6,0,0,"['arr', 'kth']","[None, None]","[None, None]",2118,"[""   ''' return kth mininimun '''\n""]","['len', 'np.partition']",2
utilmy/zarchive/py3/util.py:np_findfirst,np_findfirst,function,5,11,10,54,4.91,1,1,"['item', 'vec']","[None, None]","[None, None]",2125,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/py3/util.py:np_find,np_find,function,5,11,10,54,4.91,1,1,"['item', 'vec']","[None, None]","[None, None]",2132,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/py3/util.py:find,find,function,4,12,11,49,4.08,1,1,"['xstring', 'list_string']","[None, None]","[None, None]",2139,"['    """"""return the index of the first occurence of item in vec""""""\n']",['xi.find'],1
utilmy/zarchive/py3/util.py:findnone,findnone,function,4,11,10,54,4.91,1,1,['vec'],[None],[None],2145,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/py3/util.py:findx,findx,function,6,27,18,119,4.41,0,2,"['item', 'vec']","[None, None]","[None, None]",2151,"['    """"""return the index of the first occurence of item in vec""""""\n']","['type', 'vec.index', 'len']",3
utilmy/zarchive/py3/util.py:finds,finds,function,12,34,23,155,4.56,2,3,"['itemlist', 'vec']","[None, None]","[None, None]",2162,"['  """"""return the index of the first occurence of item in vec""""""\n']","['range', 'idlist.append']",2
utilmy/zarchive/py3/util.py:findhigher,findhigher,function,5,13,12,50,3.85,1,1,"['x', 'vec']","[None, None]","[None, None]",2173,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/py3/util.py:findlower,findlower,function,5,12,11,50,4.17,1,1,"['x', 'vec']","[None, None]","[None, None]",2179,"['    """"""return the index of the first occurence of item in vec""""""\n']",['range'],1
utilmy/zarchive/py3/util.py:np_find_minpos,np_find_minpos,function,4,7,5,95,13.57,0,0,['values'],[None],[None],2187,[],['min'],1
utilmy/zarchive/py3/util.py:np_find_maxpos,np_find_maxpos,function,17,44,33,260,5.91,1,3,['values'],[None],[None],2191,[],"['max', 'np_find_maxpos_2nd', 'float', 'enumerate']",4
utilmy/zarchive/py3/util.py:np_find_maxpos_2nd,np_find_maxpos_2nd,function,13,36,27,132,3.67,1,3,['numbers'],[None],[None],2195,[],"['float', 'enumerate']",2
utilmy/zarchive/py3/util.py:np_findlocalmax2,np_findlocalmax2,function,45,116,77,690,5.95,2,5,"['v', 'trig']","[None, None]","[None, None]",2207,[],"['n=len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'range', 'len', 'findhigher', 'np.abs']",8
utilmy/zarchive/py3/util.py:np_findlocalmin2,np_findlocalmin2,function,46,119,78,703,5.91,2,6,"['v', 'trig']","[None, None]","[None, None]",2241,[],"['n=len', 'np.zeros', 'np_find_minpos', 'enumerate', 'range', 'len', 'findlower', 'np.abs']",8
utilmy/zarchive/py3/util.py:np_findlocalmax,np_findlocalmax,function,45,116,77,690,5.95,2,5,"['v', 'trig']","[None, None]","[None, None]",2275,[],"['n=len', 'np.zeros', 'np_find_maxpos', 'enumerate', 'range', 'len', 'findhigher', 'np.abs']",8
utilmy/zarchive/py3/util.py:np_findlocalmin,np_findlocalmin,function,46,119,78,703,5.91,2,6,"['v', 'trig']","[None, None]","[None, None]",2289,[],"['n=len', 'np.zeros', 'np_find_minpos', 'enumerate', 'range', 'len', 'findlower', 'np.abs']",8
utilmy/zarchive/py3/util.py:np_stack,np_stack,function,12,53,25,323,6.09,0,7,"['v1', 'v2', 'v3', 'v4', 'v5']","[None, None, None, None, None]","[None, 'None', 'None', 'None', 'None']",2305,[],"['np.shape', 'np.row_stack', 'np.column_stack']",3
utilmy/zarchive/py3/util.py:np_uniquerows,np_uniquerows,function,6,8,8,148,18.5,0,0,['a'],[None],[None],2320,[],"['np.ascontiguousarray', 'np.unique', 'unique_a.view']",3
utilmy/zarchive/py3/util.py:np_remove_zeros,np_remove_zeros,function,2,4,4,35,8.75,0,0,"['vv', 'axis1']","[None, None]","[None, '1']",2325,[],[],0
utilmy/zarchive/py3/util.py:np_sort,np_sort,function,7,19,12,184,9.68,0,0,"['arr', 'colid', 'asc']","[None, None, None]","[None, None, '1']",2328,"[""'''\n"", 'from sqlalchemy import create_engine\n', 'engine = create_engine(""postgresql://u:p@host/database"")\n', ""'''\n""]","['pd.DataFrame', 'df.sort_values', 'np_sort']",3
utilmy/zarchive/py3/util.py:np_memory_array_adress,np_memory_array_adress,function,2,2,2,38,19.0,0,0,['x'],[None],[None],2331,[],[],0
utilmy/zarchive/py3/util.py:sql_create_dbengine,sql_create_dbengine,function,4,45,26,358,7.96,0,1,"['type1', 'dbname', 'login', 'password', 'url', 'port']","[None, None, None, None, None, None]","[""''"", ""''"", ""''"", ""''"", ""'localhost'"", '5432']",2345,"[""   ''' Return SQL Alchemy Connector\n"", '\n', ""sql_create_dbengine(type1='mysql',  dbname='', login='', password='', url='localhost', port=5432)\n"", '\n', '# psycopg2\n', ""engine = create_engine('postgresql+psycopg2://scott:tiger@localhost/mydatabase')\n"", '\n', '\n', '# MySQL-connector-python  Official one\n', ""engine = create_engine('mysql+mysqlconnector://scott:tiger@localhost/foo')\n"", 'conda install -c anaconda mysql-connector-python=2.0.4\n', ""engine = create_engine('postgresql://%s:%s@localhost:5432/%s' %(myusername, mypassword, mydatabase))\n"", '\n', ""engine = create_engine('sqlite:///  folder/foo.db')\n"", '\n', ""   '''\n""]","['sql.create_engine', 'str']",2
utilmy/zarchive/py3/util.py:sql_query,sql_query,function,6,22,18,155,7.05,0,2,"['sqlr', 'dbengine', 'output', ""dburl='sqlite""]","[None, None, None, '']","[""'SELECT ticker,shortratio,sector1_id, FROM stockfundamental'"", 'None', ""'df'"", ""'sqlite:///aaserialize/store/finviz.db'""]",2377,"["" '''\n"", "" :param sqlr:       'SELECT ticker,shortratio,sector1_id, FROM stockfundamental'\n"", ' :param output:     df   /   file1.csv\n', "" :param dburl:      'sqlite:///aaserialize/store/finviz.db'\n"", "" :param dbengine:   dbengine = sql.create_engine('postgresql+psycopg2://postgres:postgres@localhost/coke')\n"", ' :return:\n', "" '''\n""]","['sql.create_engine', 'pd.read_sql_query', 'output.find', 'df.to_csv']",4
utilmy/zarchive/py3/util.py:sql_get_dbschema,sql_get_dbschema,function,16,37,32,316,8.54,2,2,"[""dburl='sqlite"", 'dbengine', 'isprint']","['', None, None]","[""'sqlite:///aapackage/store/yahoo.db'"", 'None', '0']",2391,[],"['sql.create_engine', 'sql.inspect', 'inspector.get_table_names', 'inspector.get_columns', 'l1.append', 'print', 'np.array']",7
utilmy/zarchive/py3/util.py:sql_delete_table,sql_delete_table,function,1,8,8,89,11.12,0,0,"['name', 'dbengine']","[None, None]","[None, None]",2404,[],[],0
utilmy/zarchive/py3/util.py:sql_insert_excel,sql_insert_excel,function,50,131,97,992,7.57,5,2,"['file1', 'dbengine', 'dbtype']","[None, None, None]","[""'.xls'"", 'None', ""''""]",2411,"["" ''' http://flask-excel.readthedocs.io/en/latest/\n"", ' https://pythonhosted.org/pyexcel/tutorial_data_conversion.html#import-excel-sheet-into-a-database-table\n', ' from sqlalchemy import create_engine\n', 'from sqlalchemy.ext.declarative import declarative_base\n', 'from sqlalchemy import Column , Integer, String, Float, Date\n', 'from sqlalchemy.orm import sessionmaker\n', 'engine = create_engine(""sqlite:///birth.db"")\n', 'Base = declarative_base()\n', 'Session = sessionmaker(bind=engine)\n', '\n', 'class BirthRegister(Base):\n', ""...     __tablename__='birth'\n"", '...     id=Column(Integer, primary_key=True)\n', '...     name=Column(String)\n', '...     weight=Column(Float)\n', '...     birth=Column(Date)\n', '\n', 'Base.metadata.create_all(engine)\n', '\n', 'https://www.digitalocean.com/community/tutorials/how-to-use-celery-with-rabbitmq-to-queue-tasks-on-an-ubuntu-vps\n', 'import os\n', 'import pyexcel\n', 'import datetime\n', '\n', 'from sqlalchemy import create_engine\n', 'from sqlalchemy.ext.declarative import declarative_base\n', 'from sqlalchemy import Column, Integer, String, Float, Date\n', 'from sqlalchemy.orm import sessionmaker\n', '\n', '\n', 'engine = create_engine(""sqlite:///birth.db"")\n', 'Base = declarative_base()\n', 'Session = sessionmaker(bind=engine)\n', '\n', '\n', '# here is the destination table\n', 'class BirthRegister(Base):\n', ""    __tablename__ = 'birth'\n"", '    id = Column(Integer, primary_key=True)\n', '    name = Column(String)\n', '    weight = Column(Float)\n', '    birth = Column(Date)\n', '\n', '\n', 'Base.metadata.create_all(engine)\n', '\n', '\n', '# create fixture\n', 'data = [\n', '    [""name"", ""weight"", ""birth""],\n', '    [""Adam"", 3.4, datetime.date(2015, 2, 3)],\n', '    [""Smith"", 4.2, datetime.date(2014, 11, 12)]\n', ']\n', 'pyexcel.save_as(array=data,\n', '                dest_file_name=""birth.xls"")\n', '\n', '# import the xls file\n', 'session = Session()  # obtain a sql session\n', 'pyexcel.save_as(file_name=""birth.xls"",\n', '                name_columns_by_row=0,\n', '                dest_session=session,\n', '                dest_table=BirthRegister)\n', '\n', '# verify results\n', 'sheet = pyexcel.get_sheet(session=session, table=BirthRegister)\n', 'print(sheet)\n', '\n', 'session.close()\n', ""os.unlink('birth.db')\n"", 'os.unlink(""birth.xls"")\n', '\n', ' This code uses the openpyxl package for playing around with excel using Python code\n', ' to convert complete excel workbook (all sheets) to an SQLite database\n', ' The code assumes that the first row of every sheet is the column name\n', ' Every sheet is stored in a separate table\n', ' The sheet name is assigned as the table name for every sheet\n', "" '''\n""]","['slugify', 'text.strip', 're.sub', 'load_workbook', 'wb.get_sheet_names', 'str', 'columns.append', 'dbengine.execute', 'enumerate', 'tuprow.append', 'tup.append', 'VALUES', 'dbengine.executemany', 'dbengine.commit', 'dbengine.close']",15
utilmy/zarchive/py3/util.py:sql_insert_df,sql_insert_df,function,22,60,52,481,8.02,1,2,"['df', 'dbtable', 'dbengine', 'col_drop', 'verbose']","[None, None, None, None, None]","[None, None, None, ""['id']"", '1']",2534,[],"['df.drop', 'df.to_dict', 'print', 'sql.Table', 'Session', 'dbengine.execute', 'session.commit', 'session.close']",8
utilmy/zarchive/py3/util.py:sql_insert_csv,sql_insert_csv,function,35,164,130,1379,8.41,3,3,"['csvfile', 'dbtable', 'dbengine', 'col_drop']","[None, None, None, None]","[None, None, None, '[]']",2564,[],"['datetime.now', 'pd.read_csv', 'df.rename', 'c.replace', 'df.drop', 'df.to_dict', 'sql.Table', 'Session', 'dbengine.execute', 'session.commit', 'session.close', 'print', 'sql_insert_csv2', 'os.listdir', 'i.endswith', 'i.startswith', 'df.to_sql']",17
utilmy/zarchive/py3/util.py:sql_insert_csv2,sql_insert_csv2,function,11,63,57,484,7.68,1,2,"['csvfile', 'dbtable', 'columns', 'dbengine', 'nrows']","[None, None, None, None, None]","[""''"", ""''"", '[]', 'None', ' 10000']",2636,"['    """"""\n', '    Upload data to a temporary table first using PANDAs to identify optimal data-types for columns\n', '    PANDAS is not speed-efficient as it uses INSERT commands rather than COPY e.g. it took COPY 16mins average\n', '    to get a 15GB CSV into the database (door-to-door) whereas pandas.to_sql took 50mins\n', '    """"""\n']","['os.listdir', 'i.endswith', 'i.startswith', 'print', 'pd.read_csv', 'df.to_sql']",6
utilmy/zarchive/py3/util.py:sql_postgres_create_table,sql_postgres_create_table,function,28,125,96,1052,8.42,1,3,"['mytable', 'database', 'username', 'password']","[None, None, None, None]","[""''"", ""''"", ""''"", ""''""]",2669,"['    """""" Create table copying the structure of the temp table created using pandas  Timer to benchmark """"""\n']","['psycopg2.connect', 'con.cursor', 'print', 'sys.exit', 'cur.execute', 'time.time', 'os.listdir', 'i.endswith', 'i.startswith', 'open', 'cur.copy_expert', 'con.commit', 'old_div', 'con.close']",14
utilmy/zarchive/py3/util.py:sql_postgres_query_to_csv,sql_postgres_query_to_csv,function,12,42,41,360,8.57,0,1,"['sqlr', 'csv_out']","[None, None]","[""'SELECT ticker,shortratio,sector1_id, FROM stockfundamental'"", ""''""]",2740,"['    """""" Submit query to created PostgreSQL database and output results to a CSV  """"""\n']","['psycopg2.connect', 'con.cursor', 'print', 'open', 'cur.copy_expert', 'con.close']",6
utilmy/zarchive/py3/util.py:sql_postgres_pivot,sql_postgres_pivot,function,0,1,1,4,4.0,0,0,[],[],[],2755,"[""   '''\n"", 'Enabling the Crosstab Function\n', 'As we previously mentioned, the crosstab function is part of a PostgreSQL extension called tablefunc. To call the crosstab function,\n', 'you must first enable the tablefunc extension by executing the following SQL command:\n', 'CREATE extension tablefunc;\n', '\n', 'SELECT *\n', ""FROM crosstab( 'select student, subject, evaluation_result from evaluations order by 1,2')\n"", 'AS final_result(Student TEXT, Geography NUMERIC,History NUMERIC,Language NUMERIC,Maths NUMERIC,Music NUMERIC);\n', '\n', '\n', '##### Correct Even iF there are missing values :\n', 'http://www.vertabelo.com/blog/technical-articles/creating-pivot-tables-in-postgresql-using-the-crosstab-function\n', '\n', 'SELECT *\n', ""FROM crosstab( 'select student, subject, evaluation_result from evaluations\n"", ""                where extract (month from evaluation_day) = 7 order by 1,2',\n"", ""                'select name from subject order by 1')\n"", '     AS final_result(Student TEXT, Geography NUMERIC,History NUMERIC,Language NUMERIC,Maths NUMERIC,Music NUMERIC);\n', '\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/py3/util.py:sql_mysql_insert_excel,sql_mysql_insert_excel,function,30,73,69,712,9.75,1,0,[],[],[],2781,[],"['xlrd.open_workbook', 'list.sheet_by_index', 'database.cursor', 'range', 'sheet.cell', 'cursor.execute', 'cursor.close', 'database.commit', 'database.close', 'print', 'str', 'rows=str']",12
utilmy/zarchive/py3/util.py:sql_pivotable,sql_pivotable,function,1,3,3,10,3.33,0,0,"['dbcon', 'ss']","[None, None]","[None, ""'select  '""]",2814,"[""  '''\n"", '\n', ' 1) get the category\n', '\n', ' 2) Build the Pivot From category\n', '  SELECT *\n', ""  FROM crosstab( 'select student, subject, evaluation_result from evaluations\n"", ""                where extract (month from evaluation_day) = 7 order by 1,2',\n"", ""                'select name from subject order by 1')\n"", '   AS final_result(Student TEXT, Geography NUMERIC,History NUMERIC,Language NUMERIC,Maths NUMERIC,Music NUMERIC);\n', '\n', '  https://www.amazon.com/PostgreSQL-High-Performance-Gregory-Smith/dp/184951030X/ref=as_li_ss_tl?s=books&ie=UTF8&qid=1458352081&sr=1-6&keywords=postgres&linkCode=sl1&tag=postgres-bottom-20&linkId=c981783121cbd5542dc2b44a2297df57\n', '\n', '\n', 'http://blog.brakmic.com/data-science-for-losers-part-2/\n', '\n', 'Here we instruct Pandas to merge two tables by using certain primary keys from both when combining their rows into a new table. The parameter how instructs Pandas to use the inner-join which means it will only combine such rows which belong to both of the tables. Therefore well not receive any NaN-rows. But in some cases this could be desirable. Then use the alternative options like left, right or outer.\n', '\n', 'Pivots with Tables from SQLAlchemy\n', '\n', 'And of course its possible to generate the same pivot tables with data that came from SQLAlchemy.\n', 'Theyre nothing else but DataFrames all the way down. OK, not absolutely all the way down,\n', 'because there are also Series and NumPy arrays etc.,\n', 'but this is a little bit too much of knowledge for Losers like us. Maybe in some later articles.\n', '\n', '\n', ' :return:\n', ""  '''\n""]",[],0
utilmy/zarchive/py3/util.py:np_pivotable_create,np_pivotable_create,function,28,99,61,699,7.06,4,5,"['table', 'left', 'top', 'value']","[None, None, None, None]","[None, None, None, None]",2847,"['    """"""\n', '    Creates a cross-tab or pivot table from a normalised input table. Use this\n', ""    function to 'denormalize' a table of normalized records.\n"", '\n', '    * The table argument can be a list of dictionaries or a Table object.\n', '    (http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/334621)\n', '    * The left argument is a tuple of headings which are displayed down the\n', '    left side of the new table.\n', '    * The top argument is a tuple of headings which are displayed across the\n', '    top of the new table.\n', '    Tuples are used so that multiple element headings and columns can be used.\n', '\n', '    E.g. To transform the list (listOfDicts):\n', '\n', '    Name,   Year,  Value\n', '    -----------------------\n', ""    'Simon', 2004, 32\n"", ""    'Simon', 2005, 128\n"", ""    'Russel', 2004, 64\n"", ""    'Eric', 2004, 52\n"", ""    'Russel', 2005, 32\n"", '\n', '    into the new list:\n', '\n', ""    'Name',   2004, 2005\n"", '    ------------------------\n', ""    'Simon',  32,     128\n"", ""    'Russel',  64,     32\n"", ""    'Eric',   52,     NA\n"", '\n', '    you would call pivot with the arguments:\n', '\n', ""    newList = pivot(listOfDicts, ('Name',), ('Year',), 'Value')\n"", '\n', '    """"""\n']","['tuple', 'ysort.append', 'xsort.append', 'len', 'list', 'xsort.sort', 'headings.extend', 'sortedkeys.sort', 'row.extend', 't.append']",10
utilmy/zarchive/py3/util.py:pd_info,pd_info,function,13,25,24,255,10.2,1,2,"['df', 'doreturn']","[None, None]","[None, '1']",2932,[],"['df.info', 'print', 'type', 'pd_info_memsize', 'pd_info', 'df.memory_usage']",6
utilmy/zarchive/py3/util.py:pd_info_memsize,pd_info_memsize,function,5,6,6,84,14.0,0,1,"['df', 'memusage']","[None, None]","[None, '0']",2939,[],"['df.info', 'pd_info', 'df.memory_usage']",3
utilmy/zarchive/py3/util.py:pd_selectrow,pd_selectrow,function,11,99,53,857,8.66,1,3,"['df', '**conditions']","[None, None]","[None, None]",2947,"[""    '''Select rows from a df according to conditions\n"", '    pdselect(data, a=2, b__lt=3) __gt __ge __lte  __in  __not_in\n', ""    will select all rows where 'a' is 2 and 'b' is less than 3\n"", ""    '''\n""]","['type', 'pd.DataFrame', 'list', 'c.endswith', 'np.in1d', 'ValueError']",6
utilmy/zarchive/py3/util.py:pd_csv_randomread,pd_csv_randomread,function,11,41,37,266,6.49,0,1,"['filename', 'nsample', 'filemaxline', 'dtype']","[None, None, None, None]","[None, '10000', '-1', 'None']",2991,[],"['sum', 'open', 'np.sort', 'pd.read_csv']",4
utilmy/zarchive/py3/util.py:pd_array_todataframe,pd_array_todataframe,function,14,46,33,313,6.8,0,2,"['array', 'colname', 'index1', 'dotranspose']","[None, None, None, None]","[None, 'None', 'None', 'False']",3001,[],"['np.shape', 'len', 'pd.DataFrame', 'np.array']",4
utilmy/zarchive/py3/util.py:pd_dataframe_toarray,pd_dataframe_toarray,function,10,10,10,103,10.3,0,0,['df'],[None],[None],3011,[],['df.reset_index'],1
utilmy/zarchive/py3/util.py:pd_createdf,pd_createdf,function,2,4,4,55,13.75,0,0,"['array1', 'col1', 'idx1']","[None, None, None]","[None, 'None', 'None']",3017,[],['pd.DataFrame'],1
utilmy/zarchive/py3/util.py:pd_create_colmapdict_nametoint,pd_create_colmapdict_nametoint,function,9,12,12,77,6.42,1,0,['df'],[None],[None],3020,"[""  ''' 'close' ---> 5    '''\n""]",['enumerate'],1
utilmy/zarchive/py3/util.py:pd_extract_col_idx_val,pd_extract_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],3030,[],[],0
utilmy/zarchive/py3/util.py:pd_extract_col_uniquevalue_tocsv,pd_extract_col_uniquevalue_tocsv,function,5,10,9,101,10.1,0,0,"['df', 'colname', 'csvfile']","[None, None, None]","[None, ""''"", ""''""]",3034,"["" ''' Write one column into a file   '''\n""]","['np.array', 'pd.DataFrame', 'print']",3
utilmy/zarchive/py3/util.py:pd_split_col_idx_val,pd_split_col_idx_val,function,4,4,4,49,12.25,0,0,['df'],[None],[None],3043,[],[],0
utilmy/zarchive/py3/util.py:pd_splitdf_inlist,pd_splitdf_inlist,function,15,34,26,254,7.47,3,2,"['df', 'colid', 'type1']","[None, None, None]","[None, None, '""dict""']",3046,"[""    ''' Split df into dictionnary of dict/list '''\n""]","['list', 'l1.append']",2
utilmy/zarchive/py3/util.py:pd_find,pd_find,function,38,140,82,990,7.07,5,8,"['df', 'regex_pattern', 'col_restrict', 'isnumeric', 'doreturnposition']","[None, None, None, None, None]","[None, ""'*'"", '[]', 'False', 'False']",3061,"["" ''' Find string / numeric values inside df columns, return position where found\n"", ""     col_restrict : restrict to these columns '''\n""]","['str', 'print', 'enumerate', 'np.column_stack', 'len', 'gc.collect', 'np_dictordered_create', 'range', 'np.array']",9
utilmy/zarchive/py3/util.py:pd_dtypes_totype2,pd_dtypes_totype2,function,6,9,9,58,6.44,1,0,"['df', 'columns', 'targetype']","[None, None, None]","[None, '[]', ""'category'""]",3107,[],[],0
utilmy/zarchive/py3/util.py:pd_dtypes,pd_dtypes,function,7,12,11,91,7.58,1,0,"['df', 'columns', 'targetype']","[None, None, None]","[None, '[]', ""'category'""]",3112,[],['pd_dtypes'],1
utilmy/zarchive/py3/util.py:pd_df_todict,pd_df_todict,function,16,30,26,246,8.2,1,2,"['df', 'colkey', 'excludekey', 'onlyfirstelt']","[None, None, None, None]","[None, ""'table'"", ""['']"", ' True']",3127,[],"['df.drop_duplicates', 'range', 'dict0.setdefault']",3
utilmy/zarchive/py3/util.py:pd_applyfun_col,pd_applyfun_col,function,5,11,9,107,9.73,0,1,"['df', 'newcol', 'ff', 'use_colname']","[None, None, None, None]","[None, None, None, '""all/[colname]""']",3174,"[""   ''' use all Columns to compute values '''\n""]",['ff'],1
utilmy/zarchive/py3/util.py:pd_cleanquote,pd_cleanquote,function,10,25,23,173,6.92,1,1,['q'],[None],[None],3180,[],"['pd.to_numeric', 'q.fillna']",2
utilmy/zarchive/py3/util.py:pd_date_intersection,pd_date_intersection,function,7,18,15,157,8.72,1,0,['qlist'],[None],[None],3189,[],"['set', 'set.intersection', 'list', 'sorted']",4
utilmy/zarchive/py3/util.py:pd_is_categorical,pd_is_categorical,function,2,12,10,108,9.0,0,1,['z'],[None],[None],3197,[],['isinstance'],1
utilmy/zarchive/py3/util.py:pd_str_encoding_change,pd_str_encoding_change,function,6,8,8,76,9.5,1,0,"['df', 'cols', 'fromenc', 'toenc']","[None, None, None, None]","[None, None, ""'iso-8859-1'"", ""'utf-8'""]",3204,[],[],0
utilmy/zarchive/py3/util.py:pd_str_unicode_tostr,pd_str_unicode_tostr,function,2,4,4,60,15.0,0,0,"['df', 'targetype']","[None, None]","[None, 'str']",3210,"["" '''\n"", ' https://www.azavea.com/blog/2014/03/24/solving-unicode-problems-in-python-2-7/\n', ' Nearly every Unicode problem can be solved by the proper application of these tools;\n', ' they will help you build an airlock to keep the inside of your code nice and clean:\n', '\n', 'encode(): Gets you from Unicode -> bytes\n', 'decode(): Gets you from bytes -> Unicode\n', 'codecs.open(encoding=utf-8): Read and write files directly to/from Unicode (you can use any encoding,\n', ' not just utf-8, but utf-8 is most common).\n', 'u: Makes your string literals into Unicode objects rather than byte sequences.\n', 'Warning: Dont use encode() on bytes or decode() on Unicode objects\n', '\n', '>>> uni_greeting % utf8_name\n', 'Traceback (most recent call last):\n', ' File ""<stdin>"", line 1, in <module>\n', ""UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 3: ordinal not in range(128)\n"", '# Solution:\n', "">>> uni_greeting % utf8_name.decode('utf-8')\n"", ""u'Hi, my name is Josxe9.'\n"", '\n', "" '''\n""]",['pd_dtypes_type1_totype2'],1
utilmy/zarchive/py3/util.py:pd_dtypes_type1_totype2,pd_dtypes_type1_totype2,function,7,13,12,100,7.69,1,1,"['df', 'fromtype', 'targetype']","[None, None, None]","[None, 'str', 'str']",3234,[],['isinstance'],1
utilmy/zarchive/py3/util.py:pd_resetindex,pd_resetindex,function,3,4,4,50,12.5,0,0,['df'],[None],[None],3243,[],['list'],1
utilmy/zarchive/py3/util.py:pd_insertdatecol,pd_insertdatecol,function,8,11,11,192,17.45,0,0,"['df_insider', 'format1=""%Y-%m-%d %H']","[None, '']","[None, '""%Y-%m-%d %H:%M:%S:%f""']",3247,[],"['pd_addcol', 'date_nowtime']",2
utilmy/zarchive/py3/util.py:pd_replacevalues,pd_replacevalues,function,11,14,14,101,7.21,1,0,"['df', 'matrix']","[None, None]","[None, None]",3253,"["" ''' Matrix replaces df.values  '''\n""]",['np.shape'],1
utilmy/zarchive/py3/util.py:pd_removerow,pd_removerow,function,2,2,2,29,14.5,0,0,"['df', 'row_list_index', '45]']","[None, None, None]","[None, '[23', None]",3262,[],['df.drop'],1
utilmy/zarchive/py3/util.py:pd_removecol,pd_removecol,function,2,3,3,28,9.33,0,0,"['df1', 'name1']","[None, None]","[None, None]",3265,[],['df1.drop'],1
utilmy/zarchive/py3/util.py:pd_addcol,pd_addcol,function,3,6,6,30,5.0,0,0,"['df1', 'name1']","[None, None]","[None, ""'new'""]",3268,[],[],0
utilmy/zarchive/py3/util.py:pd_insertcol,pd_insertcol,function,4,6,6,30,5.0,0,0,"['df', 'colname', 'vec']","[None, None, None]","[None, None, None]",3280,"["" ''' Vec and Colname must be aligned '''\n""]",[],0
utilmy/zarchive/py3/util.py:pd_insertrow,pd_insertrow,function,6,10,10,102,10.2,0,0,"['df', 'rowval', 'index1', 'isreset']","[None, None, None, None]","[None, None, 'None', '1']",3296,[],"['pd_array_todataframe', 'df.append']",2
utilmy/zarchive/py3/util.py:pd_h5_cleanbeforesave,pd_h5_cleanbeforesave,function,5,12,11,91,7.58,0,0,['df'],[None],[None],3304,"[""   '''Clean Column type before Saving in HDFS: Unicode, Datetime  '''\n""]","['pd_resetindex', 'pd_str_unicode_tostr']",2
utilmy/zarchive/py3/util.py:pd_h5_addtable,pd_h5_addtable,function,5,18,17,148,8.22,0,1,"['df', 'tablename', ""dbfile='F""]","[None, None, '']","[None, None, ""'F:\\temp_pandas.h5'""]",3320,[],"['pd.HDFStore', 'find', 'list', 'print', 'store.append', 'store.close']",6
utilmy/zarchive/py3/util.py:pd_h5_tableinfo,pd_h5_tableinfo,function,4,4,4,65,16.25,0,0,"['filenameh5', 'table']","[None, None]","[None, None]",3329,[],"['pd.HDFStore', 'store.get_storer']",2
utilmy/zarchive/py3/util.py:pd_h5_dumpinfo,pd_h5_dumpinfo,function,12,23,23,304,13.22,1,0,"[""dbfile='E""]",[''],"[""'E:\\_data\\stock\\intraday_google.h5'""]",3334,[],"['pd.HDFStore', 'list', 'pd.read_hdf', 'extract.append', 'datetime_tostring', 'errsym.append', 'np.array']",7
utilmy/zarchive/py3/util.py:pd_h5_save,pd_h5_save,function,3,6,6,64,10.67,0,0,"['df', ""filenameh5='E"", 'key']","[None, '', None]","[None, ""'E:/_data/_data_outlier.h5'"", ""'data'""]",3347,"["" ''' File is release after saving it'''\n""]","['pd.HDFStore', 'store.append', 'store.close']",3
utilmy/zarchive/py3/util.py:pd_h5_load,pd_h5_load,function,4,22,16,185,8.41,0,2,"[""filenameh5='E"", 'table_id', 'exportype', 'rowstart', 'rowend', 'cols']","['', None, None, None, None, None]","[""'E:/_data/_data_outlier.h5'"", ""'data'"", '""pandas""', '-1', '-1', '[]']",3352,[],['pd.read_hdf'],1
utilmy/zarchive/py3/util.py:pd_h5_fromcsv_tohdfs,pd_h5_fromcsv_tohdfs,function,30,78,65,670,8.59,3,2,"['dircsv', 'filepattern', 'tofilehdfs', 'tablename', 'col_category', 'dtype0', 'encoding', 'chunksize', 'mode', 'format', 'complib']","[None, None, None, None, None, None, None, None, None, None, None]","[""'dir1/dir2/'"", ""'*.csv'"", ""'file1.h5'"", ""'df'"", '[]', 'None', ""'utf-8'"", ' 2000000', ""'a'"", ""'table'"", 'None']",3358,[],"['os_file_listall', 'pd.HDFStore', 'enumerate', 'pd.read_csv', 'print', 'store.append', 'store.close', 'os_file_exist']",8
utilmy/zarchive/py3/util.py:pd_np_toh5file,pd_np_toh5file,function,2,8,8,76,9.5,0,0,"['numpyarr', 'fileout', 'table1']","[None, None, None]","[None, '""file.h5""', ""'data'""]",3395,[],"['pd.DataFrame', 'pd.HDFStore', 'st.append']",3
utilmy/zarchive/py3/util.py:date_allinfo,date_allinfo,function,0,0,0,0,0.0,0,0,[],[],[],3403,"[""   '''\n"", '\n', '   https://aboutsimon.com/blog/2016/08/04/datetime-vs-Arrow-vs-Pendulum-vs-Delorean-vs-udatetime.html\n', '\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/py3/util.py:date_convert,date_convert,function,14,52,37,298,5.73,1,1,"['t1', 'fromtype', 'totype']","[None, None, None]","[None, None, None]",3412,[],"['len', 'isinstance', '_dateconvert_from', 'tlist.append']",4
utilmy/zarchive/py3/util.py:datetime_tostring,datetime_tostring,function,8,26,21,244,9.38,1,2,['datelist1'],[None],[None],3436,[],"['isinstance', 'datelist1.strftime', 'pd.to_datetime', 't.strftime', 'date2.append']",5
utilmy/zarchive/py3/util.py:date_remove_bdays,date_remove_bdays,function,16,41,28,351,8.56,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",3446,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/py3/util.py:date_add_bdays,date_add_bdays,function,16,40,29,350,8.75,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",3460,[],"['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/py3/util.py:datenumpy_todatetime,datenumpy_todatetime,function,11,36,22,420,11.67,0,3,"['tt', 'islocaltime']","[None, None]","[None, 'True']",3474,[],['type'],1
utilmy/zarchive/py3/util.py:datetime_tonumpydate,datetime_tonumpydate,function,2,2,2,22,11.0,0,0,"['t', 'islocaltime']","[None, None]","[None, 'True']",3490,[],['np.datetime64'],1
utilmy/zarchive/py3/util.py:datestring_todatetime,datestring_todatetime,function,6,15,13,125,8.33,1,1,"['datelist1', 'format1']","[None, None]","[None, ' ""%Y%m%d""']",3494,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/py3/util.py:datetime_toint,datetime_toint,function,6,15,13,155,10.33,1,1,['datelist1'],[None],[None],3502,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/py3/util.py:date_holiday,date_holiday,function,0,0,0,0,0.0,0,0,[],[],[],3509,"[""   '''\n"", '   https://jakevdp.github.io/blog/2015/07/23/learning-seattles-work-habits-from-bicycle-counts/\n', '\n', 'from pandas.tseries.holiday import USFederalHolidayCalendar\n', 'cal = USFederalHolidayCalendar()\n', ""holidays = cal.holidays('2012', '2016', return_name=True)\n"", 'holidays.head()\n', '\n', 'holidays_all = pd.concat([holidays, ""Day Before "" + holidays.shift(-1, \'D\'),  ""Day After "" + holidays.shift(1, \'D\')])\n', 'holidays_all = holidays_all.sort_index()\n', 'holidays_all.head()\n', '\n', 'from pandas.tseries.offsets import CustomBusinessDay\n', 'from pandas.tseries.holiday import USFederalHolidayCalendar\n', 'bday_us = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n', 'dateref[-1] - bday_us- bday_us\n', '\n', ""   '''\n""]",[],0
utilmy/zarchive/py3/util.py:date_add_bday,date_add_bday,function,16,40,29,350,8.75,1,3,"['from_date', 'add_days']","[None, None]","[None, None]",3529,"["" ''' str / stamp /  '''\n""]","['isint', 'dateint_todatetime', 'datetime.timedelta', 'current_date.weekday', 'datetime_toint']",5
utilmy/zarchive/py3/util.py:dateint_todatetime,dateint_todatetime,function,6,15,13,135,9.0,1,1,['datelist1'],[None],[None],3542,[],"['isinstance', 'parser.parse', 'date2.append']",3
utilmy/zarchive/py3/util.py:date_diffinday,date_diffinday,function,4,5,5,74,14.8,0,0,"['intdate1', 'intdate2']","[None, None]","[None, None]",3550,[],['dateint_todatetime'],1
utilmy/zarchive/py3/util.py:date_diffinyear,date_diffinyear,function,2,3,3,53,17.67,0,0,"['startdate', 'enddate']","[None, None]","[None, None]",3554,[],['date_as_float'],1
utilmy/zarchive/py3/util.py:date_diffinbday,date_diffinbday,function,7,11,9,112,10.18,0,0,"['intd2', 'intd1']","[None, None]","[None, None]",3557,[],"['dateint_todatetime', 'd1.date', 'd2.date', 'np.busday_count']",4
utilmy/zarchive/py3/util.py:date_gencalendar,date_gencalendar,function,9,13,11,233,17.92,0,0,"['start', 'end', 'country']","[None, None, None]","[""'2010-01-01'"", ""'2010-01-15'"", ""'us'""]",3565,[],"['CustomBusinessDay', 'np.arrray']",2
utilmy/zarchive/py3/util.py:date_finddateid,date_finddateid,function,5,75,20,437,5.83,0,10,"['date1', 'dateref']","[None, None]","[None, None]",3571,[],['np_findfirst'],1
utilmy/zarchive/py3/util.py:datestring_toint,datestring_toint,function,6,16,14,106,6.62,1,1,['datelist1'],[None],[None],3585,[],"['isinstance', 'int', 'date2.append']",3
utilmy/zarchive/py3/util.py:date_now,date_now,function,15,45,31,396,8.8,0,2,['i'],[None],['0'],3591,[],"['datetime.now', 'date_add_bdays', 'date_remove_bdays', 'str', 'date_nowtime', 'datetime.today', 'd.strftime']",7
utilmy/zarchive/py3/util.py:date_nowtime,date_nowtime,function,8,23,18,205,8.91,0,1,"['type1', 'format1= ""%Y-%m-%d %H']","[None, '']","[""'str'"", ' ""%Y-%m-%d %H:%M:%S:%f""']",3598,"["" ''' str / stamp /  '''\n""]","['datetime.today', 'd.strftime']",2
utilmy/zarchive/py3/util.py:date_tofloat,date_tofloat,function,12,28,24,291,10.39,0,1,['dt'],[None],[None],3607,[],"['old_div', 'datetime.datetime', 'isleap', 'timedelta']",4
utilmy/zarchive/py3/util.py:date_generatedatetime,date_generatedatetime,function,18,27,27,267,9.89,0,1,"['start', 'nbday', 'end']","[None, None, None]","['""20100101""', '10', '""""']",3615,[],"['datestring_todatetime', 'date_add_bdays', 'datetime.timedelta', 'list', 'np.array']",5
utilmy/zarchive/py3/util.py:np_numexpr_vec_calc,np_numexpr_vec_calc,function,13,28,22,214,7.64,0,0,"['filename', 'expr', 'i0', 'imax', ""fileout='E""]","[None, None, None, None, '']","[None, None, '0', '1000', ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",3628,"["" ''' New= xx*xx  over very large series\n"", ' #numexpr_vect_calc(filename, 0, imax=16384*4096, ""xx*xx"", \'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5\'  ):\n', ""'''\n""]","['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/py3/util.py:np_numexpr_tohdfs,np_numexpr_tohdfs,function,10,28,22,248,8.86,0,0,"['filename', 'expr', 'i0', 'imax', ""fileout='E""]","[None, None, None, None, '']","[None, None, '0', '1000', ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",3642,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/py3/util.py:np_comoment,np_comoment,function,4,6,6,70,11.67,0,0,"['xx', 'yy', 'nsample', 'kx', 'ky']","[None, None, None, None, None]","[None, None, None, None, None]",3658,[],['old_div'],1
utilmy/zarchive/py3/util.py:np_acf,np_acf,function,12,34,31,253,7.44,0,0,['data'],[None],[None],3664,[],"['len', 'np.mean', 'old_div', 'float', 'r', 'np.arange', 'np.asarray']",7
utilmy/zarchive/py3/util.py:plot_XY,plot_XY,function,25,83,60,688,8.29,0,3,"['xx', 'yy', 'zcolor', 'tsize', 'title1', 'xlabel', 'ylabel', 'figsize', '6']","[None, None, None, None, None, None, None, None, None]","[None, None, 'None', 'None', ""''"", ""''"", ""''"", '(8', None]",3680,[],"['isinstance', 'np.array', 'np.abs', 'plt.get_cmap', 'np.min', 'plt.subplots', 'fig.set_size_inches', 'fig.set_dpi', 'fig.tight_layout', 'ax1.scatter', 'ax1.set_xlabel', 'ax1.set_ylabel', 'ax1.set_title', 'ax1.grid', 'plt.savefig', 'plt.clf', 'plt.show']",17
utilmy/zarchive/py3/util.py:plot_heatmap,plot_heatmap,function,15,26,25,364,14.0,0,1,"['frame', 'ax', 'cmap', 'vmin', 'vmax', 'interpolation']","[None, None, None, None, None, None]","[None, 'None', 'None', 'None', 'None', ""'nearest'""]",3720,[],"['plt.gca', 'ax.set_xticks', 'ax.set_xticklabels', 'ax.set_yticks', 'ax.set_yticklabels', 'ax.grid', 'ax.set_aspect', 'ax.imshow']",8
utilmy/zarchive/py3/util.py:gc_map_dict_to_bq_schema,gc_map_dict_to_bq_schema,function,13,59,31,714,12.1,3,3,"['source_dict', 'schema', 'dest_dict']","[None, None, None]","[None, None, None]",3737,"[""    '''\n"", '     new_dict = {}\n', '     map_dict_to_bq_schema (my_dict, schema, new_dict)\n', '\n', '     new_dict = {}\n', '     map_dict_to_bq_schema (my_dict, schema, new_dict)\n', '\n', '    :param source_dict:\n', '    :param schema:\n', '    :param dest_dict:\n', '    :return:\n', ""    '''\n""]","['map_dict_to_bq_schema', 'format_value_bq']",2
utilmy/zarchive/py3/util.py:aws_accesskey_get,aws_accesskey_get,function,7,17,13,107,6.29,0,1,"['access', 'key']","[None, None]","[""''"", ""''""]",3782,[],[],0
utilmy/zarchive/py3/util.py:aws_conn_do,aws_conn_do,function,2,3,3,39,13.0,0,0,"['action', 'region']","[None, None]","[""''"", '""ap-northeast-2""']",3790,[],['aws_conn_create'],1
utilmy/zarchive/py3/util.py:aws_conn_getallregions,aws_conn_getallregions,function,2,2,2,28,14.0,0,0,['conn'],[None],['None'],3794,[],['conn.get_all_regions'],1
utilmy/zarchive/py3/util.py:aws_conn_create,aws_conn_create,function,21,36,34,282,7.83,1,2,"['region', 'access', 'key']","[None, None, None]","['""ap-northeast-2""', ""''"", ""''""]",3797,[],"['aws_accesskey_get', 'EC2Connection', 'aws_conn_getallregions', 'conn=EC2Connection', 'print']",5
utilmy/zarchive/py3/util.py:aws_conn_getinfo,aws_conn_getinfo,function,1,2,2,28,14.0,0,0,['conn'],[None],[None],3810,[],['print'],1
utilmy/zarchive/py3/util.py:aws_s3_url_split,aws_s3_url_split,function,4,5,5,52,10.4,0,0,['url'],[None],[None],3850,"[""  '''Split into Bucket, url '''\n""]",['url.split'],1
utilmy/zarchive/py3/util.py:aws_s3_getbucketconn,aws_s3_getbucketconn,function,13,17,16,221,13.0,0,0,['s3dir'],[None],[None],3855,[],"['aws_s3_url_split', 'aws_accesskey_get', 'boto.connect_s3', 'conn.get_bucket']",4
utilmy/zarchive/py3/util.py:aws_s3_puto_s3,aws_s3_puto_s3,function,36,114,102,1158,10.16,3,2,"['fromdir_file', 'todir']","[None, None]","[""'dir/file.zip'"", ""'bucket/folder1/folder2'""]",3863,"["" ''' Copy File or Folder to S3 '''\n""]","['aws_s3_getbucketconn', 'aws_s3_url_split', 'fromdir_file.find', 'os_file_getname', 'fromdir_file=os_file_getpath', 'os.walk', 'uploadFileNames.extend', 'percent_cb', 'print', 'bucket.initiate_multipart_upload', 'open', 'mp.upload_part_from_file', 'mp.complete_upload', 'k.set_contents_from_filename']",14
utilmy/zarchive/py3/util.py:aws_s3_getfrom_s3,aws_s3_getfrom_s3,function,21,45,45,430,9.56,1,1,"['froms3dir', 'todir', 'bucket_name']","[None, None, None]","[""'task01/'"", ""''"", ""'zdisk'""]",3907,"["" ''' Get from S3 file/folder  '''\n""]","['aws_s3_url_split', 'aws_s3_getbucketconn', 'bucket.list', 'str', 'os_file_getname', 'os_file_getpath', 'os.makedirs', 'l.get_contents_to_filename']",8
utilmy/zarchive/py3/util.py:aws_s3_folder_printtall,aws_s3_folder_printtall,function,12,18,17,234,13.0,1,0,['bucket_name'],[None],"[""'zdisk'""]",3926,[],"['aws_accesskey_get', 'boto.connect_s3', 'conn.create_bucket', 'bucket.list', 'print']",5
utilmy/zarchive/py3/util.py:aws_s3_file_read,aws_s3_file_read,function,10,14,13,187,13.36,0,0,"['filepath', 'isbinary']","[None, None]","[None, '1']",3935,"[""  ''' s3_client = boto3.client('s3')\n"", '    #Download private key file from secure S3 bucket\n', ""  s3_client.download_file('s3-key-bucket','keys/keyname.pem', '/tmp/keyname.pem')\n"", ""  '''\n""]","['S3Connection', 'conn.get_object']",2
utilmy/zarchive/py3/util.py:aws_ec2_python_script,aws_ec2_python_script,function,5,20,18,195,9.75,0,0,"['script_path', 'args1', 'host']","[None, None, None]","[None, None, None]",4179,[],['aws_ec2_cmd_ssh'],1
utilmy/zarchive/py3/util.py:aws_ec2_create_con,aws_ec2_create_con,function,35,116,89,1002,8.64,1,9,"['contype', 'host', 'port', 'username', 'keyfilepath', 'password', 'keyfiletype', 'isprint']","[None, None, None, None, None, None, None, None]","[""'sftp/ssh'"", ""'ip'"", '22', ""'ubuntu'"", ""''"", ""''"", ""'RSA'"", '1']",4184,"['    """""" Transfert File  host = \'52.79.79.1\'\n', ""        keyfilepath = 'D:/_devs/aws/keypairs/ec2_instanc'\n"", '\n', '# List files in the default directory on the remote computer.\n', ""dirlist = sftp.listdir('.')\n"", ""sftp.get('remote_file.txt', 'downloaded_file.txt')\n"", ""sftp.put('testfile.txt', 'remote_testfile.txt')\n"", '\n', 'http://docs.paramiko.org/en/2.1/api/sftp.html\n', '    """"""\n']","['paramiko.Transport', 'transport.add_server_key', 'transport.connect', 'print', 'sftp.listdir', 'paramiko.SSHClient', 'ssh.set_missing_host_key_policy', 'ssh.connect', 'ssh.exec_command', 'stdin.flush', 'stdout.read', 'sftp.close', 'transport.close', 'ssh.close']",14
utilmy/zarchive/py3/util.py:aws_ec2_allocate_elastic_ip,aws_ec2_allocate_elastic_ip,function,7,16,16,233,14.56,0,0,"['instance_id', 'region']","[None, None]","[None, '""ap-northeast-2""']",4233,[],"['aws_conn_create', 'con.associate_address', 'print']",3
utilmy/zarchive/py3/util.py:googledrive_get,googledrive_get,function,0,1,1,4,4.0,0,0,[],[],[],4388,"[""   '''\n"", '   https://github.com/ctberthiaume/gdcp\n', '   ... I am using this now to transfer thousands of mp3 files from a ubuntu vps to google drive.\n', '\n', '\n', 'http://olivermarshall.net/how-to-upload-a-file-to-google-drive-from-the-command-line/\n', 'https://github.com/prasmussen/gdrive  : Super Complete\n', '\n', 'gdrive [global] upload [options] <path>\n', '\n', 'global:\n', '  -c, --config <configDir>         Application path, default: /Users/<user>/.gdrive\n', '  --refresh-token <refreshToken>   Oauth refresh token used to get access token (for advanced users)\n', '  --access-token <accessToken>     Oauth access token, only recommended for short-lived requests because of short lifetime (for advanced users)\n', '\n', 'options:\n', '  -r, --recursive           Upload directory recursively\n', '  -p, --parent <parent>     Parent id, used to upload file to a specific directory, can be specified multiple times to give many parents\n', '  --name <name>             Filename\n', '  --no-progress             Hide progress\n', '  --mime <mime>             Force mime type\n', '  --share                   Share file\n', '  --delete                  Delete local file when upload is successful\n', '  --chunksize <chunksize>   Set chunk size in bytes, default: 8388608\n', '\n', '   :return:\n', ""   '''\n""]",[],0
utilmy/zarchive/py3/util.py:googledrive_put,googledrive_put,function,0,1,1,4,4.0,0,0,[],[],[],4418,"[""  '''\n"", '  100 GB: 2USD,  1TB: 10USD\n', '  https://gsuite.google.com/intl/en/pricing.html\n', '\n', '  :return:\n', ""  '''\n""]",[],0
utilmy/zarchive/py3/util.py:googledrive_list,googledrive_list,function,0,1,1,4,4.0,0,0,[],[],[],4428,[],[],0
utilmy/zarchive/py3/util.py:os_processify_fun,os_processify_fun,function,32,78,63,713,9.14,0,1,['func'],[None],[None],4433,"[""    '''Decorator to run a function as a process.\n"", '    Be sure that every argument and the return value is *pickable*.\n', '    The created process is joined, so the code does not  run in parallel.\n', '    @processify\n', '\n', '    def test():\n', '      return os.getpid()\n', '\n', '    @processify\n', '    def test_deadlock():\n', '      return range(30000)\n', '\n', '   @processify\n', '   def test_exception():\n', ""     raise RuntimeError('xyz')\n"", '\n', '   def test():\n', '     print os.getpid()\n', '     print test_function()\n', '     print len(test_deadlock())\n', '     test_exception()\n', '\n', ""   if __name__ == '__main__':\n"", '     test()\n', '\n', ""    '''\n""]","['process_func', 'func', 'sys.exc_info', 'q.put', 'setattr', 'wrapper', 'Queue', 'Process', 'list', 'p.start', 'q.get', 'ex_type']",12
utilmy/zarchive/py3/util.py:ztest_processify,ztest_processify,function,2,2,2,17,8.5,0,0,[],[],[],4500,[],['os.getpid'],1
utilmy/zarchive/py3/util.py:date_getspecificdate,date_getspecificdate,function,21,194,56,1312,6.76,8,19,"['datelist', 'datetype1', 'outputype1', 'includelastdate', 'includefirstdate', '']","[None, None, None, None, None, None]","[None, '""yearend""', '""intdate""', 'True', 'False', None]",4520,[],"['isint', 'dateint_todatetime', 'enumerate', 'vec2.append', 'np.mod', 'np.array']",6
utilmy/zarchive/py3/util.py:testclass,testclass,class,23,51,40,396,7.76,0,0,[],[],[],127,[],[],0
utilmy/zarchive/py3/util.py:FundingRecord,FundingRecord,class,14,59,48,394,6.68,0,0,[],[],[],1391,[],[],0
utilmy/zarchive/py3/util.py:aws_ec2_ssh,aws_ec2_ssh,class,129,339,228,4029,11.88,10,10,[],[],[],3950,[],[],0
utilmy/zarchive/py3/util.py:testclass:__init__,testclass:__init__,method,0,1,1,4,4.0,0,0,"['self', 'x']","[None, None]","[None, None]",128,[],[],0
utilmy/zarchive/py3/util.py:testclass:z_autotest,testclass:z_autotest,method,21,44,36,349,7.93,0,0,['self'],[None],[None],131,[],"['io.StringIO', 'f', 'exec', 'print', 'codeErr.getvalue', 'codeOut.getvalue', 'codeOut.close', 'codeErr.close']",8
utilmy/zarchive/py3/util.py:FundingRecord:parse,FundingRecord:parse,method,7,35,26,223,6.37,0,0,"['klass', 'row']","[None, None]","[None, None]",1394,[],"['list', 'int', 'datetime.strptime', 'klass']",4
utilmy/zarchive/py3/util.py:FundingRecord:__str__,FundingRecord:__str__,method,6,18,18,118,6.56,0,0,['self'],[None],[None],1401,[],[],0
utilmy/zarchive/py3/util.py:aws_ec2_ssh:__init__,aws_ec2_ssh:__init__,method,21,27,25,415,15.37,0,0,"['self', 'hostname', 'username', 'key_file', 'password']","[None, None, None, None, None]","[None, None, ""'ubuntu'"", 'None', 'None']",3966,[],"['socket.socket', 'paramiko.Transport', 'print']",3
utilmy/zarchive/py3/util.py:aws_ec2_ssh:command,aws_ec2_ssh:command,method,12,20,18,247,12.35,1,0,"['self', 'cmd']","[None, None]","[None, None]",4005,[],"['chan.get_pty', 'chan.invoke_shell', 'chan.settimeout', 'chan.send', 'cmd.split']",5
utilmy/zarchive/py3/util.py:aws_ec2_ssh:put,aws_ec2_ssh:put,method,17,33,29,650,19.7,2,2,"['self', 'localfile', 'remotefile']","[None, None, None]","[None, None, None]",4025,[],"['put_all', 'os.chdir', 'os.walk', 'print', 'self.put']",5
utilmy/zarchive/py3/util.py:aws_ec2_ssh:put_all,aws_ec2_ssh:put_all,method,15,30,26,575,19.17,2,2,"['self', 'localpath', 'remotepath']","[None, None, None]","[None, None, None]",4029,[],"['os.chdir', 'os.walk', 'print', 'self.put']",4
utilmy/zarchive/py3/util.py:aws_ec2_ssh:get,aws_ec2_ssh:get,method,1,1,1,35,35.0,0,0,"['self', 'remotefile', 'localfile']","[None, None, None]","[None, None, None]",4045,[],[],0
utilmy/zarchive/py3/util.py:aws_ec2_ssh:sftp_walk,aws_ec2_ssh:sftp_walk,method,20,33,26,371,11.24,3,1,"['self', 'remotepath']","[None, None]","[None, None]",4049,[],"['S_ISDIR', 'folders.append', 'files.append', 'print', 'self.sftp_walk']",5
utilmy/zarchive/py3/util.py:aws_ec2_ssh:get_all,aws_ec2_ssh:get_all,method,15,38,30,601,15.82,2,2,"['self', 'remotepath', 'localpath']","[None, None, None]","[None, None, None]",4070,[],"['os.mkdir', 'self.sftp_walk', 'print', 'self.get']",4
utilmy/zarchive/py3/util.py:aws_ec2_ssh:write_command,aws_ec2_ssh:write_command,method,2,2,2,74,37.0,0,0,"['self', 'text', 'remotefile']","[None, None, None]","[None, None, None]",4094,[],[],0
utilmy/zarchive/py3/util.py:aws_ec2_ssh:python_script,aws_ec2_ssh:python_script,method,5,12,10,119,9.92,0,0,"['self', 'script_path', 'args1']","[None, None, None]","[None, None, None]",4101,[],['self.cmd2'],1
utilmy/zarchive/py3/util.py:aws_ec2_ssh:command_list,aws_ec2_ssh:command_list,method,5,13,13,116,8.92,1,0,"['self', 'cmdlist']","[None, None]","[None, None]",4107,[],"['print', 'self.command']",2
utilmy/zarchive/py3/util.py:aws_ec2_ssh:listdir,aws_ec2_ssh:listdir,method,2,2,2,34,17.0,0,0,"['self', 'remotedir']","[None, None]","[None, None]",4114,[],[],0
utilmy/zarchive/py3/util.py:aws_ec2_ssh:jupyter_kill,aws_ec2_ssh:jupyter_kill,method,3,9,9,130,14.44,0,0,['self'],[None],[None],4117,[],"['aws_ec2_cmd_ssh', 'print']",2
utilmy/zarchive/py3/util.py:aws_ec2_ssh:jupyter_start,aws_ec2_ssh:jupyter_start,method,0,1,1,4,4.0,0,0,['self'],[None],[None],4121,[],[],0
utilmy/zarchive/py3/util.py:aws_ec2_ssh:cmd2,aws_ec2_ssh:cmd2,method,2,5,5,63,12.6,0,0,"['self', 'cmd1']","[None, None]","[None, None]",4124,[],['aws_ec2_cmd_ssh'],1
utilmy/zarchive/py3/util.py:aws_ec2_ssh:_help_ssh,aws_ec2_ssh:_help_ssh,method,18,29,29,132,4.55,0,1,['self'],[None],[None],4129,[],[],0
utilmy/viz/zarchive/toptoolbar.py:TopToolbar,TopToolbar,class,37,73,60,627,8.59,0,2,[],[],[],4,[],[],0
utilmy/viz/zarchive/toptoolbar.py:TopToolbar:__init__,TopToolbar:__init__,method,1,3,3,32,10.67,0,0,['self'],[None],[None],28,[],[],0
utilmy/templates/templist/pypi_package/setup.py:get_current_githash,get_current_githash,function,6,10,8,123,12.3,0,0,[],[],[],17,[],"['subprocess.check_output', 'label.decode']",2
utilmy/templates/templist/pypi_package/run_pipy.py:get_current_githash,get_current_githash,function,6,10,8,126,12.6,0,0,[],[],[],39,[],"['subprocess.check_output', 'label.decode']",2
utilmy/templates/templist/pypi_package/run_pipy.py:update_version,update_version,function,12,30,28,315,10.5,0,0,"['path', 'n']","[None, None]","[None, '1']",46,[],"['open', 'Version.parse', 'print', 'int', 'file.write', 'version.new_version']",6
utilmy/templates/templist/pypi_package/run_pipy.py:git_commit,git_commit,function,3,29,19,164,5.66,0,2,['message'],[None],[None],65,[],"['ask', 'exit', 'os.system']",3
utilmy/templates/templist/pypi_package/run_pipy.py:ask,ask,function,2,3,3,42,14.0,0,0,"['question', 'ans']","[None, None]","[None, ""'yes'""]",77,[],"['input', 'ans.lower']",2
utilmy/templates/templist/pypi_package/run_pipy.py:pypi_upload,pypi_upload,function,5,40,35,391,9.78,1,1,[],[],[],81,"['    """"""\n', '      It requires credential in .pypirc  files\n', '      __token__\n', '      or in github SECRETS\n', '\n', '    """"""\n']","['os.system', 'print', 'os.listdir', 'item.endswith']",4
utilmy/templates/templist/pypi_package/run_pipy.py:main,main,function,2,6,6,75,12.5,0,0,['*args'],[None],[None],102,[],"['print', 'update_version']",2
utilmy/templates/templist/pypi_package/run_pipy.py:Version,Version,class,21,58,48,650,11.21,0,1,[],[],[],10,[],[],0
utilmy/templates/templist/pypi_package/run_pipy.py:Version:__init__,Version:__init__,method,6,6,6,50,8.33,0,0,"['self', 'major', 'minor', 'patch']","[None, None, None, None]","[None, None, None, None]",13,[],[],0
utilmy/templates/templist/pypi_package/run_pipy.py:Version:__str__,Version:__str__,method,2,2,2,36,18.0,0,0,['self'],[None],[None],18,[],"[""f'Version""]",1
utilmy/templates/templist/pypi_package/run_pipy.py:Version:__repr__,Version:__repr__,method,2,2,2,20,10.0,0,0,['self'],[None],[None],21,[],['self.__str__'],1
utilmy/templates/templist/pypi_package/run_pipy.py:Version:stringify,Version:stringify,method,1,2,2,51,25.5,0,0,['self'],[None],[None],24,[],[],0
utilmy/templates/templist/pypi_package/run_pipy.py:Version:new_version,Version:new_version,method,1,3,3,53,17.67,0,0,"['self', 'orig']","[None, None]","[None, None]",27,[],['self.stringify'],1
utilmy/templates/templist/pypi_package/run_pipy.py:Version:parse,Version:parse,method,5,23,22,192,8.35,0,1,"['cls', 'string']","[None, None]","[None, None]",31,[],"['re.findall', 'len', 'Exception', 'cls']",4
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:wi,wi,function,10,24,21,160,6.67,1,1,['*args'],[None],[None],27,[],"['str', 'dx.replace', 'printinfile']",3
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:printinfile,printinfile,function,3,6,6,51,8.5,0,0,"['vv', 'file1']","[None, None]","[None, None]",38,[],"['open', 'text_file.write']",2
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:wi2,wi2,function,4,10,10,57,5.7,1,1,['*args'],[None],[None],44,[],['print'],1
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:indent,indent,function,17,43,34,344,8.0,0,4,[],[],[],49,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find']",4
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:dedent,dedent,function,17,43,34,344,8.0,0,4,[],[],[],50,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find']",4
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:describe_builtin,describe_builtin,function,17,43,34,344,8.0,0,4,['obj'],[None],[None],54,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find']",4
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:describe_func,describe_func,function,18,59,45,444,7.53,0,5,"['obj', 'method']","[None, None]","[None, 'False']",74,[],"['inspect.getargspec', 'wi', 'str', 'len']",4
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:describe_klass,describe_klass,function,12,32,30,240,7.5,1,2,['obj'],[None],[None],99,[],"['wi', 'indent', 'getattr', 'inspect.ismethod', '1;describe_func', 'print', 'dedent']",7
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:describe,describe,function,54,315,151,2625,8.33,4,20,['obj'],[None],[None],116,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find', 'describe_func', 'inspect.getargspec', 'str', 'len', 'describe_klass', 'indent', 'getattr', 'inspect.ismethod', '1;describe_func', 'print', 'dedent', 'describe', 'dir', 'inspect.isclass', 'inspect.isfunction', 'inspect.isbuiltin', 'describe_builtin', 'describe_builtin2', 'describe_func2', 'describe_klass2', 'describe2']",25
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:describe_builtin2,describe_builtin2,function,16,33,26,272,8.24,0,3,"['obj', 'name1']","[None, None]","[None, None]",143,[],"['wi', 'docstr.split', 'func_descr.replace', 's.find']",4
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:describe_func2,describe_func2,function,10,29,23,203,7.0,0,2,"['obj', 'method', 'name1']","[None, None, None]","[None, 'False', ""''""]",162,[],"['inspect.getargspec', 'wi', 'str']",3
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:describe_klass2,describe_klass2,function,9,21,21,169,8.05,1,1,"['obj', 'name1']","[None, None]","[None, ""''""]",174,[],"['getattr', 'inspect.ismethod', 'describe_func2']",3
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:describe2,describe2,function,9,33,30,313,9.48,1,1,['module'],[None],[None],184,[],"['wi', 'dir', 'getattr', 'inspect.isclass', 'describe_klass2', 'print', 'inspect.isfunction', 'describe_func2', 'inspect.isbuiltin', 'describe_builtin2']",10
utilmy/zarchive/storage/aapackage_gen/codeanalysis.py:getmodule_doc,getmodule_doc,function,19,60,42,623,10.38,3,0,"['module1', 'file1']","[None, None]","[None, ""'moduledoc.txt'""]",200,[],"['importlib.import_module', 'pkgutil.walk_packages', 'vv.append', 'wi', 'describe2', 'print', 'describe']",7
utilmy/zarchive/storage/aapackage_gen/util.py:getmodule_doc,getmodule_doc,function,4,6,6,56,9.33,0,0,"['module1', 'fileout']","[None, None]","[None, ""''""]",35,[],['ca.getmodule_doc'],1
utilmy/zarchive/storage/aapackagedev/random.py:convert_csv2hd5f,convert_csv2hd5f,function,10,21,19,181,8.62,1,0,"['filein1', 'filename']","[None, None]","[None, None]",61,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/storage/aapackagedev/random.py:getrandom_tonumpy,getrandom_tonumpy,function,6,9,9,101,11.22,0,0,"['filename', 'nbdim', 'nbsample']","[None, None, None]","[None, None, None]",74,[],['pd.read_hdf'],1
utilmy/zarchive/storage/aapackagedev/random.py:comoment,comoment,function,4,6,6,61,10.17,0,0,"['xx', 'yy', 'nsample', 'kx', 'ky']","[None, None, None, None, None]","[None, None, None, None, None]",90,[],['ne.evaluate'],1
utilmy/zarchive/storage/aapackagedev/random.py:acf,acf,function,12,34,31,244,7.18,0,0,['data'],[None],[None],98,[],"['len', 'np.mean', 'np.sum', 'float', 'r', 'np.arange', 'np.asarray']",7
utilmy/zarchive/storage/aapackagedev/random.py:getdvector,getdvector,function,7,21,20,141,6.71,1,0,"['dimmax', 'istart', 'idimstart']","[None, None, None]","[None, None, None]",126,[],['range'],1
utilmy/zarchive/storage/aapackagedev/random.py:pathScheme_std,pathScheme_std,function,3,3,3,28,9.33,0,0,"['T', 'n', 'zz']","[None, None, None]","[None, None, None]",136,[],[],0
utilmy/zarchive/storage/aapackagedev/random.py:pathScheme_bb,pathScheme_bb,function,18,32,30,200,6.25,1,0,"['T', 'n', 'zz']","[None, None, None]","[None, None, None]",144,[],"['int', 'range', 'np.sqrt']",3
utilmy/zarchive/storage/aapackagedev/random.py:pathScheme_,pathScheme_,function,3,3,3,28,9.33,0,0,"['T', 'n', 'zz']","[None, None, None]","[None, None, None]",163,"[""'''\n"", 'ttdim=1024  # dim= 2^k\n', '[[500, testdensity(500, ttdim, 400,-1)],\n', '[1000, testdensity(1000, ttdim, 400,-1)],\n', '[2000, testdensity(2000, ttdim, 400,-1)], \n', '[3000, testdensity(3000, ttdim, 400,-1)],\n', '[4000, testdensity(4000, ttdim, 400,-1)]]\n', ""'''\n""]",[],0
utilmy/zarchive/storage/aapackagedev/random.py:testdensity,testdensity,function,27,57,54,404,7.09,3,0,"['nsample', 'totdim', 'bin01', 'Ti']","[None, None, None, None]","[None, None, None, '-1']",173,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_', 'np.arange', 'np.histogram', 'dx.dN', 'np.sum']",8
utilmy/zarchive/storage/aapackagedev/random.py:plotdensity,plotdensity,function,29,65,63,531,8.17,1,0,"['nsample', 'totdim', 'bin01', 'tit0', 'Ti']","[None, None, None, None, None]","[None, None, None, None, '-1']",191,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_', 'np.arange', 'np.histogram', 'dx.dN', 'plt.plot', 'plt.axis', 'plt.title']",10
utilmy/zarchive/storage/aapackagedev/random.py:testdensity2d,testdensity2d,function,39,85,74,702,8.26,2,0,"['nsample', 'totdim', 'bin01', 'nbasset']","[None, None, None, None]","[None, None, None, None]",236,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'np.arange', 'np.histogram2d', 'dx.dN2d', 'np.meshgrid', 'plt.imshow', 'np.sum']",10
utilmy/zarchive/storage/aapackagedev/random.py:lognormal_process2d,lognormal_process2d,function,2,5,4,49,9.8,0,0,"['a1', 'z1', 'a2', 'z2', 'k']","[None, None, None, None, None]","[None, None, None, None, None]",277,[],"['np.maximum', 'np.exp']",2
utilmy/zarchive/storage/aapackagedev/random.py:testdensity2d2,testdensity2d2,function,40,91,80,750,8.24,2,0,"['nsample', 'totdim', 'bin01', 'nbasset', 'process01', 'a1', 'a2', 'kk']","[None, None, None, None, None, None, None, None]","[None, None, None, None, 'lognormal_process2d', '0.25', '0.25', '1']",283,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'np.arange', 'np.histogram2d', 'dx.dN2d', 'process01', 'np.meshgrid', 'plt.imshow', 'np.sum']",11
utilmy/zarchive/storage/aapackagedev/random.py:call_process,call_process,function,15,30,28,245,8.17,1,0,"['a', 'z', 'k']","[None, None, None]","[None, None, None]",331,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'process01', 'np.var']",6
utilmy/zarchive/storage/aapackagedev/random.py:binary_process,binary_process,function,15,30,28,245,8.17,1,0,"['a', 'z', 'k']","[None, None, None]","[None, None, None]",335,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'process01', 'np.var']",6
utilmy/zarchive/storage/aapackagedev/random.py:pricing01,pricing01,function,15,30,28,245,8.17,1,0,"['totdim', 'nsample', 'a', 'strike', 'process01', 'aa', 'itmax', 'tt']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, '0.25', '-1', '10']",341,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_std', 'process01', 'np.var']",6
utilmy/zarchive/storage/aapackagedev/random.py:plotdensity2,plotdensity2,function,35,78,74,590,7.56,2,0,"['nsample', 'totdim', 'bin01', 'tit0', 'process01', 'vol', 'tt', 'Ti']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, '0.25', '5', '-1']",365,[],"['np.zeros', 'range', 'getdvector', 'pathScheme_', 'np.arange', 'np.histogram', 'process01', 'call_process', 'plt.plot', 'plt.axis', 'plt.title']",11
utilmy/zarchive/storage/aapackagedev/random.py:Plot2D_random_show,Plot2D_random_show,function,46,106,80,855,8.07,0,0,"['dir1', 'title1', 'dimxmax', 'dimymax', 'dimstep', 'samplejump', 'nsamplegraph']","[None, None, None, None, None, None, None]","[None, None, None, None, None, None, None]",417,[],"['pd.read_hdf', 'plt.scatter', 'plt.axis', 'title1+str', 'plt.title', 'plt.show', 'np.mean', 'np.var', 'comoment']",9
utilmy/zarchive/storage/aapackagedev/random.py:Plot2D_random_save,Plot2D_random_save,function,51,106,77,723,6.82,2,0,"['dir1', 'title1', 'dimxmax', 'dimymax', 'dimstep', 'samplejump', 'nsamplegraph', '']","[None, None, None, None, None, None, None, None]","[None, None, None, None, None, None, None, None]",464,[],"['range', 'pd.read_hdf', 'plt.scatter', 'plt.axis', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",8
utilmy/zarchive/storage/aapackagedev/random.py:getoutlier_fromrandom,getoutlier_fromrandom,function,57,168,111,1242,7.39,2,2,"['filename', 'jmax1', 'imax1', 'isamplejum', 'nsample', ""fileoutlier=   'E""]","[None, None, None, None, None, '']","[None, None, None, None, None, ""   'E:\\_data\\_QUASI_SOBOL_gaussian_outlier.h5'""]",514,[],"['range', 'pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'unique_rows', 'store.append', 'store.close']",7
utilmy/zarchive/storage/aapackagedev/random.py:numexpr_vect_calc,numexpr_vect_calc,function,13,28,22,214,7.64,0,0,"['filename', 'i0', 'imax', 'expr', ""fileout='E""]","[None, None, None, None, '']","[None, '0', '1000', None, ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",582,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/storage/aapackagedev/random.py:getoutlier_fromrandom_fast,getoutlier_fromrandom_fast,function,71,183,130,1320,7.21,2,2,"['filename', 'jmax1', 'imax1', 'isamplejum', 'nsample', 'trigger1', ""fileoutlier=   'E""]","[None, None, None, None, None, None, '']","[None, None, None, None, None, '0.28', ""   'E:\\_data\\_QUASI_SOBOL_gaussian_outlier.h5'""]",602,[],"['np.zeros', 'range', 'ne.evaluate', 'pd.HDFStore', 'store.append', 'store.close']",6
utilmy/zarchive/storage/aapackagedev/random.py:outlier_clean,outlier_clean,function,7,16,15,98,6.12,1,1,['vv2'],[None],[None],670,[],['range'],1
utilmy/zarchive/storage/aapackagedev/random.py:overwrite_data,overwrite_data,function,6,11,9,112,10.18,0,0,"['fileoutlier', 'vv2']","[None, None]","[None, None]",678,[],"['pd.HDFStore', 'store.remove', 'store.append', 'store.close']",4
utilmy/zarchive/storage/aapackagedev/random.py:doublecheck_outlier,doublecheck_outlier,function,8,12,11,100,8.33,0,0,"['fileoutlier', 'ijump', 'nsample', 'trigger1', "")fileoutlier=   'E"", ""'data')    #from filevv5"", '4)', 'dtype', 'kkmax1', '1) ', '0];   dimy', '1]y0= dimy * ijump + istartyym= dimy* ijump + nsample + istartyyyu1= yy1[y0']","[None, None, None, None, '', None, None, None, None, '  #Decrasing: dimy0 to dimmindimx', None, '']","[None, None, '4000', '0.1', ""   'E:\\_data\\_QUASI_SOBOL_gaussian_outlier.h5'fileoutlier"", ' pdf.values   #to numpy vectordel pdfistartx= 0; istarty= 0nsample= 4000trigger1=  0.1crrmax = 250000kk=0(crrmax', None, ""'int')  #empty listvv5)[0]0"", None, ' vv5[kk', ' vv5[kk', ' dimy * ijump + istartyym= dimy* ijump + nsample + istartyyyu1= yy1[y0:ym];   yyu2= yy2[y0:ym];   yyu3= yy3[y0:ym]x0= dimx * ijump + istartxxm= dimx* ijump + nsample + istartxxxu1= yy1[x0:xm];   xxu2= yy2[x0:xm];   xxu3= yy3[x0:xm]""sum( xxu3 * yyu1)"") / (nsample) # X3.Y moments""sum( xxu1 * yyu3)"") / (nsample)""sum( xxu2 * yyu2)"") / (nsample)abs(c22) > trigger1)  :']",698,[],[],0
utilmy/zarchive/storage/aapackagedev/random.py:plot_outlier,plot_outlier,function,13,20,19,144,7.2,1,0,"['fileoutlier', 'kk)fileoutlier', ""'data')    #from filevv"", '0]yy', '1]xx', 'yy', 's', '1000', '00', ""1000])nsample)+'sampl D_'+str(dimx)+' X D_'+str(dimy)tit1)'_img/'+tit1+'_outlier.jpg'"", 'dpi', 'kmax)']","[None, None, None, None, None, None, None, None, None, None, None, '']","[None, None, ' df.values   #to numpy vectordel dfxx= vv[kk', ' vv[kk', None, None, '1 )[00', None, None, None, '100))yy', None]",758,"[""'''\n"", ""fileoutlier=   'E:\\_data\\_QUASI_SOBOL_gaussian_outlier.h5'   \n"", ""df=  pd.read_hdf(fileoutlier,'data')    #from file\n"", '\n', 'nn= len(vv)\n', 'vv1= np.zeros((10001,2))\n', 'for ii in range(0,   nn ):\n', '    \n', '  ix1= vv[ii,0]  \n', '  ix2= vv[ii,1]  \n', '  \n', '  vv1[ix1,0]= ix1\n', '  vv1[ix2,0]= ix2\n', '  vv1[ix1,1]+= 1\n', '  vv1[ix2,1]+= 1\n', '  \n', '\n', 'plt.scatter(xx, yy, s=1 )\n', 'plt.axis([0, 3500, 0, 100])  \n', '\n', '\n', 'np.mean(yy[:3000])    :65.163333333333327\n', 'np.var(yy[:3000])   : 66.519322222222229\n', '\n', 'np.mean(yy[3001:10000])    :29.998285469352766    35.16504786398056\n', '\n', '\n', 'xx= vv1[:,0]\n', 'yy= vv1[:,1]\n', '\n', 'yy[3001:10000] = yy[3001:10000] +  np.random.normal(35.16,6, 6999)\n', '\n', '\n', 'plt.scatter(xx, yy, s=1 )\n', 'plt.axis([0, 10000, 0, 100]) \n', '\n', 'tit1= ""Histogram of outliers per dim 1 to 10000""\n', 'plt.title(tit1)\n', ""plt.savefig('_img/'+'histogram of outliers per dim 1 to 10000.jpg',dpi=100)\n"", 'plt.clf()\n', '\n', '0.006  0.6% are defective...\n', ""'''\n""]","['int', 'np.copy', 'range']",3
utilmy/zarchive/storage/aapackagedev/random.py:permute,permute,function,24,64,53,631,9.86,2,0,"['yy', 'kmax']","[None, None]","[None, None]",841,[],"['int', 'np.copy', 'range', 'permute2', 'plt.scatter', 'plt.axis', 'title1+str', 'plt.title', 'comoment']",9
utilmy/zarchive/storage/aapackagedev/random.py:permute2,permute2,function,18,42,41,461,10.98,1,0,"['xx', 'yy', 'kmax']","[None, None, None]","[None, None, None]",854,[],"['int', 'np.copy', 'range', 'plt.scatter', 'plt.axis', 'title1+str', 'plt.title', 'comoment']",8
utilmy/templates/templist/pypi_package/mygenerator/pipeline.py:run_generate_numbers_sequence,run_generate_numbers_sequence,function,25,80,53,756,9.45,0,0,"['sequence', 'min_spacing', 'max_spacing', 'image_width', '### image_widthoutput_path', 'config_file', '']","[' str', ' int ', ' int ', ' int ', ' str ', ' str ', None]","[None, ' 1', ' 10', ' 280', ' ""./""', ' ""config/config.yaml""', None]",14,[],"['len', 'config_load', 'pathlib.Path', 'dataset.NlpDataset', 'transform.TextToImage', 'tf.fit_transform', 'ds_img.save']",7
utilmy/templates/templist/pypi_package/mygenerator/pipeline.py:run_generate_phone_numbers,run_generate_phone_numbers,function,27,76,57,697,9.17,0,0,"['num_images', 'min_spacing', 'max_spacing', 'image_width', 'output_path', 'config_file', '']","[' int ', ' int ', ' int ', ' int ', ' str ', ' str ', None]","[' 10', ' 1', ' 10', ' 280', ' ""./""', ' ""config/config.yaml""', None]",60,[],"['config_load', 'pathlib.Path', 'dataset.PhoneNlpDataset', 'transform.TextToImage', 'tf.fit_transform', 'ds_img.save']",6
utilmy/templates/templist/pypi_package/mygenerator/utils.py:log,log,function,10,39,25,357,9.15,0,0,['*s'],[None],[None],19,[],"['logger.info', 'log2', 'logger.debug', 'logw', 'logger.warning', 'loge', 'logger.error', 'logger_setup', 'logger.configure']",9
utilmy/templates/templist/pypi_package/mygenerator/utils.py:log2,log2,function,1,5,5,39,7.8,0,0,['*s'],[None],[None],23,[],['logger.debug'],1
utilmy/templates/templist/pypi_package/mygenerator/utils.py:logw,logw,function,1,5,5,41,8.2,0,0,['*s'],[None],[None],27,[],['logger.warning'],1
utilmy/templates/templist/pypi_package/mygenerator/utils.py:loge,loge,function,1,5,5,39,7.8,0,0,['*s'],[None],[None],31,[],['logger.error'],1
utilmy/templates/templist/pypi_package/mygenerator/utils.py:logger_setup,logger_setup,function,2,15,13,138,9.2,0,0,[],[],[],35,[],['logger.configure'],1
utilmy/templates/templist/pypi_package/mygenerator/utils.py:config_load,config_load,function,16,73,62,816,11.18,0,1,"['config_path', 'pathlib.Path]] ']","[' Optional[Union[str', None]","[None, ' None']",52,"['    """"""Load Config file into a dict\n', '    1) load config_path\n', '    2) If not, load in HOME USER\n', '    3) If not, create default one\n', '    Args:\n', ""        config_path: path of config or 'default' tag value\n"", '    Returns: dict config\n', '    """"""\n']","['logw', 'log2', 'yaml.load', 'str', 'log', 'os.makedirs', 'open', 'yaml.dump']",8
utilmy/templates/templist/pypi_package/mygenerator/utils.py:dataset_donwload,dataset_donwload,function,9,19,17,246,12.95,0,0,"['url', 'path_target']","[None, None]","[None, None]",96,"['    """"""Donwload on disk the tar.gz file\n', '    Args:\n', '        url:\n', '        path_target:\n', '    Returns:\n', '\n', '    """"""\n']","['log', 'os.makedirs', 'wget.download', 'url.split', 'os_extract_archive', 'log2']",6
utilmy/templates/templist/pypi_package/mygenerator/utils.py:dataset_get_path,dataset_get_path,function,17,53,44,527,9.94,0,3,['cfg'],[' dict'],[None],113,[],"['cfg.get', 'cfgd.get', 'glob.glob', 'log2', 'len', 'dataset_donwload', 'Exception']",7
utilmy/templates/templist/pypi_package/mygenerator/utils.py:os_extract_archive,os_extract_archive,function,19,63,47,671,10.65,1,8,"['file_path', 'path', 'archive_format']","[None, None, None]","[None, '"".""', '""auto""']",142,"['    """"""Extracts an archive if it matches tar, tar.gz, tar.bz, or zip formats.\n', '    Args:\n', '        file_path: path to the archive file\n', '        path: path to extract the archive file\n', '        archive_format: Archive format to try for extracting the file.\n', ""            Options are 'auto', 'tar', 'zip', and None.\n"", ""            'tar' includes tar, tar.gz, and tar.bz files.\n"", ""            The default 'auto' is ['tar', 'zip'].\n"", '            None or an empty list will return no matches found.\n', '    Returns:\n', '        True if a match was found and an archive extraction was completed,\n', '        False otherwise.\n', '    """"""\n']","['isinstance', 'is_match_fn', 'open_fn', 'archive.extractall', 'os.remove', 'shutil.rmtree']",6
utilmy/templates/templist/pypi_package/mygenerator/utils.py:to_file,to_file,function,3,7,7,51,7.29,0,0,"['s', 'filep']","[None, None]","[None, None]",189,[],"['open', 'fp.write']",2
utilmy/templates/templist/pypi_package/mygenerator/transform.py:ImageTransform,ImageTransform,class,7,24,14,245,10.21,0,0,[],[],[],14,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/transform.py:CharToImages,CharToImages,class,23,53,41,636,12.0,1,1,[],[],[],56,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/transform.py:RemoveWhitePadding,RemoveWhitePadding,class,9,23,18,319,13.87,0,0,[],[],[],112,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/transform.py:CombineImagesHorizontally,CombineImagesHorizontally,class,68,234,178,2451,10.47,1,3,[],[],[],145,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/transform.py:ScaleImage,ScaleImage,class,17,47,40,567,12.06,0,0,[],[],[],307,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/transform.py:TextToImage,TextToImage,class,18,59,46,619,10.49,1,0,[],[],[],337,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/transform.py:ImageTransform:__init__,ImageTransform:__init__,method,6,22,13,226,10.27,0,0,['self'],[None],[None],20,"['        """"""\n', '        Parameters\n', '        ----------\n', '        """"""\n']","['transform', 'fit', 'fit_transform', 'self.fit']",4
utilmy/templates/templist/pypi_package/mygenerator/transform.py:ImageTransform:transform,ImageTransform:transform,method,2,2,2,8,4.0,0,0,"['self', 'ds']","[None, ' dataset.ImageDataset']","[None, None]",26,"['        """"""\n', '        Transform Dataset into Dataset\n', '        Args:\n', '            ds: dataset.ImageDataset\n', '        Returns:dataset.ImageDataset\n', '        """"""\n']",[],0
utilmy/templates/templist/pypi_package/mygenerator/transform.py:ImageTransform:fit,ImageTransform:fit,method,3,10,9,113,11.3,0,0,"['self', 'ds']","[None, ' dataset.ImageDataset']","[None, None]",35,"['        """"""\n', '        fit the transformation\n', '        Args:\n', '            ds: dataset.ImageDataset\n', '        Returns: Object\n', '        """"""\n']","['fit_transform', 'self.fit']",2
utilmy/templates/templist/pypi_package/mygenerator/transform.py:ImageTransform:fit_transform,ImageTransform:fit_transform,method,2,2,2,32,16.0,0,0,"['self', 'ds']","[None, ' dataset.ImageDataset']","[None, None]",44,"['        """"""\n', '        Updates internal parameters of this transformation and then\n', '        applies it to the provided dataset.\n', '\n', '        Args:\n', '            ds: dataset.ImageDataset\n', '        Returns:dataset.ImageDataset\n', '        """"""\n']",['self.fit'],1
utilmy/templates/templist/pypi_package/mygenerator/transform.py:CharToImages:__init__,CharToImages:__init__,method,2,2,2,22,11.0,0,0,"['self', 'font']","[None, ' dataset.ImageDataset']","[None, None]",62,"['        """"""\n', '        Parameters\n', '        ----------\n', '        font: dataset which contains images of characters. Images are features\n', ""        of the font dataset and characters are it's labels. Each character\n"", '        can have more than one image.\n', '        """"""\n']",[],0
utilmy/templates/templist/pypi_package/mygenerator/transform.py:CharToImages:transform,CharToImages:transform,method,16,27,26,356,13.19,1,1,"['self', 'ds']","[None, ' dataset.NlpDataset']","[None, None]",72,"['        """"""\n', '        Replaces features of the input dataset by mapping feature (string type)\n', '        to a list of character images (List[np.array] type)\n', '        """"""\n']","['_get_image_fn', 'ds.get_text_only', 'len', 'img_list.append', 'dataset.ImageDataset']",5
utilmy/templates/templist/pypi_package/mygenerator/transform.py:CharToImages:fit,CharToImages:fit,method,3,10,9,111,11.1,0,0,"['self', 'ds']","[None, ' dataset.NlpDataset']","[None, None]",96,"['        """"""\n', '        Args:\n', '            ds: dataset.NlpDataset\n', '        Returns:dataset.ImageDataset\n', '        """"""\n']","['fit_transform', 'self.fit']",2
utilmy/templates/templist/pypi_package/mygenerator/transform.py:CharToImages:fit_transform,CharToImages:fit_transform,method,2,2,2,32,16.0,0,0,"['self', 'ds']","[None, ' dataset.NlpDataset']","[None, None]",104,"['        """"""\n', '        Updates internal parameters of this transformation and then\n', '        applies it to the provided dataset.\n', '        """"""\n']",['self.fit'],1
utilmy/templates/templist/pypi_package/mygenerator/transform.py:RemoveWhitePadding:transform,RemoveWhitePadding:transform,method,8,17,13,253,14.88,0,0,"['self', 'ds']","[None, ' dataset.ImageDataset']","[None, None]",26,"['        """"""\n', '        Transform Dataset into Dataset\n', '        Args:\n', '            ds: dataset.ImageDataset\n', '        Returns:dataset.ImageDataset\n', '        """"""\n']","['_get_image_fn', 'ds.get_image_only', 'self.transform_sample', 'dataset.ImageDataset', 'transform_sample', 'util_image.image_remove_extra_padding']",6
utilmy/templates/templist/pypi_package/mygenerator/transform.py:RemoveWhitePadding:transform_sample,RemoveWhitePadding:transform_sample,method,2,2,2,50,25.0,0,0,"['self', 'image']","[None, ' np.ndarray']","[None, None]",129,"['        """"""\n', '        Remove surrounding white spaces in digit image\n', '\n', '        Parameters\n', '        ----------\n', '        image: image of the digit\n', '\n', '        returns\n', '        -------\n', '        crop: cropped image\n', '        """"""\n']",['util_image.image_remove_extra_padding'],1
utilmy/templates/templist/pypi_package/mygenerator/transform.py:CombineImagesHorizontally:__init__,CombineImagesHorizontally:__init__,method,5,5,5,86,17.2,0,0,"['self', 'padding_range', 'int]', 'combined_width']","[None, ' Tuple[int', None, ' int']","[None, None, None, None]",152,"['        """"""\n', '        Parameters\n', '        ----------\n', '        spacing_range:\n', '            a (minimum, maximum) int pair (tuple), representing the min and max spacing\n', '            between digits. Unit should be pixel.\n', '        image_width:\n', '            specifies the width of the image in pixels.\n', '        """"""\n']",['super'],1
utilmy/templates/templist/pypi_package/mygenerator/transform.py:CombineImagesHorizontally:transform,CombineImagesHorizontally:transform,method,13,26,24,415,15.96,0,0,"['self', 'ds']","[None, ' dataset.ImageDataset']","[None, None]",26,"['        """"""\n', '        Transform Dataset into Dataset\n', '        Args:\n', '            ds: dataset.ImageDataset\n', '        Returns:dataset.ImageDataset\n', '        """"""\n']","['_get_image_fn', 'ds.get_image_only', 'self.transform_sample', 'dataset.ImageDataset', 'transform_sample']",5
utilmy/templates/templist/pypi_package/mygenerator/transform.py:CombineImagesHorizontally:transform_sample,CombineImagesHorizontally:transform_sample,method,51,189,141,1812,9.59,1,3,"['self', 'image_list', '1', '1)', 'combined_width', 'min_image_width', 'validate', '']","[None, ' List[np.ndarray]', None, None, None, None, None, None]","[None, None, None, None, '10', '2', 'True', None]",176,"['        """"""\n', '        Combine images of individual digits horizontally to make image of the complete number\n', '        Parameters\n', '        ----------\n', '        image_list: list of np.ndarray containing images of each digit\n', '        padding_range: (minimum space between two digits, maximum space between two digits)\n', '        combined_width: total width of the image\n', '        returns\n', '        -------\n', '        final_image: combined image of number\n', '        padding_size: padding between each digits in a number\n', '        """"""\n']","['len', 'util_image.padding_generate', 'np.sum', 'int', 'enumerate', 'cv2.resize', 'new_img_list.append', 'Exception', 'util_image.image_merge', 'cv2.bitwise_not', 'image_padding_validate', 'logw', 'np.zeros']",13
utilmy/templates/templist/pypi_package/mygenerator/transform.py:ScaleImage:__init__,ScaleImage:__init__,method,7,7,7,71,10.14,0,0,"['self', 'width', 'height', 'inter']","[None, ' Optional[int] ', ' Optional[int] ', None]","[None, ' None', ' None', 'cv2.INTER_AREA']",312,"['        """"""\n', '        width and height specify output image dimensions\n', '        """"""\n']",['super'],1
utilmy/templates/templist/pypi_package/mygenerator/transform.py:ScaleImage:transform,ScaleImage:transform,method,8,23,20,336,14.61,0,0,"['self', 'ds']","[None, ' dataset.ImageDataset']","[None, None]",26,"['        """"""\n', '        Transform Dataset into Dataset\n', '        Args:\n', '            ds: dataset.ImageDataset\n', '        Returns:dataset.ImageDataset\n', '        """"""\n']","['_get_image_fn', 'ds.get_image_only', 'self.transform_sample', 'dataset.ImageDataset', 'transform_sample', 'util_image.image_resize']",6
utilmy/templates/templist/pypi_package/mygenerator/transform.py:ScaleImage:transform_sample,ScaleImage:transform_sample,method,2,5,5,70,14.0,0,0,"['self', 'image', 'width', 'height', 'inter']","[None, None, None, None, None]","[None, None, 'None', 'None', 'cv2.INTER_AREA']",330,"['        """"""\n', '        Resizes a image and maintains aspect ratio.\n', '        """"""\n']",['util_image.image_resize'],1
utilmy/templates/templist/pypi_package/mygenerator/transform.py:TextToImage:__init__,TextToImage:__init__,method,9,19,16,255,13.42,0,0,"['self', 'font_dir', 'pathlib.Path]', 'spacing_range', 'int]', 'image_width']","[None, ' Union[str', None, ' Tuple[int', None, ' int']","[None, None, None, None, None, None]",312,"['        """"""\n', '        width and height specify output image dimensions\n', '        """"""\n']","['dataset.ImageDataset', 'RemoveWhitePadding', 'CharToImages', 'CombineImagesHorizontally']",4
utilmy/templates/templist/pypi_package/mygenerator/transform.py:TextToImage:transform,TextToImage:transform,method,5,8,7,52,6.5,1,0,"['self', 'ds']","[None, ' dataset.NlpDataset']","[None, None]",72,"['        """"""\n', '        Replaces features of the input dataset by mapping feature (string type)\n', '        to a list of character images (List[np.array] type)\n', '        """"""\n']",['tr.transform'],1
utilmy/templates/templist/pypi_package/mygenerator/transform.py:TextToImage:fit,TextToImage:fit,method,3,10,9,111,11.1,0,0,"['self', 'ds']","[None, ' dataset.NlpDataset']","[None, None]",96,"['        """"""\n', '        Args:\n', '            ds: dataset.NlpDataset\n', '        Returns:dataset.ImageDataset\n', '        """"""\n']","['fit_transform', 'self.fit']",2
utilmy/templates/templist/pypi_package/mygenerator/transform.py:TextToImage:fit_transform,TextToImage:fit_transform,method,2,2,2,32,16.0,0,0,"['self', 'ds']","[None, ' dataset.NlpDataset']","[None, None]",104,"['        """"""\n', '        Updates internal parameters of this transformation and then\n', '        applies it to the provided dataset.\n', '        """"""\n']",['self.fit'],1
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:dataset_build_meta_mnist,dataset_build_meta_mnist,function,16,51,38,409,8.02,2,3,"['path', 'pathlib.Path]] ', 'get_image_fn', 'meta', 'image_suffix', '**kwargs', '']","[' Optional[Union[str', None, None, None, None, None, None]","[None, ' None', 'None', 'None', '""*.png""', None, None]",230,"['    """"""\n', '    Args:\n', '    * path - directory of the dataset or meta-data\n', '    * get_image_fn - function for getting i-th image of the dataset\n', '    directly metadat part\n', '\n', '    """"""\n']","['isinstance', 'pathlib.Path', 'path.exists', 'path.is_dir', 'path.iterdir', 'label_dir.is_dir', 'label_dir.glob', 'meta_rows.append', 'str', 'pd.DataFrame', 'len', 'meta.to_csv']",12
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:NlpDataset,NlpDataset,class,16,63,44,471,7.48,0,1,[],[],[],34,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:PhoneNlpDataset,PhoneNlpDataset,class,19,58,43,564,9.72,2,1,[],[],[],72,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:ImageDataset,ImageDataset,class,47,185,129,1631,8.82,3,8,[],[],[],116,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:NlpDataset:__init__,NlpDataset:__init__,method,8,32,26,209,6.53,0,1,"['self', 'meta']","[None, ' pd.DataFrame']","[None, None]",40,[],"['is_int', 'int', 'len', 'logw']",4
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:NlpDataset:__len__,NlpDataset:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],57,"['        """"""Return number of elements in this dataset.""""""\n']",['len'],1
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:NlpDataset:get_sample,NlpDataset:get_sample,method,4,5,4,59,11.8,0,0,"['self', 'idx']","[None, ' int']","[None, None]",61,[],['self.get_text_only'],1
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:NlpDataset:get_text_only,NlpDataset:get_text_only,method,5,6,5,54,9.0,0,0,"['self', 'idx']","[None, ' int']","[None, None]",65,[],[],0
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:PhoneNlpDataset:__init__,PhoneNlpDataset:__init__,method,12,33,24,373,11.3,2,0,"['self', 'size']","[None, ' int ']","[None, ' 1']",77,[],"['PhoneNumber', 'int', 'range', 'self.get_phone_number', 'meta_rows.append', 'super']",6
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:PhoneNlpDataset:__len__,PhoneNlpDataset:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],57,"['        """"""Return number of elements in this dataset.""""""\n']",['len'],1
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:PhoneNlpDataset:get_phone_number,PhoneNlpDataset:get_phone_number,method,5,10,8,71,7.1,0,1,"['self', 'idx', 'islocal']","[None, None, None]","[None, None, 'False']",105,[],['s.replace'],1
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:ImageDataset:__init__,ImageDataset:__init__,method,17,50,39,396,7.92,2,3,"['self', 'path', 'pathlib.Path]] ', 'get_image_fn', 'meta', 'image_suffix', '**kwargs', '']","[None, ' Optional[Union[str', None, None, None, None, None, None]","[None, None, ' None', 'None', 'None', '""*.png""', None, None]",125,"['        """"""\n', '        Args:\n', '        * path - directory of the dataset or meta-data\n', '        * get_image_fn - function for getting i-th image of the dataset\n', '          directly from metadata part\n', '\n', '        """"""\n']","['isinstance', 'pathlib.Path', 'path.is_dir', 'path.iterdir', 'label_dir.is_dir', 'label_dir.glob', 'meta_rows.append', 'str', 'pd.DataFrame', 'len']",10
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:ImageDataset:__len__,ImageDataset:__len__,method,1,2,2,20,10.0,0,0,['self'],[None],[None],57,"['        """"""Return number of elements in this dataset.""""""\n']",['len'],1
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:ImageDataset:get_image_only,ImageDataset:get_image_only,method,5,11,10,134,12.18,0,1,"['self', 'idx']","[None, ' int']","[None, None]",167,"['        """"""Return image of the single element of the dataset""""""\n']","['self.read_image', 'self.get_image_fn']",2
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:ImageDataset:get_sample,ImageDataset:get_sample,method,6,18,15,121,6.72,0,1,"['self', 'idx']","[None, ' int']","[None, None]",177,[],['self.get_image_only'],1
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:ImageDataset:get_label_list,ImageDataset:get_label_list,method,6,11,10,123,11.18,0,1,"['self', 'label']","[None, ' Any']","[None, None]",190,"['        """"""Return indices of the elements which have certain label.""""""\n']","['isinstance', 'np.asarray', 'np.flatnonzero']",3
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:ImageDataset:read_image,ImageDataset:read_image,method,2,2,2,47,23.5,0,0,"['self', 'filepath_or_buffer', 'io.BytesIO]']","[None, ' Union[str', None]","[None, None, None]",198,"['        """"""\n', '        Read a file into an image object\n', '        Args:\n', '            filepath_or_buffer: The path to the file, a URL, or any object\n', '                with a `read` method (such as `io.BytesIO`)\n', '        """"""\n']",['util_image.image_read'],1
utilmy/templates/templist/pypi_package/mygenerator/dataset.py:ImageDataset:save,ImageDataset:save,method,13,36,33,306,8.5,1,2,"['self', 'path', 'prefix', 'suffix', 'nrows']","[None, ' str', ' str ', ' str ', ' int ']","[None, None, ' ""img""', ' ""png""', ' -1']",209,"['        """"""Serialize on Disk the dataset elements\n', '        Args:\n', '            path:\n', '        Returns: None\n', '\n', '        """"""\n']","['os.makedirs', 'len', 'min', 'range', 'self.get_sample', 'cv2.imwrite']",6
utilmy/templates/templist/pypi_package/mygenerator/validate.py:image_padding_validate,image_padding_validate,function,6,30,28,259,8.63,0,1,"['final_image', 'min_padding', 'max_padding']","[None, None, None]","[None, None, None]",12,"['    """"""\n', '    Args:\n', '        final_image:\n', '        min_padding:\n', '        max_padding:\n', '\n', '    Returns:\n', '\n', '    """"""\n']","['image_padding_get', 'sum', 'len', 'logw']",4
utilmy/templates/templist/pypi_package/mygenerator/validate.py:image_padding_load,image_padding_load,function,6,9,7,117,13.0,0,0,"['img_path', 'threshold']","[None, None]","[None, '15']",34,"['    """"""\n', '    Args:\n', '        img_path:\n', '        threshold:\n', '\n', '    Returns number of consecutive blank columns in the image.\n', '    Example return value: [4, 8, 3] means that image contains\n', '    3 blank columns. The size of each corresponding column in pixels\n', '    is 4, 8 and 3.\n', '    """"""\n']","['util_image.image_read', 'image_padding_get']",2
utilmy/templates/templist/pypi_package/mygenerator/validate.py:image_padding_get,image_padding_get,function,21,81,40,476,5.88,1,7,"['img', 'threshold', 'inverse']","[None, None, None]","[None, '0', 'True']",50,"['    """"""\n', '    Args:\n', '         img_path:\n', '         threshold:\n', '      Returns number of consecutive blank columns in the image.\n', '      Example return value: [4, 8, 3] means that image contains\n', '      3 blank columns. The size of each corresponding column in pixels\n', '      is 4, 8 and 3.\n', '    """"""\n']","['cv2.bitwise_not', 'range', 'np.sum', 'xpad_list.append', 'xchar_list.append']",5
utilmy/templates/templist/pypi_package/mygenerator/validate.py:run_image_padding_validate,run_image_padding_validate,function,14,59,47,463,7.85,1,2,"['min_spacing', 'max_spacing', 'image_width', 'input_path', 'inverse_image', 'config_file', '**kwargs', '']","[' int ', ' int ', ' int ', ' str ', ' bool ', ' str ', None, None]","[' 1', ' 1', ' 5', ' """"', ' True', ' ""default""', None, None]",104,"['    """"""\n', '    Args:\n', '        min_spacing:\n', '        max_spacing:\n', '        image_width:\n', '        input_path:\n', '        config_file:\n', '        **kwargs:\n', '    Returns: None\n', '\n', '    """"""\n']","['sorted', 'log', 'len', 'util_image.image_read', 'image_padding_get', 'logw', 'sum']",7
utilmy/templates/templist/pypi_package/mygenerator/util_image.py:padding_generate,padding_generate,function,2,5,5,80,16.0,0,0,"['paddings_number', 'min_padding', 'max_padding']","[' int ', ' int ', ' int ']","[' 1', ' 1', ' 1']",12,"['    """"""\n', '    Args:\n', '        paddings_number:  4\n', '        min_padding:      1\n', '        max_padding:    100\n', '\n', '    Returns: padding list\n', '    """"""\n']",[],0
utilmy/templates/templist/pypi_package/mygenerator/util_image.py:image_merge,image_merge,function,32,73,51,554,7.59,1,3,"['image_list', 'n_dim', 'padding_size', 'max_height', 'total_width']","[None, None, None, None, None]","[None, None, None, None, None]",26,"['    """"""\n', '    Args:\n', '        image_list:  list of image\n', '        n_dim:\n', '        padding_size: padding size max\n', '        max_height:   max height\n', '        total_width:  total width\n', '\n', '    Returns:\n', '\n', '    """"""\n']","['np.zeros', 'len', 'enumerate']",3
utilmy/templates/templist/pypi_package/mygenerator/util_image.py:image_remove_extra_padding,image_remove_extra_padding,function,31,73,63,518,7.1,0,2,"['img', 'inverse', 'removedot']","[None, None, None]","[None, 'False', 'True']",62,"['    """"""TODO: Issue with small dot noise points : noise or not ?\n', '              Padding calc has also issues with small blobs.\n', '    Args:\n', '        img: image\n', '    Returns: image cropped of extra padding\n', '    """"""\n']","['cv2.cvtColor', 'max', 'int', 'np.where', 'morphology.remove_small_objects', 'graybin.astype', 'cv2.findNonZero', 'cv2.boundingRect']",8
utilmy/templates/templist/pypi_package/mygenerator/util_image.py:image_resize,image_resize,function,8,38,27,219,5.76,0,2,"['image', 'width', 'height', 'inter']","[None, None, None, None]","[None, 'None', 'None', 'cv2.INTER_AREA']",92,"['    """"""Resizes a image and maintains aspect ratio.\n', '    Args:\n', '        image:\n', '        width:\n', '        height:\n', '        inter:\n', '\n', '    Returns:\n', '\n', '    """"""\n']","['float', 'int', 'cv2.resize']",3
utilmy/templates/templist/pypi_package/mygenerator/util_image.py:image_read,image_read,function,12,45,34,578,12.84,0,3,"['filepath_or_buffer', 'io.BytesIO]']","[' Union[str', None]","[None, None]",126,"['    """"""\n', '    Read a file into an image object\n', '    Args:\n', '        filepath_or_buffer: The path to the file, a URL, or any object\n', '            with a `read` method (such as `io.BytesIO`)\n', '    """"""\n']","['isinstance', 'hasattr', 'np.asarray', 'cv2.imdecode', 'filepath_or_buffer.endswith', 'tifffile.imread', 'cv2.imread', 'cv2.cvtColor']",8
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:log,log,function,10,39,25,357,9.15,0,0,['*s'],[None],[None],19,[],"['logger.info', 'log2', 'logger.debug', 'logw', 'logger.warning', 'loge', 'logger.error', 'logger_setup', 'logger.configure']",9
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:log2,log2,function,1,5,5,39,7.8,0,0,['*s'],[None],[None],23,[],['logger.debug'],1
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:logw,logw,function,1,5,5,41,8.2,0,0,['*s'],[None],[None],27,[],['logger.warning'],1
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:loge,loge,function,1,5,5,39,7.8,0,0,['*s'],[None],[None],31,[],['logger.error'],1
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:logger_setup,logger_setup,function,2,15,13,138,9.2,0,0,[],[],[],35,[],['logger.configure'],1
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:config_load,config_load,function,16,73,62,816,11.18,0,1,"['config_path', 'pathlib.Path]] ']","[' Optional[Union[str', None]","[None, ' None']",52,"['    """"""Load Config file into a dict\n', '    1) load config_path\n', '    2) If not, load in HOME USER\n', '    3) If not, create default one\n', '    Args:\n', ""        config_path: path of config or 'default' tag value\n"", '    Returns: dict config\n', '    """"""\n']","['logw', 'log2', 'yaml.load', 'str', 'log', 'os.makedirs', 'open', 'yaml.dump']",8
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:dataset_donwload,dataset_donwload,function,9,19,17,246,12.95,0,0,"['url', 'path_target']","[None, None]","[None, None]",96,"['    """"""Donwload on disk the tar.gz file\n', '    Args:\n', '        url:\n', '        path_target:\n', '    Returns:\n', '\n', '    """"""\n']","['log', 'os.makedirs', 'wget.download', 'url.split', 'os_extract_archive', 'log2']",6
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:dataset_get_path,dataset_get_path,function,17,53,44,527,9.94,0,3,['cfg'],[' dict'],[None],113,[],"['cfg.get', 'cfgd.get', 'glob.glob', 'log2', 'len', 'dataset_donwload', 'Exception']",7
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:os_extract_archive,os_extract_archive,function,19,63,47,671,10.65,1,8,"['file_path', 'path', 'archive_format']","[None, None, None]","[None, '"".""', '""auto""']",142,"['    """"""Extracts an archive if it matches tar, tar.gz, tar.bz, or zip formats.\n', '    Args:\n', '        file_path: path to the archive file\n', '        path: path to extract the archive file\n', '        archive_format: Archive format to try for extracting the file.\n', ""            Options are 'auto', 'tar', 'zip', and None.\n"", ""            'tar' includes tar, tar.gz, and tar.bz files.\n"", ""            The default 'auto' is ['tar', 'zip'].\n"", '            None or an empty list will return no matches found.\n', '    Returns:\n', '        True if a match was found and an archive extraction was completed,\n', '        False otherwise.\n', '    """"""\n']","['isinstance', 'is_match_fn', 'open_fn', 'archive.extractall', 'os.remove', 'shutil.rmtree']",6
utilmy/templates/templist/pypi_package/mygenerator/util_exceptions.py:to_file,to_file,function,3,7,7,51,7.29,0,0,"['s', 'filep']","[None, None]","[None, None]",189,[],"['open', 'fp.write']",2
utilmy/templates/templist/pypi_package/tests/test_validate.py:test_image_padding_get,test_image_padding_get,function,8,43,31,295,6.86,0,0,[],[],[],8,[],"['np.zeros', 'image_padding_get']",2
utilmy/templates/templist/pypi_package/tests/test_import.py:test_import,test_import,function,13,26,16,175,6.73,0,0,[],[],[],3,[],[],0
utilmy/templates/templist/pypi_package/tests/test_dataset.py:test_image_dataset_get_label_list,test_image_dataset_get_label_list,function,5,12,12,161,13.42,0,0,[],[],[],7,[],"['dataset.ImageDataset', 'ds.get_label_list', 'np.asarray']",3
utilmy/templates/templist/pypi_package/tests/test_dataset.py:test_image_dataset_len,test_image_dataset_len,function,3,11,10,92,8.36,0,0,[],[],[],15,[],"['dataset.ImageDataset', 'len']",2
utilmy/templates/templist/pypi_package/tests/test_dataset.py:test_image_dataset_get_sampe,test_image_dataset_get_sampe,function,11,37,31,318,8.59,0,0,[],[],[],23,[],"['_dummy_get_image_fn', 'dataset.ImageDataset', 'ds.get_sample']",3
utilmy/templates/templist/pypi_package/tests/test_dataset.py:test_image_dataset_get_image_only,test_image_dataset_get_image_only,function,12,35,31,351,10.03,0,0,[],[],[],40,[],"['_get_image_fn', 'np.asarray', 'dataset.ImageDataset', 'ds.get_image_only']",4
utilmy/templates/templist/pypi_package/tests/test_dataset.py:test_nlp_dataset_len,test_nlp_dataset_len,function,2,10,10,96,9.6,0,0,[],[],[],61,[],"['dataset.NlpDataset', 'range', 'len']",3
utilmy/templates/templist/pypi_package/tests/test_util_image.py:create_blank_image,create_blank_image,function,6,11,10,101,9.18,0,0,"['width', 'height', 'rgb_color', '0', '0']","[None, None, None, None, None]","[None, None, '(0', None, None]",8,"['    """"""Create new image(numpy array) filled with certain color in RGB""""""\n']","['np.zeros', 'tuple']",2
utilmy/templates/templist/pypi_package/tests/test_util_image.py:test_image_merge,test_image_merge,function,6,93,36,395,4.25,0,0,[],[],[],23,[],"['np.asarray', 'util_image.image_merge']",2
utilmy/templates/templist/pypi_package/tests/test_util_image.py:test_image_remove_extra_padding,test_image_remove_extra_padding,function,15,39,34,472,12.1,0,0,[],[],[],63,[],"['create_blank_image', 'cv2.rectangle', 'util_image.image_remove_extra_padding', 'log2']",4
utilmy/templates/templist/pypi_package/tests/test_util_image.py:test_image_resize,test_image_resize,function,4,38,21,223,5.87,0,0,[],[],[],80,[],"['np.asarray', 'util_image.image_resize']",2
utilmy/templates/templist/pypi_package/tests/test_util_image.py:test_image_read,test_image_read,function,8,12,12,203,16.92,0,0,['tmp_path'],[None],[None],107,[],"['create_blank_image', 'str', 'cv2.imwrite', 'util_image.image_read']",4
utilmy/templates/templist/pypi_package/tests/test_transform.py:test_chars_to_images_transform,test_chars_to_images_transform,function,20,92,66,803,8.73,0,0,[],[],[],13,[],"['_get_image_fn', 'np.asarray', 'dataset.ImageDataset', 'dataset.NlpDataset', 'transform.CharToImages', 'tr.fit_transform', 'len', 'ds.get_sample', 'meta.to_dict']",9
utilmy/templates/templist/pypi_package/tests/test_transform.py:test_combine_images_horizontally_transform,test_combine_images_horizontally_transform,function,22,67,53,566,8.45,0,0,[],[],[],50,[],"['_get_image_fn', 'np.zeros', 'dataset.ImageDataset', 'transform.CombineImagesHorizontally', 'len', 'ds.get_sample', 'meta.to_dict']",7
utilmy/templates/templist/pypi_package/tests/test_transform.py:test_scale_image_transform,test_scale_image_transform,function,9,78,52,571,7.32,0,0,[],[],[],88,[],"['_get_image_fn', 'np.asarray', 'dataset.ImageDataset', 'transform.ScaleImage', 'len', 'ds.get_image_only']",6
utilmy/templates/templist/pypi_package/tests/test_transform.py:create_font_files,create_font_files,function,19,44,42,425,9.66,1,0,['font_dir'],[None],[None],143,"['    """"""\n', '    Args:\n', '        font_dir:  image directory\n', '    Returns:\n', '\n', '    """"""\n']","['range', 'str', 'dig_folder.mkdir', 'np.zeros', 'cv2.putText', 'cv2.imwrite']",6
utilmy/templates/templist/pypi_package/tests/test_transform.py:test_text_to_image_transform,test_text_to_image_transform,function,23,75,50,742,9.89,0,0,['tmp_path'],[None],[None],169,[],"['random.seed', 'create_font_files', 'dataset.NlpDataset', 'transform.TextToImage', 'len', 'ds.get_sample', 'np.mean', 'meta.to_dict']",8
utilmy/templates/templist/pypi_package/tests/test_pipeline.py:test_generate_phone_numbers,test_generate_phone_numbers,function,19,51,42,579,11.35,1,0,['tmp_path'],[None],[None],10,[],"['random.seed', 'pipeline.run_generate_phone_numbers', 'output_path=str', 'meta_file.exists', 'meta_file.is_file', 'pd.read_csv', 'len', 'output_path.glob', 'cv2.imread']",9
utilmy/zarchive/storage/aapackage_gen/old/Working Copy of util34.py:getmodule_doc,getmodule_doc,function,6,8,8,69,8.62,0,0,"['module1', 'fileout']","[None, None]","[None, ""''""]",18,[],['ca.getmodule_doc'],1
utilmy/zarchive/storage/aapackage_gen/old/util34.py:getmodule_doc,getmodule_doc,function,6,8,8,69,8.62,0,0,"['module1', 'fileout']","[None, None]","[None, ""''""]",18,[],['ca.getmodule_doc'],1
utilmy/zarchive/storage/aapackage_gen/old/util27.py:getmodule_doc,getmodule_doc,function,6,8,8,69,8.62,0,0,"['module1', 'fileout']","[None, None]","[None, ""''""]",20,[],['ca.getmodule_doc'],1
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:numexpr_vect_calc,numexpr_vect_calc,function,13,28,22,214,7.64,0,0,"['filename', 'i0', 'imax', 'expr', ""fileout='E""]","[None, None, None, None, '']","[None, '0', '1000', None, ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",14,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:numexpr_topanda,numexpr_topanda,function,10,28,22,248,8.86,0,0,"['filename', 'i0', 'imax', 'expr', ""fileout='E""]","[None, None, None, None, '']","[None, '0', '1000', None, ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",27,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:convertcsv_topanda,convertcsv_topanda,function,9,22,19,182,8.27,1,0,"['filein1', 'filename', 'tablen']","[None, None, None]","[None, None, ""'data'""]",39,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:getpanda_tonumpy,getpanda_tonumpy,function,6,10,10,92,9.2,0,0,"['filename', 'nsize', 'tablen']","[None, None, None]","[None, None, ""'data'""]",51,[],['pd.read_hdf'],1
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:getrandom_tonumpy,getrandom_tonumpy,function,6,9,9,101,11.22,0,0,"['filename', 'nbdim', 'nbsample', 'tablen']","[None, None, None, None]","[None, None, None, ""'data'""]",56,[],['pd.read_hdf'],1
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:comoment,comoment,function,4,6,6,61,10.17,0,0,"['xx', 'yy', 'nsample', 'kx', 'ky']","[None, None, None, None, None]","[None, None, None, None, None]",87,[],['ne.evaluate'],1
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:acf,acf,function,12,34,31,244,7.18,0,0,['data'],[None],[None],95,[],"['len', 'np.mean', 'np.sum', 'float', 'r', 'np.arange', 'np.asarray']",7
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:unique_rows,unique_rows,function,6,8,8,148,18.5,0,0,['a'],[None],[None],116,[],"['np.ascontiguousarray', 'np.unique', 'unique_a.view']",3
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:remove_zeros,remove_zeros,function,2,4,4,35,8.75,0,0,"['vv', 'axis1']","[None, None]","[None, '1']",121,[],[],0
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:sort_array,sort_array,function,2,8,8,74,9.25,0,0,['vv'],[None],[None],124,[],[],0
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:save_topanda,save_topanda,function,7,16,16,199,12.44,0,0,"['vv', 'filenameh5']","[None, None]","[None, None]",129,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",6
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:load_frompanda,load_frompanda,function,7,16,16,199,12.44,0,0,['filenameh5'],[None],[None],134,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",6
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:plotsave,plotsave,function,7,16,16,199,12.44,0,0,"['xx', 'yy', 'title1']","[None, None, None]","[None, None, '""""']",145,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",6
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:plotshow,plotshow,function,6,15,15,159,10.6,0,0,"['xx', 'yy', 'title1']","[None, None, None]","[None, None, '""""']",156,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.show']",5
utilmy/zarchive/storage/aapackage_gen/old/utils27.py:parsePDF,parsePDF,function,38,80,69,838,10.47,1,0,['url'],[None],[None],320,[],"['urllib2.urlopen', 'StringIO', 'PDFParser', 'PDFDocument', 'PDFResourceManager', 'LAParams', 'TextConverter', 'PDFPageInterpreter', 'PDFPage.create_pages', 'interpreter.process_page', 'retstr.getvalue', 're.findall', 're.search', 'float', 'match.group']",15
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:numexpr_vect_calc,numexpr_vect_calc,function,13,28,22,214,7.64,0,0,"['filename', 'i0', 'imax', 'expr', ""fileout='E""]","[None, None, None, None, '']","[None, '0', '1000', None, ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",14,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:numexpr_topanda,numexpr_topanda,function,10,28,22,248,8.86,0,0,"['filename', 'i0', 'imax', 'expr', ""fileout='E""]","[None, None, None, None, '']","[None, '0', '1000', None, ""'E:\\_data\\_QUASI_SOBOL_gaussian_xx3.h5'""]",27,[],"['pd.read_hdf', 'ne.evaluate', 'pd.HDFStore', 'store.append']",4
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:convertcsv_topanda,convertcsv_topanda,function,9,22,19,182,8.27,1,0,"['filein1', 'filename', 'tablen']","[None, None, None]","[None, None, ""'data'""]",39,[],"['pd.read_csv', 'pd.HDFStore', 'store.append', 'store.close']",4
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:getpanda_tonumpy,getpanda_tonumpy,function,6,10,10,92,9.2,0,0,"['filename', 'nsize', 'tablen']","[None, None, None]","[None, None, ""'data'""]",51,[],['pd.read_hdf'],1
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:getrandom_tonumpy,getrandom_tonumpy,function,6,9,9,101,11.22,0,0,"['filename', 'nbdim', 'nbsample', 'tablen']","[None, None, None, None]","[None, None, None, ""'data'""]",56,[],['pd.read_hdf'],1
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:comoment,comoment,function,4,6,6,61,10.17,0,0,"['xx', 'yy', 'nsample', 'kx', 'ky']","[None, None, None, None, None]","[None, None, None, None, None]",87,[],['ne.evaluate'],1
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:acf,acf,function,12,34,31,244,7.18,0,0,['data'],[None],[None],95,[],"['len', 'np.mean', 'np.sum', 'float', 'r', 'np.arange', 'np.asarray']",7
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:unique_rows,unique_rows,function,6,8,8,148,18.5,0,0,['a'],[None],[None],116,[],"['np.ascontiguousarray', 'np.unique', 'unique_a.view']",3
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:remove_zeros,remove_zeros,function,2,4,4,35,8.75,0,0,"['vv', 'axis1']","[None, None]","[None, '1']",121,[],[],0
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:sort_array,sort_array,function,2,8,8,74,9.25,0,0,['vv'],[None],[None],124,[],[],0
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:save_topanda,save_topanda,function,7,16,16,199,12.44,0,0,"['vv', 'filenameh5']","[None, None]","[None, None]",129,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",6
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:load_frompanda,load_frompanda,function,7,16,16,199,12.44,0,0,['filenameh5'],[None],[None],134,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",6
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:plotsave,plotsave,function,7,16,16,199,12.44,0,0,"['xx', 'yy', 'title1']","[None, None, None]","[None, None, '""""']",145,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.savefig', 'plt.clf']",6
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:plotshow,plotshow,function,6,15,15,159,10.6,0,0,"['xx', 'yy', 'title1']","[None, None, None]","[None, None, '""""']",156,[],"['plt.scatter', 'plt.autoscale', 'title1+str', 'plt.title', 'plt.show']",5
utilmy/zarchive/storage/aapackage_gen/old/utils34.py:parsePDF,parsePDF,function,38,80,69,838,10.47,1,0,['url'],[None],[None],320,[],"['urllib2.urlopen', 'StringIO', 'PDFParser', 'PDFDocument', 'PDFResourceManager', 'LAParams', 'TextConverter', 'PDFPageInterpreter', 'PDFPage.create_pages', 'interpreter.process_page', 'retstr.getvalue', 're.findall', 're.search', 'float', 'match.group']",15
utilmy/zarchive/storage/aapackage_gen/34/Working Copy of util34.py:getmodule_doc,getmodule_doc,function,6,8,8,69,8.62,0,0,"['module1', 'fileout']","[None, None]","[None, ""''""]",18,[],['ca.getmodule_doc'],1
utilmy/zarchive/storage/aapackage_gen/34/util34.py:getmodule_doc,getmodule_doc,function,6,8,8,69,8.62,0,0,"['module1', 'fileout']","[None, None]","[None, ""''""]",18,[],['ca.getmodule_doc'],1
